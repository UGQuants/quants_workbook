[["index.html", "UG Quantitative Methods in the Social Sciences lab workbook A step-by-step guide for conducting quantitative research with R Welcome", " UG Quantitative Methods in the Social Sciences lab workbook A step-by-step guide for conducting quantitative research with R by J Rafael Verudzco Torres and Mark Wong 2024-12-06 Welcome Welcome to the Quantitative Methods in the Social Sciences lab! This workbook is targeted to University of Glasgow students enrolled in the Undergraduate Quantitative Research Methods course of the School of Social &amp; Political Sciences. The activities are designed for RStudio Cloud. The book was written using R bookdown package based on the GitHub repository: https://github.com/rstudio/bookdown-demo. The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["lab-intro.html", "Lab 1 Introduction to R 1.1 Why R? 1.2 Getting started 1.3 Hands on R 1.4 Activity", " Lab 1 Introduction to R For this course we will be using R (R Core Team 2021) and R Studio as the main tools for conducting quantitative analysis. R and the basic versions of R Studio are open-source and thus free software. Even though R appeared in the early 90s, it has been gaining a lot of popularity in recent years. In fact, it is now one of the most common software for doing statistics in academia. R and R Studio are two separate things. R is the actual programming language and the main processing tool which does the computations in the background, whereas RStudio integrates all functionalities in a friendly and interactive interface. In short, for this course (and most of the times in practice) you chiefly RStudio whilst R is silently doing all the work in the background. Thereafter, we will refer to R, as the integrated interface. R works in a command-based line environment. This means that you need to call the commands (or functions, as called in R) through text. This can look intimidating at first glance. But do not worry, we will guide you step by step. At this point you may be wondering why you need to bother learning these tools. In the next section you will see some of the advantages and examples that can be achieved using R. 1.1 Why R? 1.1.1 R: a flexible tool R can be applied in a wide variety of fields and subjects, including not only those in the social sciences (e.g. sociology, politics or policy research), but also in humanities (e.g. history, digital humanities), natural and physical sciences (e.g. biology, chemistry or geography), health (e.g. medical studies, public health, epidemiology), business and management (e.g. finance, economics, marketing), among many others. The broad application of R is due to its flexibility which allows to perform a range of tasks related to data. These cover tasks at initial stages, such as downloading, mining, or importing data. But it is also useful to manipulate, edit, transform, and organize information. Furthermore and most important for us, there are a set of tools that allow us to analyse data using a range of statistical techniques. These are useful to understand, summarize and draw conclusions about samples, e.g. people. Lastly, R is powerful to communicate and share information and documents. There are several extensions (called packages in R) that can help to produce static and interactive plots/charts, maps, written reports, interactive applications or even entire books! In fact this workbook was written from RStudio. 1.1.2 Advantages of using R Some of the advantages of using R are the following: It is free and open source. You do not need to pay for a license. Thus you can use it anywhere at anytime even if you do not have an affiliation to an institution or organization (e.g. University or workplace); It is a collaborative project. This means that it is the users who maintain, extend and update its applications; It is reproducible. Research can be more transparent since you will get the same results every time you run your analysis through a specific pathway (i.e. through scripts); High compatibility. You can read and produce most types of file extensions; There are a number of easy-access web resources to support you in the learning process. 1.2 Getting started 1.2.1 Setting up RStudio At this point, you need to know that there are at least two alternatives to start using RStudio. One, and by far the most common, is to download both R and RStudio and install the applications in your local drive. The other option is RStudio Cloud. This is an on-line version of RStudio that does not require installing any additional software. You can run it directly from your browser (e.g. GoogleChrome, Safari, Firefox, etc). For now, we will use the cloud version. To get started, follow the next steps: Part 1 Create an RStudio Cloud account. Click on this link RStudio Cloud - SSO, which should automatically open a new tab in your web browser or go directly to the browser and copy this URL: https://sso.rstudio.cloud/glasgow; Enter your University of Glasgow email address in the login page as normal; Then it gets linked to the SSO sign-in page, which you input your GUID and password (same page as if you’re logging into the library portal/e-reading list); Figure 1.1: SSO Login Done! You will be taken you into your own Rstudio Cloud work space. Part 2 Join your lab group. You will receive a link from your tutor to join your lab group on RStudio Cloud (the link will be posted on Moodle too). N.b. you must use this specific link to join and access your lab group workspace, as each link is unique to your group. So only use your group’s specific link. Copy and paste the link in your web browser. You should see the following window: Figure 1.2: Join Space. Join your lab by clicking on the ‘Join space’ button shown above. Open the shared space form the left-hand side pane called ‘Quants Lab Group..’ and start the Lab 1 project by clicking on the ‘Start’ button as shown below: Figure 1.3: Start project. 1.2.2 RStudio environment 1.2.2.1 RStudio screen Once you have started ‘Lab 1’ you will see the screen below: Figure 1.4: Project name. Now, go to the “File” tab and create a R Script as follows File &gt; New file &gt; R Script Figure 1.5: New R Script. Once you have created your first R Script, save it by clicking on File &gt; Save as.. &gt; [write the name of your file]. After this, your RStudio screen will be split in four important windows or panes as shown below: Figure 1.6: R Studio panes. In Pane 1, you have your newly created R script. This is the area where you will be working most of the time. From here, you will write functions. To run an R script line, you can click on the Run green arrow situated on the top of pane 1 or more commonly you can run a code line by typing alt + enter. The things you write in this section will be saved in your R script file. In Pane 2, you have the “Global Environment”, this is one of the most useful tabs in this pane. It shows you the active ‘objects’ that you have available/loaded in your current session (this will probably make more sense in the coming sections). In Pane 3, you have the R Console, this is where you will see most of the results of the functions you run from your script (pane 1). You can also write and run functions from here, by typing the function and hitting enter. NOTE that what you do here will NOT be saved, this is usually used to quickly call functions that you do not want to save in your script. Figure 1.7: Console. Finally, in Pane 4 you have multiple useful tabs. In the File tab you can see the files and directories that you have in your R project. In the Plot tab you will see a preview of the static plots/charts you will be producing from your script. In Packages, you have a list of the extensions or plug-ins (called ‘packages’ in R) that are installed in your working environment. The Help contains some resources that clarify or expand what each of the functions does. Again, probably this will make more sense once you get started. We will come back to this later. Finally, the Viewer displays interactive outputs. 1.3 Hands on R Now you are ready! It is your turn to start exploring and getting familiar with R by completing the following activities. 1.3.0.1 R as calculator Go to your console (pane 3, bottom-left pane), write some simple calculations and run them by typing ‘enter’ after each of them, as shown below. Figure 1.8: R Console as calculator. Try different operations such as 50 / 20 or 3 * 5. Fairly simple, right? And don’t forget, it is entirely normal to copy/paste and tweak any existing codes. Unlike writing an essay or an exam, you don’t actually need to know and write codes “off the cuff” or recite/memorise any syntax. You are only expected to know how to run the codes and tweak them as you go along, there is a huge amount of trial and error when you work in R. So don’t worry if you feel like you are just making minor changes to the codes, that’s how it’s supposed to work, and the first few weeks is all about getting comfortable in using R, then the level of challenge will go up. Let’s continue with the next activities! 1.3.0.2 Testing logical operators Now, write and run the following lines in your console (pane 3) and take some time to observe the result in detail for each of them: 10 == 10 10 != 10 1 == 5 1 &gt; 5 'a' == 'a' 'a' == 'b' What do you see? … …That’s it! When you use the double equal sign == you are asking R whether the value on the left hand-side of the operator is equal to the one on the right hand-side. Likewise, when you combine the exclamation mark ! with other operator, you get the reversed result. In the past exercises you used !=, this was interpreted as “is not equal to”, that is why 10 != 10 returns FALSE, but 10 == 10 returns TRUE. R can process different classes of inputs. In this case we used letters and we asked R whether ‘a’ was equal to ‘a’, and of course the result is TRUE. Note that when you want to input text (referred as character values in R), you need quotation marks '. If you want to enter numeric values, you simply input the raw number. These are different ‘class’ values. Perhaps logical operators do not make much sense at this point, but you will find out later that they are useful to manipulate data. For example, these are essential to filter a data set based on specific rules or patterns. 1.3.0.3 Assigning values to ‘objects’ In R, it is very common (and practical) to store values or data as ‘objects’. These are temporally stored in your current session. Let’s try it! Now, we will work in the R script file (Pane 1, top-left pane), write the following and run it by clicking the green arrow or using alt + enter: a &lt;- 10 a + 5 What do you observe?… …That’s right! The operator &lt;- assigned the numeric value 10 to the object a (on the left hand-side of the arrow). Later, you used the object (a) to compute a sum (i.e, a + 5). Now, write and run the following in your R script (Pane 1) c &lt;- 3 a * c As you can see, you stored the numeric value 3 in the variable c. Then, you called the previously created object a in a multiplication. In the same way as you assigned these simple variables, you will store other types of objects later, e.g. vectors, data frames or lists. This is useful because those objects will be ready in your session to do some computations. There are a few things to note when assigning objects to variables. If you use a different value to the same variable, e.g. by typing a &lt;- 5, you will replace the old value with the new. So, instead of having a representing the value 10, you will have 5. You can see the objects available in your session on the Global Environment (‘Environment’ tab in Pane 2) as shown below. Figure 1.9: ‘Environment’ tab. This is a very good start, great job! Note that the changes made in your script are saved automatically in RStudio Cloud. To verify this, have a look at the name of your script in the top-left of pane 1. If changes are due to be saved, the name will be written in red. If it is in red, save changes manually by clicking on the disk icon. After you have made sure your changes are saved, end your session simply by closing the RStudio Cloud tab in your browser. 1.4 Activity Discuss the following questions with your neighbour or tutor: What are the main differences between working on a R script file (pane 1) and directly on the console (pane 3)? Can you describe what happens when your run the following code? (tip: look at the environment tab in pane 2) object1 &lt;- 10 object1 &lt;- 30 References "],["lab2.html", "Lab 2 Data in R 2.1 Welcome back! 2.2 R Packages 2.3 Types of variables 2.4 More operators and some essential symbols 2.5 Black lives matter! 2.6 Activities", " Lab 2 Data in R 2.1 Welcome back! In our previous lab, we set up an RStudio Cloud session and we got familiar with the RStudio environment and some of the purpose and contents of its panes. In this Lab we will learn about R packages, how to install them and load them. Also, we will use different types of data. You will have the chance to practice with additional R operators. Lastly, we will load a real-world data set and put in practice your new skills. 2.2 R Packages As mentioned in our last lab, R (R Core Team 2021) is a collaborative project. This means that R users are developing, maintaining, and extending the functionalities constantly. When you set up R and RStudio for the first time, as we did it last week, it comes only with the ‘basic’ functionalities by default. However, there are literally thousands of extensions that are developed by other users. In R, these non-default extensions are called packages. Most of the times, we use packages because they simplify our work in R or they allow us to extend the capabilites of base R. 2.2.1 Installing packages Let’s put hands-on to install and load some useful packages. We will start with tidyverse (Wickham 2021).1 2.2.2 Activity: Part 1. Access your lab group in R Studio Cloud Make sure you have a free, institutional-subscription RStudio Cloud account (in case you have not created one yet, please follow the guidance provided in Lab 1); You will receive a link from your tutor to join your lab group in a shared space. Copy and paste it in your web browser (log in if necessary). If you already joined your lab group in RStudio Cloud, simply access the ‘Lab 2’ project and omit steps 3 to 5. Otherwise, continue with steps 3, 4 and 5. If you did not join your lab group yet, you should see the following window: Figure 2.1: Join Space Click on the ‘Join space’ button shown above. Open the shared space form the left-hand side pane called ‘Quants Lab Group..’ and start the Lab 2 project by clicking on the ‘Start’ button as shown below: Figure 2.2: Start Lab 2. Part 2. Working on your script Once you have accessed the ‘Lab 2’ project, write or copy the following line in your script (pane 1) and run it: install.packages(&#39;tidyverse&#39;) Wait until you get the message ‘The downloaded source packages are in…’. The installing process can take up to a couple of minutes to finish. Once the package is installed, you need to load it using the library() function. Please, copy and paste the following line, and run it: library(tidyverse) And that’s it, tidyverse is ready to be used in your current session! There are couple of things you should know. First, the packages need to be installed only per project in RStudio Cloud (and only once if you are working in RStudio Desktop version). However, packages must be loaded using the library() function every time you restart an R session. Another thing to notice is that when you install a package you need to use quotation marks, whereas in library() you only need to write the plain package name within brackets. Usually, you will load the packages at the beginning of your script. 2.3 Types of variables R can handle many classes of data. It is crucial that you can distinguish the main ones. Broadly speaking there are two types of variables, categorical and; numeric (formally know as interval or ratio). Categorical variables are distinctive because they are limited in the number of categories it can take, e.g., country, name, political party, or gender. Ordinal data is a sub-type of the categorical, and it is used when the categories can be ranked and their order is meaningful, e.g., education level or level of satisfaction. Numeric values can be continuous (these are usually measured and can take infinite values, e.g. speed or time).2 In R, the basic types of data are known as ‘atomic vectors’ and there are 6 of them (logical, integer, double, character, complex and raw). In the social sciences, we often use the following: numeric, factor and character. Numeric vectors are used to represent continuous numerical data.3 On the other hand, factor vectors are used to represent categorical and ordinal data. In R, there are couple of functions that will help us to identify the type of data. First, we have glimpse(). This prints some of the main characteristics of a data set, namely its overall dimension, name of each variable (column), the first values for each variable, and the type of the variable. Second we have the function class(), that will help us to determine the overall class(type) of on R object. 2.3.1 Activity: We are now going to use some datasets that are pre-loaded in the R session by default. Please go to your ‘Lab_2’ project in RStudio Cloud and do the following: We will start with a classic dataset example in R called iris. This contains measurements of various flowers species (for more info type ?iris in your console). Please go to your console and type the line below. glimpse(iris) What do you observe from the output?… First, it tells you the number of rows and the columns on the top. Later, it lists the name of each variable. Additionally, it tells you the type of the variable between these symbols &lt; &gt;. The first five variables in this dataset are of type &lt;dbl&gt; which is a type of numeric variable. The last, Species, is a factor &lt;fct&gt;. In sum, there is information of the species and four types of continuous measures associated to each flower in this dataset. Now you know that each flower belongs to a species, but what are the specific categories in this data set? To find out, type the following in your console. levels(iris$Species) As you can see, there are three categories, which are three types of flower species. In R the categories in factor vectors are are called levels. Note the syntax above. Inside the function, we used the name of the dataset followed by the dollar sign ($) which is is needed to access the specific column/variable Species. Now, let’s get serious and explore Star Wars. Yes, the famous film series! The starwars data set from the dplyr package contains information about the characters, including height, hair colour, and sex (to get more information type ?starwars in your console). At this time we will use a reduced version of the full data set. Please complete the following activities from your R script (pane 1). First, we will run the next couple of lines to reduce the data set, and then we will glimpse the Star Wars characters: starwars2 &lt;- starwars[ ,1:11] glimpse(starwars2) What do you observe this time? … It seems that the data type is not consistent with their content. For example, the variables species, gender, and hair_color are of type &lt;chr&gt; (that is character), when according to what we just learnt they should be a factor. To transform them, we will use the function ´factor()´. This process is known as coercing a variable, that is when you change from one type to another. Let’s coerce the species variable from character to factor and assign the result to the same column in the dataset. starwars2$species &lt;- factor(starwars2$species) Let’s check if the type of variable really changed by glimpsing the data and checking the levels of species. glimpse(starwars2) levels(starwars2$species) The glimpse result now is telling us that species is a &lt;fct&gt;, as expected. Furthermore, the levels() function reveals that there are 37 types of species, including Human, Ewok, Droid, and more. Hopefully, these examples will help you to identify the the main vector types and more importantly to coerce them in an appropriate type. Be aware that many data sets represent categories with numeric values, for example, using ‘0’ for males and ‘1’ for females. Usually, large data sets are accompanied by extra information in a code book or documentation file, which specifies the values of the numeric code and their respective meaning. It’s important to read the code book/documentation of every dataset as the conventions and meanings can vary. 2.4 More operators and some essential symbols A useful operator is the pipe %&gt;%. This is part of the tidyverse package. So, it is ready for you to use. This operator passes the result of one operation to the next. Check the results of the following operations in your console: 1 %&gt;% + 1 1 %&gt;% + 1 %&gt;% + 5 Observe what happened…The result from the first line was 2. This is because this line can be read as: ‘take 1, THEN sum 1’. Therefore, the result is 2. Similarly, the second line follows this process: ‘take 1, THEN sum 1, take the result of this (which is ’2’) and THEN sum 5’. Therefore, the result is 7. This can sound a bit abstract at this point, but we will practice with some data in the next section. 2.5 Black lives matter! In this section we will work with data originally collected by The Guardian in 2015, for more information click here. The data set we will use today is an extended version which was openly shared in GitHub by the American news website FiveThirtyEight. This data set contains information about the people that were killed by police or other law enforcement bodies in the US, such as age, gender, race/ethnicity, etc. Additionally, it includes information about the city or region where the event happened. For more information click here. 2.5.1 Downloading and reading the data For the following excercices, please make sure that your are working in your R script. First, we will create a new folder in our project directory to store the data. To do it from R, run this line in your script (Don’t worry if you get a warning. This appears because you already have a folder with this name): dir.create(&quot;data&quot;) Note that in the ‘Files’ tab of Pane 4, there is a new folder called data. Now, download the data from the GitHub repository using the function download.file(). This function takes two arguments separated by a comma: (1) the URL and (2) the destination (including the directory, file name, and file extension), as shown below. Also, since the file we downloaded is wrapped in a .zip file, we will need to unzip it using unzip(). Copy, paste in your script, the following lines: download.file(&quot;https://projects.fivethirtyeight.com/data-webpage-data/datasets/police-killings.zip&quot;, &quot;data/police-killings.zip&quot;) unzip(&quot;data/police-killings.zip&quot;, exdir = &quot;data&quot;) After following the previous steps, we are ready to read the data. As you can see in the ‘File’ tab, the data comes as a .csv file. Thus, we can use the read_csv() function included in the tidyverse package (make sure you the package is loaded in your session as explained in a previous section). We will assign the data in an object called police. police &lt;- read_csv(&quot;data/police-killings/police_killings.csv&quot;) 2.5.2 Examining the data If you look at your ‘Environment’ tab in pane 2, you will see there is a new object called police, which has 467 observations and 34 variables (or columns). To start exploring the contents, we will glimpse the police data as following: glimpse(police) As you can see, there are several variables included in the dataset, such as age, gender, law enforcement agency (lawenforcementagency), or whether the victim was armed (armed). You will see some of these variables are not in the appropriate type. For instance, some are categorical and should be type &lt;fct&gt; instead of &lt;chr&gt;. 2.5.3 Data wrangling Before coercing these variables, we will create a smaller subset selecting only the variables that we are interested in. To do so, we can use the select() function. The select function takes the name of the data first and then the name of the variables we want to keep (no quotation marks needed). We will select a few variables and assign the result to a new object called police_2. police_2 &lt;- select(police, age, gender, raceethnicity, lawenforcementagency, armed) If you look again to the ‘Environment’ tab, there is a second data set with the same number of observations but only 5 variables. You can glimpse this object to have a better idea of its contents. glimpse(police_2) Having a closer look at the reduced version, we can see that in fact all the variables are of type &lt;chr&gt;, including age. Let’s coerce the variables in to their correct type. We will start with age, from character to numeric: police_2 &lt;- police_2 %&gt;% mutate(age = as.numeric(age)) Age is not known for some cases. Thus, it is recorded as ‘Unknown’ in the dataset. Since this is not recognized as a numeric value in the coercion process, R automatically sets it as a missing value, NA. This is why it will give you a warning message. We can continue coercing raceethnicity and gender from character to a factor: police_2 &lt;- police_2 %&gt;% mutate(raceethnicity = factor(raceethnicity)) police_2 &lt;- police_2 %&gt;% mutate(gender = factor(gender)) Let’s run a summary of your data. This shows the number of observations in each category or a summary of a numeric variable: summary(police_2) There are some interesting figures coming out from the summary. For instance, in age you can see that the youngest is… 16 years old(?!), and the oldest 87 years old. Also, the vast majority are male individuals (445 vs 22). In relation to race/ethnicity, roughly half of them is ‘White’, whereas ‘Black’ individuals represent an important share. One may question about the proportion of people killed in terms of race/ethnicity compared to the composition of the total population (considering Black is a minority group in the US). Let’s suppose that we only want observations in which race/ethnicity is not unknown. To ‘remove’ undesired observation we can use the filter() function. We will assign the result of filter in a variable called police_2. police_2 &lt;- police_2 %&gt;% filter(raceethnicity != &#39;Unknown&#39;) So, what just happened in the code above? First, the pipe operator, %&gt;%: What we are doing verbally is take the object police_2, THEN filter raceethnicity based on a condition. Later, what is happening inside filter? Lets have a look at what R does in the background for us (Artwork by @alison_horst): Figure 2.3: Filter. Source: Artwork by Horst (n.d.). In the example above, we are keeping the observations in raceethnicity that are NOT EQUAL to ‘Unknown’. Finally, when we assigned the result to an object named as the same as our previous object, we replaced the old dataset with the filtered version. 2.6 Activities Discuss the following questions with your neighbour or tutor: What is the main purpose of the functions select() and filter? What does coerce mean in the context of R? and Why do we need to coerce some variables? What is the mutate() function useful for? Using the police_2 dataset: Filter how many observations are ‘White’ in raceethnicity? How may rows/observations are left? How many ‘Latino/Hispanic’ are there in the dataset? Using the example of Figure 2.3, could you filter how many were killed that were (a) ‘Black’ and (b) killed by firearm (‘firearm’)? What about ‘White’ and ‘firearm’? Extra activities: 1. Why did you have to use quotes in the following: filter(police, raceethnicity==”White” &amp; raceethnicity==”firearm”)? 2. What do you have to repeat the variable raceethnicity twice? This is the end of Lab 2. Again, the changes in your script should be saved automatically in R Studio Cloud. However, make sure this is the case as you were taught in Lab 1. After this, you can close the tab in your web browser. Hope you had fun! References "],["data-wrangling-1.html", "Lab 3 Data wrangling 3.1 Importing and data wrangling 3.2 Data wrangling 3.3 Read the clean dataset", " Lab 3 Data wrangling Welcome to Lab 3! In our previous session we learned about R packages, including how to install and load them. We talked about the main types of data used in social science research and how to represent them in R. Also, we played around with some datasets using some key functions, such as: filter(), select(), and mutate(). In this session we will learn how to import data in R, clean and format the data using a real-world dataset. These is a common and important phase in quantitative research. 3.1 Importing and data wrangling Today, we will be working with data generated by the Access Research Knowledge (ARK) hub. ARK conducts a series of surveys about society and life in Northern Ireland. For this lab, we will be working with the results of the Northern Ireland Life and Times Survey (NILT) in the year 2012. In particular, we will be using a teaching dataset that focuses on community relations and political attitudes. This includes background information of the participants and their household. Please take 5-10 minutes to read the documentation of this dataset (click here to access the documentation). p.s. You will have to regularly consult this document to understand and use the data in NILT. So, I recommend you to save the PDF file in your local drive if you can. This NILT teaching dataset is also what you will be using for the research report assignment in this course (smart, isn’t it?) - so it’s worth investing the time to learn how to work with this data through the next few labs, as part of the preparation and practice for your assignemnt. 3.1.1 Downloading and reading the data We will continue using R Studio Cloud, as we did in our previous labs. Please follow the next steps: Go to your ‘Quants lab group’ in RStudio Cloud (if you have not joined a shared space, follow the instructions in Section 2.2 of Lab 2). Start the project called ‘NILT’ located in your lab group. Once you have initialized the project, generate a new R scrip file, and save it as ‘Exploratory analysis’. Load the tidyverse and haven packages. This last package is useful to import data from SPSS (the tidyverse package was pre-installed in your session). You can copy, paste, and run the following functions from your script: library(tidyverse) library(haven) Next, we will create a folder to store the data. Then, download and read the NILT data set, following the next steps: From your script, create a new folder called ‘data’: dir.create(&#39;data&#39;) Download the data using the download.file() function. Remember that you have to specify the URL first, and the destination of the file second (including the folder). download.file(&#39;https://www.ark.ac.uk/teaching/NILT2012GR.sav&#39;, &#39;data/nilt2012.sav&#39;) Take a look to the ‘Files’ tab in pane 3, you will see a folder called ‘data’, click on it, and you will see the nilt2012.sav file. Figure 3.1: Cloud files. To read this type of file use the read_sav() function. Read the .sav file and assign it to an object called nilt. nilt &lt;- read_sav(&quot;data/nilt2012.sav&quot;) And that’s it! You should see a new data object in your ‘Environment’ tab (Pane 2) ready to be used. You can also see that this contains 1204 observations (rows) and 133 variables (columns). Lets glimpse our newly imported data and see the type of variables included. glimpse(nilt) 3.2 Data wrangling As you can see from the result of glimpse, the class for practically all the variables is &lt;dbl+lbl&gt;. What does this mean? This happened because usually datasets use numbers to represent each of the categories/levels in categorical variables. These numbers are labelled with their respective meaning. This is why we have a combination of value types (&lt;dbl+lbl&gt;). Take the example of the variable called rsex, as you can see from the values displayed using glimpse(), this includes numbers only, e.g. 1,1,2,2.... This is because ‘1’ represents ‘Male’ respondents and ‘2’ represents ‘Female’ respondents in the NILT dataset (n.b. the authors of this lab workbook recognise that sex and gender are different concepts, and we acknowledge this tension and that it will be problematic to imply or define gender identities as binary, as with any dataset. More recent surveys normally approach this in a more inclusive way by offering self-describe options). You can check the pre-defined parameters of the variable in NILT in the documentation or running print_labels(nilt$rsex) in your console, which returns the numeric value and its respective label. As with rsex, this is the case for many other variables in this data set. You should be aware that this type of ‘mix’ variable is a special case since we imported a file from a foreign file that saves metadata for each variable (containing the names of the categories). As you learned in the last lab, in R we treat categorical variables as factor. Therefore, we will coerce some variables as factor. This time we will use the function as_factor() instead of the simple factor() that we used before. This is because as_factor() allows us to keep the names of each category in the variables. The syntax is exactly the same as before. Copy and run the following from your script: # Gender of the respondent nilt &lt;- nilt %&gt;% mutate(rsex = as_factor(rsex)) # Highest Educational qualification nilt &lt;- nilt %&gt;% mutate(highqual = as_factor(highqual)) # Religion nilt &lt;- nilt %&gt;% mutate(religcat = as_factor(religcat)) # Politic identification nilt &lt;- nilt %&gt;% mutate(uninatid = as_factor(uninatid)) # Happiness nilt &lt;- nilt %&gt;% mutate(ruhappy = as_factor(ruhappy)) Notice from the code above that we are replacing the ‘old’ dataset with the result of the mutated variables that are of type factor. This is why we assigned the result with the assigning operator &lt;-. What about the numeric variables? In the documentation file there is a table in which you will see a type of measure ‘scale’. This usually refers to continuous numeric variables (e.g. age or income).4 Let’s coerce some variables to the appropriate type. In the previous operation we coerced the variables as factor one by one, but we can transform several variables at once within the mutate function. As we did before, copy and run the following code in your script: # Coerce several variables as numeric nilt &lt;- nilt %&gt;% mutate( rage = as.numeric(rage), rhourswk = as.numeric(rhourswk), persinc2 = as.numeric(persinc2), ) Before doing some analyses, we will drop unused levels (or categories) in our dataset using the function droplevels(), as following: # drop unused levels nilt &lt;- droplevels(nilt) The previous function is useful to remove some categories that are not being used in the dataset (e.g. categories including 0 observations). Finally, save the NILT survey in an .rds file (this is the R format). We will not use this file now, but this will save us time formatting the dataset in next labs (So, we do not have to repeat the steps above every time). saveRDS(nilt, &quot;data/nilt_r_object.rds&quot;) 3.3 Read the clean dataset Phew! Good job. You have completed the basics for wrangling the data and producing a workable dataset. As a final step, just double check that things went as expected. For this purpose, we will re-read the clean dataset. 3.3.1 Activity Using the readRDS() function, read the .rds file that you just created in the last step and assign it to an object called cleanesed_data. Remember to include the full directory of the file using quotation marks inside the function. Run the glimpse function on the cleanesed_data object. Run the glimpse function on the nilt object. Do they look the same? If yes, it means that you successfully saved your work. Be careful, in some cases these actually correspond to discrete numeric values in this dataset (things that can be counted, e.g. number of…).↩︎ "],["exploratory-data-analysis.html", "Lab 4 Exploratory data analysis 4.1 Getting started 4.2 Exploratory analysis", " Lab 4 Exploratory data analysis In our previous session we learned about wrangling data in R by implementing useful function such as filter(), select(), and mutate(). In this session we will focus on descriptive statistics. This includes the exploration and description of quantitative data. 4.1 Getting started We will continue working on the same project and dataset that you created in the last lab on R Studio Cloud. Please follow the next steps: Go to your ‘Quants lab group’ in RStudio Cloud. Open the project called ‘NILT’ located in your lab group’. Continue working at the bottom of the ‘Exploratory analysis’ script that you created in the last lab. Load nilt dataset that you created in the last session using the following code: # Load the data from the .rds file we created in the last lab nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) Create a subset of the nilt data keeping only few variables using the select() function as shown below: #Subset nilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2) 4.2 Exploratory analysis 4.2.1 Data overview Are your summary statistics hiding something interesting? Figure 4.1: Exploratory analysis. To start exploring our data it essential to distinguish the adequate tools and measures available for the type of data in question. As you know now, there are two broad types: (1) categorical and (2) numeric. There are several ways in which we can summarise our data. Today, we will use a useful package called vtable. Install it in your session running the following line from your console: install.packages(&#39;vtable&#39;) Once it is installed, make sure to load it with the next line. This time copy and paste it in your R script, so you can run it every time you restart your session. library(vtable) 4.2.2 Categorical data A usual way to explore the categorical data is using contingency and proportion tables. The contingency tables include the count for each category while the proportion tables contain the count divided by the total number of observations. Try to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown. Let’s say we are interested in the data’s break down by respondents’ sex (called rsex in the dataset). We will use function sumtable() of the vtable package to produce a contingency table for a single variable (known as One-Way contingency table). sumtable(nilt_subset, vars = c(&#39;rsex&#39;)) Table 4.1: Summary Statistics Variable N Percent rsex 1204 … Male 537 45% … Female 667 55% From the result, we see that there are more female respondents than males. Specifically, we see that males respondents represent 44.6% of the total sample, whereas females 55.4%. We can do this with any type of categorical variable. Let’s see how the sample is split by religion (religcat). So, we will add it to in the vars argument. sumtable(nilt_subset, vars = c(&#39;rsex&#39;, &#39;religcat&#39;)) Table 4.2: Summary Statistics Variable N Percent rsex 1204 … Male 537 45% … Female 667 55% religcat 1168 … Catholic 491 42% … Protestant 497 43% … No religion 180 15% As you can see, about the same number of people are identified as being catholic or protestant, and a relatively small number with no religion. What if we want to know the religious affiliation break-down by males and females. This is where Two-Way contingency tables are useful and very common in quantitative research. To produce it, we have to specify the group argument in the sumtable function as following: sumtable(nilt_subset, vars = c(&#39;religcat&#39;), group = &#39;rsex&#39;) Table 4.3: Summary Statistics rsex Male Female Variable N Percent N Percent religcat 520 648 … Catholic 209 40% 282 44% … Protestant 211 41% 286 44% … No religion 100 19% 80 12% There are some interesting results from this table. You can see that there are proportionally more female respondents who are either Catholic or Protestant than males, namely 43.5% vs 40.2% and 44.1% vs 40.6%, respectively. We also see that there are almost 20% of male respondents who do not self-identify with a religion which contrast to the 12% of female participants. 4.2.3 Activity 1 From your R Studio Cloud script, do the following activities using the data in the nilt_subset object (feel free to copy and adapt the code shown above): Create a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function; Using the variables religcat and uninatid, generate a Two-Way contingency table; Are your summary statistics hiding something interesting? Discuss your results with your neighbour or your tutor. 4.2.4 Continuous (numeric) data In the previous section we’ve learnt how to summarise categorical data. But very often we want to work with continuous numeric variables or a combination of both. To summarise and understand numeric data there are two main types: measures of centrality and measures of spread. As before, try to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown. 4.2.4.1 Measures of centrality In quantitative research, we usually have access to many observations in a sample which contains different attributes for each of them. It would be difficult (and probably not very useful) to talk about each of the NILT respondents one by one. Instead, to describe this sample we need measures that roughly represent all participants. This is actually an important step in quantitative research, since it allows us to characterise the people that we are studying. For example, in the previous section we only talked about the respondents’ sex and political affiliation, but who are the people we are talking about? Probably a place to start digging deeper is to know their age. The first tool that we will use to understand numeric values is a histogram. Let’s see how the age of NILT respondents is distributed. hist(nilt_subset$rage) This plot show us on the X axis (horizontal) the age and the frequency on the Y axis. We can see that the youngest age in the sample is somewhere close to 20, and the oldest is almost 100. We also observe that the total number of observations (represented by the frequency on the vertical axis) for extreme values (close to 20 on the left-hand side and 100 on the right-hand side), tends to be lower than the values in the centre of the plot (somewhere between 30 and 45). For instance, we can see that there are approximately 120 respondents who are around 40 years old, that seems to be the most popular/frequent age in our sample. Now, we can represent these central values with actual measures, typically mean or median. The median is the mid-point value in a numeric series. If you sort the values and split it by half, the value right in the middle is the median. Luckily there is a function ready to be used called… You guessed it - median(). median(nilt_subset$rage, na.rm = TRUE) ## [1] 48 The median age is 48, that means that 50% (or half) of the respondents are equal or younger than this, and the other 50% is equal or older.5 To compute the mean manually, we need to sum all our values and divide it by the total number of the observations as follows: \\[ mean =\\frac{ x_1 + x_2 + x_3 ...+x_n } {n} \\] The formula above is for you to know that this measure considers the magnitude of all values included in the numeric series. Therefore, the average is sensitive to extreme numbers (e.g. a very very old person). To compute the mean you need the mean() function. mean(nilt_subset$rage, na.rm = T) ## [1] 49.61532 As you can see, the above measures try to approximate values that fall somewhere in the centre of the histogram plot, and represent all observations in the sample. They tell different things and are sometimes more (or less) suitable in a specific situation. 4.2.4.2 Measures of spread By contrast, there are measures that helps us to describe how far away a series of numeric values are from the centre. The common measures of spread are quartiles, variance and standard deviation. The quartiles are very useful to quickly see how numeric data is distributed. Imagine that we sort all ages in the sample and split it in four equal parts. The first quartile includes the lowest 25% values, the second the other 25%, the third other other 25%, and the fourth the highest 25%. To compute quartiles we can use the quantile function. quantile(nilt_subset$rage, na.rm = T) ## 0% 25% 50% 75% 100% ## 18 35 48 64 97 In our sample, the youngest quarter of the respondents is between 18 and 35 years old. The second quarter is between 35 and 48 years old. The next quartile is between 48 and 64. The oldest 25% of the respondents is between 64 and 97. The variance is useful to obtain a singe measure of spread (instead of four values, as the above) taking the mean as a reference. This is given by the following formula: \\[ var = \\frac{ \\sum(x - \\bar{x})^2 }{n-1 } \\] To decipher the formula above, the \\(\\bar{x}\\) represents the mean, the \\(x\\) represents each of the values in the numeric series. The formula takes each of the \\(x\\) values and subtract it from the mean \\(\\bar{x}\\). Later, it squares the result of the subtraction (that is multiply it by itself). This is done to obtain a positive value, since some numbers in the series will be lower than the mean (resulting in negative values). Then, we sum all of them and divide the sum by the size/length of the numeric sequence \\(n\\) minus 1. To estimate the variance in R we only need the var() function. var(nilt_subset$rage, na.rm = T) ## [1] 343.3486 As you can see, the result is not very intuitive. That is because we squared the subtraction. Luckily, there is a measure that put it in readable scale. This is the standard deviation. In essence this takes the square root of the variance (the reversed operation of squaring it): \\[sd=\\sqrt{var}\\] To compute it in R, use the sd() function. sd(nilt_subset$rage, na.rm = T) ## [1] 18.52967 This measure is more human readable than the variance. Don’t worry too much about the formula. An important thing to remember is what the measure represents. An informal definition of the standard deviation is the average distance from the mean. In essence, it tell us how far the values in our data are from the mean. 4.2.5 Putting it all together Phew, that was a lot!… … Luckily we can use the sumtable function to compute all these measures at the same time! It is very simple. You can compute a quick summary for age as following: sumtable(nilt_subset, vars = c(&#39;rage&#39;)) Table 4.4: Summary Statistics Variable N Mean Std. Dev. Min Pctl. 25 Pctl. 75 Max rage 1201 50 19 18 35 64 97 The result displays the number of observations used (N), the mean, the standard deviation, minimum, the 1st (same as ‘Pctl. 25’) and 3rd quartile (same as ‘Pctl. 75’), as well as the maximum (i.e., eldest respondent). 4.2.5.1 Categorical and numeric data Lastly, there will be times in which you will need to compute a summary combining categorical and numeric data, to compare groups for example. The good news is that we can use exactly the same function and syntax to do this. Let’s take the following example to compute the summary of the respondent’s age (rage) by gender: sumtable(nilt_subset, vars = c(&#39;rage&#39;), group = &#39;rsex&#39;) Table 4.5: Summary Statistics rsex Male Female Variable N Mean SD N Mean SD rage 535 51 18 666 49 19 In the code above, we are simple specifying the variable rage and grouping the summary by rsex. This produces a small summary included the number of observations in each category an the main measure of centrality and spread, namely the mean and the std. dev. 4.2.6 Activity 2 Can money buy happiness? Using the data in the nilt_subset dataset, complete the following activities. This will be a good practice run for doing the research report when you run your own analysis on the NILT teaching dataset. Handy! Using the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions (use the $ symbol after the name of the dataset and then the name of the variable inside the function); Create a summary of the personal income persinc2 variable, using the sumtable() function; Finally, compute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy. What do you observe? Discuss the results with your neighbour or your tutor. Note that the argument na.rm equal TRUE is used in the function. The ‘na’ bit refers to missing values, and the ‘rm’ refers to remove. So we are telling R to remove the missing values when computing the median. This is because we do not know the age of 3 of the respondents in the sample.↩︎ "],["rmarkdown.html", "Lab 5 Reporting in R Markdown 5.1 Introduction 5.2 R Markdown 5.3 Activity", " Lab 5 Reporting in R Markdown 5.1 Introduction In the previous labs you were exploring the 2012 Northern Ireland Life and Times Survey (NILT). You’ve learnt how to download, read and format the data. Also, you’ve learnt how to explore categorical and numeric data and a mix of them. In this lab, you will learn about how to efficiently report quantitative results directly from R, using R Markdown, which is used by many academics and professionals in a workplace setting to communicate quantitative findings to a wider audience. R Markdown is also what you will use to write your research report assignment for this course. So, Let’s dive in and learn more! 5.2 R Markdown R Markdown (Rmd) is a different type of file included in R Studio (and it is actually a different programming language). This allows you to generate reports in common file types, such as .html (the same one used for this lab workbook you’re reading right now), .pdf or word (.doc). The interesting thing is that the Rmd file allows you to integrate text, code, and plots directly into your report (so you do not have to copy and paste tables or graphs into a Word document, for example, which is often very messy and time-consuming). You have already seen how well this works in the lab workbooks so far, which are written entirely in R Markdown. The basic components of an Rmd file are: the code, text and metadata. The code is integrated by blocks called ‘chunks’, and the metadata contains information to format the report. We believe the best way to learn is by doing it. So, let’s create your first Rmd document! 5.2.1 First R Markdown document We will continue working in the same project called NILT in RStudio Cloud. Please go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary); Open your own copy of the ‘NILT’ project from the ‘Quants lab group’; Create a new Rmd file, this is similar as creating and R Script, from the ‘File’ tab on the top-left: File&gt;New File&gt;R Markdown... (Rstudio may ask to install some packages, click ‘Yes’); Type ‘Test1’ in ‘Tile’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML. Then, click ‘OK’. Save the Rmd file clicking on File&gt;Save as..., type Test1 in the ‘File name’ box, and click on the Save button. After completing the previous steps, your screen should look like this (You can minimize the console to expand pane 1): Figure 5.1: Rmd file. Note that now you have two files open in Pane 1, one tab includes the R script that we created in the last lab (called Lab_3.R), and the other is the Rmd document that you just created. The Rmd document Test1 contains an example by default. The first bit on the top enclosed by the dashes ---, contains the general metadata to format the output, as shown in the Figure 5.2. This bit is called YAML. In the default example, it contains the title, name of the author, data and the type of output (html). You can adjust this information directly by typing the relevant info (e.g. date or name). Figure 5.2: YAML and code chunk. Below the YAML shown in Figure 5.2, there is another box. This is an R code ‘chunk’. To run a chunk of code individually (that is to visualize a partial result of an Rmd document), you can click on the green arrow pointed on the top-left of the first chunk. In line 12, you have a second-level header, which contains the name of a section in the document. As you can see this is preceded by double hash tag ##. If you want a first-level header section, you would require only one hash tag like this #, and three for a third-level header. Finally the ‘Knit’ word is enclosed by double asterisk **. This is to format the characters enclosed in bold. In line 26, you will see a chunk including a basic plot. Let’s check the results that this example in ‘Test1’ produce. To render the document from Rmd to HTML, we need to Knit it by clicking on the icon shown below. Try it! Knit button RStudio may ask you if you want to update some packages, click ‘Yes’. After you knit the document, a window with the output will pop-up automatically. As you can see, this document contains the main title, followed by your name and date, as specified in the YAML. After, there is a second-level header which includes the first section of this example document. Also the the word ‘Knit’ is shown in bold, as it was wrapped by double asterisk **. An interesting thing is that we can integrate the result of our code in the output as we did with the second chunk (which starts in line 18). Here, we can use a summary of the data set cars (which is an in-built data set in R that contains only two variables). Similarly, you can create a .pdf file. To do this, instead of clicking on on the Knit icon directly, click on the black arrow next to it. Then click ‘Knit to PDF’. Try it and see the result. When you knitted the document, R Studio actually created a new .html or .pdf file named as same as your Rmd document. You can confirm this in the ‘File’ tab in Pane 4. You can to download this file from the cloud to your local drive by clicking on the box of the output file. Then, click More&gt;Export... in the same pane (or clicking on the gear icon in pane 4). IMPORTANT: Rmd files are different from simple R scripts. While everything you write in an R script is interpreted as code, only the bits within the code chunks will be interpreted as code in Rmd files. Everything else not within chunks in an Rmd file is interpreted as text. 5.3 Activity In the Test1.Rmd file that you just created, do the following: Change the title of the document in the YAML to ‘My first R Markdown document’. In the code chunk in line 21, replace the existing line (summary(car)) with the following: glimpse(iris). In the code chunk called ‘pressure’, change echo=FALSE to echo=TRUE. At the very bottom of the script, create a new paragraph and write one or two lines briefly describing how you think quantitative methods are improving your discipline (e.g. politics, sociology, social and public policy, or central and eastern European studies). Knit the document in html format. Download the newly edited version of the Test1.html document to your machine. Discuss how each of the edits suggested above modify the output with your neighbour or your tutor. Make sure you’ve got the basics of R Markdown, since this is the tool which you will use to write your final assignment (i.e. research report). If there is something not very clear, or you are curious about, feel free to ask your tutor. They will be happy to answer your questions. "],["visual-exploratory-analysis.html", "Lab 6 Visual exploratory analysis 6.1 Introduction 6.2 Data visualization 6.3 Categorical variables 6.4 Numeric variables 6.5 Mixed data 6.6 Activity", " Lab 6 Visual exploratory analysis 6.1 Introduction In this lab, we will extend your skills to explore data by visualizing it…and R is great for this! It is actually a highly-demanded skill in the job market. Visualizing data is an important process in at least two stages in quantitative research: First, for you as a researcher to get familiar with the data; and second, to communicate your findings. R includes two important tools to achieve this: First, the ggplot2 package (included in tidyverse), a powerful tool to explore and plot data. Second, R Markdown which allows you to create integrated quantitative reports. Combining and mastering the two can create very effective results. 6.2 Data visualization Visual data exploration with ggplot2 (Artwork by @alison_horst). Figure 6.1: Visual data exploration. Source: Horst (n.d.) Different plot types serve different types of data. In the last lab, we introduced some functions to summarise your data and to generate summaries for specific types of variable. This will help you to decide what the most suitable plot is for your data or variable. The table below presents a minimal guide to choose the type of plot as a function of the type of data you intend to visualize. In addition, it splits the type of plot by the number of variables included in the visualization, one for univariate, or two for bivariate. Bivariate plots are useful to explore the relationship between variables. Univariate Bivariate Categorical Bar plot / Pie chart Bar plot Numeric Histogram / boxplot Scatter plot Categorical + Numeric - Box plot 6.2.1 Prepare the Rmd document and data As we did in the last session, let’s learn by doing. We will continue working in the same project called NILT in RStudio Cloud. Set up your session as follows: Go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary); Open your own copy of the ‘NILT’ project from the ‘Quants lab group’; Next, create a new Rmd document following the next steps. Create a new Rmd file titled ‘Lab 6. Data visualization’, and write your name in the ‘Author’ space. Save the Rmd file clicking on File&gt;Save as..., using Lab_6 in the ‘File name’ box, and click on the ‘Save’ button. Delete all the contents in the default example with the exception of the first bit which contains the YAML and the first code chunk (that is keeping from line 1 to 10 and deleting everything from line 11). In the setup chunk (line 8), change echo from TRUE to FALSE (this will hide the code for all chunks in the output). Once your Rmd document is ready, insert a new R code chunk. To do this, click on the ‘Insert’ button on pane 1, and then click on ‘R’. This will insert an R code chunk as the ones we explored in ‘Test1’. Figure 6.2: Insert a code chunk. In the body of the first chunk, load the packages that you will need. For now, it will be only tidyverse. library(tidyverse) After the previous, insert a second chunk. In the body of the code copy and paste the lines below. This is to load the data we downloaded and formatted in the last lab in R format using the readRDS() function (note that we are reading the .rds file and not the original .sav file). Also, we will create a subset called nilt_subset selecting only the variables that we are interested in for now, as we did in the previous lab. # Load the data from the .rds file we created in the last lab nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) #Create subset nilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2) Run both of the previous chunks individually by clicking on the green arrow located on the top-right of the chunk, as described earlier. Now that we read the data in, we are ready to start creating our own own plots using the 2012 NILT survey. You do not need to reproduce each of the examples in the following section, but we suggest you to carefully read the description and try to understand the code syntax. 6.3 Categorical variables Let’s start using the same variables we summarised in our last lab. We started by computing the total number of respondents by gender in a One-Way contingency table. We can easily visualize this using a bar plot with ggplot. This package always takes at least three layers, namely data, aesthetics and geometry. Here, we define the data as the first argument of the ggplot()function with the nilt_subset. The second argument, aesthetics, is separated by a comma and is introduced using the function aes(). In this case we define the X axis x = of the plot by the categories included in the variable rsex. The geometry is specified with the function geom_bar() without arguments for now. Note that we added the geometry using the plus sign + at the end of the previous line code. As an extra, I included the main title and the name of the x axis using the labs() function (you don’t need to copy or run the following code chunks in your .Rmd. This is only for demonstration purposes). ggplot(nilt_subset, aes(x = rsex)) + geom_bar() + labs(title = &#39;Gender&#39;, x = &#39;Gender of respondent&#39;) From the plot above, we can graphically see what we found out previously: there are more female respondents than males in our sample. The advantage is that we can have a sense of the magnitude of the difference by visualising it. 6.3.1 Bivariate categorical vs categorical In Lab 3, we computed a Two-Way contingency table, which included the count of two categorical variables. This summary can be visualized using a stacked bar plot. This is quite similar to the above, with the addition that the area of the vertical plot is coloured by the size of each group. If we wanted to know how gender is split by religion, we can add the fill argument with a second variable in aesthetics, as shown below. ggplot(nilt_subset, aes(x = rsex, fill = religcat)) + geom_bar() + labs(title = &#39;Gender by religion&#39;, x = &#39;Gender of respondent&#39;) This plot is not very informative, since the total size of female and male respondents is different. The type of visualization will also depend on your specific research question or the topic you are interested in. For example, if I think it is worthwhile visualizing the religion by respondents’ sex. A plot can show us the magnitudes and composition by respondents’ sex for each religion. To do this, we need to change the aesthetics, specifying the religion by category variable religcat on the x axis and fill with gender rsex. ggplot(nilt_subset, aes(x = religcat, fill = rsex)) + geom_bar() + labs(title = &#39;Religion by gender&#39;, x = &#39;Religion&#39;) As we can see, catholic and protestant religion are equally popular among the respondents. Also, we can see that these are composed by similar proportions of males and females. One interesting thing is that there are more male respondents with no religion than female participants. Again, we found this out with the descriptive statistics computed in the last lab. However, we have the advantage that we can graphically reprsent and inspect the magnitude of these differences. 6.4 Numeric variables 6.4.1 Univariate numeric Last time we talked about some measures of centrality and spread for numeric variables. The histogram plot is similar to the bar plot, the difference is that it splits the numeric range in fix “bins” and computes the frequency/count for each bin instead of counting the number of respondents for each numeric value. The syntax is practically the same as the simple bar plot. At this time, we set the the x aesthetic with the numeric variable age rage. Also, the geometry is defined as a histogram, using the geom_histogram() function. ggplot(nilt_subset, aes(x = rage)) + geom_histogram() + labs(title = &#39;Age distribution&#39;) From the histogram, we have age (in bins) on the X axis, and the frequency/count on the y axis. This plot is useful to visualize how respondent’s age is distributed in our sample. For instance, we can quickly see the minimum and maximum value, or the most popular age, or a general trend indicating the largest age group. A second option to visualize numeric variables is the box plot. Essentially this draws the quartiles of a numeric vector. For this time, rage is defined in the y axis. This is just a personal preference. The geometry is set by the geom_boxplot() function. ggplot(nilt_subset, aes(y = rage)) + geom_boxplot() + labs(title = &#39;Age boxplot&#39;) What we see from this plot is the first, second and third quartile. The second quartile (or median) is represented by the black line in the middle of the box. As you can see this is close to 50 years old, as we computed using the quantile() function. The lower edge of the box represents the 2nd quartile, which is somewhere around 35 years old. Similarly the 3rd quartile is represented by the upper edge of the box. We can confirm this by computing the quantiles for this variable. quantile(nilt_subset$rage, na.rm = T) ## 0% 25% 50% 75% 100% ## 18 35 48 64 97 6.4.2 Bivariate numeric A useful plot to explore the relationship between two numeric variables is the scatter plot. This plot locates a dot for each observation according to to their respective numeric values. In the example below, we use age rage on the X axis (horizontal), and personal income persinc2 on the Y axis (vertical). This type of plot is useful to explore a relationship between variables. To generate a scatter plot, we need to define x and y in aesthetics aes(). The geometry is a point, expressed by geom_point(). Note that we are specifying some further optional arguments. First, in aes() alpha regulates the opacity of the dots. This goes from 0.0 (completely translucent) to 1.0 (completely solid fill). Second, in geom_point() we defined position as jitter. This arguments slightly moves the point away from their exact location. These two arguments are desired in this plot because the personal income bands are overlapped. Adding some transparency and noise to their position, can allow to visualize possible patterns easily. ggplot(nilt_subset, aes(x = rage, y = persinc2, alpha = 0.8)) + geom_point(position = &quot;jitter&quot;) + labs(title = &#39;Personal income vs age&#39;, x = &#39;Age&#39;, y = &#39;Personal income (£)&#39;) There is not a clear pattern in our previous plot. However, it is interesting to note that most of the people younger than 25 years old earn less than £20K a year. Similarly, most of the people older than 75 earn less than £20K. And only very few earn over £60k a year (looking at the top of the plot). 6.5 Mixed data Very often we want to summarise central or spread measure by categories or groups. For example, let’s go back to the example of age and respondents’ sex. We can visualize these two variables (which include one numeric and one categorical) using a box plot. To create this, we need to specify the x and y value in aes() and include the geom_boxplot() geometry. ggplot(nilt_subset, aes(y = rage, x= rsex)) + geom_boxplot() + ggtitle(&#39;Age by sex&#39;) From this, we can visualize that female participants are slightly younger than their male counterparts in the sample. 6.6 Activity Using the nilt_subset object, complete the tasks below in the Rmd file Lab_6, which you created earlier. Insert a new code chunk for each of these activities and include brief comments as text (outside the chunk) in the Rmd document to introduce or describe the plots. Feel free to copy and adapt the code to create the plots in the examples above. Create a first-level header to start a section called “Categorical analysis”; Create simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid; Based on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables; Create a new first-level header to start a section called “Numeric analysis”; Create a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis; Finally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime); Add your own (brief) comments to each of the plots as text in your Rmd file; Knit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish. 6.6.1 R Cheatsheets There are a number of features that you can customize in your plots, including the background, text size, colours, adding more variables. But you don’t have to memorise or remember all this, one thing that is very commonly used by R data scientists are R cheat sheets! They are extremely handy when you try to create a visualisation from scratch, check out the Data Visualization with ggplot2 Cheat Sheet. An extra tip is that you can change the overall look of the plot by adding pre-defined themes. You can read more about it here. Another interesting site is the The R Graph Gallery, which includes a comprehensive showcase of plot types and their respective code. References "],["correlation.html", "Lab 7 Correlation 7.1 What is correlation? 7.2 Measuring correlation", " Lab 7 Correlation 7.1 What is correlation? When conducting empirical research, we are often interested in associations between two variables, for example, personal income and attitudes towards migrants. In this lab we will focus on visualizing relationship between variables and how to measure it. In quantitative research, the main variable of interest in an analysis is called the dependent or response variable, and the second is known as the independent or explanatory. In the example above, we can think of personal income as the independent variable and attitudes as the dependent. The relationship between variables can be positive, negative or non-existent. The figure below shows these type of relationships to different extents. The association is positive when one of the variables increases and the second variable tends to go in the same direction (that is increasing as well). The first plot on the left-hand side shows a strong positive relationship. As you can see, the points are closely clustered around the straight line. The next plot also shows a positive relationship. This time the relationship is moderate. Therefore, the points are more dispersed in relation to the line compared to the previous one. Figure 7.1: Types of correlation. The plot in the middle, shows two variables that are not correlated. The location of the points is not following any pattern and the line is flat. By contrast, the last two plots on the right hand-side show a negative relationship. When the values on the X axis increase, the values on the Y axis tend to decrease. 7.1.1 Data and R environment We will continue working on the same R Studio Cloud project as in the previous session and using the 2012 Northern Ireland Life and Times Survey (NILT) data. To set the R environment please follow the next steps: Please go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary); Open your own copy of the ‘NILT’ project from the ‘Quants lab group’; Create a new Rmd file, type ‘Correlation analysis’ in ‘Tile’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML. Save the Rmd document under the name ‘Lab7_correlation’. Delete all the contents in the Rmd default example with the exception of the first bit which contains the YAML and the first chunk, which contains the default chunk options (that is all from line 12 and on). In the setup chunk, change echo from TRUE to FALSE in line 9 (this will hide the code for all chunks in your final document). Within the first chunk, copy and paste the following code below line 9 knitr::opts_chunk$set(message = FALSE, warning = FALSE). This will hide the warnings and messages when you load the packages. In the Rmd document insert a new a chunk, copy and paste the following code. Then, run the individual chunk by clicking on the green arrow on the top-right of the chunk. # Load the packages library(tidyverse) library(haven) # Load the data from the .rds file we created in lab 3 nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) This time we will use new variables from the survey. Therefore, we need to coerce them into their appropriate type first. Insert a second chunk, copy and paste the code below. Then, run the individual chunk. # Age of respondent’s spouse/partner nilt$spage &lt;- as.numeric(nilt$spage) # Migration nilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric) Also, we will create a new variable called mig_per by summing the respondent’s opinion in relation to migration using the following variables: mil10yrs, miecono and micultur (see the documentation p. 14 here to know more about these variables). Again, insert a new chunk, copy and paste the code below, and run the individual chunk. # overall perception towards migrants nilt &lt;- rowwise(nilt) %&gt;% # sum values mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T )) %&gt;% ungroup() %&gt;% # assign NA to values that sum 0 mutate(mig_per = na_if(mig_per, 0)) 7.1.2 Visualizing correlation Visualizing two or more variables can help to uncover or understand the relationship between these variables. As briefly introduced in the previous session, different types of plots are appropriate for different types of variables. Therefore, we split the following sections according to the type of data to be analysed. You do not need to run or reproduce the examples shown in the following sections in your R session with the exception of exercises that are under the activity headers. 7.1.2.1 Numeric vs numeric To illustrate this type of correlation, let’s start with a relatively obvious but useful example. Suppose we are interested in how people choose their spouse or partner. The first characteristic that we might look at is age. We might suspect that there is a correlation between the the nilt respondents’ won age and their partner’s age. Since both ages are numeric variables, a scatter plot is appropriate to visualize the correlation. To do this, let’s use the functions ggplot() and geom_point(). In aesthetics aes() let’s define the respondent’s age rage in the X axis and the respondent’s spouse/partner age spage in the Y axis. As a general convention in quantitative research, the response/dependent variable is visualized on the Y axis and the independent on the X axis (you do not need to copy and reproduce the example below). ggplot(nilt, aes(x = rage, y = spage)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Respondent&#39;s age vs respondent’s spouse/partner age&quot;, x = &quot;Respondent&#39;s age&quot;, y = &quot;Respondent’s spouse/partner age&quot; ) Note that in this plot the function geom_smooth() was used. This is to plot a straight line which describes the best all the points in the graph. From the plot above, we see that there is a strong positive correlation between the respondent’s age and their partner’s age. We see that for some individuals their partner’s age is older, whereas others is younger. Also, there are some dots that are far away from the straight line. For example, in one case the respondent is around 60 years old and the age of their partner is around 30 years old (can you find that dot on the plot?). These extreme values are known as outliers. We may also suspect that the respondents’ sex is playing a role in this relationship. We can include this as a third variable in the plot by colouring the dots by the respondents’ sex. To do this, let’s specify the colour argument in aesthetics aes() with a categorical variable rsex. ggplot(nilt, aes(x = rage, y = spage, colour = rsex)) + geom_point() + geom_abline(slope = 1, intercept = 0, colour = &quot;gray20&quot;) + labs(title = &quot;Respondent&#39;s age vs respondent’s spouse/partner age&quot;, x = &quot;Respondent&#39;s age&quot;, y = &quot;Respondent’s spouse/partner age&quot; ) In the previous plot, we included a line which describes what it would look like if the partner’s age were exactly the same as the respondent’s age. We observe a clear pattern in which female participants are on one side of the line and males on the other. As we can see, most female respondents tend to choose/have partners who are older, whereas males younger ones. 7.1.3 Activity 1 In the Lab7_correlation file, use the nilt data object to visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text (outside the code chunks). Create a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function set the method argument to 'lm'. What type of relationship do you observe? Comment as text in the Rmd the overall result of the plot and whether this is in line with your previous expectation. 7.1.3.1 Numeric vs categorical As briefly introduced in the last lab, correlations often occur between categorical and numeric data. A good way to observe the relationship between these type of variables is using a box plot. Which essentially shows the distribution of the numeric values by category/group. Let’s say we are interested in the relationship between education level and perception of migration. The variable highqual contains the respondent’s highest education qualification. Using ggplot(), we can situate mig_per on the X axis and highqual on the Y axis, and plot it with the gem_boxplot() function. Note that before passing the dataset to ggplot, we can filter out two categories of the variable highqual where education level is unknown (i.e. “Other, level unknown” or “Unclassified”). nilt %&gt;% filter(highqual != &quot;Other, level unknown&quot; &amp; highqual != &quot;Unclassified&quot;) %&gt;% ggplot(aes(x = mig_per, y = highqual )) + geom_boxplot() From the plot above, we see that respondents with higher education level (on the bottom) appear to have more positive opinion on migration when compared to respondents with lower education level or no qualifications (on the top). Overall, the data shows a pattern that the lower one’s education level is, the worse their opinion towards migration is likely to be. Since education level is an ordinal variable, we can say this is a positive relationship. 7.1.4 Activity 2 Using the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you can observe from the results as text in the Rmd file to introduce the plot. Create a boxplot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the political party which the respondent identify with uninatid. Use ggplot() in combination with geom_boxplot(). Make sure to specify mig_per on the Y axis and uninatid on the X axis in aes(). Do you think the opinion towards migration differs among the groups in the plot? Comment on the overall results in the Rmd document. 7.2 Measuring correlation So far we have examined correlation by visualizing variables only. A useful practice in quantitative research is to actually measure the magnitude of the relationship between these variables. One common measure is the Pearson correlation coefficient. This measure results in a number that goes from -1 to 1. A coefficient below 0 implies a negative correlation whereas a coefficient over 0 a positive one. When the coefficient is close to positive one (1) or negative one (-1), it implies that the relationship is strong. By contrast, coefficients close to 0 indicate a weak relationship. This technique is appropriate to measure linear numeric relationships, which is when we have numeric variables with a normal distribution, e.g. age in our dataset. Let’s start measuring the relationship between the respondent’s age and their partner’s age. To do this in R, we should use the cor() function. In the R syntax, first we specify the variables separated by a comma. We need to be explicit by specifying the object name, the dollar sign, and the name of the variable, as shown below. Also, I set the use argument as 'pairwise.complete.obs'. This is because one or both of the variables contain more than one missing value. Therefore, we are telling R to use complete observations only. cor(nilt$rage, nilt$spage, use = &#39;pairwise.complete.obs&#39;) ## [1] 0.9481297 The correlation coefficient between this variables is 0.95. This is close to positive 1. Therefore, it is a strong positive correlation. The result is completely in line with the plot above, since we saw how the dots were close to the straight line. What about the relationship between age and mig_per that you plotted earlier? cor(nilt$rage, nilt$mig_per, use = &#39;pairwise.complete.obs&#39;) ## [1] -0.05680918 The coefficient is very close to 0, which means that the correlation is practically non-existent. The absence of correlation is also interesting in research. For instance, one might expect that younger people would be more open to migration. However, it seems that age does not play a role on people’s opinion about migration in NI according to this data. Let’s say that we are interested in the correlation between mig_per and all other numeric variables in the dataset. Instead of continuing computing the correlation one by one, we can run a correlation matrix. The code syntax can be read as follows: from the nilt data select these variables, then compute the correlation coefficient using complete cases, and then round the result to 3 decimals. nilt %&gt;% select(mig_per, rage, spage, rhourswk, persinc2) %&gt;% cor(use = &#39;pairwise.complete.obs&#39;) %&gt;% round(3) ## mig_per rage spage rhourswk persinc2 ## mig_per 1.000 -0.057 -0.132 0.082 0.228 ## rage -0.057 1.000 0.948 -0.013 -0.036 ## spage -0.132 0.948 1.000 -0.182 -0.090 ## rhourswk 0.082 -0.013 -0.182 1.000 0.383 ## persinc2 0.228 -0.036 -0.090 0.383 1.000 From the result above, we have a correlation matrix that computes the Person correlation coefficient for the selected variables. In the first row we have migration perception. You will notice that the first value is 1.00, this is because it is measuring the correlation against the same variable (i.e. itself). The next value in the first row is age, which is nearly 0. The next variables also result in low coefficients, with the exception of the personal income, where we see a moderate/low positive correlation. This can be interpreted that respondents with high income are associated with more positive opinion towards migration compared to low-income respondents. 7.2.1 Activity 3 Insert a new chunk in your Rmd file; Using the nilt data object, compute a correlation matrix using the following variables: rage, persinc2, mil10yrs, miecono and micultur, setting the use argument to 'pairwise.complete.obs' and rounding the result to 3 decimals; Run the chunk individually and comment whether personal income or age is correlated with the perception of migrants in relation to the specific aspects asked in the variables measured (consult the documentation in p. 14 to get a description of these variables); Knit the Lab7_correlation Rmd document to .html or .pdf. The output document will automatically be saved in your project. Discuss your previous results with your neighbour or tutor. "],["simple-lm.html", "Lab 8 Linear model: Simple linear regression 8.1 Welcome 8.2 Introduction to simple linear regression 8.3 Fitting linear regression in R 8.4 Model evaluation 8.5 Lab activities", " Lab 8 Linear model: Simple linear regression 8.1 Welcome In the previous lab we learned about correlation. We visualised the relationship of different types of variables. Also, we computed one correlation measure for two numeric variables, Pearson correlation. This measure is useful to compute the strength and the direction of the association. However, it presents some limitations, e.g. it can only be used for numeric variables, it allows only one variable at a time, and it is appropriate to describe a linear relationship. Linear regression can overcome some of these limitations and it can be extended to achieve further purposes (e.g. use multiple variables or estimate scenarios). This technique is in fact very common and one of the most popular in quantitative research in social sciences. Therefore, getting familiar with it will be important for you not only to perform your own analyses, but also to interpret and critically read literature. 8.2 Introduction to simple linear regression IMPORTANT: Linear regression is appropriate only when the dependent variable is numeric (interval/ratio). However, the independent variables can be categorical, ordinal, or numeric. Also, you can include more than one independent variable to evaluate how these relate to the dependent variable. In this lab we will start using only one explanatory variable. This is know as simple linear regression. 8.2.1 The idea behind For this lab, we will continue using the 2012 NILT survey to introduce linear models. To do so, please set your RStudio environment as follows: Go to your ‘Quants lab group’ in RStudio Cloud; Create a copy of the project called ‘NILT2’ which is located in your ‘Quants lab group’ by clicking on the ‘Start’ button; Once in the project, create a new ‘R Script’ file (a simple R script NOT an .Rmd file); Save the R file as ‘linear_model_intro’. Reproduce the code below, by copying, pasting and running it from your new script and focus on the intuitive part of this section. First, load the tidyverse library and read the nilt_r_object.rds file which is stored in the folder called ‘data’ and contains the NILT survey (tidyverse was installed in your session already). ## Load the packages library(tidyverse) # Read the data from the .rds file nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) This dataset is exactly the same as the one you processed in previous labs. We will re-revisit the example of the respondent’s spouse/partner characteristics. To keep things simple, let’s create a minimal sample of the dataset including only 40 random observations: # select a small random sample set.seed(3) # Filter where partner&#39;s age is not NA and take a random sample of 40 nilt_sample &lt;- filter(nilt, !is.na(spage)) %&gt;% sample_n(40) # Select only respondent&#39;s age and spouse/partner&#39;s age nilt_sample &lt;- select(nilt_sample, rage, spage) We will start by creating a scatter plot using the respondent’s spouse/partner age spage on the Y axis, and the respondent’s rage on the X axis. # plot ggplot(nilt_sample, aes(x = rage, y = spage)) + geom_point(position = &quot;jitter&quot;) + labs(title = &quot;Respondent&#39;s age vs respondent’s spouse/partner age&quot;, x = &quot;Respondent&#39;s age&quot;, y = &quot;Respondent’s spouse/partner age&quot; ) As you can see, even with this minimal example using 40 observations, there is a clear trend. From the plot, it seems intuitive to draw a line that describes this general trend. Imagine a friend will draw this line for you. You will have to tell them some basic references on how to do it. You can, for example, specify a start point and and end point in the plot. Alternatively, which is what we will do below, you can specify a start point and a value that describes the slope of the line. For now, our friend is ggplot. You have to pass these two values (start point and slope) in the geom_abline() function to draw a line which describes these points. ggplot(nilt_sample, aes(x = rage, y = spage)) + geom_point() + geom_abline(slope = 1, intercept = 0, colour = &quot;red&quot;) + labs(title = &quot;Respondent&#39;s age vs respondent’s spouse/partner age&quot;, x = &quot;Respondent&#39;s age&quot;, y = &quot;Respondent’s spouse/partner age&quot; ) In the guess above, it is assumed that the age of the spouse/partner might be exactly the same as the respondent. To draw a line like this, we used 0 as the starting value in the geom_abline() function. In statistics, this is known as the intercept and it is often represented with a Greek letter and a zero sub-index as \\(\\beta_0\\) (beta-naught) or with \\(\\alpha\\) (alpha). The location of the intercept can be found along the vertical axis (in this case it is not visible). The second value we passed to describe the line is the slope, which uses the X axis as the reference. The slope value is multiplied by the value of the X axis. Therefore, a slope of 0 is completely horizontal. In this example, a slope of 1 produces a line at 45º. The slope is often represented with the Greek letter beta and a sequential numeric sub-index like this: \\(\\beta_1\\) (beta-one). We will create some points in the data frame based on these two values, the start point and the steepness factor or, formally speaking, the intercept and the slope respectively. We will store the results in a column called line1 and print the head of this data set. # create values for the guess line 1 nilt_sample &lt;- nilt_sample %&gt;% mutate(line1 = 0 + rage * 1) # print results head(nilt_sample) ## # A tibble: 6 × 3 ## rage spage line1 ## &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; ## 1 40 39 40 ## 2 54 51 54 ## 3 66 59 66 ## 4 60 71 60 ## 5 35 34 35 ## 6 67 57 67 From the above, you can notice that the respondent’s age rage and the column that we just created line1, are identical…So, that is our guess for now. The partner’s age is the same as the respondent’s. Is the line we just drew good enough to represent the general trend in the relationship above? To answer this question, we can measure the distance of each point of the plot to the line 1, as shown by the dashed segment in the plot below. ggplot(nilt_sample, aes(x = rage, y = spage)) + geom_point() + geom_abline(slope = 1, intercept = 0, colour = &quot;red&quot;) + geom_segment(aes(xend = rage, yend = line1), linetype = &quot;dashed&quot;) + labs(title = &quot;Respondent&#39;s age vs respondent’s spouse/partner age&quot;, x = &quot;Respondent&#39;s age&quot;, y = &quot;Respondent’s spouse/partner age&quot; ) This distance difference is known as the residual or the error and it is often expressed with the Greek letter \\(\\epsilon\\) (epsilon). Numerically, we can calculate this by computing the difference between the known value (the observed) and the guessed number. Formally, the guessed number is called the expected value (also known as predicted/estimated value). Normally, the expected values in statistics are represented by putting a hat like this \\(\\hat{}\\) on the letters. Since the independent variable is usually located on the Y axis, the expected value is differentiated from the observed values by putting the hat on the Y like this: \\(\\hat{y}\\) (y-hat). Let’s estimate the residuals for each of the observations by subtracting \\(\\hat{y}_i\\) from \\(y_i\\) (spage - line1) and store it in a column called residuals. Print the head of the data set after. # estimate residuals nilt_sample &lt;- nilt_sample %&gt;% mutate(residuals = spage - line1) # print first 6 values head(nilt_sample) ## # A tibble: 6 × 4 ## rage spage line1 residuals ## &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 40 39 40 -1 ## 2 54 51 54 -3 ## 3 66 59 66 -7 ## 4 60 71 60 11 ## 5 35 34 35 -1 ## 6 67 57 67 -10 Coming back to the question, is my line good enough? We could sum all these residuals as an overall measure to know how good my line is. From the previous plot, we see that some of the expected ages exceed the actual values and others are below. This produces negative and positive residuals. If we simply sum them, they would compensate each other and this would not tell us much about the overall magnitude of the difference. To overcome this, we can multiply the differences by themselves (to make all numbers positive) and then sum them all together. This is known as the sum of squared residuals (SSR) and we can use this as the criterion to measure how good my line is with respect to the overall trend of the points. For line 1, we can easily calculate the SSR as follows: sum((nilt_sample$residuals) ^ 2) ## [1] 1325 The sum of squared residuals for line 1 is 1,325. We can try different lines to find the combination of intercept and slope that produces the smallest error (SSR). To our luck, we can find the best values using a well established technique called Ordinary Least Squares (OLS). This technique finds the optimal solution by the principle of maxima and minima. We do not need to know the details for now. The important thing is that this procedure guarantees to find a line that produces the smallest possible error (SSR). In R, it is very simple to fit a linear model and we do not need to go through each of the steps above manually nor to memorise all the steps. To do this, simply use the function lm(), save the result in an object called m1 and print it. m1 &lt;- lm(spage ~ rage, nilt_sample) m1 ## ## Call: ## lm(formula = spage ~ rage, data = nilt_sample) ## ## Coefficients: ## (Intercept) rage ## 6.299 0.875 So, this is the optimal intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) that produces the least sum of the square. Let’s see what the SSR is compared to my arbitrary guess above: sum(residuals(m1) ^ 2) ## [1] 1175.209 Yes, this is better than before (because the value is smaller)! Let’s plot the line we guessed and the optimal line together: ggplot(nilt_sample, aes(x = rage, y = spage)) + geom_point() + geom_abline(slope = 1, intercept = 0, colour = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Respondent&#39;s age vs respondent’s spouse/partner age&quot;, x = &quot;Respondent&#39;s age&quot;, y = &quot;Respondent’s spouse/partner age&quot; ) The blue is the optimal solution, whereas the red was just an arbitrary guess. We can see that the optimal is more balanced than the red in relation to the observed data points. 8.2.2 Formal specification Now you are ready for the the formal specification of the linear model. After the introduction above, it is easy to take the pieces apart. In essence, the simple linear model is telling us that the dependent value \\(y\\) is defined by a line that intersects the vertical axis at \\(\\beta_0\\), plus the multiplication of the slope \\(\\beta_1\\) by the value of \\(x_1\\), plus some residual \\(\\epsilon\\). The second part of the equation is just telling us that the residuals are normally distributed (that is when a histogram of the residuals follows a bell-shaped curve). \\[ \\begin{align} y = \\beta_0 + \\beta_1 * x_1 + \\epsilon, &amp;&amp; \\epsilon ~ N(0, \\sigma) \\end{align} \\] You do not need to memorize this equation, but being familiar to this type of notation will help you to expand your skills in quantitative research. 8.3 Fitting linear regression in R 8.3.1 R syntax We already fitted our first linear model using the lm() above, which of course stands for linear model, but we did not discuss the specifications needed in this function. The first argument takes the dependent variable, then, we use tilde ~ to express that this is followed by the independent variable. Then, we specify the data set separated by a comma, as shown below (this is just a conceptual example, it does not actually run!): lm(dependent_variable ~ independent_variable, data) 8.3.2 Running a simple linear regression Now, let’s try with the full data set. First, we will fit a linear model using the same variables as the example above but including all the observations (remember that before we were working only with a small random sample). The first argument is the respondent’s spouse/partner age spage, followed by the respondent’s age rage. Let’s assign the model to an object called m2 and print it after. m2 &lt;- lm(spage ~ rage, nilt) m2 ## ## Call: ## lm(formula = spage ~ rage, data = nilt) ## ## Coefficients: ## (Intercept) rage ## 3.7039 0.9287 The output first displays the formula employed to estimate the model. Then, it show us the estimated values for the intercept and the slope for the age of the respondent, these estimated values are called coefficients. In this model, the coefficients differ from the ones in the example above (m1). This is because we now have more information to fit this line. Despite the difference, we see that the relationship is still in the same direction (positive). In fact, the value of the slope \\(\\beta_1\\) did not change much if you look at it carefully (0.87 vs 0.92). 8.3.3 Interpreting results But what are the slope and intercept telling us? The first thing to note is that the slope is positive, which means that the relationship between the respondent age and their partner are in the same direction. When the age of the respondent goes up, the age of their partner is expected to go up as well. In our model, the intercept does not have a meaningful interpretation on its own, it would not be logical to say that when the respondent’s age is 0 their partner would be expected to be 3.70 years old. Interpretations should be made only within the range of the values used to fit the model (the youngest age in this data set is 18). The slope can be interpreted as following: For every year older the respondent is, the partner’s age is expected to change by 0.92. In other words, the respondents are expected to have a partner who is 0.92 times years older than themselves. This means that their partners are expected to be slightly younger. In this case, the units of the dependent variable are the same as the independent (age vs age) which make it easy to interpret. In reality, there might be more factors that potentially affect how people select their partners (e.g. genders, education, race/ethnicity, etc.). For now, we are keeping things simple using only these two variables. Let’s practice with another example. What if we are interested in income? We can use personal income persinc2 as the dependent variable and the number of hours worked a week rhourswk as the independent variable. We’ll assign the result to an object called m3. m3 &lt;- lm(persinc2 ~ rhourswk, data = nilt) m3 ## ## Call: ## lm(formula = persinc2 ~ rhourswk, data = nilt) ## ## Coefficients: ## (Intercept) rhourswk ## 5170.4 463.2 This time, the coefficients look quite different. The results are telling us that for every additional hour worked a week a respondent is expected to earn £463.2 more a year than other respondent. Note that the input units are not the same in the interpretation. The dependent variable is in pounds earned a year and the independent is in hours worked a week. Does this mean that someone who works 0 hours a week is expected to earn £5170.4 a year (given the fitted model \\(\\hat{persinc2} = 5170.4 + 0*463.2\\)) or that someone who works 110 hours a week (impossible!) is expected to earn £56K a year? We should only make reasonable interpretations of the coefficients within the range analysed. Finally, an important thing to know when interpreting linear models using cross sectional data (data collected in a single time point only) is: correlation does not imply causation. Variables are often correlated with other non-observed variables which may cause the effects observed on the independent variable in reality. This is why we cannot always be sure that X is causing the observed changes on Y. 8.4 Model evaluation In the next section, we suggest you to focus on the interpretation of the results of the linear model. You do not need to reproduce the code. 8.4.1 Model summary So far, we have talked about the basics of the linear model. If we print a summary of the model, we can obtain more information to evaluate it. summary(m2) ## ## Call: ## lm(formula = spage ~ rage, data = nilt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -33.211 -2.494 0.075 2.505 17.578 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.70389 0.66566 5.564 4e-08 *** ## rage 0.92869 0.01283 72.386 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.898 on 589 degrees of freedom ## (613 observations deleted due to missingness) ## Multiple R-squared: 0.8989, Adjusted R-squared: 0.8988 ## F-statistic: 5240 on 1 and 589 DF, p-value: &lt; 2.2e-16 There are a lot of numbers from the result. Let’s break down the information into different pieces: first the call, second the residuals, third the coefficients, and fourth the goodness-of-fit measures, as shown below: Model summary The first part, Call, is simply printing the variables that were used to produce the models. The second part, the Residuals, provides a small summary of the residuals by quartile, including minimum and maximum residual values. Remember the second part of the formal specification of the model? Well, this is where we can see in practice the distribution of the residuals. If we look at the first and third quartile, we can see that that 50% of the predicted values by the model are \\(\\pm\\) 2.5 years old away form the observed partner’s age…not that bad. The third part is about the coefficients. This time, the summary provides more information about the intercept and the slope than just the estimated coefficients as before. Usually, we focus on the independent variables in this section, rage in our case. The second column, standard error, tell us how large the error of the slope is, which is small for our variable. The third and fourth columns provide measures about the statistical significance of the relationship between the dependent and independent variable. The most commonly reported is the p-value (the fourth column). When these values contains many digits, R reports the result in scientific notation. In our result the p-value is &lt;2e-16 (the real equivalent number is &lt;0.0000000000000002, if you want to know more about scientific notation click here). As a general agreed principle, if the p-value is equal or less than 0.05 (\\(p\\) ≤ 0.05), we can reject the null hypothesis, and say that the relationship between the dependent variable and the independent is significant. In our case, the p-value is far lower than 0.05. So, we can say that the relationship between the respondent’s spouse/partner age and the respondent’s age is significant. In fact, R includes codes for each coefficient to help us to identify the significance. In our result, we got three stars/asterisks. This is because the value is less than 0.001. R assigns two stars if the p-value is less than 0.01, and one star if it is lower than 0.05. In the fourth area, we have various goodness-of-fit measures. Overall, these measures tell us how well the model represents our data. For now, let’s focus on the adjusted r-squared and the sample size. The r-squared is a value that goes from 0 to 1. 0 means that model is not explaining any of the variance, whereas 1 would mean a perfect model. In social sciences we do not see high values often. The adjusted r-squared can be understood as the percentage variance that the model is able to explain. In the example above, the model explains 89.88% percent of the total variance. This is a good fit, but this is because we are modelling a quite “obvious” relationship. The r-squared does not represent how good or bad our work is. You may wonder why we do not report the sum of squared residuals (SSR), as we just learned. This is because the scale of the SSR is in the units used to fit the model, which would make it difficult to compare our model with other models that use different units or different variables. Conversely, the adjusted r-squared is expressed in relative terms which makes it a more suitable measure to compare with other models. An important thing to note is that the output is telling us that this model excluded 613 observations which were missing in our dataset. This is because we do not have the age of the respondent’s partner in 613 of the rows. This missingness may be because the respondent do not have a partner, refused to provide the information, etc. In academic papers in the social sciences, usually the measures reported are the coefficients, p-values, the adjusted r-squared, and the size of the sample used to fit the model (number of observations). 8.4.2 Further checks The linear model, as many other techniques in statistics, relies on assumptions. These refer to characteristics of the data that are taken for granted to generate the estimates. It is the task of the modeller/analyst to make sure the data used follows these assumptions. One simple yet important check is to examine the distribution of the residuals, which ought to follow a normal distribution. Does the residuals follow a normal distribution in m1? We can go further and plot a histogram to graphically evaluate it. Use the function residuals() to extract the residuals from our object m2, and then plot a histogram with the r-base function hist(). hist(residuals(m2), breaks = 20) Overall, it seems that the residuals follow the normal distribution reasonably well, with the exception of the negative value to the left of the plot. Very often when there is a strange distribution or different to the normal is because one of the assumptions is violated. If that is the case we cannot trust the coefficient estimates. In the next lab we will talk more about the assumptions of the linear model. For now, we will leave it here and get some practice. 8.5 Lab activities In the linear_model_intro.R script, use the nilt dataset in to: Plot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point() jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside. Print the summary of m3 using the summary() function. Is the the relationship of hours worked a week significant? What is the adjusted r-squared? How would you interpret it? What is the sample size to fit the model? What is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model? Plot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)? Discuss your answers with your neighbour or tutor. "],["multi-lm.html", "Lab 9 Multivariate linear model 9.1 Introduction 9.2 Multivariate model 9.3 R syntax for the multivariate linear model 9.4 Assumptions 9.5 Lab activities", " Lab 9 Multivariate linear model 9.1 Introduction In the last lab we mentioned that one of the advantages of linear regression is that we can include more than one independent (explanatory) variable to evaluate their relationship with the dependent variable. This technique is known as multivariate linear regression. In practice, few studies in quantitative social research rely only on simple linear models. This is because often in reality there is more than one variable associated to a social phenomenon. This is why it is important to familiarise yourself with the multivariate linear model. 9.2 Multivariate model The multivariate linear model is based on the same principles as the simple linear model. It is worth remembering that this model is appropriate only when we have a numeric (interval/ratio) dependent variable. As a rule-of-thumb, a variable can be treated as numeric when you have at least 7 ordered categories (Fogarty, 2019). As mentioned before, we can use any type of independent variables, such as ordinal, categorical and numeric. More than one and a combination of these can be analysed simultaneously. 9.2.1 Paralell slopes We will follow-up with the example we used before introducing the simple linear model to further illustrate the advantages of using more than one variable. To do so, please set your RStudio environment as follows: Go to your ‘Quants lab group’ in RStudio Cloud; Open your own ‘NILT2’ project from your ‘Quants lab group’; Once in the project, create a new R Script file (a simple R Script, NOT an .Rmd file). Save the script document as ‘multivariate_linear_model’. Reproduce the code below, by copying, pasting and running it from your new script. First, install the moderndivepackage. install.packages(&quot;moderndive&quot;) Load the tidyverse and moderndive libraries (tidyverse was pre-installed for you before, you do not need to re-install it in this project) and read the nilt_r_object.rds file which is stored in the ‘data’ folder and contains the NILT survey. ## Load the packages library(tidyverse) library(moderndive) # Read the data from the .rds file nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) Once again, create a minimal random sample for the following example using the code below: # select a small random sample set.seed(3) # Filter where partner&#39;s age is not NA and take a random sample of 40 nilt_sample &lt;- filter(nilt, !is.na(spage)) %&gt;% sample_n(40) # Select only respondent&#39;s age and spouse/partner&#39;s age nilt_sample &lt;- select(nilt_sample, rage, spage, rsex) As a follow-up, we will draw a scatter plot using ggplot specifying the age of the respondent rage on the X axis and the respondent’s partner/spouse age spage on the Y axis. This time, we will add sex rsex as a third variable defining the color argument. # plot ggplot(nilt_sample, aes(x = rage, y = spage, color = rsex)) + geom_point(position = &quot;jitter&quot;) + labs(title = &quot;Respondent&#39;s age vs respondent’s spouse/partner age&quot;, x = &quot;Respondent&#39;s age&quot;, y = &quot;Respondent’s spouse/partner age&quot; ) What do you observe in the plot above? … The first thing to note is that females and males go in the same direction. When the respondent’s age increases the age of the partner increases as well. An interesting thing is that females tend to be located higher with respect to the Y axis compared to the male dots. Therefore, we can imagine that not only the age of the respondent is involved in the decision on how people choose their partner, but also the respondents’ sex. We can draw two lines, one for male and the other for female respondents, instead of only a general one as we did in the previous lab. To do this we will use the ggplot function in combination with the geom_parallel_slopes function from the moderndive package: # plot ggplot(nilt_sample, aes(x = rage, y = spage, color = rsex)) + geom_point(position = &quot;jitter&quot;) + geom_parallel_slopes(se = FALSE) + labs(title = &quot;Respondent&#39;s age vs respondent’s spouse/partner age&quot;, x = &quot;Respondent&#39;s age&quot;, y = &quot;Respondent’s spouse/partner age&quot; ) Well, our suspicion that the points representing female respondents were generally above the male ones is turning out to be true. What we have in the plot above are the optimal parallel lines that describe our points the best for each group. The interpretation of our visualizations so far is: both males and females partner’s age is positively associated with respondent’s age. This association is different for males and females. Overall, females choose older partners compared to males. But what is the magnitude of these relationships? We can easily extend the simple linear model by adding rsex as a second explanatory variable as follows: m1 &lt;- lm(spage ~ rage + rsex, nilt_sample) m1 ## ## Call: ## lm(formula = spage ~ rage + rsex, data = nilt_sample) ## ## Coefficients: ## (Intercept) rage rsexFemale ## 1.7383 0.9138 4.9190 From the output, we see that there is one intercept \\(\\beta_0\\), one slope coefficient \\(\\beta_1\\) for the numeric variable rage, and another coefficient \\(\\beta_2\\) for the categorical variable. If you observe closer, the output appended only one of the two categories for rsex. This is because categorical variables take one of the categories as the reference. The variable that is not shown is the reference, ‘Male’, in this case. Being more precise with our previous interpretation, we can say there is a positive relationship between both males and female participants’ age and their partner’s age by a factor of 0.91 for every additional year in age. Also, female respondent’s partners are expected to be 4.9 years older compared to male respondents. Before we move on, it is worth mentioning that the criterion to fit the coefficient is the same as in the linear model. This procedure guarantees to produce the smallest possible sum of squared residuals (SSR) using the ordinary least square method (OLS). We can check if the SSR was reduced by adding this extra variable by computing the SSR as we did before: sum(residuals(m1) ^ 2) ## [1] 948.263 Yes, it improved! Before, it was about 1,175, as we saw in the last lab workbook. 9.2.2 Multivariate model specification After the introduction of the simple linear model, the formal definition of the multivariate linear model should not look that scary, right? In essence, this equation tells us how \\(\\hat{y}\\) is described/explained by other variables. \\[ \\begin{aligned} \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}*x_1 + \\hat{\\beta_2}*x_2 + ... + \\hat{\\epsilon}, &amp;&amp; \\epsilon ~ N(0, \\sigma) \\end{aligned} \\] We can break down the equation above in smaller pieces as follows: \\(\\hat{y}\\) is the dependent variable, which is explained by \\(\\hat{\\beta_0}\\) the intercept, plus \\(\\hat{\\beta_1}\\) the slope coefficient for the first independent variable times the value of \\(x_1\\) variable 1, plus \\(\\hat{\\beta_2}\\) the slope coefficient for the second independent variable times the value of \\(x_2\\), plus \\(...\\) any number of independent variables, plus \\(\\hat{\\epsilon}\\) the error/residual term. \\(\\epsilon ~ N(0, \\sigma)\\) this bit tell us that the residuals are normally distributed. 9.3 R syntax for the multivariate linear model (You don’t need to reproduce the code of this section in your script). The general syntax in R is as follows: lm(formula, data) Where the formula is given by a dependent variable which is followed by ~ one or more independent variables joined by a plus sign +: dependent_variable ~ independent_variable1 + independent_variable2 ... In the previous section, we used a small random sample of 40 people only. Now, let’s fit a multivariate linear model using all the observations in the nilt dataset. In addition to the respondent age rage and rsex, we might be interested to know whether the type of place where they live (e.g. big city, village, farm/country, etc.) placeliv plays a role in how people choose their partner. First, we need to coerce the placeliv variable as factor. And we will have a quick look at a table of this variable to have an idea of how respondents are distributed and see how the factor levels are ordered. # Coerce to factor nilt &lt;- nilt %&gt;% mutate(placeliv = as_factor(placeliv)) # Create table table(nilt$placeliv) ## ## Dont know ...a big city ## 0 168 ## the suburbs or outskirts of a big city a small city or town ## 233 477 ## a country village a farm or home in the country ## 176 146 We see that most of the respondents live in ‘a small city or town’. Also, note that the category ‘Dont know’ contains 0 observations. Let’s use the function droplevels() to remove the unused category in our variables. This also means that the first level in this variable will be ‘a big city’. Then, we will fit the model and store it in an object called m2. # remove unused levels nilt &lt;- droplevels(nilt) # boxplot m2 &lt;- lm(spage ~ rage + rsex + placeliv, nilt) m2 ## ## Call: ## lm(formula = spage ~ rage + rsex + placeliv, data = nilt) ## ## Coefficients: ## (Intercept) ## 1.34251 ## rage ## 0.94147 ## rsexFemale ## 3.79426 ## placelivthe suburbs or outskirts of a big city ## 0.02099 ## placeliva small city or town ## -0.71196 ## placeliva country village ## 0.25645 ## placeliva farm or home in the country ## -0.01280 From the result: We confirm the positive relationship between the respondent’s age and their partner by a factor of 0.94. The difference in years by sex now is smaller. We see that the age of a female respondent’s partner is expected to be 3.8 years older compared to males. As before, we see that the result appended the name of the categories to the placeliv variables. This is because it is comparing each of the categories with the category of reference, in this case, ‘a big city’. The interpretation must be in comparative terms. For instance, the age of the respondent’s partner in ‘the suburbs or outskirts of a big city’ is expected to be 0.02 years older than the partner of someone living in a ‘a big city’. Overall, the coefficients do not differ much as a function of the type of place of where they live. The largest expected difference is for people living in ‘a small city or town’, where partner’s age is expected to be -0.7 years old compared to those living in ‘a big city’. In general, our model can be expressed as: \\[ \\hat{spage}_i = \\hat{\\beta_0} + \\hat{\\beta_1}*rage_i + \\hat{\\beta_2}*rsex_i + \\hat{\\beta_3} * placeliv_i \\] So, if we wanted to know the expected age of the partner of a 40 years old female who lives in a small city or town, we can use and apply the formula above: 1.34 + 0.94*40 + 3.80*1 + -0.7*1 ## [1] 42.04 The expected age of a 40 year old female’s partner is, therefore, 41.7 years old. 9.3.1 Model evaluation We can obtain more details from our model using the summary() function. summary(m2) ## ## Call: ## lm(formula = spage ~ rage + rsex + placeliv, data = nilt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -34.737 -2.176 -0.139 2.214 19.530 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 1.34251 0.86727 1.548 ## rage 0.94147 0.01202 78.311 ## rsexFemale 3.79426 0.37761 10.048 ## placelivthe suburbs or outskirts of a big city 0.02099 0.72637 0.029 ## placeliva small city or town -0.71196 0.65391 -1.089 ## placeliva country village 0.25645 0.75903 0.338 ## placeliva farm or home in the country -0.01280 0.75787 -0.017 ## Pr(&gt;|t|) ## (Intercept) 0.122 ## rage &lt;2e-16 *** ## rsexFemale &lt;2e-16 *** ## placelivthe suburbs or outskirts of a big city 0.977 ## placeliva small city or town 0.277 ## placeliva country village 0.736 ## placeliva farm or home in the country 0.987 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.524 on 583 degrees of freedom ## (614 observations deleted due to missingness) ## Multiple R-squared: 0.9143, Adjusted R-squared: 0.9134 ## F-statistic: 1037 on 6 and 583 DF, p-value: &lt; 2.2e-16 From the summary, we note that the 1st and 3rd quartile of residuals are symmetric (-2.18 to 2.21). This means that 50% of the estimated/predicted ages are approximately \\(\\pm\\) 2.2 years away from the observed partner’s age. Though, the minimum and the maximum residuals are not symmetric, this could be due to some extreme observed values. We can have a look to a histogram of the residuals to have a clearer idea. hist(m2$residuals) Overall they seem normally distributed with the exception of the negative value to the left. Secondly, from the summary we can see that respondent age rage and respondent sex rsex have a significant association with the dependent variable (indicated by the p-value &lt; 0.001). You can confirm this in the fourth column of the coefficients (‘Pr(&gt;|t|)’). Also, the std. error for the coefficients of this variables is small. However, the std. error for the categories of the place of where they live placeliv is large when compared to the estimated coefficients in the first column. In fact, when we look at the fourth column we see that the p-values are much larger than 0.05, which means that there is not a significant relationship between any of the type of places where they live and the partner’s respondent age. What this means is that the place where people live does not play a role in the age of the respondent’s partner. Third, this time we see that the multiple R-squared is slightly different from the adjusted R-squared. This is because the adjusted considers the number of independent variables included in the model. This is why the adjusted r-squared is preferred for multivariate models. This shows our model explains 91.34% of the variance of the respondent’s partner age. Lastly, it is important to note that even though we use the full nilt data set, 614 of the observations were not considered in the analysis. This is because we do not have information for these respondents. Probably some do not have a partner or preferred not to say their sex, for example. 9.4 Assumptions You need to know that these estimates are backed by well-established patterns studied in probability and statistics. Therefore, the linear model, as many other statistical techniques, provides reliable estimates if the data follows certain characteristics. These are known as assumptions. We will not go into detail with these. However, it is important that you have them in mind when you evaluate your results and also evaluate others. There are many assumption for the linear regression model, but we will introduce four of the most commonly assessed (Boston University School of Public Health, 2016): Linearity: The relationship between X and the mean of Y is linear. Homoscedasticity: The variance of residual is the same for any value of X. Independence: Observations are independent of each other. Normality: For any fixed value of X, Y is normally distributed. A first step to identify potential violations of the points above is to assess the distribution of the residuals by looking to the quartiles, mean and histogram. There are many other specific tests and techniques that can help us to know whether we met the assumptions and more importantly that can help to correct them. For the moment, we will leave it here. The important thing is to be aware of the existence of these potential problems and to be transparent with the results you produce. For now, I would suggest you to acknowledge the limitations of your assumptions checks. 9.5 Lab activities Set an R Markdown document in your ‘NILT2’ project as following: Create a new Rmd file, type ‘Multivariate linear model’ in ‘Tile’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML. Save the Rmd document as ‘Multivariate_lab’. Erase all the contents in the Rmd default example with the exception of the first bit (that contains the YAML) and the first R chunk (which contains the default chunk options), that is all from line 12 and on. Using the nilt object do the following by inserting a new chunk for each bullet points below (remember to write the comments and observations for the results as simple text outside the chunks): Load the packages, and the data that you will need in your file using the code below: ## Load the packages library(tidyverse) library(moderndive) # Read the data from the .rds file nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) Print a table for the highest level of qualification highqual using the table() function. Generate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot. Fit a linear model model using the lm() function to analyse the personal income persinc2 using the number of works worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary. Comment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model. Plot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model? Discuss your answers with your neighbour or tutor. "],["interpretivereport.html", "Lab 10 UG Quants summative assessment: Interpreting Quantitative Findings Report 10.1 Introduction 10.2 Create your research report template from GitHub 10.3 About the research report template 10.4 Activity 1 10.5 Activity 2 10.6 Conclusion", " Lab 10 UG Quants summative assessment: Interpreting Quantitative Findings Report 10.1 Introduction This is our final practical session. Thank you for tuning in and all your hard work! Today, we will discuss some practical aspects of the summative assessment Interpreting Quantitative Findings Report. You will actually start working on the assignment today by creating a template you can use to write your report in RStudio. Also, you will get familiar with the structure and the contents of the template. Remember, this is the perfect time to clarify as many questions as possible. Your tutor will be more than happy to help! Before proceeding, please take about 10 minutes to read the instructions for the assignment available in Moodle. 10.2 Create your research report template from GitHub We’ve created a GitHub repository which contains an RStudio project and all the essentials you need to start writing your report. There are two options for you. One is writing in RStudio Cloud, as we have been doing all along this course. Alternatively, you can also use the RStudio Desktop version (this requires having R and RStudio installed locally on your device). The result will be the same and the process is very similar. So, the choice is totally up to you and what your preference is. Here, we explain how to start your own project for both options. For this session, try only one of these. Don’t worry about which one you choose now, you can change your mind later. If you can’t make up your mind, then try the RStudio Cloud route first, as it means you can work on your assignment on any devices, regardless of the computational power of your device, but it does mean you need to work on the assignment online using a web browser. If online connection is an issue for you, then choose the RStudio Desktop route. 10.2.1 RStudio Cloud version If you wish to write your project using RStudio Cloud, please follow the next steps: Access RStudio Cloud as usual. Make sure you are in your Quants lab group (not in ‘Your Workspace’). Click on the ‘New Project’ button and select ‘New Project from Git Repository’. Figure 10.1: New project from GitHub. Paste the following URL in the box and click ‘OK’: https://github.com/UGQuants/UGQuant-assignment2. You are all set! Just click on the project to access the contents. 10.2.2 RStudio Desktop version If alternately you decide to write your assessment using the RStudio Desktop version, consider the following steps: Access the following GitHub repository copying and pasting the following URL in your browser: https://github.com/UGQuants/UGQuant-assignment2. Click on the ‘Code’ button and select the ‘Download ZIP’ option, as shown below. Figure 10.2: Downlowad ZIP from GitHub. Go to your local downloads folder, right click on the UGQuant-assignment2-main.zip and choose the ‘Extract all’ option (here, you can also chose the folder where you want to store the RStudio project and write your assignment). Click on the ‘Extract’ button. As you can see, this folder contains an RStudio project. Open the UGQuant-assignment2.Rproj which will initialize a new RStudio session. 10.3 About the research report template Once in your UGQuant-assignment2 project, open the Assignmet2-template.Rmd file in Pane 4 under the ‘Files’ tab, as shown below. Figure 10.3: Template. This template contains the following: Suggested structure of the report. Suggested word count for each section. The code necessary to run and present the results of a multivariate linear regression. In essence, this is the basic structure to start writing your assignment. Of course, you can add, edit, and customize as much as you consider appropriate. Remember, this is just a generic suggestion and you should still address all the points to the best of your abilities. Remember: There are no hard and fast rules to say what’s right or wrong. You are the one who can determine and justify why you did what you did. There are also many ways to write and approach this assignment, so make choices based on your own interest(s) and disciplinary background. It is truly your time to shine! The course handbook also gives important pointers on how your report will be assessed, offering some guiding questions to tackle the report–don’t skip this crucial step. 10.4 Activity 1 In the template, fill in the ‘author’ and ‘date’ space in the YAML at the top of the file with your student number and appropriate information using quotation marks. Knit the Rmd file as html (RStudio will ask to install some packages, click ‘Yes’). In the output, look at the results of the table under the ‘Results’ section and identify the dependent and the independent variables. You can learn about the meaning of these variables in the NILT documentation (click here to access the documentation). Identify the variables that are significant in this model and the direction of the relationship. Discuss your interpretations with your neighbour or tutor. You can refer back to Lab 8 and Lab 9 to refresh your memory. 10.5 Activity 2 Write one introductory paragraph in the ‘Introduction’ section of the template according to the guidance provided and your preliminary insights. Knit the document again. 10.6 Conclusion You are on the right track now! We hope that by getting familiar with this setting, you will easily succeed in writing your research report using all the knowledge and skills acquired during this course. Take this session to ask questions about the assignment or the course in general as much as possible. This is the right time to have specialized one-to-one support from your tutors. We wish you the best of luck! Get in touch with your tutors via email or, even better, post any questions you have about the assignment on your lab group discussion forum on Moodle, if you don’t know where to start or get stuck. Don’t suffer in silence. Remember your Tutors, lab group mates, and the teaching and admin team are here for you. Also don’t forget coding is all about trial and error, so it’s completely normal to write / copy and paste some code, then get an error message, and basically for your code to not work. Keep chipping at it, which sometimes can take hours (if not days), until you can get it to work. That’s a normal process even for professional data scientists and quantitative researchers! So do persevere and don’t panic if you don’t get it to work right away, because troubleshooting your code is part of the work, in addition to making your own choices in the analysis and reporting. For the code, the error message you get often gives you clues about, well, where the error is. So read the error message carefully, it might be you haven’t loaded the package required to run the code (remember to do this every time you open RStudio) or you might have missed a parentheses, or you might have not capitalised a word in the R syntax when you need to. Double check the R cheatsheets or previous sections in the lab workbook to ensure you have specified R syntax/arguments correctly. Again, trial and error is your friend, unlike writing an essay/doing an exam. Good luck and have fun! You got this. "],["answers.html", "Workbook suggested answers Introduction Lab 3. Data wrangling Lab 4. Exploratory data analysis Lab 6. Visual exploratory analysis Lab 7. Correlation Lab 8. Linear model. Simple linear regression Lab 9. Multivariate linear model", " Workbook suggested answers Introduction This chapter presents the suggested R code to answer the workbook activities and exercises throughout the course labs in Quantitative Research Methods for Social Sciences. This covers from Lab 3 to Lab 9. Before looking at the answers, try asking your tutor for help. Also, we strongly recommend web resources, such as https://stackoverflow.com/ or https://community.rstudio.com/. By solving the issues, you will learn a lot! ;) Lab 3. Data wrangling ## Load the packages library(tidyverse) # Read the data from the .rds file clean_data &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) # Glimpse clean_data glimpse(clean_data) # Glimpse the nilt data glimpse(nilt) Lab 4. Exploratory data analysis Preamble code ## Load the packages library(tidyverse) # Read the data from the .rds file nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) #Subset nilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2) 10.6.1 Activity #1 From your R Studio Cloud script, do the following activities using the data stored in the nilt_subset object: Create a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function; # Load the vtable package to create summary tables library(vtable) # Create table sumtable(nilt_subset, vars = c(&#39;uninatid&#39;)) Table 10.1: Summary Statistics Variable N Percent uninatid 1183 … Unionist 348 29% … Nationalist 255 22% … Neither 580 49% Using the variables religcat and uninatid, generate a Two-Way contingency table; sumtable(nilt_subset, vars = c(&#39;religcat&#39;), group = &#39;uninatid&#39;) Table 10.2: Summary Statistics uninatid Unionist Nationalist Neither Variable N Percent N Percent N Percent religcat 341 255 558 … Catholic 2 1% 245 96% 238 43% … Protestant 305 89% 5 2% 180 32% … No religion 34 10% 5 2% 140 25% 10.6.2 Activity #2 Using the data in the nilt_subset object, complete the following activities. Using the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions; hist(nilt_subset$persinc2) Create a summary of the personal income persinc2 variable, using the sumtable() function. sumtable(nilt_subset, vars = c(&#39;persinc2&#39;)) Table 10.3: Summary Statistics Variable N Mean Std. Dev. Min Pctl. 25 Pctl. 75 Max persinc2 897 16395 13466 260 6760 22100 75000 Compute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy. sumtable(nilt_subset, vars = c(&#39;persinc2&#39;), group = &#39;ruhappy&#39;) Table 10.4: Summary Statistics ruhappy Very happy Fairly happy Not very happy Not at all happy Can’t choose Variable N Mean SD N Mean SD N Mean SD N Mean SD N Mean SD persinc2 305 17569 13429 500 16262 14015 62 13445 9754 9 12451 11501 15 11457 6113 Lab 6. Visual exploratory analysis Preamble code ## Load the packages library(tidyverse) # Load the data from the .rds file we created in the last lab nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) #Create subset nilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2) 10.6.3 Excercices Using the nilt_subset object, complete the tasks below in the Rmd file ‘Lab_4’, which you created earlier. Insert a new chunk for each of these activities and include brief comments as text in the Rmd document to introduce the plots and discuss the results (tip —leave an empty line between your text and the next chunk to separate the description and the plots): Create a first-level header to start a section called “Categorical analysis”; ## Categorical analysis Create a simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid; ggplot(nilt_subset, aes(x=uninatid)) + geom_bar() + labs(title = &quot;Political afiliation&quot;, x= &quot;Party&quot;) Based on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables; ggplot(nilt_subset, aes(x=uninatid, fill = religcat)) + geom_bar() + labs(title = &quot;Political afiliation by religion&quot;, x= &quot;Party&quot;, fill = &quot;Religion&quot;) Create a new first-level header to start a section called “Numeric analysis”; ## Numeric analysis Create a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis; ggplot(nilt_subset, aes(x= rhourswk, y=persinc2)) + geom_point() + labs(title= &#39;Income and number of hours worked a week&#39;, x = &#39;Number of hours worked a week&#39;, y= &#39;Personal income (£ a year)&#39; ) Finally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime); ggplot(nilt_subset, aes(x= ruhappy, y=persinc2)) + geom_boxplot() + labs(title= &#39;Personal income and happiness&#39;, x=&#39;Hapiness level&#39;, y=&#39;Personal income (£ a year)&#39;) Briefly comment each of the plots as text in your Rmd file; Knit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish. Lab 7. Correlation ## Load the packages library(tidyverse) # Load the data from the .rds file we created in lab 3 nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) # Age of respondent’s spouse/partner nilt$spage &lt;- as.numeric(nilt$spage) # Migration nilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric) # overall perception towards migrants nilt &lt;- rowwise(nilt) %&gt;% # sum values mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T )) %&gt;% ungroup() %&gt;% # assign NA to values that sum 0 mutate(mig_per = na_if(mig_per, 0)) 10.6.4 Activity 1 Using the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text in the Rmd file (remember to leave an empty line between your text and the chunk). Create a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use the ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function, set the method argument to 'lm'. ggplot(nilt, aes(x=rage, mig_per)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + labs(title = &#39;Perception of migration vs age&#39;, x= &#39;Respondent age&#39;, y= &#39;Perception of migration (0-30)&#39;) What type of relationship do you observe? Comment the overall result of the plot and whether this is in line with your previous expectation. Lab 8. Linear model. Simple linear regression ## Load the packages library(tidyverse) # Read the data from the .rds file nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) m3 &lt;- lm(persinc2 ~ rhourswk, data = nilt) 10.6.5 Lab activities Use the nilt data set object in your linear_model_intro file to: Plot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point(), jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside. ggplot(nilt, aes(x= rhourswk, y= persinc2)) + geom_point(position = &#39;jitter&#39;) + geom_smooth(method = &#39;lm&#39;) Print the summary of m3 using the summary() function. summary(m3) ## ## Call: ## lm(formula = persinc2 ~ rhourswk, data = nilt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -43694 -8148 -3070 4990 58249 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5170.4 1966.2 2.63 0.00884 ** ## rhourswk 463.2 52.4 8.84 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13860 on 455 degrees of freedom ## (747 observations deleted due to missingness) ## Multiple R-squared: 0.1466, Adjusted R-squared: 0.1447 ## F-statistic: 78.15 on 1 and 455 DF, p-value: &lt; 2.2e-16 Is the the relationship of hours worked a week significant? Re: Yes. The p-value (fourth column of the ‘Coefficients’ table) is lower than 0.05. What is the adjusted r-squared? How would you interpret it? Re: the adjusted R-squared is 0.14. This can be interpreted in terms of percentage, e.g. 14% of the variance in personal income can be explained by the number of hours worked a week. What is the sample size to fit the model? Re: The total number of observations in the data set is 1,204 and the model summary says that 747 observations were deleted due to missingness. Therefore, the sample size is 457 (1204-747). What is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model? 5170.4 + 463.2 * 30 ## [1] 19066.4 Plot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)? hist(residuals(m3)) Overall, the residuals look normally distributed with the exception of the values to the right-hand side of the plot (between 40000 and 60000). Lab 9. Multivariate linear model 10.6.6 Lab activities Load the packages, and the data that you will need in your file using the code below: ## Load the packages library(moderndive) library(tidyverse) # Read the data from the .rds file nilt &lt;- readRDS(&quot;data/nilt_r_object.rds&quot;) Print a table for the highest level of qualification highqual using the table() function. table(nilt$highqual) ## ## Degree level or higher Higher education GCE A level or equiv ## 230 102 243 ## GCSE A-C or equiv GCSE D-G or equiv No qualifications ## 185 82 281 ## Other, level unknown Unclassified ## 27 54 Generate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot. ggplot(nilt, aes(x = rhourswk, y= persinc2, color = highqual)) + geom_point(position = &#39;jitter&#39;) + moderndive::geom_parallel_slopes(se = FALSE) + labs(title = &quot;Personal income&quot;, subtitle = &#39;Personal income and number of hours worked a week by education level&#39;, x= &#39;Number of hours worked a week&#39;, y= &#39;Personal income (£ a year)&#39;, color = &#39;Highest education level&#39;) Fit a linear model model using the lm() function to analyse the personal income persinc2 using the number of works worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary. m4 &lt;- lm(persinc2 ~ rhourswk + rage + highqual, nilt) summary(m4) ## ## Call: ## lm(formula = persinc2 ~ rhourswk + rage + highqual, data = nilt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36228 -6425 -1411 4635 54749 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3904.64 2714.25 1.439 0.151 ## rhourswk 444.79 45.32 9.814 &lt; 2e-16 *** ## rage 236.59 48.47 4.881 1.47e-06 *** ## highqualHigher education -8164.91 1939.50 -4.210 3.09e-05 *** ## highqualGCE A level or equiv -12439.26 1563.42 -7.956 1.47e-14 *** ## highqualGCSE A-C or equiv -13037.47 1703.07 -7.655 1.20e-13 *** ## highqualGCSE D-G or equiv -11622.07 2665.38 -4.360 1.61e-05 *** ## highqualNo qualifications -12968.10 2339.78 -5.542 5.11e-08 *** ## highqualOther, level unknown 15445.70 3334.75 4.632 4.76e-06 *** ## highqualUnclassified -12399.58 2786.16 -4.450 1.08e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11830 on 447 degrees of freedom ## (747 observations deleted due to missingness) ## Multiple R-squared: 0.3887, Adjusted R-squared: 0.3764 ## F-statistic: 31.58 on 9 and 447 DF, p-value: &lt; 2.2e-16 Comment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model. Re: All the independent variables including the number of hours worked a week, age, and all the categories of highest qualification level compared to ‘Degree or higher’ are significant to predict personal income in the model ‘m4’, considering that the p-value is lower than 0.05. We can confirm this from the fourth column of the ‘Coefficients’ table. The adjusted R-squared of the model is 0.37. This means that 37.6% of the variance in personal income can be explained by these variables. The size of the sample used to fit this model is 457, considering that the ‘nilt’ data set contains 1204 observations but 747 were deleted due to missingness (1204 - 747). Plot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model? hist(residuals(m4)) The distribution of the residuals in ‘m4’ look overall normally distributed. However, the distribution is not perfectly symmetric. Therefore, we would advice to conduct further checks to test the linear model assumptions. "],["InterpretiveReportTemplate.html", "Appendix 1: RMarkdown &amp; the Interpretive Report Template Introduction RMarkdown Interpretive Report Template", " Appendix 1: RMarkdown &amp; the Interpretive Report Template Introduction This appendix provides additional information on using RMarkdown and an overview of the structure of the RMarkdown document provided with the interpretive report project template. Please ensure to review the relevant lab materials: Lab 5 session on RMarkdown for an intro to RMarkdown. Lab 10 Session for Interpreting Quantitative Findings Report summative assessment for instructions on how to create a project on RStudio Cloud for your interpretive report using the provided template. Similarly, if you encounter any errors, please ensure to check the R Issues FAQ first for any existing solutions. The first half of this page provides an overview of RMarkdown and the second half covers the Interpretive Report Template in more detail. Importantly, please ensure to at least read the Preamble Code Chunk section for information on best practice for installing and loading any additional packages. Outline: RMarkdown Formatting Line spacing Code Chunks Code Chunk Labels Code Chunk Options Running Code Chunks Knitting RMarkdown Documents RMarkdown Cheat Sheet Interpretive Report Template YAML Block Setup Code Chunk Preamble Code Chunk Word Count Code Template Outline Regression Results Code Chunk RMarkdown RMarkdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with Code Chunks that can be converted to multiple file formats, such as HTML or PDF. This makes RMarkdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis. RMarkdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text (see the Formatting section below). Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. RMarkdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text and passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format. Formatting RMarkdown uses simple, human-readable syntax for basic formatting. Whilst different to how you would write text in a Word document, it is easy to learn. This simple transparent formatting also helps avoid the hidden complexities of Word document, where formatting issues often arise from inconsistent syntax that is hidden from the user. Below is a screenshot from RStudio with text using the key basic syntax for formatting your main text: And, this is how it appears when knitted: Headings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level: And, how it appears when knitted: Line spacing The main thing in RMarkdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between: Each paragraph Before and after a list Before and after a header Here’s some example text in an RMarkdown document, the first without line spacing, the second with: And, how this looks when knitted: Side-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in RMarkdown. Code Chunks There are three main ways to create a code chunk: Click the ‘Insert Code Chunk’ button and from the drop-down select ‘R’ at the top. (See gif below) Press Ctrl + Alt + I (Windows &amp; Linux) or Cmd + Option + I (macOS) Manually typing three back-ticks with ‘r’ in curly brackets before your code and adding three back-ticks after. Curly brackets at the start of a code chunk are used to specify {programming-language optional name, options = values}. Since we are using R, all our code chunks have {r ...}. Within code chunks, the pound sign / hash, #, at the start of a line is used to add any comments: Side-note, this may seem confusing given hash signs are used for headers in the main text. However, inside of a code chunk, all text is treated as code for the language specified in the curly brackets. In R, hash signs are used for comments, so they are treated as comments within the code chunk rather than as Markdown headers. Code Chunk Labels Adding a label is useful as a quick way to remind yourself the purpose of each code chunk. The quick outline (button at bottom of Source panel) can be used to jump to specific sections and code chunks in your document. If you provide a label for your code chunk it will also appear here: Code Chunk Options Options are used to specify how code chunks are handled when run/knitted. Two key ones we have used in the labs and in the project template: echo - whether the code chunk is displayed in knitted files. include - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files. By default the code chunk and the output are displayed in knitted files. For the interpretive report, you do not want to display the code chunks in your knitted HTML file, so ensure to add echo=FALSE to the options for new code chunk you create. Alternatively, see Setup Code Chunk section for information on how to set echo=FALSE as a ‘global option’. Examples when working in RMarkdown document: And when knitted: Running Code Chunks There are multiple ways to run your R code. Within the top-right of each code chunk there are two buttons: Run Current Chunk - which will run the code within that code chunks only. Run All Chunks Above - which will run all code chunks from the top of your document down to this code chunk. The second is useful when a code chunk depends upon others being run first. Remember the R environment maintains the results of any previously run code, so you do not need to continuously run all previous chunks. However, this can be useful when debugging issues or if you restart your R environment, losing the results of previously run code, and needing to re-run everything before this chunk. Further options can be found in the ‘Run’ drop-down menu, accessed from the top-right of the Source Panel: Useful additional ones here are: Run Selected Line(s) - running all lines you have manually selected using the mouse / text cursor. Restart R and Run All Chunks - incredibly useful when you need to reset your R environment, this can be useful when debugging an error to figure out whether the issue stems from your current code or code you previously ran but now removed. Note as well that many of these options have keyboard shortcuts listed. Learning these pays off long-term as you will be able to write and run your code without needing to move your hands from the keyboard. Knitting RMarkdown Documents Knitting a document runs all your R code from top to bottom, then combines the results from this with the Markdown text to convert these into different file formats. YAML is used to specify which file formats to convert to, see the YAML Block section for information on how this is setup within the project template. Importantly, when you knit an RMarkdown document, all code is run sequentially from top to bottom in a clean R environment. This ensures the document is reproducible. Anyone with a copy can knit it and produce the same results. This requires though that code to load any required packages are included within a code chunk in the document. Simply loading a package via the Console adds it to your current R environment and manually run code chunks will be able to access the package. However, when knitting, the clean R environment won’t have access to the package, and you’ll receive an error message when the knit process tries to run a function that requires the package. This is why it is important to include a code chunk at the top of your document that loads all required packages. See the Preamble Code Chunk for how to do this in the RMarkdown file provided in the interpretive report project template. Assuming you have YAML specifying which file format(s) to convert to, all you then need to do each time is simply click the ‘Knit’ button: RMarkdown Cheat Sheet It is possible to access an RMarkdown cheat sheet (as well as ones for ggplot2 and dplyr!) from within RStudio: Note, from the same ‘Help’ menu, just below ‘Cheat Sheets’ is an option for ‘Keyboard Shortcuts Help’. This displays a screen with most of the shortcuts available within RStudio. This also includes a link to ‘See All Shortcuts…’. Interpretive Report Template YAML Block The top of the RMarkdown document includes a YAML block. YAML is often used with Markdown as it provides a simple human-readable structure for configuring metadata, such as the document title, author, date, and desired output when knitting: Please remember to add your student number - and not your name - to line 3. To save having to remember to update the date before knitting and submitting your final version, you can use the following in the YAML block: date: &quot;`r format(Sys.time(), &#39;%d/%m/%y&#39;)`&quot; When knitted that code will automatically add the current date in dd/mm/yy format (e.g. 04/12/24): Setup Code Chunk After the YAML Block, there is a ‘setup’ code chunk that sets global options for knitr. The include=FALSE in its options means that this chunk and its output are not displayed in your knitted files: Global options apply to all code chunks in the document. This means anything set in the global options does not have to be individually added to each code chunk. Note, you can set a global option and still set a different one for a specific code chunk where needed. For example, if you set option=FALSE globally, but needed it to be TRUE for a specific code chunk, all you need to do is add option=TRUE to its options to override the global default. The global options set in the setup code chunk are: message=FALSE, which hides all non-warning messages when knitting code chunks. For example, when loading a library it will sometimes display non-warning messages. warning=FALSE, which hides all warning messages when knitting code chunks. Again, making sure that no warnings messages displayed when running code is included in your knitted file. Importantly, you do not want to display your code chunks in your knitted file, only the tables and graphs they generate. To achieve this you could add echo=FALSE to the options for each individual code chunk. Or – you can simply add it as a global option in the setup code chunk! Preamble Code Chunk It is best practice to install and load any required packages and read in any data being used at the top of the RMarkdown file. This keeps all package management and data reading in a single location at the start of the document, making your code simpler and clearer. It further ensures that when running/knitting your code that all required packages are loaded and dataframes created before the rest of your code is run. The preamble code block within the template provides a structure following this practice. Install packages if missing, outlined in pink. This code may look complex, but all it does is create a list of packages that are going to be used, assigning it to the object list.of.packages. This list is checked against packages already installed, creating a new list new.packages containing only the names of packages not already installed. The final line then basically says “if the length of new.packages is 1 or more, then install all packages in the new.packages list”. Wordcount addin if missing, outlined in blue. There is not a R package for calculating wordcounts, but there is what is known as an ‘addin’ for RStudio. The code here checks if the addin is already installed and if not it downloads a copy of the addin from GitHub. Please see Word Count Code section for further information on how this addin is used in the document. Load packages, outlined in yellow. This is where all packages used in the analysis are loaded. Read data, outlined in red. The dataset used for the assignment in read in and assigned to the nilt dataframe object. Important, as a dataset is provided with the project template and read in already for you, you do not need to download or read in any other dataset. The assignment uses a version of the NILT dataset that includes more variables than the version of the data we used in the lab sessions. Downloading and reading in the dataset used in the labs will result in variables disappearing from the regression results table. Please see the R Issues FAQ for more info. If you require additional packages, the best way to add this would be: Within the Install packages if missing section, add the package name at the end of the list that is being assigned to ‘list.of.packages’ Within the Load packages section, add a new library() line under the existing ones. For example, if needing to install and load ‘vtable’ these are the changes that you would make: As a gif: Note, moving all code for installing and loading of packages into this preamble code chunk means you do not need, and can safely remove, any other install.package() and library() lines from the rest of your code chunks. This will reduce risk of encountering error messages and make it easier to bug them if any do occur. Word Count Code Tables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. Note, you will need to knit your document each time you want to check the updated word count. This is how the code to calculate the word count looks within the RMarkdown file: Side-note, surrounding code with single back-ticks, (`), creates an “inline code chunk”, enabling you to add short snippets of code. The r at the start specifies that the code is R, similar to adding r in curly brackets for code chunks. This is then how it looks when knitting the project template (assuming you haven’t added any additional text yet): We use this addin as the built-in RStudio word count, accessed via the ‘Edit’ menu at the top of the screen, includes all code and comments. So, despite the addin calculating ‘0’, RStudio will provide: Important, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to wordcountaddin::word_count(\"Assignmet2-template.Rmd\") - 193. Template Outline The RMarkdown document provided in the project template includes headed sections you can use alongside suggested word count for each and brief summary reminder of what to include in each section. The Course Handbook, pages 26-30, provide a more detailed breakdown of what to include in each section. Screenshot of the outline and commented suggestions: Important, within the “Results and discussion” section, ensure to add your interpretation after the code chunk that creates the regression results table: Regression Results Code Chunk The code chunk that runs the multiple linear regression model and creates the table with the regression results can be found in the ‘Results and discussion’ section: First the model is run, outlined in pink, and assigned to the model object. Next a list with labels to use for the independent and control variables in the regression table is created, outlined in blue, and assigned to the cov_labels object. Finally, the stargazer package is used to create the regression results table, outlined in red. It is passed the model and cov_label objects, highlighted in yellow, alongside arguments for outputting the table in HTML, a title, and caption &amp; label to use for the dependent variable. Stargazer produces well-formatted regression tables, but with the downside that when running the code chunk in RStudio, you will only see the raw HTML syntax that is created: In order to view the table, you will need to knit your document and view the outputted HTML file: Your knitted HTML file should then open in a new popup window. However, you might instead receive a dialogue window saying that a pop-up was prevented from opening, if so just click ‘Try again’: Alternatively, you can open the last knitted version of your HTML file from the ‘Files’ panel. Click the HTML file and select ‘View in Web Browser’: Below is how your regression results table should look in the knitted HTML file: If it doesn’t, please see the R Issues FAQ. "],["RFAQ.html", "Appendix 2: R Issues FAQ Introduction Reporting R Issues Regression Results Table Issues Object ‘nilt_subset’ Not Found Wrong Word Count Unable to Knit Turnitin Unable to Open HTML File", " Appendix 2: R Issues FAQ Introduction This appendix contains information for solving common issues students have reported when working on their interpretive findings report. Updates will continue being added to this appendix with information for any new issues being reported. So, if you experience an issue, this page will be a good first port of call to check for any existing solution. Reporting R Issues When making a Moodle post or emailing about an issue it is vital to provide sufficient context to enable others to help find a solution. This includes: The full text of any error message that you are receiving. Any relevant code that is producing the error. Error messages will often flag the relevant code chunk where the error occurred: So, based on the above error message you would include the code within the code block at lines 49-50. Additionally, when emailing, please also include: Your lab group number. If already made a Moodle post about the issue, a link to the post. Regression Results Table Issues If variables go missing from your regression results table or the regression results are different to how they originally appeared when you knitted the template, you may have unintentionally replaced the data included with the template. The preamble code block at the top of the RMarkdown file provide with the template reads in the ‘fullnilt_2012.rds’ dataset and assigns it to the ‘nilt’ data frame object. You therefore do not need to download and read in anther dataset, this is all setup for you already within the preamble code block. To resolve the issue, remove all code where you download and assign another dataset to the nilt data frame. For example, these would be lines to remove: The ‘NILT2012GR’ that we used in the labs has a different set of the NILT variables, and does not contain all the variables in the ‘fullnilt_2012’ dataset provided in the template. The above code then would result in variables disappearing from your regression results table. Similarly, this would also be code to remove: The above code replaces the original nilt data frame with a version that only includes the 5 variables listed within the selection function. Again, resulting in variables disappearing from the regression results table. The select() function keeps variables passed to it and drops the others. The first argument is always the dataframe being selected from, select(dataframe, ...), with variables to keep from it listed after, select(dataframe, variable1, variable2, ...). Assigning, &lt;-, select() to the same dataframe, dataframe &lt;- select(dataframe, ..), is effectively saying, ‘from this dataframe keep these variables and permanently remove the others’. Unless your whole analysis is only going to use that selection, always assign it to a new dataframe instead, dataframe_subset &lt;- select(dataframe, ...). If you need to create a subset of the data, ensure to assign it to a new data frame: Note, always consider whether you actually need to create a subset. Most functions have arguments for defining which variables to use, so you do not necessarily need to provide a subset to them. Object ‘nilt_subset’ Not Found ‘Object not found’ errors often arise when no data has been assigned to a data frame object. For example, if you ran code using ‘nilt_subset’, such as sumtable(nilt_subset, ...) without first assigning data to it nilt_subset &lt;- ..., you would receive the following error message when running/knitting your code: When working with a subset of the data, ensure before the lines identified in the error message that you included code that assigns data to it (dataframe &lt;- ...). For example, if you need to work with a subset of data that used filter() or select(): If you already have code assigning data to the object and do so before the code that is producing the error message, try running the lines of code where you assigning the data again, or ‘Run All’. If you still receive an error message, double-check for typos and capitalisation, nilt_subset, NILT_subset, and ni1t_subset will be treated as different objects. If the code was previously working, it may be the case that you have accidentally deleted code blocks / lines that assigned data to the object, added new code above the lines where you assigned data, or moved code where the lines assigning data to the object and calling it within a function are now in the wrong order. Note, always consider whether you actually need to create a subset. Most functions have arguments for defining which variables to use, so you do not necessarily need to provide a subset to them. Wrong Word Count If the word count displayed at the top of your knitted HTML is wrong, check whether the line of code that calculates the word count refers to the correct RMarkdown file you are using. Within the template RMarkdown document, we include an inline code block that uses a word count addin to calculate your word count within the knitted HTML file: Which when knitted will look as follows in the HTML file: However, the code to calculate the word count refers to a specific file “Assignmet2-template.Rmd”. if you created a new RMarkdown file, such as one that included your student number ‘Assignment2-255615q.Rmd’, you also need to update the file name in the code: Note, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to “wordcountaddin::word_count(”Assignmet2-template.Rmd”) - 193”. Unable to Knit Package could not be loaded If receiving a “package ‘…’ could not be loaded” error message, check the YAML code block at the top of your RMarkdown file. Where this error message has occurred previously, it is due to using the ‘vtable’ package and the YAML code block includes pdf as an output. You should have either of the following for output: ...: But, will probably have the something like the following if receiving the error message: This can happen unintentionally as RStudio can automatically add a line for PDF outline, such as if accidentally clicked ‘Knit to PDF’: Annoyingly, even if you click ‘Knit to HTML’ from the options, if your YAML block includes pdf_document in its outputs, R will still try to create a PDF document and keep running into the error message. So, if you are receiving the error message and have the two output lines, you can easily fix it by removing the pdf_document line: Note, as the YAML block is already specifying to create an HTML output, you can just click the ‘Knit’ button directly each time rather than clicking the wee down arrow to select ‘Knit to HTML’ specifically. Execution Halted The most common cause of this error is from renaming the RMarkdown file, but not updating the line of code that calculates the word count. See Wrong Word Count section for general info. For example, if you renamed the RMarkdwon file from ‘Assignmet2-template.Rmd’ to ‘Assignment2-255615q.Rmd’, you will receive an ‘Execution halted’ error when trying to knit: To fix it, just update the line of code that calculates the word count so it refers to the new name you have given the file: Turnitin Unable to Open HTML File To avoid such potential issues, please ensure to submit an exported version of your knitted HTML file rather than one saved through your browser. If Turnitin is providing an error message that it is unable to open your submitted HTML file, it may be due to having saved the file through your browser (e.g. right-clicking and ‘Save as…’) rather than exporting it from RStudio. Whilst HTML files saved from the browser and exported from RStudio will look absolutely identical when opened, some browsers - and browser extensions - will add additional code within the HTML file. This additional code does not change how the file looks when opened, but creates issues for Turnitin. To export your knitted HTML file - 1. Check the box next to your knitted HTML file in the ‘Files’ panel. By default, the Files panel is on the bottom right of the screen. Click ‘More’ from the toolbar at the top of the Files panel. Click ‘Export’ from the menu options that pop-up. This will pop open an ‘Export Files’ dialogue. Here, you can rename the file to include your student number. Just make sure the “.html” at the end remains included. After naming the file, click ‘Download’. This will open a dialogue to select which location to save the file. Here’s a short gif running through all the steps together. Please note: Don’t panic if Turnitin is unable to open your file. We are aware of the issue and you will not be penalised if you submit a file on time, but then have to resubmit after the deadline due to Turnitin being unable to open your original submission. "],["references.html", "References", " References "]]
