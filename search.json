[
  {
    "objectID": "02-Lab2.html",
    "href": "02-Lab2.html",
    "title": "Data in R",
    "section": "",
    "text": "In the previous lab, we set up an RStudio session in Posit Cloud and we got familiar with the RStudio environment and the purpose and contents of RStudio’s panes. In this Lab we will learn about R packages, how to install them and load them. We will also use different types of data. You will have the chance to practice with additional R operators. Lastly, we will load a real-world data set and put in practice your new skills.\n\n\n\n\n\n\nOverview\n\n\n\nBy the end of this lab you will know how to:\n\ninstall and load packages in R\ndownload a dataset from a URL and assign it to a named object\nuse the pipe (|&gt;) operator to chain steps\nexamine, wrangle, and subset a dataset using functions\n\n\n\n\nAs mentioned in our last lab, R (R Core Team 2021) is a collaborative project. This means that R users are developing, maintaining, and extending the functionalities constantly. When you set up R and RStudio for the first time, as we did last week, it comes only with the ‘basic’ functionalities by default, sometimes referred to as ‘base R’. However, there are literally thousands of extensions that are developed by other users. In R, these non-default extensions are called packages.\nMost of the time, we use packages because they simplify our work in R - such as by replacing what would take dozens of lines of complex code in base R with a simple one-line function - or they allow us to extend beyond the capabilities of base R.\nLet’s install and load some useful packages. We will start with one of R’s most famous packages, and one we will use across the labs, the tidyverse (Wickham 2021) package.1 First, sign-in to Posit Cloud, create/open your Labs-1-2 project, and then open the Lab-2 R Markdown file.\n\nFor those who missed Lab 1:\n\nMake sure you have a free, institutional-subscription Posit Cloud account (in case you have not created one yet, please follow the guidance provided in Lab 1);\nIf you enrolled before teaching started, you will have received an email with a link to join your lab group. If you joined the course after the start of teaching, you can find a link in your Lab Group’s forum on Moodle. You tutor will also be able to provide you with a link.\nFollow the rest of the Lab 1 guidance in the ‘Create a New Project’ section.\n\nFor those who already joined and created a project in Lab 1, log into Posit Cloud and:\n\nFrom the left-hand menu, click “Lab Group ..” where “..” is your lab group number.\nWithin the main screen, click “Labs-1-2” to open the Posit Cloud project you created last week.\n\n\n\nSelect your lab group space and then open ‘Labs-1-2’ project\n\n\nYou will find your project in the same state as you left it last week. To open a tab for the Lab 2 R Markdown file, click to open ‘Lab-02.Rmd’ in the “Files” tab in the bottom-right pane.\n\n\nClick ‘Lab-02.Rmd’ in the “Files” tab in the bottom-right pane\n\nWithin Lab 1, you may have noticed the key: value lines between two ---. This is called YAML. YAML is a simple human-readable markup language for data serialization. It consists of what are referred to as key-value pairs. Within R Markdown, a YAML header can be included at the very top and must be fenced with the three hyphens, ---, at start and end.\nThe R Markdown file for this lab has a couple more lines in the YAML header this time:\n\n\nYAML block at top of Lab-02.Rmd\n\ntitle and author are self-explanatory, but make sure to replace [your name here] with your name, making sure to keep the quotation marks around it. Without the quotation marks, you will receive an error message when trying to knit. (Note: Don’t knit your file just yet anyway as it will result in an error message until you complete the sections below.)\ndate can be any text. However, format(Sys.time(), '%d/%m/%y') is a nice single line of R code that runs when the file is knitted to retrieve the current date and formats it as dd/mm/yyyy, such as 12/09/2025. This then effectively makes the date field “date this file was knitted” and saves you having to manually update it each time.\noutput specifies what file format(s) to output to when knitting the R Markdown file. Across the labs and assessments we are using ‘html_document’ as it is easiest to work with across the packages we are using in the labs and assessments.\nAfter the YAML header, there is a ‘setup’ code chunk setting knitr options:\n\nThis may look intimidating, but it is relatively simple when parsed bit by bit:\n\n\nknitr is the package used to knit R Markdown files\n\nknitr::opts_chunk is an object that stores default settings applied to all code chunks\n\nknitr::opts_chunk$set(...) runs the set() function to change the knitr::opts_chunk default settings\n\nSo, what the code in the setup chunk does is set messages and warnings to “FALSE”. This prevents any messages or warnings that get raised when running code chunks from also appearing in any knitted files. (Why we do this will become clear in a bit.)\n\n\n\n\n\n\nMore R Operators\n\n\n\n\n\n:: is the namespace operator for accessing an object (such as a function or dataset) from a package, in the format package::object. If you load a package using the library() function, as we will do later, you can call objects from it directly as object without needing to include package:: before it.\n\n$ is the component operator for accessing named elements within an object, in the format object$component. In data analysis it is often used to access a column (variable) from a data frame. For example, if a data frame named df has a column named age, you can access it with df$age.\n\n[] (which is introduced later in this lab) is the subsetting operator and is used to extract elements from an object, in the format object[...]. In contrast to $ where we used the name for the component we wanted, [] uses numbers. If we had a data frame (i.e. a table) called df and ran the code df[2, 4], R would return the value found in the fourth column of the second row.\n\n\n\n\n\nOnce you have the Lab 2 R Markdown file open, type the following in the Console (bottom-right pane) and hit ‘Enter’ to run it:\n\n\ninstall.packages(\"tidyverse\")\n\n\nWait until you get the message ‘The downloaded source packages are in …’. The install process can take a couple of minutes to finish.\n\n/\n\nOnce the package is installed, you next need to load it using the library() function. In the preamble code chunk already setup in the R Markdown file, add the following in the line under the # Load packages comment:\n\n\nlibrary(tidyverse)\n\n\nNow run the code chunk. Either by pressing Ctrl+Shift+Enter with your text cursor within the code chunk, or by clicking the green triangle in the top-right of the code chunk.\n\n\nAnd that’s it, tidyverse is ready to use in your current session!\nNote, we run install.packages(\"package_name\") in the Console as we only need to install the package once for each Posit Cloud project. We load packages with library(package_name) in a code chunk at the top of the R Markdown file as packages need to be loaded for every R session. If we ran library(...) in the Console instead of adding it to a code chunk, our packages would not be available in the fresh R session that is created when knitting and would result in an error message.\nAnother thing to note is that when you install a package, you need to use quotation marks, install.packages(\"package_name\"), whereas when loading a package you just use the package name without quotation marks, library(package_name).\nWhat you may also notice is the message raised in the Console when loading the tidyverse. (If you already ran the setup code chunk that set messages and warnings to FALSE you won’t see any text). This is one of the reasons why we have the setup chunk to exclude messages and warnings in knitted files. If we knitted the Lab-02.Rmd file without the setup chunk, the text on loading the tidyverse would also appear in the knitted HTML file after the preamble chunk. We will cover other options you can set to customise what appears in your knitted files in later weeks.\n\nThis text often causes users new to R to think an error has occurred. However, it is merely a ‘message’ about conflicts that can be ignored.\nThe tidyverse is a meta package that bundles a collection of packages together, such as dplyr and ggplot2. These packages share a common design philosophy and are often used together in data analysis. Loading the tidyverse meta package, library(tidyverse), saves us from having to load each of these packages individually with library(dplyr), library(ggplot2), and so on.\nWhat the message is showing then is that each of the core tidyverse packages were loaded OK, and there are conflicts for functions provided by the dplyr package. Two of its functions, filter() and lag(), share the same name as functions in the base R stats package. After loading the tidyverse, any code calling filter() will use the dplyr function with that name rather than the stats ones. (This applies only to filter() directly, the stats function can still be accessed when the tidyverse is loaded by using the :: namespace operator, stats::filter().)\n\n\n\n\n\n\nNew Terms\n\n\n\n\n\nYAML: a human-readable data serialisation language, often used for configuration/settings; in R Markdown the YAML front matter between --- sets document metadata and other options.\n\ntidyverse: a collection of R packages that share a consistent design philosophy and work together for importing, wrangling, and visualising data.\n\ndata frame: an object in R that stores data in a two-dimensional table with rows (observations) and columns (variables). We load a file containing a data set into a data frame within R.\n\ntibble: a modern version of a data frame used in tidyverse packages that has more consistent data handling than base R’s data frames. For simplicity, we will still refer to these in the labs as data frames.\n\n\n\n\n\n\n\n\n\ninstall.packages() function\n\n\n\nInstall an R package that is available from the Comprehensive R Archive Network.\nUsage: install.packages(\"package_name\")\nArguments: \"package_name\" with the name of the package you want to install that must be in quotes, such as install.packages(\"tidyverse\").\nNotes: Only needs to be run once per computer, or once per project in Posit Cloud.\n\n\n\n\n\n\n\n\nlibrary() function\n\n\n\nLoad an installed package so its functions are available in the R session.\nUsage: library(package_name)\nArguments: package_name with the name of the installed package you want to load.\nNotes:\n\nA package needs to be installed first using install.packages(\"package_name\") in the Console.\nIt is best practice to load packages in a code chunk at the top of R Markdown files.\n\n\n\n\nIn quantitative analysis, we often distinguish two main types of variables:\n\n\nNumeric variables have values that describe measurable or countable quantities.\n\n\nContinuous variables have values that can fall anywhere within a range, such as time and speed.\n\nDiscrete variables have values that are whole numbers that count something, such as number of children in a household.\n\n\n\nCategorical variables have values that describe distinct, mutually exclusive categories.\n\n\nNominal variables have categories without any order, such as country, name, political party, and gender.\n\nOrdinal variables have categories with a meaningful order, such as education level or level of satisfaction.\n\n\n\nIn R, the most basic data building blocks are atomic vectors. There are six atomic types: logical, integer, double, character, complex, and raw.\nIn relation to our two main types of variables, we use the following vectors:\n\n\nNumeric vectors (integer or double) to store continuous and discrete values.\n\nFactor vectors (a class built on integer codes + character labels) to store nominal and ordinal categorical values.\n\nA numeric vector is usually stored as a double internally by default. The difference between integer and double is based on how they are computationally stored. Base R and most R packages handle the difference behind the scenes, so the difference in how they are stored does not matter to us.\nA factor is a class combining integer codes and a levels attribute containing the human-readable character labels for those codes. Each stored integer points to a character label in levels, so 1 might correspond to \"Conservative\", 2 might correspond to \"Labour\", and so on. For ordered factors, used for ordinal variables, R also records an explicit order of the levels to enable comparisons and sorting by that order.\nTwo other vectors we commonly use are:\n\n\nCharacter vectors (character) to store text, such as unique respondent IDs (“A102”, “C006”) and free-text answers.\n\nLogical vectors (logical) to store boolean TRUE / FALSE values, useful for filtering and conditions, such as age &gt; 30.\n\nIn R, there are couple of functions that will help us to identify the type of data. First, we have glimpse(). This prints some of the main characteristics of a data set, namely its overall dimension, name of each variable (column), the first values for each variable, and the type of the variable. Second we have the function class(), that will help us to determine the overall class(type) of on R object.\n\nWe are now going to use some datasets that are available to us in the R session. R comes with some example datasets out of the box, such as iris with 150 observations of iris flowers. Some R packages also include additional example datasets, such as starwars - included in the dplyr tidyverse package - with info about 87 Star Wars characters. Unlike other datasets that we have to manually load, and will cover in later section, we can access these in any R session out the box or after loading the relevant package that provides them.\nPlease go to the ‘Types of variables’ section in your Lab-02.Rmd file. You will see a few code chunks already setup for you:\n\nWe will start with a classic dataset example in R called iris. (For more info about the dataset, you can type ?iris or help(iris) in the Console). Please go to the “iris-glimpse” code chunk in your R Markdown file and run it.\n\n\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nWhat do you observe from the output?\nFirst, it tells you the number of rows and the columns on the top. After, it lists the name of each variable. Additionally, it tells you the type of the variable between these symbols &lt; &gt;. The first five variables in this dataset are of type &lt;dbl&gt; (double) which as covered above is a type of numeric variable. The last, Species, is a factor &lt;fct&gt;. So, for each of the 150 iris flowers observed, there is information on its species and four types of continuous measures. Though, as glimpse only provides a preview, we only see the values for the first few observations.\n\nNow you know that each iris flower belongs to a species, but what are the specific categories in this data set? To find out, add the following in the ‘iris-levels’ code chunk and then run it.\n\n\nlevels(iris$Species)\n\n(If you receive ‘NULL’ when running the chunk, it because you have species and not Species.)\nAs you can see, there are three categories, or levels as they are called in R factor variables, which are three types of iris flower species. Notice here we used the $ component operator mentioned earlier. Here it basically means, from the iris data frame select the Species column/variable. levels() is then a nice little utility function that shows all the levels for a column given to it as an argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s get serious and explore Star Wars.\nThe starwars data set from the dplyr package contains information about the characters, including height, hair colour, and sex. (Again, to get more information run ?starwars or help(starwars) in the Console). For the purpose of keeping things relatively simple to focus on variable types, we will use a reduced version of the full data set.\n\nFirst, run the ‘starwars-glimpse’ code chunk to create a reduced version of the data set named starwars2 and then glimpse the Star Wars characters in that reduced data set:\n\n\nstarwars2 &lt;- starwars[, 1:11]\nglimpse(starwars2)\n\nCreating a reduced version of the data frame uses another selection operator, [...]. The basic format is data_frame[rows, columns]. Here starwars[, 1:11] leaves the rows argument blank, which effectively means select all rows, and the columns argument has 1:11, which effectively means select columns 1 to 11. So, the code returns a copy of the data frame with all rows but only the first 11 columns.\nWhat do you observe this time?\nIt seems that the data type is not consistent with their content. For example, the variables species, gender, and hair_color are of type &lt;chr&gt; (that is character), when according to what we covered above these should be factors. To transform them, we will use the function ´factor()´. This process is known as coercing a variable, that is when you change a variable from one type to another. factor() is another nice little helper function\n\n\n\n\n\n\n\n\n\nLet’s coerce the species variable from character to factor and assign the result to the same column in the dataset. Add the following to the ‘starwars-factor’ code chunk and run it.\n\n\nstarwars2$species &lt;- factor(starwars2$species)\n\nNote, most functions in R for data analysis do not directly change the object -such as a data frame - passed to it. Instead, the functions return a new object with the changes. An explicit assignment back to the original object is required if you want to change it. For example, if you run factor(starwars2$species on its own in the Console, it will return all the values for species and the levels created. However, for the species column in starwars2 to be updated with the results returned by factor(starwars2$species) we need the starwars2$species &lt;- ... assignment as well. What the code is effectively saying is “replace the species column in starwars2 with the version returned by the factor function”.\n\nLet’s check if the type of variable really changed by glimpsing the data and checking the levels of species. Add the following to the ‘starwars-glimpse2’ chunk and run it.\n\n\nglimpse(starwars2)\nlevels(starwars2$species)\n\nThe glimpse result now is telling us that species is a &lt;fct&gt;, as expected. Furthermore, the levels() function reveals that there are 37 types of species, including Human, Ewok, Droid, and more.\nHopefully, these examples will help you to identify the main vector types and, more importantly, an initial understanding of how to coerce them into an appropriate type. Be aware that many data sets represent categories with numeric values without the labels, for example, using ‘0’ for male and ‘1’ for female - and as part of the initial process turning these variables into factors you will also need to provide a list mapping these numbers to the labels to use. Usually, large data sets are accompanied by extra information in a code book or documentation file, which specifies the values for the numeric code and their respective meaning. It’s important to read the code book/documentation of every data set rather than assuming what the numbers stand for as the conventions and meanings can vary. For example, some surveys now put gender in alphabetical order, with 0 for female and 1 for male.\n\n\n\n\n\n\nglimpse() function\n\n\n\nProvides a ‘glimpse’ of an object. From the dplyr package, and loaded with the tidyverse package.\nUsage: glimpse(x)\nArguments:\n\n\nx - The object (usually a data frame) to glimpse.\n\nReturns: For data frames, it returns counts for rows and columns and a compact, transposed preview showing column names, types, and a few example values. Types are shown as &lt;int&gt; (integer), &lt;dbl&gt; (double), &lt;chr&gt; (character), &lt;lgl&gt; (logical), &lt;fct&gt; (factor).\n\n\n\n\n\n\n\n\nfactor() function\n\n\n\nCreate a categorical variable (factor) with fixed, named levels.\nUsage: factor(x, levels, ordered = is.ordered(x))\nArguments:\n\n\nx - A vector, such as a data frame column, to turn into a factor.\n\nlevels - The allowed values and their order. If omitted, all unique values in x are used to create the levels.\n\nordered - Set to TRUE where order matters when creating an ordered factor.\n\nNotes: Whenever you see text like ordered = is.ordered(x) in a function documentation, this shows the name of the argument and its default value. In this example, the default value is for ordered is calculated using another function. The is.ordered(x) checks whether x is already an ordered factor and returns TRUE or FALSE. If you are using factor() to adjust the levels/labels of an existing ordered factor then, you can leave the ordered = TRUE out.\n\n\n\n\n\n\n\n\nlevels() function\n\n\n\nGet the category labels (“levels”) of a factor.\nUsage: levels(x)\nArguments: x - A factor variable, such as data_frame$country.\nReturns: A character vector of level labels for the factor variable.\n\n\n\n\n\nA useful operator is the pipe |&gt;. This is the base R version of the pipe. This operator is what is known in programming as syntactic sugar. It is syntax which helps make code easier to read and write without changing any functionality. The pipe operator passes the result of one function as the first argument to the next (left to right).\nRun the ‘pipe-1’ code chunk and check its results.\n\n1 |&gt; sum(1)\n1 |&gt; sum(1) |&gt; sum(5)\n\nThe sum() function adds the value of two numbers in format sum(x, y). Using a pipe for 1 |&gt; sum(1) is the equivalent to sum(1, 1). If we were to write sum(x, y) with a pipe, it would be x |&gt; sum(y). So, in our example code chunk, the pipe let’s us ‘unpack’ the first argument in each function, whereby line 1 reads equivalent to saying ‘take 1, then add it with 1’ and line 2 ‘take 1, then add it with 1, then add it with 5’.\nThis may seem as if it is overcomplicating things, but as the code becomes more complex the value of the pipe operator becomes clearer. The second line, 1 |&gt; sum(1) |&gt; sum(5) would be written as sum(sum(1,1), 5) without pipes. Going left to right, 1 |&gt; sum(1) would be sum(1,1) and then sum(1,1) |&gt; sum(5) would be sum(sum(1,1), 5).\nPipes then help us avoid foo(foo(foo(...), ...) ...) monstrosities in our code. In coding examples, you will often see words like foo, bar and baz used as placeholder names when explaining how coding syntax works rather than using real functions. Similarly, x, y, and z often get used as placeholder names for variables / arguments.\nAs we will cover in examples in the next section below, plenty of functions take more than two arguments. Let’s imagine we had a data frame named df and we were updating it by using - in order - foo, bar, and baz functions that each take arguments x, y, and z. Without pipes our code would be:\n\ndf &lt;- baz(bar(foo(df, y, z), y, z), y, z)\n\nThis is a tangled mess, we want the result from foo passed to bar and its result in turn passed to baz. This ordering though gets visually reversed when nesting each function within the other - we have to read it inside out for it to make sense.\nWith pipes, it instead becomes:\n\ndf &lt; - df |&gt;\n  foo(y, z) |&gt;\n  bar(y, z) |&gt;\n  baz(y, z)\n\nNow it is easy to read and the ordering of the functions matches how we could verbally describe what the code does, “we are updating a data frame called df by running the functions foo, bar and baz on it in turn”. In the code chunk above, we make use of another wonderful thing about pipes - it let’s us break complex code over multiple lines. To do so you add the pipe at the end of the line before each function, then start each new line with an indent followed by the function. If you type |&gt; at the end of a line and hit enter, RStudio will add the spaces at the start of the new line for you.\nNote, in the readings and other materials, you may come across another %&gt;% pipe. This pipe comes from the tidyverse. The tidyverse pipe came before the base R one, where due to its popularity an equivalent was added to base R. Whilst you will still see it in use, and it remains supported in the tidyverse, the tidyverse itself recommends people start using the base R |&gt; pipe. To see that they work the same, run the ‘pipe-2’ chunk.\n\nIn this section we will work with data originally collected by The Guardian in 2015, for more information click here. The data set we will use today is an extended version which was openly shared in GitHub by the American news website FiveThirtyEight. This data set contains information about the people that were killed by police or other law enforcement bodies in the US, such as age, gender, race/ethnicity, etc. Additionally, it includes information about the city or region where the event happened. For more information click here.\n\nAs with installing packages, we only want to download the data once. For this lab, we will download the data running code in the Console. Next week though, we will go through how to set up an R script for downloading data. This is a common strategy for sharing the code used to download data whilst still keeping it separate to your main R Markdown file.\nFirst, we will create a new folder in our project directory to store the data. To do it from the Console, run this line. (Don’t worry if you get a warning. This appears because you already have a folder with this name):\n\ndir.create(\"data\")\n\nNote that in the ‘Files’ tab of Pane 4, the bottom-right pane, there is a new folder called data.\n\nNow, download the data from the GitHub repository using the function download.file(). This function takes two arguments separated by a comma: (1) the URL and (2) the destination (including the directory, file name, and file extension), as shown below.\nCopy and paste the following lines into the Console:\n\ndownload.file(\"https://github.com/fivethirtyeight/data/raw/master/police-killings/police_killings.csv\", \"data/police_killings.csv\")\n\n(Note, to avoid any issues typing out the URL, you can click the clipboard icon in the top-right of the code above to copy the full contents to your clipboard.)\nAfter that, we are ready to read the data. As the data comes as a .csv file, we can use the read_csv() function included in the tidyverse package (make sure the package is loaded in your session as explained in a previous section).\nThe read_csv() reads the data in the file and returns it as a data frame in R. We need then to assign this data frame to a named object so we can continue working with it. In our example below we will do this by assigning it to a named object police. So, in effect, our new police named object will be a data frame containing the data from the .csv file we downloaded.\nImportantly, as with loading packages, when loading data it is good practice to do so at the top of the file. So, go all the way back up to the preamable code chunk and add the following line after the ‘# Read data’ comment.\n\npolice &lt;- read_csv(\"data/police_killings.csv\")\n\nYou should now have:\n\nRun the code chunk and you’ll know it has run OK if can see ‘police’ in your Global Environment in the top-right pane. It will have 467 observations and 34 variables (columns).\n\n\nOK, before we look at this police data, a reminder about creating code chunks. Scroll back in your R Markdown file to “## Black lives matter!”. For this section, you will need to create your own code chunks.\nTo create a new code chunk, you have four options:\n\nPut your text cursor on a line in your R Markdown file and press Ctrl+Alt+I on the keyboard.\nFrom the menu bar at the top of RStudio, you can select Code &gt; Insert Chunk.\nManually type three backticks, on UK layout keyboards the key to the left of the 1 key, and then {r} (remember to add three backticks on a line below to close the code chunk as well).\nClick on the icon that is a c in green square with a ‘+’ in a circle in top-left corner, and then click on R to create an R code chunk.\n\n\nBy default, a new code chunk will be plain {r}, but you can edit this to give it an optional name:\n\n\nBack to the data. Let’s first glimpse the police data. Create a code chunk then add and run the following:\n\nglimpse(police)\n\nAs you can see, there are several variables included in the dataset, such as age, gender, law enforcement agency (lawenforcementagency), or whether the victim was armed (armed). You will see some of these variables are not in the appropriate type. For instance, some are categorical and should be a factor (&lt;fct&gt;) instead of character (&lt;chr&gt;).\n\nBefore coercing these variables, we will create a smaller subset of the data frame by selecting only the variables that we are interested in. To do so, we can use the select() function. The select() function takes the name of the data frame as the first argument and then the names of the variables we want to keep separated by commas (no quotation marks needed).\nRemember with pipes we can replace select(data_frame, variable1, variable2, ...) with data_frame |&gt; select(variable1, variable2, ... making it easier to read as “from our data frame select the following variables”. Let’s create a code chunk to select a few variables and assign the result to a new object called police_2.\n\npolice_2 &lt;- police |&gt; select(age, gender, raceethnicity, lawenforcementagency, armed)\n\nThe select() function is from the dplyr tidyverse package. Many tidyverse functions that take a data frame as the first argument, foo(date_frame, ...) or when using a pipe data_frame |&gt; foo(...), will assume all variables listed are from that data frame. Within the function, this then let’s us type age and gender rather than police$age and police$gender.\nIf you look again to the ‘Environment’ tab, there is now a second data frame with the same number of observations but only 5 variables. Create a new code chunk, then add and run the following code to glimpse its contents.\n\nglimpse(police_2)\n\nHaving a closer look at the reduced version, we can see that in fact all the variables are of type &lt;chr&gt; (character), including age.\nLet’s coerce the variables into their correct type. Start by creating a code chunk and naming it ‘coerce-police_2’ or similar.\nThen within the chunk, let’s start with age, coercing it from character to numeric by adding the following lines:\n\n# Coerce numeric\npolice_2 &lt;- police_2 |&gt; mutate(age = as.numeric(age))\n\nThe # Coerce numeric line let’s us add a comment to our code. Any text within an R code chunk after a # is treated as a comment and is not run as part of the code. This is useful for recording the ‘why’ behind the code, so future-you (and others) can quickly read and understand it.\n(You may be wondering though why outside the code chunks there are # but these become headers when knitting the document. The reason is within Markdown syntax # at the start of a line is used for Headings, with the number of # denoting the Heading level - # for Heading 1, ## for Heading 2, and so on. As R and Markdown developed separately before the creation of R Markdown brought the two together, they both had by that point decided to use # for different things. Whilst confusing at first, it all boils down to “# inside code chunk = comments” and “# outside code chunks = headers”.)\nAge is not known for some cases. Thus, it is recorded as ‘Unknown’ in the dataset. Since this is not recognized as a numeric value in the coercion process, R automatically sets it as a missing value, NA. This is why it will give you a warning message. (Though you will not see a warning message if you ran the knitr setup chunk that disabled warning messages.)\nWe can continue coercing raceethnicity and gender from character to a factor by adding the following lines to our chunk:\n\n# Coerce factors\npolice_2 &lt;- police_2 |&gt; mutate(raceethnicity = factor(raceethnicity))\npolice_2 &lt;- police_2 |&gt; mutate(gender = factor(gender))\n\nRun the chunk and then create and run a new chunk with the following to glimpse the data frame again:\n\nglimpse(police_2)\n\nYou should hopefully see the three coerced variables now have the correct types:\n\n\nage as numeric &lt;dbl&gt;\n\n\ngender and raceethnicity as factor &lt;fct&gt;\n\n\nNow, let’s run a summary of the data using the summary() function. This shows the number of observations in each category or a summary of a numeric variable. Create a code chunk, then add and run the following:\n\nsummary(police_2)\n\nThere are some interesting figures coming out from the summary. For instance, in age you can see that the youngest (Min.) is… 16 years old(?!), and the oldest (Max.) 87 years old. Also, the vast majority are male (445 vs 22). In relation to race/ethnicity, roughly half are ‘White’, whereas ‘Black’ individuals represent an important share. One may question the proportion of people killed in terms of race/ethnicity compared to the composition of the total population (considering Black is a minority group in the US).\nLet’s suppose that we only want observations in which race/ethnicity is not unknown. To ‘remove’ undesired observation we can use the filter() function. Remember, to make changes to an object we need to assign the result of a function back to it. We will then need to assign the result of filter back to the police_2 object.\nCreate a chunk, then add and run the following:\n\npolice_2 &lt;- police_2 |&gt; filter(raceethnicity != \"Unknown\")\n\nSo, what just happened in the code above? First, the pipe operator, |&gt;: What we are doing verbally is take the object police_2, THEN filter raceethnicity based on a condition.\nThen, what is happening inside the filter() function? Let’s have a look at what R does in the background for us (Artwork by @alison_horst):\n\n\n\n\nFilter. Source: Artwork by Horst (n.d.).\n\n\n\nNote, df for us is police_2. Since we are using the pipe operator, police_2 |&gt; filter(raceethnicity != \"Unkown\") is equivalent to writing filter(police_2, raceethnicity != \"Unkown\") without the pipe operator.\nImportantly, the filter() function returns a data frame with only the rows from the data frame that was passed to it that meet the conditions specified. In our example then, filter() returns a data frame that only has the rows from the police_2 data frame where the row’s value in the raceethnicity column is NOT EQUAL (!=) to ‘Unknown’. When we then assign the result to an object named the same as our existing object, police_2 &lt;- ..., we replace the old police_2 data frame with the new filtered version. In effect, our police_2 data frame is updated to a version with all rows where the value for raceethnicity was “Unknown” removed.\nFinally then, create another code chunk to get a glimpse and summary of the data frame again:\n\nglimpse(police_2)\n\nRows: 452\nColumns: 5\n$ age                  &lt;dbl&gt; 16, 27, 26, 25, 29, 29, 22, 35, 44, 31, 76, 40, N…\n$ gender               &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, M…\n$ raceethnicity        &lt;fct&gt; Black, White, White, Hispanic/Latino, White, Whit…\n$ lawenforcementagency &lt;chr&gt; \"Millbrook Police Department\", \"Rapides Parish Sh…\n$ armed                &lt;chr&gt; \"No\", \"No\", \"No\", \"Firearm\", \"No\", \"No\", \"Firearm…\n\nsummary(police_2)\n\n      age           gender                   raceethnicity lawenforcementagency\n Min.   :16.00   Female: 20   Asian/Pacific Islander: 10   Length:452          \n 1st Qu.:28.00   Male  :432   Black                 :135   Class :character    \n Median :35.00                Hispanic/Latino       : 67   Mode  :character    \n Mean   :37.15                Native American       :  4                       \n 3rd Qu.:45.00                Unknown               :  0                       \n Max.   :87.00                White                 :236                       \n NA's   :2                                                                     \n    armed          \n Length:452        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nTo make it easier to compare, the results shown when running the code chunks are included here as well. Within your R Markdown file, scroll up to your previous glimpse and summary chunks for the police_2 data frame. Now compare that to the results we now receive after filtering/ You should see in the filtered version that under raceethnicity, ‘Unkown’ is now 0, when it was 15 before we filtered the dataset. Correspondingly, the total number of rows has dropped by 15, going from 467 initially, to 452 after filtering.\n\n\n\n\n\n\n\n\n\nAs we introduced a lot of new functions in this section, here is a final sub-section with the mini help documentation for them all together -\n\n\n\n\n\n\nsummary() function\n\n\n\nGet a quick summary for an object. Supports many different object types.\nUsage: summary(object)\nArguments: object - The named object to summarise. This object can be a data frame, a specific column from a data frame, etc.\nReturns: Depends on object.\nFor data frame columns it summarises:\n\n\nnumeric columns: Min, 1st Qu., Median, Mean, 3rd Qu., Max, NA count\n\nfactor columns: frequency counts of each category, and NA count\n\nIf provided a data frame it will provide this summary for each column.\n\n\n\n\n\n\n\n\nas.numeric() function\n\n\n\nCoerce a vector to numeric (double) if possible.\nUsage: as.numeric(x)\nArguments: x - The object, such as a data frame column, to coerce.\nReturns: A numeric vector. Any text such as “10” will become a numeric value, but text such as “house”, “missing”, “unknown”, will all become NA values with a warning.\n\n\n\n\n\n\n\n\nmutate() function\n\n\n\nModify existing column(s) or add new column(s). From the dplyr package, which is loaded with the tidyverse package.\nUsage: mutate(data_frame, column = expression, ...) or with pipe data_frame |&gt; mutate(column1 = expression, column2 = expression, ...)\nArguments:\n\n\ndata_frame - The data frame to modify columns from.\n\ncolumn = expression - On the left, the column name want to create or overwrite. On the right, an expression using existing columns and functions. For example age = as.numeric(age) will overwrite the existing age column, but new_age = as.numeric(age) would create a new new_age column.\n\n\n\n\n\n\n\n\n\nselect() function\n\n\n\nSelect specific columns (i.e. variables) from a data frame. From the dplyr package, which is loaded with the tidyverse package.\nUsage: select(data_frame, column1, column2, ...) or with pipe data_frame |&gt; select(column1, column2, ...)\nArguments:\n\n\ndata_frame - The data frame to select columns from.\n\ncolumn1, column2, ... - The columns in the data frame to keep, separated by commas.\n\nReturns: A data frame containing only the selected columns, in the order they were listed in the function.\n\n\n\n\n\n\n\n\nfilter() function\n\n\n\nFilter rows in a data frame to keep only those meeting given conditions. From the dplyr package, which is loaded with the tidyverse package.\nUsage: filter(data_frame, condition1, condition2, ...) or with pipe data_frame |&gt; filter(condition1, condition2, ...)\nArguments:\n\n\ndata_frame - The data frame to filter.\n\ncondition1, condition2, ... - Conditions stated as logical tests, such as age &gt; 18, country == \"Scotland\".\n\nReturns: A data frame containing only the rows where the conditions are met.\n\n\n\nAs a final side note - and a wee preview of code you will see in later labs and in R textbooks - all the steps we did above for creating and modifying police_2 can also be written in a single code chunk. The code chunk below does all the same steps. It creates a police_2 data frame by first selecting variables from the police data frame, then coerces the types of some variables, and finally filters the data.\n\npolice_2 &lt;- police |&gt;\n  select(age, gender, raceethnicity, lawenforcementagency, armed) |&gt;\n  mutate(\n    age = as.numeric(age),\n    raceethnicity = factor(raceethnicity),\n    gender = factor(gender)\n  ) |&gt;\n  filter(raceethnicity != \"Unknown\")\n\nNotice here that we can also pass multiple variables to mutate() and able to create a line for each variable being coerced. We can do this as R also lets us split anything after a bracket or comma onto a new line. With long functions, it is common to start a new line after the (, then add each element that is separated by a comma on a line of its own, then add a final line with the closing ). Notice as well that the elements are indented another two spaces (making the indent for them 4 spaces in total) and the closing ) is indented the same as mutate(.\nIt may take a while to get the hang of reading code with ease, but this formatting helps a lot. The initial 2 space indent after the first line let’s us know that “all the following lines are still part of the same series of steps”. The additional 2 space indent for the lines between the opening ( and closing ) for the mutate() function then also visually lets us know, “all the following lines are arguments within the mutate() function”. This code would be incredibly difficult to read at a glance if it was written instead as a single line and without any pipes.\n\nDiscuss the following questions with your neighbour or tutor:\n\nWhat is the main purpose of the functions select() and filter()?\nWhat does coerce mean in the context of R? and Why do we need to coerce some variables?\nWhat is the mutate() function useful for?\n\nUsing the police_2 dataset:\n\nFilter how many observations are ‘White’ in raceethnicity? How may rows/observations are left?\nHow many ‘Hispanic/Latino’ are there in the dataset?\nUsing the example of Figure 2.3, could you filter how many people were killed that were (a) ‘Black’ and (b) possessed a firearm on them at the time (‘Firearm’)?\nWhat about ‘White’ and ‘Firearm’?\n\nFinally, ‘Knit’ your R Markdown file.\nExtra activities:\n\nWhy did you have to use quotes in the following: police |&gt; filter(raceethnicity==\"White\" & armed==\"Firearm\")?\nWhat do you have to use == rather than =?\n\nNote, police |&gt; filter(raceethnicity==\"White\" & armed==\"Firearm\") could also be written as police |&gt; filter(raceethnicity==\"White\", armed==\"Firearm\"). However, using & makes it clearer that we want to keep rows that meet both conditions. If we wanted to filter for rows where either condition is true, we would use | (which is often used as an operator for ‘or’ in programing languages) instead of &.\nThis is the end of Lab 2. Again, the changes in your R Markdown file should be saved automatically in RStudio. However, make sure this is the case as covered in Lab 1. After this, you can close the tab in your web browser. Hope you had fun!",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#welcome-back",
    "href": "02-Lab2.html#welcome-back",
    "title": "Data in R",
    "section": "",
    "text": "In the previous lab, we set up an RStudio session in Posit Cloud and we got familiar with the RStudio environment and the purpose and contents of RStudio’s panes. In this Lab we will learn about R packages, how to install them and load them. We will also use different types of data. You will have the chance to practice with additional R operators. Lastly, we will load a real-world data set and put in practice your new skills.\n\n\n\n\n\n\nOverview\n\n\n\nBy the end of this lab you will know how to:\n\ninstall and load packages in R\ndownload a dataset from a URL and assign it to a named object\nuse the pipe (|&gt;) operator to chain steps\nexamine, wrangle, and subset a dataset using functions",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#r-packages",
    "href": "02-Lab2.html#r-packages",
    "title": "Data in R",
    "section": "",
    "text": "As mentioned in our last lab, R (R Core Team 2021) is a collaborative project. This means that R users are developing, maintaining, and extending the functionalities constantly. When you set up R and RStudio for the first time, as we did last week, it comes only with the ‘basic’ functionalities by default, sometimes referred to as ‘base R’. However, there are literally thousands of extensions that are developed by other users. In R, these non-default extensions are called packages.\nMost of the time, we use packages because they simplify our work in R - such as by replacing what would take dozens of lines of complex code in base R with a simple one-line function - or they allow us to extend beyond the capabilities of base R.\nLet’s install and load some useful packages. We will start with one of R’s most famous packages, and one we will use across the labs, the tidyverse (Wickham 2021) package.1 First, sign-in to Posit Cloud, create/open your Labs-1-2 project, and then open the Lab-2 R Markdown file.\n\nFor those who missed Lab 1:\n\nMake sure you have a free, institutional-subscription Posit Cloud account (in case you have not created one yet, please follow the guidance provided in Lab 1);\nIf you enrolled before teaching started, you will have received an email with a link to join your lab group. If you joined the course after the start of teaching, you can find a link in your Lab Group’s forum on Moodle. You tutor will also be able to provide you with a link.\nFollow the rest of the Lab 1 guidance in the ‘Create a New Project’ section.\n\nFor those who already joined and created a project in Lab 1, log into Posit Cloud and:\n\nFrom the left-hand menu, click “Lab Group ..” where “..” is your lab group number.\nWithin the main screen, click “Labs-1-2” to open the Posit Cloud project you created last week.\n\n\n\nSelect your lab group space and then open ‘Labs-1-2’ project\n\n\nYou will find your project in the same state as you left it last week. To open a tab for the Lab 2 R Markdown file, click to open ‘Lab-02.Rmd’ in the “Files” tab in the bottom-right pane.\n\n\nClick ‘Lab-02.Rmd’ in the “Files” tab in the bottom-right pane\n\nWithin Lab 1, you may have noticed the key: value lines between two ---. This is called YAML. YAML is a simple human-readable markup language for data serialization. It consists of what are referred to as key-value pairs. Within R Markdown, a YAML header can be included at the very top and must be fenced with the three hyphens, ---, at start and end.\nThe R Markdown file for this lab has a couple more lines in the YAML header this time:\n\n\nYAML block at top of Lab-02.Rmd\n\ntitle and author are self-explanatory, but make sure to replace [your name here] with your name, making sure to keep the quotation marks around it. Without the quotation marks, you will receive an error message when trying to knit. (Note: Don’t knit your file just yet anyway as it will result in an error message until you complete the sections below.)\ndate can be any text. However, format(Sys.time(), '%d/%m/%y') is a nice single line of R code that runs when the file is knitted to retrieve the current date and formats it as dd/mm/yyyy, such as 12/09/2025. This then effectively makes the date field “date this file was knitted” and saves you having to manually update it each time.\noutput specifies what file format(s) to output to when knitting the R Markdown file. Across the labs and assessments we are using ‘html_document’ as it is easiest to work with across the packages we are using in the labs and assessments.\nAfter the YAML header, there is a ‘setup’ code chunk setting knitr options:\n\nThis may look intimidating, but it is relatively simple when parsed bit by bit:\n\n\nknitr is the package used to knit R Markdown files\n\nknitr::opts_chunk is an object that stores default settings applied to all code chunks\n\nknitr::opts_chunk$set(...) runs the set() function to change the knitr::opts_chunk default settings\n\nSo, what the code in the setup chunk does is set messages and warnings to “FALSE”. This prevents any messages or warnings that get raised when running code chunks from also appearing in any knitted files. (Why we do this will become clear in a bit.)\n\n\n\n\n\n\nMore R Operators\n\n\n\n\n\n:: is the namespace operator for accessing an object (such as a function or dataset) from a package, in the format package::object. If you load a package using the library() function, as we will do later, you can call objects from it directly as object without needing to include package:: before it.\n\n$ is the component operator for accessing named elements within an object, in the format object$component. In data analysis it is often used to access a column (variable) from a data frame. For example, if a data frame named df has a column named age, you can access it with df$age.\n\n[] (which is introduced later in this lab) is the subsetting operator and is used to extract elements from an object, in the format object[...]. In contrast to $ where we used the name for the component we wanted, [] uses numbers. If we had a data frame (i.e. a table) called df and ran the code df[2, 4], R would return the value found in the fourth column of the second row.\n\n\n\n\n\nOnce you have the Lab 2 R Markdown file open, type the following in the Console (bottom-right pane) and hit ‘Enter’ to run it:\n\n\ninstall.packages(\"tidyverse\")\n\n\nWait until you get the message ‘The downloaded source packages are in …’. The install process can take a couple of minutes to finish.\n\n/\n\nOnce the package is installed, you next need to load it using the library() function. In the preamble code chunk already setup in the R Markdown file, add the following in the line under the # Load packages comment:\n\n\nlibrary(tidyverse)\n\n\nNow run the code chunk. Either by pressing Ctrl+Shift+Enter with your text cursor within the code chunk, or by clicking the green triangle in the top-right of the code chunk.\n\n\nAnd that’s it, tidyverse is ready to use in your current session!\nNote, we run install.packages(\"package_name\") in the Console as we only need to install the package once for each Posit Cloud project. We load packages with library(package_name) in a code chunk at the top of the R Markdown file as packages need to be loaded for every R session. If we ran library(...) in the Console instead of adding it to a code chunk, our packages would not be available in the fresh R session that is created when knitting and would result in an error message.\nAnother thing to note is that when you install a package, you need to use quotation marks, install.packages(\"package_name\"), whereas when loading a package you just use the package name without quotation marks, library(package_name).\nWhat you may also notice is the message raised in the Console when loading the tidyverse. (If you already ran the setup code chunk that set messages and warnings to FALSE you won’t see any text). This is one of the reasons why we have the setup chunk to exclude messages and warnings in knitted files. If we knitted the Lab-02.Rmd file without the setup chunk, the text on loading the tidyverse would also appear in the knitted HTML file after the preamble chunk. We will cover other options you can set to customise what appears in your knitted files in later weeks.\n\nThis text often causes users new to R to think an error has occurred. However, it is merely a ‘message’ about conflicts that can be ignored.\nThe tidyverse is a meta package that bundles a collection of packages together, such as dplyr and ggplot2. These packages share a common design philosophy and are often used together in data analysis. Loading the tidyverse meta package, library(tidyverse), saves us from having to load each of these packages individually with library(dplyr), library(ggplot2), and so on.\nWhat the message is showing then is that each of the core tidyverse packages were loaded OK, and there are conflicts for functions provided by the dplyr package. Two of its functions, filter() and lag(), share the same name as functions in the base R stats package. After loading the tidyverse, any code calling filter() will use the dplyr function with that name rather than the stats ones. (This applies only to filter() directly, the stats function can still be accessed when the tidyverse is loaded by using the :: namespace operator, stats::filter().)\n\n\n\n\n\n\nNew Terms\n\n\n\n\n\nYAML: a human-readable data serialisation language, often used for configuration/settings; in R Markdown the YAML front matter between --- sets document metadata and other options.\n\ntidyverse: a collection of R packages that share a consistent design philosophy and work together for importing, wrangling, and visualising data.\n\ndata frame: an object in R that stores data in a two-dimensional table with rows (observations) and columns (variables). We load a file containing a data set into a data frame within R.\n\ntibble: a modern version of a data frame used in tidyverse packages that has more consistent data handling than base R’s data frames. For simplicity, we will still refer to these in the labs as data frames.\n\n\n\n\n\n\n\n\n\ninstall.packages() function\n\n\n\nInstall an R package that is available from the Comprehensive R Archive Network.\nUsage: install.packages(\"package_name\")\nArguments: \"package_name\" with the name of the package you want to install that must be in quotes, such as install.packages(\"tidyverse\").\nNotes: Only needs to be run once per computer, or once per project in Posit Cloud.\n\n\n\n\n\n\n\n\nlibrary() function\n\n\n\nLoad an installed package so its functions are available in the R session.\nUsage: library(package_name)\nArguments: package_name with the name of the installed package you want to load.\nNotes:\n\nA package needs to be installed first using install.packages(\"package_name\") in the Console.\nIt is best practice to load packages in a code chunk at the top of R Markdown files.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#types-of-variables",
    "href": "02-Lab2.html#types-of-variables",
    "title": "Data in R",
    "section": "",
    "text": "In quantitative analysis, we often distinguish two main types of variables:\n\n\nNumeric variables have values that describe measurable or countable quantities.\n\n\nContinuous variables have values that can fall anywhere within a range, such as time and speed.\n\nDiscrete variables have values that are whole numbers that count something, such as number of children in a household.\n\n\n\nCategorical variables have values that describe distinct, mutually exclusive categories.\n\n\nNominal variables have categories without any order, such as country, name, political party, and gender.\n\nOrdinal variables have categories with a meaningful order, such as education level or level of satisfaction.\n\n\n\nIn R, the most basic data building blocks are atomic vectors. There are six atomic types: logical, integer, double, character, complex, and raw.\nIn relation to our two main types of variables, we use the following vectors:\n\n\nNumeric vectors (integer or double) to store continuous and discrete values.\n\nFactor vectors (a class built on integer codes + character labels) to store nominal and ordinal categorical values.\n\nA numeric vector is usually stored as a double internally by default. The difference between integer and double is based on how they are computationally stored. Base R and most R packages handle the difference behind the scenes, so the difference in how they are stored does not matter to us.\nA factor is a class combining integer codes and a levels attribute containing the human-readable character labels for those codes. Each stored integer points to a character label in levels, so 1 might correspond to \"Conservative\", 2 might correspond to \"Labour\", and so on. For ordered factors, used for ordinal variables, R also records an explicit order of the levels to enable comparisons and sorting by that order.\nTwo other vectors we commonly use are:\n\n\nCharacter vectors (character) to store text, such as unique respondent IDs (“A102”, “C006”) and free-text answers.\n\nLogical vectors (logical) to store boolean TRUE / FALSE values, useful for filtering and conditions, such as age &gt; 30.\n\nIn R, there are couple of functions that will help us to identify the type of data. First, we have glimpse(). This prints some of the main characteristics of a data set, namely its overall dimension, name of each variable (column), the first values for each variable, and the type of the variable. Second we have the function class(), that will help us to determine the overall class(type) of on R object.\n\nWe are now going to use some datasets that are available to us in the R session. R comes with some example datasets out of the box, such as iris with 150 observations of iris flowers. Some R packages also include additional example datasets, such as starwars - included in the dplyr tidyverse package - with info about 87 Star Wars characters. Unlike other datasets that we have to manually load, and will cover in later section, we can access these in any R session out the box or after loading the relevant package that provides them.\nPlease go to the ‘Types of variables’ section in your Lab-02.Rmd file. You will see a few code chunks already setup for you:\n\nWe will start with a classic dataset example in R called iris. (For more info about the dataset, you can type ?iris or help(iris) in the Console). Please go to the “iris-glimpse” code chunk in your R Markdown file and run it.\n\n\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nWhat do you observe from the output?\nFirst, it tells you the number of rows and the columns on the top. After, it lists the name of each variable. Additionally, it tells you the type of the variable between these symbols &lt; &gt;. The first five variables in this dataset are of type &lt;dbl&gt; (double) which as covered above is a type of numeric variable. The last, Species, is a factor &lt;fct&gt;. So, for each of the 150 iris flowers observed, there is information on its species and four types of continuous measures. Though, as glimpse only provides a preview, we only see the values for the first few observations.\n\nNow you know that each iris flower belongs to a species, but what are the specific categories in this data set? To find out, add the following in the ‘iris-levels’ code chunk and then run it.\n\n\nlevels(iris$Species)\n\n(If you receive ‘NULL’ when running the chunk, it because you have species and not Species.)\nAs you can see, there are three categories, or levels as they are called in R factor variables, which are three types of iris flower species. Notice here we used the $ component operator mentioned earlier. Here it basically means, from the iris data frame select the Species column/variable. levels() is then a nice little utility function that shows all the levels for a column given to it as an argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s get serious and explore Star Wars.\nThe starwars data set from the dplyr package contains information about the characters, including height, hair colour, and sex. (Again, to get more information run ?starwars or help(starwars) in the Console). For the purpose of keeping things relatively simple to focus on variable types, we will use a reduced version of the full data set.\n\nFirst, run the ‘starwars-glimpse’ code chunk to create a reduced version of the data set named starwars2 and then glimpse the Star Wars characters in that reduced data set:\n\n\nstarwars2 &lt;- starwars[, 1:11]\nglimpse(starwars2)\n\nCreating a reduced version of the data frame uses another selection operator, [...]. The basic format is data_frame[rows, columns]. Here starwars[, 1:11] leaves the rows argument blank, which effectively means select all rows, and the columns argument has 1:11, which effectively means select columns 1 to 11. So, the code returns a copy of the data frame with all rows but only the first 11 columns.\nWhat do you observe this time?\nIt seems that the data type is not consistent with their content. For example, the variables species, gender, and hair_color are of type &lt;chr&gt; (that is character), when according to what we covered above these should be factors. To transform them, we will use the function ´factor()´. This process is known as coercing a variable, that is when you change a variable from one type to another. factor() is another nice little helper function\n\n\n\n\n\n\n\n\n\nLet’s coerce the species variable from character to factor and assign the result to the same column in the dataset. Add the following to the ‘starwars-factor’ code chunk and run it.\n\n\nstarwars2$species &lt;- factor(starwars2$species)\n\nNote, most functions in R for data analysis do not directly change the object -such as a data frame - passed to it. Instead, the functions return a new object with the changes. An explicit assignment back to the original object is required if you want to change it. For example, if you run factor(starwars2$species on its own in the Console, it will return all the values for species and the levels created. However, for the species column in starwars2 to be updated with the results returned by factor(starwars2$species) we need the starwars2$species &lt;- ... assignment as well. What the code is effectively saying is “replace the species column in starwars2 with the version returned by the factor function”.\n\nLet’s check if the type of variable really changed by glimpsing the data and checking the levels of species. Add the following to the ‘starwars-glimpse2’ chunk and run it.\n\n\nglimpse(starwars2)\nlevels(starwars2$species)\n\nThe glimpse result now is telling us that species is a &lt;fct&gt;, as expected. Furthermore, the levels() function reveals that there are 37 types of species, including Human, Ewok, Droid, and more.\nHopefully, these examples will help you to identify the main vector types and, more importantly, an initial understanding of how to coerce them into an appropriate type. Be aware that many data sets represent categories with numeric values without the labels, for example, using ‘0’ for male and ‘1’ for female - and as part of the initial process turning these variables into factors you will also need to provide a list mapping these numbers to the labels to use. Usually, large data sets are accompanied by extra information in a code book or documentation file, which specifies the values for the numeric code and their respective meaning. It’s important to read the code book/documentation of every data set rather than assuming what the numbers stand for as the conventions and meanings can vary. For example, some surveys now put gender in alphabetical order, with 0 for female and 1 for male.\n\n\n\n\n\n\nglimpse() function\n\n\n\nProvides a ‘glimpse’ of an object. From the dplyr package, and loaded with the tidyverse package.\nUsage: glimpse(x)\nArguments:\n\n\nx - The object (usually a data frame) to glimpse.\n\nReturns: For data frames, it returns counts for rows and columns and a compact, transposed preview showing column names, types, and a few example values. Types are shown as &lt;int&gt; (integer), &lt;dbl&gt; (double), &lt;chr&gt; (character), &lt;lgl&gt; (logical), &lt;fct&gt; (factor).\n\n\n\n\n\n\n\n\nfactor() function\n\n\n\nCreate a categorical variable (factor) with fixed, named levels.\nUsage: factor(x, levels, ordered = is.ordered(x))\nArguments:\n\n\nx - A vector, such as a data frame column, to turn into a factor.\n\nlevels - The allowed values and their order. If omitted, all unique values in x are used to create the levels.\n\nordered - Set to TRUE where order matters when creating an ordered factor.\n\nNotes: Whenever you see text like ordered = is.ordered(x) in a function documentation, this shows the name of the argument and its default value. In this example, the default value is for ordered is calculated using another function. The is.ordered(x) checks whether x is already an ordered factor and returns TRUE or FALSE. If you are using factor() to adjust the levels/labels of an existing ordered factor then, you can leave the ordered = TRUE out.\n\n\n\n\n\n\n\n\nlevels() function\n\n\n\nGet the category labels (“levels”) of a factor.\nUsage: levels(x)\nArguments: x - A factor variable, such as data_frame$country.\nReturns: A character vector of level labels for the factor variable.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#pipes",
    "href": "02-Lab2.html#pipes",
    "title": "Data in R",
    "section": "",
    "text": "A useful operator is the pipe |&gt;. This is the base R version of the pipe. This operator is what is known in programming as syntactic sugar. It is syntax which helps make code easier to read and write without changing any functionality. The pipe operator passes the result of one function as the first argument to the next (left to right).\nRun the ‘pipe-1’ code chunk and check its results.\n\n1 |&gt; sum(1)\n1 |&gt; sum(1) |&gt; sum(5)\n\nThe sum() function adds the value of two numbers in format sum(x, y). Using a pipe for 1 |&gt; sum(1) is the equivalent to sum(1, 1). If we were to write sum(x, y) with a pipe, it would be x |&gt; sum(y). So, in our example code chunk, the pipe let’s us ‘unpack’ the first argument in each function, whereby line 1 reads equivalent to saying ‘take 1, then add it with 1’ and line 2 ‘take 1, then add it with 1, then add it with 5’.\nThis may seem as if it is overcomplicating things, but as the code becomes more complex the value of the pipe operator becomes clearer. The second line, 1 |&gt; sum(1) |&gt; sum(5) would be written as sum(sum(1,1), 5) without pipes. Going left to right, 1 |&gt; sum(1) would be sum(1,1) and then sum(1,1) |&gt; sum(5) would be sum(sum(1,1), 5).\nPipes then help us avoid foo(foo(foo(...), ...) ...) monstrosities in our code. In coding examples, you will often see words like foo, bar and baz used as placeholder names when explaining how coding syntax works rather than using real functions. Similarly, x, y, and z often get used as placeholder names for variables / arguments.\nAs we will cover in examples in the next section below, plenty of functions take more than two arguments. Let’s imagine we had a data frame named df and we were updating it by using - in order - foo, bar, and baz functions that each take arguments x, y, and z. Without pipes our code would be:\n\ndf &lt;- baz(bar(foo(df, y, z), y, z), y, z)\n\nThis is a tangled mess, we want the result from foo passed to bar and its result in turn passed to baz. This ordering though gets visually reversed when nesting each function within the other - we have to read it inside out for it to make sense.\nWith pipes, it instead becomes:\n\ndf &lt; - df |&gt;\n  foo(y, z) |&gt;\n  bar(y, z) |&gt;\n  baz(y, z)\n\nNow it is easy to read and the ordering of the functions matches how we could verbally describe what the code does, “we are updating a data frame called df by running the functions foo, bar and baz on it in turn”. In the code chunk above, we make use of another wonderful thing about pipes - it let’s us break complex code over multiple lines. To do so you add the pipe at the end of the line before each function, then start each new line with an indent followed by the function. If you type |&gt; at the end of a line and hit enter, RStudio will add the spaces at the start of the new line for you.\nNote, in the readings and other materials, you may come across another %&gt;% pipe. This pipe comes from the tidyverse. The tidyverse pipe came before the base R one, where due to its popularity an equivalent was added to base R. Whilst you will still see it in use, and it remains supported in the tidyverse, the tidyverse itself recommends people start using the base R |&gt; pipe. To see that they work the same, run the ‘pipe-2’ chunk.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#black-lives-matter",
    "href": "02-Lab2.html#black-lives-matter",
    "title": "Data in R",
    "section": "",
    "text": "In this section we will work with data originally collected by The Guardian in 2015, for more information click here. The data set we will use today is an extended version which was openly shared in GitHub by the American news website FiveThirtyEight. This data set contains information about the people that were killed by police or other law enforcement bodies in the US, such as age, gender, race/ethnicity, etc. Additionally, it includes information about the city or region where the event happened. For more information click here.\n\nAs with installing packages, we only want to download the data once. For this lab, we will download the data running code in the Console. Next week though, we will go through how to set up an R script for downloading data. This is a common strategy for sharing the code used to download data whilst still keeping it separate to your main R Markdown file.\nFirst, we will create a new folder in our project directory to store the data. To do it from the Console, run this line. (Don’t worry if you get a warning. This appears because you already have a folder with this name):\n\ndir.create(\"data\")\n\nNote that in the ‘Files’ tab of Pane 4, the bottom-right pane, there is a new folder called data.\n\nNow, download the data from the GitHub repository using the function download.file(). This function takes two arguments separated by a comma: (1) the URL and (2) the destination (including the directory, file name, and file extension), as shown below.\nCopy and paste the following lines into the Console:\n\ndownload.file(\"https://github.com/fivethirtyeight/data/raw/master/police-killings/police_killings.csv\", \"data/police_killings.csv\")\n\n(Note, to avoid any issues typing out the URL, you can click the clipboard icon in the top-right of the code above to copy the full contents to your clipboard.)\nAfter that, we are ready to read the data. As the data comes as a .csv file, we can use the read_csv() function included in the tidyverse package (make sure the package is loaded in your session as explained in a previous section).\nThe read_csv() reads the data in the file and returns it as a data frame in R. We need then to assign this data frame to a named object so we can continue working with it. In our example below we will do this by assigning it to a named object police. So, in effect, our new police named object will be a data frame containing the data from the .csv file we downloaded.\nImportantly, as with loading packages, when loading data it is good practice to do so at the top of the file. So, go all the way back up to the preamable code chunk and add the following line after the ‘# Read data’ comment.\n\npolice &lt;- read_csv(\"data/police_killings.csv\")\n\nYou should now have:\n\nRun the code chunk and you’ll know it has run OK if can see ‘police’ in your Global Environment in the top-right pane. It will have 467 observations and 34 variables (columns).\n\n\nOK, before we look at this police data, a reminder about creating code chunks. Scroll back in your R Markdown file to “## Black lives matter!”. For this section, you will need to create your own code chunks.\nTo create a new code chunk, you have four options:\n\nPut your text cursor on a line in your R Markdown file and press Ctrl+Alt+I on the keyboard.\nFrom the menu bar at the top of RStudio, you can select Code &gt; Insert Chunk.\nManually type three backticks, on UK layout keyboards the key to the left of the 1 key, and then {r} (remember to add three backticks on a line below to close the code chunk as well).\nClick on the icon that is a c in green square with a ‘+’ in a circle in top-left corner, and then click on R to create an R code chunk.\n\n\nBy default, a new code chunk will be plain {r}, but you can edit this to give it an optional name:\n\n\nBack to the data. Let’s first glimpse the police data. Create a code chunk then add and run the following:\n\nglimpse(police)\n\nAs you can see, there are several variables included in the dataset, such as age, gender, law enforcement agency (lawenforcementagency), or whether the victim was armed (armed). You will see some of these variables are not in the appropriate type. For instance, some are categorical and should be a factor (&lt;fct&gt;) instead of character (&lt;chr&gt;).\n\nBefore coercing these variables, we will create a smaller subset of the data frame by selecting only the variables that we are interested in. To do so, we can use the select() function. The select() function takes the name of the data frame as the first argument and then the names of the variables we want to keep separated by commas (no quotation marks needed).\nRemember with pipes we can replace select(data_frame, variable1, variable2, ...) with data_frame |&gt; select(variable1, variable2, ... making it easier to read as “from our data frame select the following variables”. Let’s create a code chunk to select a few variables and assign the result to a new object called police_2.\n\npolice_2 &lt;- police |&gt; select(age, gender, raceethnicity, lawenforcementagency, armed)\n\nThe select() function is from the dplyr tidyverse package. Many tidyverse functions that take a data frame as the first argument, foo(date_frame, ...) or when using a pipe data_frame |&gt; foo(...), will assume all variables listed are from that data frame. Within the function, this then let’s us type age and gender rather than police$age and police$gender.\nIf you look again to the ‘Environment’ tab, there is now a second data frame with the same number of observations but only 5 variables. Create a new code chunk, then add and run the following code to glimpse its contents.\n\nglimpse(police_2)\n\nHaving a closer look at the reduced version, we can see that in fact all the variables are of type &lt;chr&gt; (character), including age.\nLet’s coerce the variables into their correct type. Start by creating a code chunk and naming it ‘coerce-police_2’ or similar.\nThen within the chunk, let’s start with age, coercing it from character to numeric by adding the following lines:\n\n# Coerce numeric\npolice_2 &lt;- police_2 |&gt; mutate(age = as.numeric(age))\n\nThe # Coerce numeric line let’s us add a comment to our code. Any text within an R code chunk after a # is treated as a comment and is not run as part of the code. This is useful for recording the ‘why’ behind the code, so future-you (and others) can quickly read and understand it.\n(You may be wondering though why outside the code chunks there are # but these become headers when knitting the document. The reason is within Markdown syntax # at the start of a line is used for Headings, with the number of # denoting the Heading level - # for Heading 1, ## for Heading 2, and so on. As R and Markdown developed separately before the creation of R Markdown brought the two together, they both had by that point decided to use # for different things. Whilst confusing at first, it all boils down to “# inside code chunk = comments” and “# outside code chunks = headers”.)\nAge is not known for some cases. Thus, it is recorded as ‘Unknown’ in the dataset. Since this is not recognized as a numeric value in the coercion process, R automatically sets it as a missing value, NA. This is why it will give you a warning message. (Though you will not see a warning message if you ran the knitr setup chunk that disabled warning messages.)\nWe can continue coercing raceethnicity and gender from character to a factor by adding the following lines to our chunk:\n\n# Coerce factors\npolice_2 &lt;- police_2 |&gt; mutate(raceethnicity = factor(raceethnicity))\npolice_2 &lt;- police_2 |&gt; mutate(gender = factor(gender))\n\nRun the chunk and then create and run a new chunk with the following to glimpse the data frame again:\n\nglimpse(police_2)\n\nYou should hopefully see the three coerced variables now have the correct types:\n\n\nage as numeric &lt;dbl&gt;\n\n\ngender and raceethnicity as factor &lt;fct&gt;\n\n\nNow, let’s run a summary of the data using the summary() function. This shows the number of observations in each category or a summary of a numeric variable. Create a code chunk, then add and run the following:\n\nsummary(police_2)\n\nThere are some interesting figures coming out from the summary. For instance, in age you can see that the youngest (Min.) is… 16 years old(?!), and the oldest (Max.) 87 years old. Also, the vast majority are male (445 vs 22). In relation to race/ethnicity, roughly half are ‘White’, whereas ‘Black’ individuals represent an important share. One may question the proportion of people killed in terms of race/ethnicity compared to the composition of the total population (considering Black is a minority group in the US).\nLet’s suppose that we only want observations in which race/ethnicity is not unknown. To ‘remove’ undesired observation we can use the filter() function. Remember, to make changes to an object we need to assign the result of a function back to it. We will then need to assign the result of filter back to the police_2 object.\nCreate a chunk, then add and run the following:\n\npolice_2 &lt;- police_2 |&gt; filter(raceethnicity != \"Unknown\")\n\nSo, what just happened in the code above? First, the pipe operator, |&gt;: What we are doing verbally is take the object police_2, THEN filter raceethnicity based on a condition.\nThen, what is happening inside the filter() function? Let’s have a look at what R does in the background for us (Artwork by @alison_horst):\n\n\n\n\nFilter. Source: Artwork by Horst (n.d.).\n\n\n\nNote, df for us is police_2. Since we are using the pipe operator, police_2 |&gt; filter(raceethnicity != \"Unkown\") is equivalent to writing filter(police_2, raceethnicity != \"Unkown\") without the pipe operator.\nImportantly, the filter() function returns a data frame with only the rows from the data frame that was passed to it that meet the conditions specified. In our example then, filter() returns a data frame that only has the rows from the police_2 data frame where the row’s value in the raceethnicity column is NOT EQUAL (!=) to ‘Unknown’. When we then assign the result to an object named the same as our existing object, police_2 &lt;- ..., we replace the old police_2 data frame with the new filtered version. In effect, our police_2 data frame is updated to a version with all rows where the value for raceethnicity was “Unknown” removed.\nFinally then, create another code chunk to get a glimpse and summary of the data frame again:\n\nglimpse(police_2)\n\nRows: 452\nColumns: 5\n$ age                  &lt;dbl&gt; 16, 27, 26, 25, 29, 29, 22, 35, 44, 31, 76, 40, N…\n$ gender               &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, M…\n$ raceethnicity        &lt;fct&gt; Black, White, White, Hispanic/Latino, White, Whit…\n$ lawenforcementagency &lt;chr&gt; \"Millbrook Police Department\", \"Rapides Parish Sh…\n$ armed                &lt;chr&gt; \"No\", \"No\", \"No\", \"Firearm\", \"No\", \"No\", \"Firearm…\n\nsummary(police_2)\n\n      age           gender                   raceethnicity lawenforcementagency\n Min.   :16.00   Female: 20   Asian/Pacific Islander: 10   Length:452          \n 1st Qu.:28.00   Male  :432   Black                 :135   Class :character    \n Median :35.00                Hispanic/Latino       : 67   Mode  :character    \n Mean   :37.15                Native American       :  4                       \n 3rd Qu.:45.00                Unknown               :  0                       \n Max.   :87.00                White                 :236                       \n NA's   :2                                                                     \n    armed          \n Length:452        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nTo make it easier to compare, the results shown when running the code chunks are included here as well. Within your R Markdown file, scroll up to your previous glimpse and summary chunks for the police_2 data frame. Now compare that to the results we now receive after filtering/ You should see in the filtered version that under raceethnicity, ‘Unkown’ is now 0, when it was 15 before we filtered the dataset. Correspondingly, the total number of rows has dropped by 15, going from 467 initially, to 452 after filtering.\n\n\n\n\n\n\n\n\n\nAs we introduced a lot of new functions in this section, here is a final sub-section with the mini help documentation for them all together -\n\n\n\n\n\n\nsummary() function\n\n\n\nGet a quick summary for an object. Supports many different object types.\nUsage: summary(object)\nArguments: object - The named object to summarise. This object can be a data frame, a specific column from a data frame, etc.\nReturns: Depends on object.\nFor data frame columns it summarises:\n\n\nnumeric columns: Min, 1st Qu., Median, Mean, 3rd Qu., Max, NA count\n\nfactor columns: frequency counts of each category, and NA count\n\nIf provided a data frame it will provide this summary for each column.\n\n\n\n\n\n\n\n\nas.numeric() function\n\n\n\nCoerce a vector to numeric (double) if possible.\nUsage: as.numeric(x)\nArguments: x - The object, such as a data frame column, to coerce.\nReturns: A numeric vector. Any text such as “10” will become a numeric value, but text such as “house”, “missing”, “unknown”, will all become NA values with a warning.\n\n\n\n\n\n\n\n\nmutate() function\n\n\n\nModify existing column(s) or add new column(s). From the dplyr package, which is loaded with the tidyverse package.\nUsage: mutate(data_frame, column = expression, ...) or with pipe data_frame |&gt; mutate(column1 = expression, column2 = expression, ...)\nArguments:\n\n\ndata_frame - The data frame to modify columns from.\n\ncolumn = expression - On the left, the column name want to create or overwrite. On the right, an expression using existing columns and functions. For example age = as.numeric(age) will overwrite the existing age column, but new_age = as.numeric(age) would create a new new_age column.\n\n\n\n\n\n\n\n\n\nselect() function\n\n\n\nSelect specific columns (i.e. variables) from a data frame. From the dplyr package, which is loaded with the tidyverse package.\nUsage: select(data_frame, column1, column2, ...) or with pipe data_frame |&gt; select(column1, column2, ...)\nArguments:\n\n\ndata_frame - The data frame to select columns from.\n\ncolumn1, column2, ... - The columns in the data frame to keep, separated by commas.\n\nReturns: A data frame containing only the selected columns, in the order they were listed in the function.\n\n\n\n\n\n\n\n\nfilter() function\n\n\n\nFilter rows in a data frame to keep only those meeting given conditions. From the dplyr package, which is loaded with the tidyverse package.\nUsage: filter(data_frame, condition1, condition2, ...) or with pipe data_frame |&gt; filter(condition1, condition2, ...)\nArguments:\n\n\ndata_frame - The data frame to filter.\n\ncondition1, condition2, ... - Conditions stated as logical tests, such as age &gt; 18, country == \"Scotland\".\n\nReturns: A data frame containing only the rows where the conditions are met.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#code-formatting",
    "href": "02-Lab2.html#code-formatting",
    "title": "Data in R",
    "section": "",
    "text": "As a final side note - and a wee preview of code you will see in later labs and in R textbooks - all the steps we did above for creating and modifying police_2 can also be written in a single code chunk. The code chunk below does all the same steps. It creates a police_2 data frame by first selecting variables from the police data frame, then coerces the types of some variables, and finally filters the data.\n\npolice_2 &lt;- police |&gt;\n  select(age, gender, raceethnicity, lawenforcementagency, armed) |&gt;\n  mutate(\n    age = as.numeric(age),\n    raceethnicity = factor(raceethnicity),\n    gender = factor(gender)\n  ) |&gt;\n  filter(raceethnicity != \"Unknown\")\n\nNotice here that we can also pass multiple variables to mutate() and able to create a line for each variable being coerced. We can do this as R also lets us split anything after a bracket or comma onto a new line. With long functions, it is common to start a new line after the (, then add each element that is separated by a comma on a line of its own, then add a final line with the closing ). Notice as well that the elements are indented another two spaces (making the indent for them 4 spaces in total) and the closing ) is indented the same as mutate(.\nIt may take a while to get the hang of reading code with ease, but this formatting helps a lot. The initial 2 space indent after the first line let’s us know that “all the following lines are still part of the same series of steps”. The additional 2 space indent for the lines between the opening ( and closing ) for the mutate() function then also visually lets us know, “all the following lines are arguments within the mutate() function”. This code would be incredibly difficult to read at a glance if it was written instead as a single line and without any pipes.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#activities",
    "href": "02-Lab2.html#activities",
    "title": "Data in R",
    "section": "",
    "text": "Discuss the following questions with your neighbour or tutor:\n\nWhat is the main purpose of the functions select() and filter()?\nWhat does coerce mean in the context of R? and Why do we need to coerce some variables?\nWhat is the mutate() function useful for?\n\nUsing the police_2 dataset:\n\nFilter how many observations are ‘White’ in raceethnicity? How may rows/observations are left?\nHow many ‘Hispanic/Latino’ are there in the dataset?\nUsing the example of Figure 2.3, could you filter how many people were killed that were (a) ‘Black’ and (b) possessed a firearm on them at the time (‘Firearm’)?\nWhat about ‘White’ and ‘Firearm’?\n\nFinally, ‘Knit’ your R Markdown file.\nExtra activities:\n\nWhy did you have to use quotes in the following: police |&gt; filter(raceethnicity==\"White\" & armed==\"Firearm\")?\nWhat do you have to use == rather than =?\n\nNote, police |&gt; filter(raceethnicity==\"White\" & armed==\"Firearm\") could also be written as police |&gt; filter(raceethnicity==\"White\", armed==\"Firearm\"). However, using & makes it clearer that we want to keep rows that meet both conditions. If we wanted to filter for rows where either condition is true, we would use | (which is often used as an operator for ‘or’ in programing languages) instead of &.\nThis is the end of Lab 2. Again, the changes in your R Markdown file should be saved automatically in RStudio. However, make sure this is the case as covered in Lab 1. After this, you can close the tab in your web browser. Hope you had fun!",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#footnotes",
    "href": "02-Lab2.html#footnotes",
    "title": "Data in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.tidyverse.org/↩︎",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "assessments/formative.html",
    "href": "assessments/formative.html",
    "title": "Formative Assessment",
    "section": "",
    "text": "Formative Submission Deadline\n\n\n\n12 noon, Friday 31st October 2025\n\n\nFor the Formative Assessment, you will be asked to consolidate the main elements of the course across Weeks 1-5 in the form of a short report (500 words maximum, requiring the use of R Markdown). This should be submitted as an R Markdown report. You will learn how to do this in the Labs.\nBy Week 6, you will have become familiar with the NILT Northern Ireland Life and Times 2012 dataset. You will be using this dataset, and for the final summative assessment report we are going to be using the following variables:\n\nAnnual Personal Income\nSex\nReligion\nSexual Orientation\nNI Constitutional View\nTrade Union Membership\nSupervisor \nAge \n\nThis formative assessment is designed to help you get some initial practice on and feedback on the summative assessment. Although it is not graded therefore, it will help directly inform your final summative assessment grade.  \nYou will be asked to write a report with the following headings: \n\n\nIn this part you need to set up your Research Question and explain its rationale. And then break the Research Question down into a hypothesis or hypotheses that can be tested in a quantitative research design.\n\n\nState your research question clearly here. Remember from the lectures, that the research question tends to have two elements:\n\na relationship\na specific context\n\nNext, break your Research Question down into a hypothesis or, if relevant, hypotheses. Again from the lectures, these need to be in a specific form. Please revisit the guidance here to help form your hypothesis/hypotheses correctly.\n\n\n\nWhy is your Research Question necessary to ask in the first place? Give some context and background to help the reader understand the rationale for your study and use some literature to help you establish this: 2-3 references are fine as a minimum for the formative assessment.\nDon’t worry overly about being original. You are using a dataset from 2012 with restricted number of variables already chosen for you. The main thing is to demonstrate you can contextualize your Research Question by explaining its background and by using some relevant literature to support this.\n\n\n\nYour Research Question will contain a dependent variable and an independent variable (or possibly two maximum). These need to be identified:\n\nDependent variable: Be clear on what your dependent variable is: state it for the reader.\nIndependent variable: You also should have one key independent variable (two maximum), which you want to focus on in your study. Again, state what this variable is, clearly, for the reader.\n\n\n\n\n\nYou have now set up your Research Question and explained its background. You have identified the key variables (dependent and independent variables) contained within it. And you have then used these to break your Research Question down into a hypothesis i.e. into a relationship between variables that you can now go onto test.\nTo do so though, you now need a dataset which collects data on these variables of interest. For our course, this dataset is NILT 2012.\n\n\nDescribe the dataset you are using:\n\nWhat is the data set? Who collected the data and for what purpose? What does the data describe?\nWhat is the sample size? How was the data collected? Why is this reliable and are there any potential shortcomings that could limit the interpretation of the data?\n\nAs per the lectures, think of the general details, technical details and any limitations we need to know.\n\n\n\n\nPresent a table of descriptive statistics for the variables included in the model.\nDiscuss the descriptive findings for your dependent variable and independent variable(s).\nDoes the distribution of the dependent variable have any implications for the model?\n\nYou should submit this formative assessment report as an R Markdown report.\nFinally, this formative is designed to help you practice the summative assessment. Specifically, note both:\n\nPart 1: Introduction and Part 2: Data Collection and Descriptive Statistics of this formative assessment\n\nRelate directly to:\n\nPart 1: Introduction and Part 2: Data and Method of the summative report (Interpreting Quantitative Findings).\n\nMake sure you utilise the feedback given to you on your Formative Assessment report therefore in your final Summative Assessment report:\n\nby using it to inform your Interpreting Quantitative Findings report\nand explaining in your Reflective Course Summary how you used this important source of formative feedback to help your learning on the course",
    "crumbs": [
      "Assessments",
      "Formative Assessment"
    ]
  },
  {
    "objectID": "assessments/formative.html#setting-up-a-quantitative-project",
    "href": "assessments/formative.html#setting-up-a-quantitative-project",
    "title": "Formative Assessment",
    "section": "",
    "text": "Formative Submission Deadline\n\n\n\n12 noon, Friday 31st October 2025\n\n\nFor the Formative Assessment, you will be asked to consolidate the main elements of the course across Weeks 1-5 in the form of a short report (500 words maximum, requiring the use of R Markdown). This should be submitted as an R Markdown report. You will learn how to do this in the Labs.\nBy Week 6, you will have become familiar with the NILT Northern Ireland Life and Times 2012 dataset. You will be using this dataset, and for the final summative assessment report we are going to be using the following variables:\n\nAnnual Personal Income\nSex\nReligion\nSexual Orientation\nNI Constitutional View\nTrade Union Membership\nSupervisor \nAge \n\nThis formative assessment is designed to help you get some initial practice on and feedback on the summative assessment. Although it is not graded therefore, it will help directly inform your final summative assessment grade.  \nYou will be asked to write a report with the following headings: \n\n\nIn this part you need to set up your Research Question and explain its rationale. And then break the Research Question down into a hypothesis or hypotheses that can be tested in a quantitative research design.\n\n\nState your research question clearly here. Remember from the lectures, that the research question tends to have two elements:\n\na relationship\na specific context\n\nNext, break your Research Question down into a hypothesis or, if relevant, hypotheses. Again from the lectures, these need to be in a specific form. Please revisit the guidance here to help form your hypothesis/hypotheses correctly.\n\n\n\nWhy is your Research Question necessary to ask in the first place? Give some context and background to help the reader understand the rationale for your study and use some literature to help you establish this: 2-3 references are fine as a minimum for the formative assessment.\nDon’t worry overly about being original. You are using a dataset from 2012 with restricted number of variables already chosen for you. The main thing is to demonstrate you can contextualize your Research Question by explaining its background and by using some relevant literature to support this.\n\n\n\nYour Research Question will contain a dependent variable and an independent variable (or possibly two maximum). These need to be identified:\n\nDependent variable: Be clear on what your dependent variable is: state it for the reader.\nIndependent variable: You also should have one key independent variable (two maximum), which you want to focus on in your study. Again, state what this variable is, clearly, for the reader.\n\n\n\n\n\nYou have now set up your Research Question and explained its background. You have identified the key variables (dependent and independent variables) contained within it. And you have then used these to break your Research Question down into a hypothesis i.e. into a relationship between variables that you can now go onto test.\nTo do so though, you now need a dataset which collects data on these variables of interest. For our course, this dataset is NILT 2012.\n\n\nDescribe the dataset you are using:\n\nWhat is the data set? Who collected the data and for what purpose? What does the data describe?\nWhat is the sample size? How was the data collected? Why is this reliable and are there any potential shortcomings that could limit the interpretation of the data?\n\nAs per the lectures, think of the general details, technical details and any limitations we need to know.\n\n\n\n\nPresent a table of descriptive statistics for the variables included in the model.\nDiscuss the descriptive findings for your dependent variable and independent variable(s).\nDoes the distribution of the dependent variable have any implications for the model?\n\nYou should submit this formative assessment report as an R Markdown report.\nFinally, this formative is designed to help you practice the summative assessment. Specifically, note both:\n\nPart 1: Introduction and Part 2: Data Collection and Descriptive Statistics of this formative assessment\n\nRelate directly to:\n\nPart 1: Introduction and Part 2: Data and Method of the summative report (Interpreting Quantitative Findings).\n\nMake sure you utilise the feedback given to you on your Formative Assessment report therefore in your final Summative Assessment report:\n\nby using it to inform your Interpreting Quantitative Findings report\nand explaining in your Reflective Course Summary how you used this important source of formative feedback to help your learning on the course",
    "crumbs": [
      "Assessments",
      "Formative Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html",
    "href": "11-Lab11.html",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "This is our final practical session. Thank you for tuning in and all your hard work! Today, we will discuss some practical aspects of the summative assessment Interpreting Quantitative Findings Report. You will actually start working on the assignment today by creating a template you can use to write your report in RStudio. Also, you will get familiar with the structure and the contents of the template. Remember, this is the perfect time to clarify as many questions as possible. Your tutor will be more than happy to help!\nBefore proceeding, please take about 10 minutes to read the instructions for the assignment available in Moodle.\n\nWe’ve created a GitHub repository which contains an RStudio project and all the essentials you need to start writing your report. There are two options for you. One is writing in RStudio Cloud, as we have been doing all along this course. Alternatively, you can also use the RStudio Desktop version (this requires having R and RStudio installed locally on your device). The result will be the same and the process is very similar. So, the choice is totally up to you and what your preference is. Here, we explain how to start your own project for both options.\nFor this session, try only one of these. Don’t worry about which one you choose now, you can change your mind later. If you can’t make up your mind, then try the RStudio Cloud route first, as it means you can work on your assignment on any devices, regardless of the computational power of your device, but it does mean you need to work on the assignment online using a web browser. If online connection is an issue for you, then choose the RStudio Desktop route.\n\nIf you wish to write your project using RStudio Cloud, please follow the next steps:\n\nAccess RStudio Cloud as usual.\nMake sure you are in your Quants lab group (not in ‘Your Workspace’).\nClick on the ‘New Project’ button and select ‘New Project from Git Repository’.\n\n\n\n\n\nNew project from GitHub.\n\n\n\n\nPaste the following URL in the box and click ‘OK’: https://github.com/UGQuants/UGQuant-assignment2.\nYou are all set! Just click on the project to access the contents.\n\nIf alternately you decide to write your assessment using the RStudio Desktop version, consider the following steps:\n\nAccess the following GitHub repository copying and pasting the following URL in your browser: https://github.com/UGQuants/UGQuant-assignment2.\nClick on the ‘Code’ button and select the ‘Download ZIP’ option, as shown below.\n\n\n\n\n\nDownlowad ZIP from GitHub.\n\n\n\n\nGo to your local downloads folder, right click on the UGQuant-assignment2-main.zip and choose the ‘Extract all’ option (here, you can also chose the folder where you want to store the RStudio project and write your assignment).\nClick on the ‘Extract’ button.\nAs you can see, this folder contains an RStudio project. Open the UGQuant-assignment2.Rproj which will initialize a new RStudio session.\n\nOnce in your UGQuant-assignment2 project, open the Assignmet2-template.Rmd file in Pane 4 under the ‘Files’ tab, as shown below.\n\n\n\n\nTemplate.\n\n\n\nThis template contains the following:\n\nSuggested structure of the report.\nSuggested word count for each section.\nThe code necessary to run and present the results of a multivariate linear regression.\n\nIn essence, this is the basic structure to start writing your assignment. Of course, you can add, edit, and customize as much as you consider appropriate. Remember, this is just a generic suggestion and you should still address all the points to the best of your abilities. Remember: There are no hard and fast rules to say what’s right or wrong. You are the one who can determine and justify why you did what you did. There are also many ways to write and approach this assignment, so make choices based on your own interest(s) and disciplinary background. It is truly your time to shine! The course handbook also gives important pointers on how your report will be assessed, offering some guiding questions to tackle the report–don’t skip this crucial step.\n\n\nIn the template, fill in the ‘author’ and ‘date’ space in the YAML at the top of the file with your student number and appropriate information using quotation marks.\nKnit the Rmd file as html (RStudio may ask to install some packages; click ‘Yes’).\nIn the output, look at the results of the table under the ‘Results’ section and identify the dependent and the independent variables. You can learn about the meaning of these variables in the NILT documentation (click here to access the documentation).\nIdentify the variables that are significant in this model and the direction of the relationship.\nDiscuss your interpretations with your neighbour or tutor. You can refer back to Lab 8 and Lab 9 to refresh your memory.\n\n\nWrite one introductory paragraph in the ‘Introduction’ section of the template according to the guidance provided and your preliminary insights.\nKnit the document again.\n\nYou are on the right track now!\nWe hope that by getting familiar with this setting, you will easily succeed in writing your research report using all the knowledge and skills acquired during this course. Take this session to ask questions about the assignment or the course in general as much as possible. This is the right time to have specialized one-to-one support from your tutors.\nWe wish you the best of luck! Get in touch with your tutors via email or, even better, post any questions you have about the assignment on your lab group discussion forum on Moodle, if you don’t know where to start or get stuck. Don’t suffer in silence. Remember your Tutors, lab group mates, and the teaching and admin team are here for you. Also don’t forget coding is all about trial and error, so it’s completely normal to write / copy and paste some code, then get an error message, and basically for your code to not work. Keep chipping at it, which sometimes can take hours (if not days), until you can get it to work. That’s a normal process even for professional data scientists and quantitative researchers! So do persevere and don’t panic if you don’t get it to work right away, because troubleshooting your code is part of the work, in addition to making your own choices in the analysis and reporting. For the code, the error message you get often gives you clues about, well, where the error is. So read the error message carefully, it might be you haven’t loaded the package required to run the code (remember to do this every time you open RStudio) or you might have missed a parentheses, or you might have not capitalised a word in the R syntax when you need to. Double check the R cheatsheets or previous sections in the lab workbook to ensure you have specified R syntax/arguments correctly. Again, trial and error is your friend, unlike writing an essay/doing an exam.\nGood luck and have fun! You got this.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#introduction",
    "href": "11-Lab11.html#introduction",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "This is our final practical session. Thank you for tuning in and all your hard work! Today, we will discuss some practical aspects of the summative assessment Interpreting Quantitative Findings Report. You will actually start working on the assignment today by creating a template you can use to write your report in RStudio. Also, you will get familiar with the structure and the contents of the template. Remember, this is the perfect time to clarify as many questions as possible. Your tutor will be more than happy to help!\nBefore proceeding, please take about 10 minutes to read the instructions for the assignment available in Moodle.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#create-your-research-report-template-from-github",
    "href": "11-Lab11.html#create-your-research-report-template-from-github",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "We’ve created a GitHub repository which contains an RStudio project and all the essentials you need to start writing your report. There are two options for you. One is writing in RStudio Cloud, as we have been doing all along this course. Alternatively, you can also use the RStudio Desktop version (this requires having R and RStudio installed locally on your device). The result will be the same and the process is very similar. So, the choice is totally up to you and what your preference is. Here, we explain how to start your own project for both options.\nFor this session, try only one of these. Don’t worry about which one you choose now, you can change your mind later. If you can’t make up your mind, then try the RStudio Cloud route first, as it means you can work on your assignment on any devices, regardless of the computational power of your device, but it does mean you need to work on the assignment online using a web browser. If online connection is an issue for you, then choose the RStudio Desktop route.\n\nIf you wish to write your project using RStudio Cloud, please follow the next steps:\n\nAccess RStudio Cloud as usual.\nMake sure you are in your Quants lab group (not in ‘Your Workspace’).\nClick on the ‘New Project’ button and select ‘New Project from Git Repository’.\n\n\n\n\n\nNew project from GitHub.\n\n\n\n\nPaste the following URL in the box and click ‘OK’: https://github.com/UGQuants/UGQuant-assignment2.\nYou are all set! Just click on the project to access the contents.\n\nIf alternately you decide to write your assessment using the RStudio Desktop version, consider the following steps:\n\nAccess the following GitHub repository copying and pasting the following URL in your browser: https://github.com/UGQuants/UGQuant-assignment2.\nClick on the ‘Code’ button and select the ‘Download ZIP’ option, as shown below.\n\n\n\n\n\nDownlowad ZIP from GitHub.\n\n\n\n\nGo to your local downloads folder, right click on the UGQuant-assignment2-main.zip and choose the ‘Extract all’ option (here, you can also chose the folder where you want to store the RStudio project and write your assignment).\nClick on the ‘Extract’ button.\nAs you can see, this folder contains an RStudio project. Open the UGQuant-assignment2.Rproj which will initialize a new RStudio session.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#about-the-research-report-template",
    "href": "11-Lab11.html#about-the-research-report-template",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "Once in your UGQuant-assignment2 project, open the Assignmet2-template.Rmd file in Pane 4 under the ‘Files’ tab, as shown below.\n\n\n\n\nTemplate.\n\n\n\nThis template contains the following:\n\nSuggested structure of the report.\nSuggested word count for each section.\nThe code necessary to run and present the results of a multivariate linear regression.\n\nIn essence, this is the basic structure to start writing your assignment. Of course, you can add, edit, and customize as much as you consider appropriate. Remember, this is just a generic suggestion and you should still address all the points to the best of your abilities. Remember: There are no hard and fast rules to say what’s right or wrong. You are the one who can determine and justify why you did what you did. There are also many ways to write and approach this assignment, so make choices based on your own interest(s) and disciplinary background. It is truly your time to shine! The course handbook also gives important pointers on how your report will be assessed, offering some guiding questions to tackle the report–don’t skip this crucial step.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#activity-1",
    "href": "11-Lab11.html#activity-1",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "In the template, fill in the ‘author’ and ‘date’ space in the YAML at the top of the file with your student number and appropriate information using quotation marks.\nKnit the Rmd file as html (RStudio may ask to install some packages; click ‘Yes’).\nIn the output, look at the results of the table under the ‘Results’ section and identify the dependent and the independent variables. You can learn about the meaning of these variables in the NILT documentation (click here to access the documentation).\nIdentify the variables that are significant in this model and the direction of the relationship.\nDiscuss your interpretations with your neighbour or tutor. You can refer back to Lab 8 and Lab 9 to refresh your memory.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#activity-2",
    "href": "11-Lab11.html#activity-2",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "Write one introductory paragraph in the ‘Introduction’ section of the template according to the guidance provided and your preliminary insights.\nKnit the document again.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#conclusion",
    "href": "11-Lab11.html#conclusion",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "You are on the right track now!\nWe hope that by getting familiar with this setting, you will easily succeed in writing your research report using all the knowledge and skills acquired during this course. Take this session to ask questions about the assignment or the course in general as much as possible. This is the right time to have specialized one-to-one support from your tutors.\nWe wish you the best of luck! Get in touch with your tutors via email or, even better, post any questions you have about the assignment on your lab group discussion forum on Moodle, if you don’t know where to start or get stuck. Don’t suffer in silence. Remember your Tutors, lab group mates, and the teaching and admin team are here for you. Also don’t forget coding is all about trial and error, so it’s completely normal to write / copy and paste some code, then get an error message, and basically for your code to not work. Keep chipping at it, which sometimes can take hours (if not days), until you can get it to work. That’s a normal process even for professional data scientists and quantitative researchers! So do persevere and don’t panic if you don’t get it to work right away, because troubleshooting your code is part of the work, in addition to making your own choices in the analysis and reporting. For the code, the error message you get often gives you clues about, well, where the error is. So read the error message carefully, it might be you haven’t loaded the package required to run the code (remember to do this every time you open RStudio) or you might have missed a parentheses, or you might have not capitalised a word in the R syntax when you need to. Double check the R cheatsheets or previous sections in the lab workbook to ensure you have specified R syntax/arguments correctly. Again, trial and error is your friend, unlike writing an essay/doing an exam.\nGood luck and have fun! You got this.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "05-Lab5.html",
    "href": "05-Lab5.html",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "In the previous labs you were exploring the 2012 Northern Ireland Life and Times Survey (NILT). You’ve learnt how to download, read and format the data. Also, you’ve learnt how to explore categorical and numeric data and a mix of them. In this lab, you will learn about how to efficiently report quantitative results directly from R, using R Markdown, which is used by many academics and professionals in a workplace setting to communicate quantitative findings to a wider audience. R Markdown is also what you will use to write your research report assignment for this course. So, Let’s dive in and learn more!\n\nR Markdown (Rmd) is a different type of file included in RStudio (and it is actually a different markup language). This allows you to generate reports in common file types, such as .html (the same one used for this lab workbook you’re reading right now), .pdf, or Word (.doc). The interesting thing is that the Rmd file allows you to integrate text, code, and plots directly into your report (so you do not have to copy and paste tables or graphs into a Word document, for example, which is often very messy and time-consuming). You have already seen how well this works in the lab workbooks so far, which are written entirely in R Markdown.\nThe basic components of an Rmd file are: the code, text and metadata. The code is integrated by blocks called ‘chunks’, and the metadata contains information to format the report. We believe the best way to learn is by doing it. So, let’s create your first Rmd document!\n\nWe will continue working in the same project called NILT in RStudio Cloud.\n\nPlease go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\nCreate a new Rmd file, this is similar as creating and R Script, from the ‘File’ tab on the top-left: File&gt;New File&gt;R Markdown... (Rstudio may ask to install some packages, click ‘Yes’);\nType ‘Test1’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML. Then, click ‘OK’.\nSave the Rmd file clicking on File&gt;Save as..., type Test1 in the ‘File name’ box, and click on the Save button.\n\nAfter completing the previous steps, your screen should look like this (You can minimize the console to expand pane 1):\n\n\n\n\nRmd file.\n\n\n\nNote that now you have two files open in Pane 1, one tab includes the R script that we created in the last lab (called Lab_3.R), and the other is the Rmd document that you just created.\nThe Rmd document Test1 contains an example by default. The first bit on the top enclosed by the dashes ---, contains the general metadata to format the output, as shown in the Figure @ref(fig:yaml-chunk). This bit is called YAML. In the default example, it contains the title, name of the author, data and the type of output (html). You can adjust this information directly by typing the relevant info (e.g. date or name).\n\n\n\n\nYAML and code chunk.\n\n\n\nBelow the YAML shown in Figure @ref(fig:yaml-chunk), there is another box. This is an R code ‘chunk’. To run a chunk of code individually (that is to visualize a partial result of an Rmd document), you can click on the green arrow pointed on the top-left of the first chunk.\nIn line 12, you have a second-level header, which contains the name of a section in the document. As you can see this is preceded by double hash tag ##. If you want a first-level header section, you would require only one hash tag like this #, and three for a third-level header. Finally the ‘Knit’ word is enclosed by double asterisk **. This is to format the characters enclosed in bold.\nIn line 26, you will see a chunk including a basic plot. Let’s check the results that this example in ‘Test1’ produce.\nTo render the document from Rmd to HTML, we need to Knit it by clicking on the icon shown below. Try it!\n\n\nKnit button\n\nRStudio may ask you if you want to update some packages, click ‘Yes’.\nAfter you knit the document, a window with the output will pop up automatically. As you can see, this document contains the main title, followed by your name and date, as specified in the YAML. Afterwards, there is a second-level header which includes the first section of this example document. Also, the word ‘Knit’ is shown in bold, as it was wrapped by double asterisk **.\nAn interesting thing is that we can integrate the result of our code in the output as we did with the second chunk (which starts in line 18). Here, we can use a summary of the data set cars (which is an in-built data set in R that contains only two variables).\n\nSimilarly, you can create a .pdf file. To do this, instead of clicking on the Knit icon directly, click on the black arrow next to it. Then click ‘Knit to PDF’. Try it and see the result.\nWhen you knitted the document, RStudio actually created a new .html or .pdf file with the same name as your Rmd document. You can confirm this in the ‘Files’ tab in Pane 4. You can download this file from the cloud to your local drive by clicking on the box of the output file. Then, click More &gt; Export... in the same pane (or click on the gear icon in Pane 4).\n\n\nIMPORTANT: Rmd files are different from simple R scripts. While everything you write in an R script is interpreted as code, only the bits within the code chunks will be interpreted as code in Rmd files. Everything else not within chunks in an Rmd file is interpreted as text.\n\nIn the Test1.Rmd file that you just created, do the following:\n\nChange the title of the document in the YAML to ‘My first R Markdown document’.\nIn the code chunk in line 21, replace the existing line (summary(car)) with the following: glimpse(iris).\nIn the code chunk called ‘pressure’, change echo=FALSE to echo=TRUE.\nAt the very bottom of the script, create a new paragraph and write one or two lines briefly describing how you think quantitative methods are improving your discipline (e.g. politics, sociology, social and public policy, or central and eastern European studies).\nKnit the document in html format.\nDownload the newly edited version of the Test1.html document to your machine.\nDiscuss how each of the edits suggested above modify the output with your neighbour or your tutor.\n\n\nMake sure you’ve got the basics of R Markdown, since this is the tool which you will use to write your final assignment (i.e. research report). If there is something not very clear, or you are curious about, feel free to ask your tutor. They will be happy to answer your questions.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#introduction",
    "href": "05-Lab5.html#introduction",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "In the previous labs you were exploring the 2012 Northern Ireland Life and Times Survey (NILT). You’ve learnt how to download, read and format the data. Also, you’ve learnt how to explore categorical and numeric data and a mix of them. In this lab, you will learn about how to efficiently report quantitative results directly from R, using R Markdown, which is used by many academics and professionals in a workplace setting to communicate quantitative findings to a wider audience. R Markdown is also what you will use to write your research report assignment for this course. So, Let’s dive in and learn more!",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#r-markdown",
    "href": "05-Lab5.html#r-markdown",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "R Markdown (Rmd) is a different type of file included in RStudio (and it is actually a different markup language). This allows you to generate reports in common file types, such as .html (the same one used for this lab workbook you’re reading right now), .pdf, or Word (.doc). The interesting thing is that the Rmd file allows you to integrate text, code, and plots directly into your report (so you do not have to copy and paste tables or graphs into a Word document, for example, which is often very messy and time-consuming). You have already seen how well this works in the lab workbooks so far, which are written entirely in R Markdown.\nThe basic components of an Rmd file are: the code, text and metadata. The code is integrated by blocks called ‘chunks’, and the metadata contains information to format the report. We believe the best way to learn is by doing it. So, let’s create your first Rmd document!\n\nWe will continue working in the same project called NILT in RStudio Cloud.\n\nPlease go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\nCreate a new Rmd file, this is similar as creating and R Script, from the ‘File’ tab on the top-left: File&gt;New File&gt;R Markdown... (Rstudio may ask to install some packages, click ‘Yes’);\nType ‘Test1’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML. Then, click ‘OK’.\nSave the Rmd file clicking on File&gt;Save as..., type Test1 in the ‘File name’ box, and click on the Save button.\n\nAfter completing the previous steps, your screen should look like this (You can minimize the console to expand pane 1):\n\n\n\n\nRmd file.\n\n\n\nNote that now you have two files open in Pane 1, one tab includes the R script that we created in the last lab (called Lab_3.R), and the other is the Rmd document that you just created.\nThe Rmd document Test1 contains an example by default. The first bit on the top enclosed by the dashes ---, contains the general metadata to format the output, as shown in the Figure @ref(fig:yaml-chunk). This bit is called YAML. In the default example, it contains the title, name of the author, data and the type of output (html). You can adjust this information directly by typing the relevant info (e.g. date or name).\n\n\n\n\nYAML and code chunk.\n\n\n\nBelow the YAML shown in Figure @ref(fig:yaml-chunk), there is another box. This is an R code ‘chunk’. To run a chunk of code individually (that is to visualize a partial result of an Rmd document), you can click on the green arrow pointed on the top-left of the first chunk.\nIn line 12, you have a second-level header, which contains the name of a section in the document. As you can see this is preceded by double hash tag ##. If you want a first-level header section, you would require only one hash tag like this #, and three for a third-level header. Finally the ‘Knit’ word is enclosed by double asterisk **. This is to format the characters enclosed in bold.\nIn line 26, you will see a chunk including a basic plot. Let’s check the results that this example in ‘Test1’ produce.\nTo render the document from Rmd to HTML, we need to Knit it by clicking on the icon shown below. Try it!\n\n\nKnit button\n\nRStudio may ask you if you want to update some packages, click ‘Yes’.\nAfter you knit the document, a window with the output will pop up automatically. As you can see, this document contains the main title, followed by your name and date, as specified in the YAML. Afterwards, there is a second-level header which includes the first section of this example document. Also, the word ‘Knit’ is shown in bold, as it was wrapped by double asterisk **.\nAn interesting thing is that we can integrate the result of our code in the output as we did with the second chunk (which starts in line 18). Here, we can use a summary of the data set cars (which is an in-built data set in R that contains only two variables).\n\nSimilarly, you can create a .pdf file. To do this, instead of clicking on the Knit icon directly, click on the black arrow next to it. Then click ‘Knit to PDF’. Try it and see the result.\nWhen you knitted the document, RStudio actually created a new .html or .pdf file with the same name as your Rmd document. You can confirm this in the ‘Files’ tab in Pane 4. You can download this file from the cloud to your local drive by clicking on the box of the output file. Then, click More &gt; Export... in the same pane (or click on the gear icon in Pane 4).\n\n\nIMPORTANT: Rmd files are different from simple R scripts. While everything you write in an R script is interpreted as code, only the bits within the code chunks will be interpreted as code in Rmd files. Everything else not within chunks in an Rmd file is interpreted as text.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#activity",
    "href": "05-Lab5.html#activity",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "In the Test1.Rmd file that you just created, do the following:\n\nChange the title of the document in the YAML to ‘My first R Markdown document’.\nIn the code chunk in line 21, replace the existing line (summary(car)) with the following: glimpse(iris).\nIn the code chunk called ‘pressure’, change echo=FALSE to echo=TRUE.\nAt the very bottom of the script, create a new paragraph and write one or two lines briefly describing how you think quantitative methods are improving your discipline (e.g. politics, sociology, social and public policy, or central and eastern European studies).\nKnit the document in html format.\nDownload the newly edited version of the Test1.html document to your machine.\nDiscuss how each of the edits suggested above modify the output with your neighbour or your tutor.\n\n\nMake sure you’ve got the basics of R Markdown, since this is the tool which you will use to write your final assignment (i.e. research report). If there is something not very clear, or you are curious about, feel free to ask your tutor. They will be happy to answer your questions.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "07-Lab7.html",
    "href": "07-Lab7.html",
    "title": "Correlation",
    "section": "",
    "text": "When conducting empirical research, we are often interested in associations between two variables, for example, personal income and attitudes towards migrants. In this lab we will focus on visualizing relationship between variables and how to measure it. In quantitative research, the main variable of interest in an analysis is called the dependent or response variable, and the second is known as the independent or explanatory. In the example above, we can think of personal income as the independent variable and attitudes as the dependent.\nThe relationship between variables can be positive, negative or non-existent. The figure below shows these type of relationships to different extents. The association is positive when one of the variables increases and the second variable tends to go in the same direction (that is increasing as well). The first plot on the left-hand side shows a strong positive relationship. As you can see, the points are closely clustered around the straight line. The next plot also shows a positive relationship. This time the relationship is moderate. Therefore, the points are more dispersed in relation to the line compared to the previous one.\n\n\n\n\n Types of correlation.\n\n\n\nThe plot in the middle, shows two variables that are not correlated. The location of the points is not following any pattern and the line is flat. By contrast, the last two plots on the right hand-side show a negative relationship. When the values on the X axis increase, the values on the Y axis tend to decrease.\n\nWe will continue working on the same RStudio Cloud project as in the previous session and using the 2012 Northern Ireland Life and Times Survey (NILT) data. To set the R environment please follow the next steps:\n\nPlease go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\nCreate a new Rmd file, type ‘Correlation analysis’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML.\nSave the Rmd document under the name ‘Lab7_correlation’.\nDelete all the contents in the Rmd default example with the exception of the first bit which contains the YAML and the first chunk, which contains the default chunk options (that is all from line 12 and on).\nIn the setup chunk, change echo from TRUE to FALSE in line 9 (this will hide the code for all chunks in your final document).\nWithin the first chunk, copy and paste the following code below line 9 knitr::opts_chunk$set(message = FALSE, warning = FALSE). This will hide the warnings and messages when you load the packages.\n\nIn the Rmd document insert a new a chunk, copy and paste the following code. Then, run the individual chunk by clicking on the green arrow on the top-right of the chunk.\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Load the data from the .rds file we created in lab 3\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nThis time we will use new variables from the survey. Therefore, we need to coerce them into their appropriate type first. Insert a second chunk, copy and paste the code below. Then, run the individual chunk.\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n# Migration\nnilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric)\n\nAlso, we will create a new variable called mig_per by summing the respondent’s opinion in relation to migration using the following variables: mil10yrs, miecono and micultur (see the documentation p. 14 here to know more about these variables). Again, insert a new chunk, copy and paste the code below, and run the individual chunk.\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) %&gt;%\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\n\nVisualizing two or more variables can help to uncover or understand the relationship between these variables. As briefly introduced in the previous session, different types of plots are appropriate for different types of variables. Therefore, we split the following sections according to the type of data to be analysed.\nYou do not need to run or reproduce the examples shown in the following sections in your R session with the exception of exercises that are under the activity headers.\n\nTo illustrate this type of correlation, let’s start with a relatively obvious but useful example. Suppose we are interested in how people choose their spouse or partner. The first characteristic that we might look at is age. We might suspect that there is a correlation between the nilt respondents’ own age and their partner’s age. Since both ages are numeric variables, a scatter plot is appropriate to visualise the correlation. To do this, let’s use the functions ggplot() and geom_point(). In aesthetics aes(), define the respondent’s age rage on the X axis and the respondent’s spouse/partner age spage on the Y axis. As a general convention in quantitative research, the response/dependent variable is visualised on the Y axis and the independent on the X axis (you do not need to copy and reproduce the example below).\n\nggplot(nilt, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nNote that in this plot the function geom_smooth() was used. This is to plot a straight line which describes the best all the points in the graph.\nFrom the plot above, we see that there is a strong positive correlation between the respondent’s age and their partner’s age. We see that for some individuals their partner’s age is older, whereas others is younger. Also, there are some dots that are far away from the straight line. For example, in one case the respondent is around 60 years old and the age of their partner is around 30 years old (can you find that dot on the plot?). These extreme values are known as outliers.\nWe may also suspect that the respondents’ sex is playing a role in this relationship. We can include this as a third variable in the plot by colouring the dots by the respondents’ sex. To do this, let’s specify the colour argument in aesthetics aes() with a categorical variable rsex.\n\nggplot(nilt, aes(x = rage, y = spage, colour = rsex)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"gray20\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nIn the previous plot, we included a line that describes what it would look like if the partner’s age were exactly the same as the respondent’s age. We observe a clear pattern in which female participants are on one side of the line and males on the other. As we can see, most female respondents tend to choose/have partners who are older, whereas males choose/have younger partners.\n\nIn the Lab7_correlation file, use the nilt data object to visualise the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text (outside the code chunks).\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function set the method argument to 'lm'.\nWhat type of relationship do you observe? Comment as text in the Rmd the overall result of the plot and whether this is in line with your previous expectation.\n\n\nAs briefly introduced in the last lab, correlations often occur between categorical and numeric data. A good way to observe the relationship between these type of variables is using a box plot. Which essentially shows the distribution of the numeric values by category/group.\nLet’s say we are interested in the relationship between education level and perception of migration. The variable highqual contains the respondent’s highest education qualification. Using ggplot(), we can situate mig_per on the X axis and highqual on the Y axis, and plot it with the geom_boxplot() function. Note that before passing the dataset to ggplot, we can filter out two categories of the variable highqual where education level is unknown (i.e. “Other, level unknown” or “Unclassified”).\n\nnilt %&gt;%\n  filter(highqual != \"Other, level unknown\" & highqual != \"Unclassified\") %&gt;%\n  ggplot(aes(x = mig_per, y = highqual)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFrom the plot above, we see that respondents with higher education level (on the bottom) appear to have more positive opinion on migration when compared to respondents with lower education level or no qualifications (on the top). Overall, the data shows a pattern that the lower one’s education level is, the worse their opinion towards migration is likely to be. Since education level is an ordinal variable, we can say this is a positive relationship.\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you can observe from the results as text in the Rmd file to introduce the plot.\n\nCreate a boxplot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the political party which the respondent identify with uninatid. Use ggplot() in combination with geom_boxplot(). Make sure to specify mig_per on the Y axis and uninatid on the X axis in aes().\nDo you think the opinion towards migration differs among the groups in the plot? Comment on the overall results in the Rmd document.\n\nSo far we have examined correlation by visualizing variables only. A useful practice in quantitative research is to actually measure the magnitude of the relationship between these variables. One common measure is the Pearson correlation coefficient. This measure results in a number that goes from -1 to 1. A coefficient below 0 implies a negative correlation whereas a coefficient over 0 a positive one. When the coefficient is close to positive one (1) or negative one (-1), it implies that the relationship is strong. By contrast, coefficients close to 0 indicate a weak relationship. This technique is appropriate to measure linear numeric relationships, which is when we have numeric variables with a normal distribution, e.g. age in our dataset.\nLet’s start measuring the relationship between the respondent’s age and their partner’s age. To do this in R, we should use the cor() function. In the R syntax, first we specify the variables separated by a comma. We need to be explicit by specifying the object name, the dollar sign, and the name of the variable, as shown below. Also, I set the use argument as 'pairwise.complete.obs'. This is because one or both of the variables contain more than one missing value. Therefore, we are telling R to use complete observations only.\n\ncor(nilt$rage, nilt$spage, use = \"pairwise.complete.obs\")\n\n[1] 0.9481297\n\n\nThe correlation coefficient between this variables is 0.95. This is close to positive 1. Therefore, it is a strong positive correlation. The result is completely in line with the plot above, since we saw how the dots were close to the straight line.\nWhat about the relationship between age and mig_per that you plotted earlier?\n\ncor(nilt$rage, nilt$mig_per, use = \"pairwise.complete.obs\")\n\n[1] -0.05680918\n\n\nThe coefficient is very close to 0, which means that the correlation is practically non-existent. The absence of correlation is also interesting in research. For instance, one might expect that younger people would be more open to migration. However, it seems that age does not play a role on people’s opinion about migration in NI according to this data.\nLet’s say that we are interested in the correlation between mig_per and all other numeric variables in the dataset. Instead of continuing computing the correlation one by one, we can run a correlation matrix. The code syntax can be read as follows: from the nilt data select these variables, then compute the correlation coefficient using complete cases, and then round the result to 3 decimals.\n\nnilt %&gt;%\n  select(mig_per, rage, spage, rhourswk, persinc2) %&gt;%\n  cor(use = \"pairwise.complete.obs\") %&gt;%\n  round(3)\n\n         mig_per   rage  spage rhourswk persinc2\nmig_per    1.000 -0.057 -0.132    0.082    0.228\nrage      -0.057  1.000  0.948   -0.013   -0.036\nspage     -0.132  0.948  1.000   -0.182   -0.090\nrhourswk   0.082 -0.013 -0.182    1.000    0.383\npersinc2   0.228 -0.036 -0.090    0.383    1.000\n\n\nFrom the result above, we have a correlation matrix that computes the Person correlation coefficient for the selected variables. In the first row we have migration perception. You will notice that the first value is 1.00, this is because it is measuring the correlation against the same variable (i.e. itself). The next value in the first row is age, which is nearly 0. The next variables also result in low coefficients, with the exception of the personal income, where we see a moderate/low positive correlation. This can be interpreted that respondents with high income are associated with more positive opinion towards migration compared to low-income respondents.\n\n\nInsert a new chunk in your Rmd file;\nUsing the nilt data object, compute a correlation matrix using the following variables: rage, persinc2, mil10yrs, miecono and micultur, setting the use argument to 'pairwise.complete.obs' and rounding the result to 3 decimals;\nRun the chunk individually and comment whether personal income or age is correlated with the perception of migrants in relation to the specific aspects asked in the variables measured (consult the documentation in p. 14 to get a description of these variables);\nKnit the Lab7_correlation Rmd document to .html or .pdf. The output document will automatically be saved in your project.\nDiscuss your previous results with your neighbour or tutor.",
    "crumbs": [
      "**Lab 7** Correlation"
    ]
  },
  {
    "objectID": "07-Lab7.html#what-is-correlation",
    "href": "07-Lab7.html#what-is-correlation",
    "title": "Correlation",
    "section": "",
    "text": "When conducting empirical research, we are often interested in associations between two variables, for example, personal income and attitudes towards migrants. In this lab we will focus on visualizing relationship between variables and how to measure it. In quantitative research, the main variable of interest in an analysis is called the dependent or response variable, and the second is known as the independent or explanatory. In the example above, we can think of personal income as the independent variable and attitudes as the dependent.\nThe relationship between variables can be positive, negative or non-existent. The figure below shows these type of relationships to different extents. The association is positive when one of the variables increases and the second variable tends to go in the same direction (that is increasing as well). The first plot on the left-hand side shows a strong positive relationship. As you can see, the points are closely clustered around the straight line. The next plot also shows a positive relationship. This time the relationship is moderate. Therefore, the points are more dispersed in relation to the line compared to the previous one.\n\n\n\n\n Types of correlation.\n\n\n\nThe plot in the middle, shows two variables that are not correlated. The location of the points is not following any pattern and the line is flat. By contrast, the last two plots on the right hand-side show a negative relationship. When the values on the X axis increase, the values on the Y axis tend to decrease.\n\nWe will continue working on the same RStudio Cloud project as in the previous session and using the 2012 Northern Ireland Life and Times Survey (NILT) data. To set the R environment please follow the next steps:\n\nPlease go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\nCreate a new Rmd file, type ‘Correlation analysis’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML.\nSave the Rmd document under the name ‘Lab7_correlation’.\nDelete all the contents in the Rmd default example with the exception of the first bit which contains the YAML and the first chunk, which contains the default chunk options (that is all from line 12 and on).\nIn the setup chunk, change echo from TRUE to FALSE in line 9 (this will hide the code for all chunks in your final document).\nWithin the first chunk, copy and paste the following code below line 9 knitr::opts_chunk$set(message = FALSE, warning = FALSE). This will hide the warnings and messages when you load the packages.\n\nIn the Rmd document insert a new a chunk, copy and paste the following code. Then, run the individual chunk by clicking on the green arrow on the top-right of the chunk.\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Load the data from the .rds file we created in lab 3\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nThis time we will use new variables from the survey. Therefore, we need to coerce them into their appropriate type first. Insert a second chunk, copy and paste the code below. Then, run the individual chunk.\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n# Migration\nnilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric)\n\nAlso, we will create a new variable called mig_per by summing the respondent’s opinion in relation to migration using the following variables: mil10yrs, miecono and micultur (see the documentation p. 14 here to know more about these variables). Again, insert a new chunk, copy and paste the code below, and run the individual chunk.\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) %&gt;%\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\n\nVisualizing two or more variables can help to uncover or understand the relationship between these variables. As briefly introduced in the previous session, different types of plots are appropriate for different types of variables. Therefore, we split the following sections according to the type of data to be analysed.\nYou do not need to run or reproduce the examples shown in the following sections in your R session with the exception of exercises that are under the activity headers.\n\nTo illustrate this type of correlation, let’s start with a relatively obvious but useful example. Suppose we are interested in how people choose their spouse or partner. The first characteristic that we might look at is age. We might suspect that there is a correlation between the nilt respondents’ own age and their partner’s age. Since both ages are numeric variables, a scatter plot is appropriate to visualise the correlation. To do this, let’s use the functions ggplot() and geom_point(). In aesthetics aes(), define the respondent’s age rage on the X axis and the respondent’s spouse/partner age spage on the Y axis. As a general convention in quantitative research, the response/dependent variable is visualised on the Y axis and the independent on the X axis (you do not need to copy and reproduce the example below).\n\nggplot(nilt, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nNote that in this plot the function geom_smooth() was used. This is to plot a straight line which describes the best all the points in the graph.\nFrom the plot above, we see that there is a strong positive correlation between the respondent’s age and their partner’s age. We see that for some individuals their partner’s age is older, whereas others is younger. Also, there are some dots that are far away from the straight line. For example, in one case the respondent is around 60 years old and the age of their partner is around 30 years old (can you find that dot on the plot?). These extreme values are known as outliers.\nWe may also suspect that the respondents’ sex is playing a role in this relationship. We can include this as a third variable in the plot by colouring the dots by the respondents’ sex. To do this, let’s specify the colour argument in aesthetics aes() with a categorical variable rsex.\n\nggplot(nilt, aes(x = rage, y = spage, colour = rsex)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"gray20\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nIn the previous plot, we included a line that describes what it would look like if the partner’s age were exactly the same as the respondent’s age. We observe a clear pattern in which female participants are on one side of the line and males on the other. As we can see, most female respondents tend to choose/have partners who are older, whereas males choose/have younger partners.\n\nIn the Lab7_correlation file, use the nilt data object to visualise the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text (outside the code chunks).\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function set the method argument to 'lm'.\nWhat type of relationship do you observe? Comment as text in the Rmd the overall result of the plot and whether this is in line with your previous expectation.\n\n\nAs briefly introduced in the last lab, correlations often occur between categorical and numeric data. A good way to observe the relationship between these type of variables is using a box plot. Which essentially shows the distribution of the numeric values by category/group.\nLet’s say we are interested in the relationship between education level and perception of migration. The variable highqual contains the respondent’s highest education qualification. Using ggplot(), we can situate mig_per on the X axis and highqual on the Y axis, and plot it with the geom_boxplot() function. Note that before passing the dataset to ggplot, we can filter out two categories of the variable highqual where education level is unknown (i.e. “Other, level unknown” or “Unclassified”).\n\nnilt %&gt;%\n  filter(highqual != \"Other, level unknown\" & highqual != \"Unclassified\") %&gt;%\n  ggplot(aes(x = mig_per, y = highqual)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFrom the plot above, we see that respondents with higher education level (on the bottom) appear to have more positive opinion on migration when compared to respondents with lower education level or no qualifications (on the top). Overall, the data shows a pattern that the lower one’s education level is, the worse their opinion towards migration is likely to be. Since education level is an ordinal variable, we can say this is a positive relationship.\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you can observe from the results as text in the Rmd file to introduce the plot.\n\nCreate a boxplot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the political party which the respondent identify with uninatid. Use ggplot() in combination with geom_boxplot(). Make sure to specify mig_per on the Y axis and uninatid on the X axis in aes().\nDo you think the opinion towards migration differs among the groups in the plot? Comment on the overall results in the Rmd document.",
    "crumbs": [
      "**Lab 7** Correlation"
    ]
  },
  {
    "objectID": "07-Lab7.html#measuring-correlation",
    "href": "07-Lab7.html#measuring-correlation",
    "title": "Correlation",
    "section": "",
    "text": "So far we have examined correlation by visualizing variables only. A useful practice in quantitative research is to actually measure the magnitude of the relationship between these variables. One common measure is the Pearson correlation coefficient. This measure results in a number that goes from -1 to 1. A coefficient below 0 implies a negative correlation whereas a coefficient over 0 a positive one. When the coefficient is close to positive one (1) or negative one (-1), it implies that the relationship is strong. By contrast, coefficients close to 0 indicate a weak relationship. This technique is appropriate to measure linear numeric relationships, which is when we have numeric variables with a normal distribution, e.g. age in our dataset.\nLet’s start measuring the relationship between the respondent’s age and their partner’s age. To do this in R, we should use the cor() function. In the R syntax, first we specify the variables separated by a comma. We need to be explicit by specifying the object name, the dollar sign, and the name of the variable, as shown below. Also, I set the use argument as 'pairwise.complete.obs'. This is because one or both of the variables contain more than one missing value. Therefore, we are telling R to use complete observations only.\n\ncor(nilt$rage, nilt$spage, use = \"pairwise.complete.obs\")\n\n[1] 0.9481297\n\n\nThe correlation coefficient between this variables is 0.95. This is close to positive 1. Therefore, it is a strong positive correlation. The result is completely in line with the plot above, since we saw how the dots were close to the straight line.\nWhat about the relationship between age and mig_per that you plotted earlier?\n\ncor(nilt$rage, nilt$mig_per, use = \"pairwise.complete.obs\")\n\n[1] -0.05680918\n\n\nThe coefficient is very close to 0, which means that the correlation is practically non-existent. The absence of correlation is also interesting in research. For instance, one might expect that younger people would be more open to migration. However, it seems that age does not play a role on people’s opinion about migration in NI according to this data.\nLet’s say that we are interested in the correlation between mig_per and all other numeric variables in the dataset. Instead of continuing computing the correlation one by one, we can run a correlation matrix. The code syntax can be read as follows: from the nilt data select these variables, then compute the correlation coefficient using complete cases, and then round the result to 3 decimals.\n\nnilt %&gt;%\n  select(mig_per, rage, spage, rhourswk, persinc2) %&gt;%\n  cor(use = \"pairwise.complete.obs\") %&gt;%\n  round(3)\n\n         mig_per   rage  spage rhourswk persinc2\nmig_per    1.000 -0.057 -0.132    0.082    0.228\nrage      -0.057  1.000  0.948   -0.013   -0.036\nspage     -0.132  0.948  1.000   -0.182   -0.090\nrhourswk   0.082 -0.013 -0.182    1.000    0.383\npersinc2   0.228 -0.036 -0.090    0.383    1.000\n\n\nFrom the result above, we have a correlation matrix that computes the Person correlation coefficient for the selected variables. In the first row we have migration perception. You will notice that the first value is 1.00, this is because it is measuring the correlation against the same variable (i.e. itself). The next value in the first row is age, which is nearly 0. The next variables also result in low coefficients, with the exception of the personal income, where we see a moderate/low positive correlation. This can be interpreted that respondents with high income are associated with more positive opinion towards migration compared to low-income respondents.\n\n\nInsert a new chunk in your Rmd file;\nUsing the nilt data object, compute a correlation matrix using the following variables: rage, persinc2, mil10yrs, miecono and micultur, setting the use argument to 'pairwise.complete.obs' and rounding the result to 3 decimals;\nRun the chunk individually and comment whether personal income or age is correlated with the perception of migrants in relation to the specific aspects asked in the variables measured (consult the documentation in p. 14 to get a description of these variables);\nKnit the Lab7_correlation Rmd document to .html or .pdf. The output document will automatically be saved in your project.\nDiscuss your previous results with your neighbour or tutor.",
    "crumbs": [
      "**Lab 7** Correlation"
    ]
  },
  {
    "objectID": "Answers.html",
    "href": "Answers.html",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "This chapter presents the suggested R code to answer the workbook activities and exercises throughout the course labs in Quantitative Research Methods for Social Sciences. This covers from Lab 3 to Lab 9.\nBefore looking at the answers, try asking your tutor for help. Also, we strongly recommend web resources, such as https://stackoverflow.com/ or https://community.rstudio.com/. By solving the issues, you will learn a lot! ;)\n\n\n## Load the packages\nlibrary(tidyverse)\n\n# Read the data from the .rds file\nclean_data &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Glimpse clean_data\nglimpse(clean_data)\n\n# Glimpse the nilt data\nglimpse(nilt)\n\n\nPreamble code\n\n## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nFrom your RStudio Cloud script, do the following activities using the data stored in the nilt_subset object:\n\nCreate a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\n\n\n# Load the vtable package to create summary tables\nlibrary(vtable)\n# Create table\nsumtable(nilt_subset, vars = c(\"uninatid\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nuninatid\n1183\n\n\n\n... Unionist\n348\n29%\n\n\n... Nationalist\n255\n22%\n\n\n... Neither\n580\n49%\n\n\n\n\n\n\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\n\n\nsumtable(nilt_subset, vars = c(\"religcat\"), group = \"uninatid\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nuninatid\n\n\nUnionist\n\n\nNationalist\n\n\nNeither\n\n\n\nVariable\nN\nPercent\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n341\n\n255\n\n558\n\n\n\n... Catholic\n2\n1%\n245\n96%\n238\n43%\n\n\n... Protestant\n305\n89%\n5\n2%\n180\n32%\n\n\n... No religion\n34\n10%\n5\n2%\n140\n25%\n\n\n\n\n\n\nUsing the data in the nilt_subset object, complete the following activities.\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions;\n\n\nhist(nilt_subset$persinc2)\n\n\n\n\n\n\n\n\nCreate a summary of the personal income persinc2 variable, using the sumtable() function.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\npersinc2\n897\n16395\n13466\n260\n6760\n22100\n75000\n\n\n\n\n\nCompute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"), group = \"ruhappy\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nruhappy\n\n\nVery happy\n\n\nFairly happy\n\n\nNot very happy\n\n\nNot at all happy\n\n\nCan't choose\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\n\n\n\npersinc2\n305\n17569\n13429\n500\n16262\n14015\n62\n13445\n9754\n9\n12451\n11501\n15\n11457\n6113\n\n\n\n\n\nPreamble code\n\n## Load the packages\nlibrary(tidyverse)\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Create subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nUsing the nilt_subset object, complete the tasks below in the Rmd file ‘Lab_4’, which you created earlier. Insert a new chunk for each of these activities and include brief comments as text in the Rmd document to introduce the plots and discuss the results (tip —leave an empty line between your text and the next chunk to separate the description and the plots):\n\nCreate a first-level header to start a section called “Categorical analysis”;\n\n\n## Categorical analysis\n\n\nCreate a simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid;\n\n\nggplot(nilt_subset, aes(x = uninatid)) +\n  geom_bar() +\n  labs(title = \"Political afiliation\", x = \"Party\")\n\n\n\n\n\n\n\n\nBased on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables;\n\n\nggplot(nilt_subset, aes(x = uninatid, fill = religcat)) +\n  geom_bar() +\n  labs(\n    title = \"Political affiliation by religion\",\n    x = \"Party\", fill = \"Religion\"\n  )\n\n\n\n\n\n\n\n\nCreate a new first-level header to start a section called “Numeric analysis”;\n\n\n## Numeric analysis\n\n\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\n\n\nggplot(nilt_subset, aes(x = rhourswk, y = persinc2)) +\n  geom_point() +\n  labs(\n    title = \"Income and number of hours worked a week\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime);\n\n\nggplot(nilt_subset, aes(x = ruhappy, y = persinc2)) +\n  geom_boxplot() +\n  labs(\n    title = \"Personal income and happiness\",\n    x = \"Happiness level\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nBriefly comment each of the plots as text in your Rmd file;\nKnit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish.\n\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Load the data from the .rds file we created in lab 3\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n# Migration\nnilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric)\n\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) %&gt;%\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text in the Rmd file (remember to leave an empty line between your text and the chunk).\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use the ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function, set the method argument to 'lm'.\n\n\nggplot(nilt, aes(x = rage, mig_per)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Perception of migration vs age\",\n    x = \"Respondent age\", y = \"Perception of migration (0-30)\"\n  )\n\n\n\n\n\n\n\n\nWhat type of relationship do you observe? Comment the overall result of the plot and whether this is in line with your previous expectation.\n\n\n## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\n\n\nUse the nilt data set object in your linear_model_intro file to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point(), jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nPrint the summary of m3 using the summary() function.\n\n\nsummary(m3)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43694  -8148  -3070   4990  58249 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5170.4     1966.2    2.63  0.00884 ** \nrhourswk       463.2       52.4    8.84  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13860 on 455 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.1466,    Adjusted R-squared:  0.1447 \nF-statistic: 78.15 on 1 and 455 DF,  p-value: &lt; 2.2e-16\n\n\n\nIs the relationship of hours worked a week significant? Re: Yes. The p-value (fourth column of the ‘Coefficients’ table) is lower than 0.05.\nWhat is the adjusted r-squared? How would you interpret it? Re: the adjusted R-squared is 0.14. This can be interpreted in terms of percentage, e.g. 14% of the variance in personal income can be explained by the number of hours worked a week.\nWhat is the sample size to fit the model? Re: The total number of observations in the data set is 1,204 and the model summary says that 747 observations were deleted due to missingness. Therefore, the sample size is 457 (1204-747).\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\n\n\n5170.4 + 463.2 * 30\n\n[1] 19066.4\n\n\n\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\n\n\nhist(residuals(m3))\n\n\n\n\n\n\n\nOverall, the residuals look normally distributed with the exception of the values to the right-hand side of the plot (between 40000 and 60000).\n\n\n\nLoad the packages, and the data that you will need in your file using the code below:\n\n\n## Load the packages\nlibrary(moderndive)\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\n\n\ntable(nilt$highqual)\n\n\nDegree level or higher       Higher education   GCE A level or equiv \n                   230                    102                    243 \n     GCSE A-C or equiv      GCSE D-G or equiv      No qualifications \n                   185                     82                    281 \n  Other, level unknown           Unclassified \n                    27                     54 \n\n\n\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2, color = highqual)) +\n  geom_point(position = \"jitter\") +\n  moderndive::geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Personal income\",\n    subtitle = \"Personal income and number of hours worked a week by education level\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\",\n    color = \"Highest education level\"\n  )\n\n\n\n\n\n\n\n\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary.\n\n\nm4 &lt;- lm(persinc2 ~ rhourswk + rage + highqual, nilt)\nsummary(m4)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk + rage + highqual, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-36228  -6425  -1411   4635  54749 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    3904.64    2714.25   1.439    0.151    \nrhourswk                        444.79      45.32   9.814  &lt; 2e-16 ***\nrage                            236.59      48.47   4.881 1.47e-06 ***\nhighqualHigher education      -8164.91    1939.50  -4.210 3.09e-05 ***\nhighqualGCE A level or equiv -12439.26    1563.42  -7.956 1.47e-14 ***\nhighqualGCSE A-C or equiv    -13037.47    1703.07  -7.655 1.20e-13 ***\nhighqualGCSE D-G or equiv    -11622.07    2665.38  -4.360 1.61e-05 ***\nhighqualNo qualifications    -12968.10    2339.78  -5.542 5.11e-08 ***\nhighqualOther, level unknown  15445.70    3334.75   4.632 4.76e-06 ***\nhighqualUnclassified         -12399.58    2786.16  -4.450 1.08e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11830 on 447 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.3887,    Adjusted R-squared:  0.3764 \nF-statistic: 31.58 on 9 and 447 DF,  p-value: &lt; 2.2e-16\n\n\n\nComment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model. Re: All the independent variables including the number of hours worked a week, age, and all the categories of highest qualification level compared to ‘Degree or higher’ are significant to predict personal income in the model ‘m4’, considering that the p-value is lower than 0.05. We can confirm this from the fourth column of the ‘Coefficients’ table. The adjusted R-squared of the model is 0.37. This means that 37.6% of the variance in personal income can be explained by these variables. The size of the sample used to fit this model is 457, considering that the ‘nilt’ data set contains 1204 observations but 747 were deleted due to missingness (1204 - 747).\nPlot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model?\n\n\nhist(residuals(m4))\n\n\n\n\n\n\n\nThe distribution of the residuals in ‘m4’ look overall normally distributed. However, the distribution is not perfectly symmetric. Therefore, we would advice to conduct further checks to test the linear model assumptions.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#introduction",
    "href": "Answers.html#introduction",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "This chapter presents the suggested R code to answer the workbook activities and exercises throughout the course labs in Quantitative Research Methods for Social Sciences. This covers from Lab 3 to Lab 9.\nBefore looking at the answers, try asking your tutor for help. Also, we strongly recommend web resources, such as https://stackoverflow.com/ or https://community.rstudio.com/. By solving the issues, you will learn a lot! ;)",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-3.-data-wrangling",
    "href": "Answers.html#lab-3.-data-wrangling",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "## Load the packages\nlibrary(tidyverse)\n\n# Read the data from the .rds file\nclean_data &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Glimpse clean_data\nglimpse(clean_data)\n\n# Glimpse the nilt data\nglimpse(nilt)",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-4.-exploratory-data-analysis",
    "href": "Answers.html#lab-4.-exploratory-data-analysis",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "Preamble code\n\n## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nFrom your RStudio Cloud script, do the following activities using the data stored in the nilt_subset object:\n\nCreate a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\n\n\n# Load the vtable package to create summary tables\nlibrary(vtable)\n# Create table\nsumtable(nilt_subset, vars = c(\"uninatid\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nuninatid\n1183\n\n\n\n... Unionist\n348\n29%\n\n\n... Nationalist\n255\n22%\n\n\n... Neither\n580\n49%\n\n\n\n\n\n\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\n\n\nsumtable(nilt_subset, vars = c(\"religcat\"), group = \"uninatid\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nuninatid\n\n\nUnionist\n\n\nNationalist\n\n\nNeither\n\n\n\nVariable\nN\nPercent\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n341\n\n255\n\n558\n\n\n\n... Catholic\n2\n1%\n245\n96%\n238\n43%\n\n\n... Protestant\n305\n89%\n5\n2%\n180\n32%\n\n\n... No religion\n34\n10%\n5\n2%\n140\n25%\n\n\n\n\n\n\nUsing the data in the nilt_subset object, complete the following activities.\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions;\n\n\nhist(nilt_subset$persinc2)\n\n\n\n\n\n\n\n\nCreate a summary of the personal income persinc2 variable, using the sumtable() function.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\npersinc2\n897\n16395\n13466\n260\n6760\n22100\n75000\n\n\n\n\n\nCompute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"), group = \"ruhappy\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nruhappy\n\n\nVery happy\n\n\nFairly happy\n\n\nNot very happy\n\n\nNot at all happy\n\n\nCan't choose\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\n\n\n\npersinc2\n305\n17569\n13429\n500\n16262\n14015\n62\n13445\n9754\n9\n12451\n11501\n15\n11457\n6113",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-6.-visual-exploratory-analysis",
    "href": "Answers.html#lab-6.-visual-exploratory-analysis",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "Preamble code\n\n## Load the packages\nlibrary(tidyverse)\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Create subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nUsing the nilt_subset object, complete the tasks below in the Rmd file ‘Lab_4’, which you created earlier. Insert a new chunk for each of these activities and include brief comments as text in the Rmd document to introduce the plots and discuss the results (tip —leave an empty line between your text and the next chunk to separate the description and the plots):\n\nCreate a first-level header to start a section called “Categorical analysis”;\n\n\n## Categorical analysis\n\n\nCreate a simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid;\n\n\nggplot(nilt_subset, aes(x = uninatid)) +\n  geom_bar() +\n  labs(title = \"Political afiliation\", x = \"Party\")\n\n\n\n\n\n\n\n\nBased on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables;\n\n\nggplot(nilt_subset, aes(x = uninatid, fill = religcat)) +\n  geom_bar() +\n  labs(\n    title = \"Political affiliation by religion\",\n    x = \"Party\", fill = \"Religion\"\n  )\n\n\n\n\n\n\n\n\nCreate a new first-level header to start a section called “Numeric analysis”;\n\n\n## Numeric analysis\n\n\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\n\n\nggplot(nilt_subset, aes(x = rhourswk, y = persinc2)) +\n  geom_point() +\n  labs(\n    title = \"Income and number of hours worked a week\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime);\n\n\nggplot(nilt_subset, aes(x = ruhappy, y = persinc2)) +\n  geom_boxplot() +\n  labs(\n    title = \"Personal income and happiness\",\n    x = \"Happiness level\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nBriefly comment each of the plots as text in your Rmd file;\nKnit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-7.-correlation",
    "href": "Answers.html#lab-7.-correlation",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Load the data from the .rds file we created in lab 3\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n# Migration\nnilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric)\n\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) %&gt;%\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text in the Rmd file (remember to leave an empty line between your text and the chunk).\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use the ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function, set the method argument to 'lm'.\n\n\nggplot(nilt, aes(x = rage, mig_per)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Perception of migration vs age\",\n    x = \"Respondent age\", y = \"Perception of migration (0-30)\"\n  )\n\n\n\n\n\n\n\n\nWhat type of relationship do you observe? Comment the overall result of the plot and whether this is in line with your previous expectation.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-8.-linear-model.-simple-linear-regression",
    "href": "Answers.html#lab-8.-linear-model.-simple-linear-regression",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\n\n\nUse the nilt data set object in your linear_model_intro file to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point(), jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nPrint the summary of m3 using the summary() function.\n\n\nsummary(m3)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43694  -8148  -3070   4990  58249 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5170.4     1966.2    2.63  0.00884 ** \nrhourswk       463.2       52.4    8.84  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13860 on 455 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.1466,    Adjusted R-squared:  0.1447 \nF-statistic: 78.15 on 1 and 455 DF,  p-value: &lt; 2.2e-16\n\n\n\nIs the relationship of hours worked a week significant? Re: Yes. The p-value (fourth column of the ‘Coefficients’ table) is lower than 0.05.\nWhat is the adjusted r-squared? How would you interpret it? Re: the adjusted R-squared is 0.14. This can be interpreted in terms of percentage, e.g. 14% of the variance in personal income can be explained by the number of hours worked a week.\nWhat is the sample size to fit the model? Re: The total number of observations in the data set is 1,204 and the model summary says that 747 observations were deleted due to missingness. Therefore, the sample size is 457 (1204-747).\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\n\n\n5170.4 + 463.2 * 30\n\n[1] 19066.4\n\n\n\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\n\n\nhist(residuals(m3))\n\n\n\n\n\n\n\nOverall, the residuals look normally distributed with the exception of the values to the right-hand side of the plot (between 40000 and 60000).",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-9.-multivariate-linear-model",
    "href": "Answers.html#lab-9.-multivariate-linear-model",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "Load the packages, and the data that you will need in your file using the code below:\n\n\n## Load the packages\nlibrary(moderndive)\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\n\n\ntable(nilt$highqual)\n\n\nDegree level or higher       Higher education   GCE A level or equiv \n                   230                    102                    243 \n     GCSE A-C or equiv      GCSE D-G or equiv      No qualifications \n                   185                     82                    281 \n  Other, level unknown           Unclassified \n                    27                     54 \n\n\n\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2, color = highqual)) +\n  geom_point(position = \"jitter\") +\n  moderndive::geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Personal income\",\n    subtitle = \"Personal income and number of hours worked a week by education level\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\",\n    color = \"Highest education level\"\n  )\n\n\n\n\n\n\n\n\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary.\n\n\nm4 &lt;- lm(persinc2 ~ rhourswk + rage + highqual, nilt)\nsummary(m4)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk + rage + highqual, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-36228  -6425  -1411   4635  54749 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    3904.64    2714.25   1.439    0.151    \nrhourswk                        444.79      45.32   9.814  &lt; 2e-16 ***\nrage                            236.59      48.47   4.881 1.47e-06 ***\nhighqualHigher education      -8164.91    1939.50  -4.210 3.09e-05 ***\nhighqualGCE A level or equiv -12439.26    1563.42  -7.956 1.47e-14 ***\nhighqualGCSE A-C or equiv    -13037.47    1703.07  -7.655 1.20e-13 ***\nhighqualGCSE D-G or equiv    -11622.07    2665.38  -4.360 1.61e-05 ***\nhighqualNo qualifications    -12968.10    2339.78  -5.542 5.11e-08 ***\nhighqualOther, level unknown  15445.70    3334.75   4.632 4.76e-06 ***\nhighqualUnclassified         -12399.58    2786.16  -4.450 1.08e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11830 on 447 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.3887,    Adjusted R-squared:  0.3764 \nF-statistic: 31.58 on 9 and 447 DF,  p-value: &lt; 2.2e-16\n\n\n\nComment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model. Re: All the independent variables including the number of hours worked a week, age, and all the categories of highest qualification level compared to ‘Degree or higher’ are significant to predict personal income in the model ‘m4’, considering that the p-value is lower than 0.05. We can confirm this from the fourth column of the ‘Coefficients’ table. The adjusted R-squared of the model is 0.37. This means that 37.6% of the variance in personal income can be explained by these variables. The size of the sample used to fit this model is 457, considering that the ‘nilt’ data set contains 1204 observations but 747 were deleted due to missingness (1204 - 747).\nPlot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model?\n\n\nhist(residuals(m4))\n\n\n\n\n\n\n\nThe distribution of the residuals in ‘m4’ look overall normally distributed. However, the distribution is not perfectly symmetric. Therefore, we would advice to conduct further checks to test the linear model assumptions.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "03-Lab3.html",
    "href": "03-Lab3.html",
    "title": "Data wrangling",
    "section": "",
    "text": "Welcome to Lab 3!\nIn our previous session we learned about R packages, including how to install and load them. We talked about the main types of data used in social science research and how to represent them in R. We also played around with some datasets using some key functions, such as: filter(), select(), and mutate(). In this session, we will build of these principles and learn how to import data in R, clean and format the data using a real-world dataset. This is a common and important phase in quantitative research.\n\n\n\n\n\n\nOverview\n\n\n\nBy the end of this lab you will know how to:\n\nset up an RStudio project file from scratch\ncreate an R script to download, wrangle, and save a dataset\nload a saved dataset within an R Markdown file\nwork with the dataset used in the formative and summative assessments\n\n\n\n\nToday, we will be working with data generated by the Access Research Knowledge (ARK) hub. ARK conducts a series of surveys about society and life in Northern Ireland. For this lab, we will be working with the results of the Northern Ireland Life and Times Survey (NILT) in the year 2012. In particular, we will be using a teaching dataset that focuses on community relations and political attitudes. This includes background information of the participants and their household.\n\nWe will continue using Posit Cloud, as we did in the previous labs. This time though we are going to make a new RStudio project from scratch.\nWithin your “Lab Group …” workspace in Posit Cloud (if you have not joined a shared space, follow the instructions in Lab 2) and:\n\nClick the blue ‘New Project’ button in the top-right.\nFrom the list of options select ‘New RStudio Project’.\n\n\nOnce the project has loaded, click on ‘Untitled Project’ at the top of the screen.\n\nYou can now give your project a name. This is how it will appear in your list of projects in your lab group workspace. Let’s name it “NILT” as we will use this project to work with the NILT data. Type the name and hit Enter to confirm.\n\n\nAlthough this project is just for the labs, we are going to set it up in a ‘reproducible’ way. This means anyone with a copy of our project would be able to run the code and receive the same results as us. Last week, we downloaded a data set using the Console. This week we will instead cover one way to include the code used to download and wrangle our data in a file separate to the R Markdown files we use for the analysis.1\n(As you will see with the RStudio templates used for the assessments, it is possible to share an RStudio Project with the data. However, that is not always possible or desirable. For instance, the data whilst available for anyone to download from an organisation’s website or data archive may have license restrictions prohibiting making a version of it available elsewhere.)\nTo do this we will use an R script file. In our first two labs we used R Markdown, which lets us create code chunks for adding our R code. An R script file is simply a file that contains only R code, like a giant code chunk.\nBy convention, R scripts for doing set up, data prep, and similar are placed in an ‘R’ subfolder. To create one:\n\nIn the “Files” pane (bottom-right) click the folder icon that also has a green circle with white plus in it.\n\n\nThen, within the ‘New Folder’ dialog that will pop up:\n\nType a single letter capital ‘R’ as the name.\nClick the ‘OK’ button to confirm creation of the folder.\n\n\nYou should now see the new folder in the bottom-right panel, so -\n\nClick the ‘R’ folder to navigate into it (Note, you need to click the ‘R’ and not the folder icon.)\n\n\nYou should now see an empty folder, and can double-check you are in the right folder by looking at the navigation bar which should be “Cloud &gt; project &gt; R”.\n\nOK, now:\n\nClick the white document icon that is to the right of the new folder icon you clicked before. (See screenshot below!)\nFrom the list of options click ‘R Script’\n\n\nThat will bring up a ‘Create a New File in Current Directory’ dialog:\n\nName your file 01_prep_nilt_2012.R\n\nClick the ‘OK’ button to confirm creation of the script\n\n\nThe file should then auto open in the top-left pane.\n\nLastly, before we move to writing our R script, let’s navigate back to the top-level folder of our project in the ‘Files’ pane (bottom-right). To navigate back up to the top-level folder, click either the ‘..’ at the top of the folder content list or in the navigation bar click the ‘project’ text.\n\n\nBack to our top-left pane with our R script. It is good practice to include some key information in this file, such as what it does, when it was written, and who wrote it. Given R script is all R code, we can add this info using comments. As a reminder, any line in R that begins with a ‘#’ is treated as a comment and will not be run as part of the code.\nBelow is a template you can use.\n\n## Download and process NILT 2012 survey\n\n# Date: [add today's date here]\n# Author: [add your name here]\n\nThe ‘##’, with an extra ‘#’, at the top doesn’t do anything extra or different, we just include it to make visually clear this is the title / short description of what the script does.\nNext, it is good practice to include details for the data set being used, especially important if the data is not our own to ensure proper attribution. In addition to being good practice, it is often a condition for data sets where it is permitted to re-share them elsewhere that you can do so only as long as you include an attribution (i.e. a reference).\nLet’s then add a reference section. (Feel free to use the clipboard button in top-right of the box below to copy and paste the text.)\n\n# References  -----------------------------------------------------------------\n\n# https://www.ark.ac.uk/teaching/\n# 2012 Good relations: ARK. Northern Ireland Life and Times Survey Teaching Dataset 2012, Good relations [computer file]. ARK www.ark.ac.uk/nilt [distributor], March 2014.\n\nThe ----- after # References again doesn’t do anything aside from help make the section visually distinct. This may seem odd at the moment, but as the file grows in length, the importance of making it easy to find sections quickly from visual glance starts to make sense.\nOK, let’s now install packages that are needed for our project. Remember we need to do this every time we create a new project on Posit Cloud. If using RStudio Desktop, we would only need to install packages once per device we are using rather than per project.\nHowever, compared to last week, we want to set up our project so that anyone can run the code and achieve the same results. (If we run into irrepairable errors we may also want to re-run our R scipt to ‘reset’ our project). We will not know though what packages someone will already have installed or not.\nThankfully, we can write R code in our script that will check whether specific packages are already installed, and then install any packages needed that are missing.\nFirst, add a comment to make clear there is a new section:\n\n# Packages -------------------------------------------------------------------\n\nNext, add the following comment and code:\n\n# Install tidyverse if needed\nif (!\"tidyverse\" %in% rownames(installed.packages())) install.packages(\"tidyverse\")\n\nThis code may look complex, but it is largely the number of brackets making it looks more complex than it actually is:\n\n\nif checks whether a given object is TRUE or FALSE, and is used in format if x y, where if x returns TRUE then y code is run.\n\n(...) we use to enclose a chunk of code to make clear this is our x.\n\n!\"tidyverse\" %in% uses the NOT ! logical operator and the %in% operator, which is equivalent then of saying “If the word ‘tidyverse’ is not in…”\n\nrownames() gets the label names for rows in a data frame or matrix (a matrix is like a simpler data frame).\n\ninstalled.packages() (note installed) is a function that returns a matrix with all packages currently installed on the system, so within Posit Cloud it returns all the installed packages within the current project. Each row’s label name matches the installed package name.\n\ninstall.packages(\"tidyverse\") as we know already from last week, this code installs the tidyverse package.\n\nPutting it all today, we have if the tidyverse is not in our currently installed packages then install the tidyverse. Logically then, if the tidyverse is installed our if check will return FALSE and the code to install the tidyverse will not be run. That way we - and anyone we share the project with - avoid installing and re-installing the tidyverse needlessly when the script is run. (Also, whilst anyone we would share our project with should only need to run this file once, they can also run it again to fix any issues resulting from things like accidentally deleting or making irreversible changes to the data.)\nWe can test it by putting the text cursor on the line with the code and pressing Ctrl+Enter, or click the Run button at the top of the file.\n\nYou should see the tidyverse installing in the Console. Once it has finished installing, try running the same line of code again.\n\nAs can see, the code will be sent to the Console but the tidyverse is not installed again. As the if now returns FALSE as the tidyverse is in the names of our installed packages, then the code to install it is not run this time.\nNow, we need to load the packages we will be using. As you can probably guess the tidyverse is one, and we will be adding a new one as well haven. Add the following lines to your script.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(haven)\n\nUse your mouse to select both the library(...) lines and press Ctrl+Enter or click the run button to load the tidyverse and haven.\nWhy did we not need to install haven first? It is one of the additional tidyverse packages for importing data. On the main tidyverse packages page, there is an explanation of all the packages that you have access to when installing the tidyverse. The ‘core tidyverse’ set of packages are the ones that get loaded with library(tidyverse). These are the ones that most data analysis will likely use. The tidyverse though also comes with additional packages that you might not need to use as often and these we library() load on an as needed basis.\nThe haven package is used for loading data saved in file formats from three of the proprietary software tools that used to dominate quantitative analysis before R - SAS, SPSS, and Stata. The tidyverse core package for loading data, readr, works with comma-separated value (CSV) and tab-separated value files (TSV). All this means is either a comma or tab is used to separate values, such as “value1, value2, …” with new lines used for each row of values. As the NILT 2012 data we are interested in is made available as either an SPSS or Stata file, and not CSV, we will need to use the haven package.\n\nOK, let’s actually download the NILT data we will be using across the labs.\nStart by using a comment to make clear in your R script that we are starting a new section:\n\n### Download and read data ----------------------------------------------------\n\nNext, we will create a folder to store the data. Last week, we used dir.create() in the Console to create a folder for our data (‘dir’ is merely short for directory, another name used for folders). As we are setting up our project in a reproducible way we will want to create a folder this way in our script. This way R automatically creates the folder for anyone running the code, rather than them having to do it manually. It also then ensures the data is downloaded to and read in from the location the rest of our R code will assume.\nIn your script, add the code to create a new folder called ‘data’:\n\n# create folder\ndir.create(\"data\")\n\nAgain, with your text cursor on the dir.create(...) line of code, press Ctrl+Enter or click the run button. You should see the data folder appear in the files pane (bottom-right). (Though, if not, check whether you are still in the R sub-folder, and if so navigate back up to the parent directory.)\nWe now need the code to download the file with the NILT data set. We can do this by using another function we used last week, the download.file() function. The arguments for this are download.file(\"URL\", \"folder/filename.type\"). So, we need a URL for where the file is available online and we need to then specify where and with what name we want to download it.\nAdd the following then next within your R script. As this involves a URL, it is OK to copy and paste this code rather than typing it out manually.\n\n# Download NILT 2012\ndownload.file(\n  \"https://www.ark.ac.uk/teaching/NILT2012GR.sav\",\n  \"data/nilt2012.sav\"\n)\n\nAgain select the code and run it.\nTake a look in the ‘Files’ tab in bottom-right, you should now have a folder called ‘data’, click on it, and you will see the nilt2012.sav file.\n\nRemember within the Files tab to navigate back to the main project folder to also click project in the nav-bar or the .. at the top of the list of files - so, given we only have one file, right-above the nilt2012.sav.\nThe .sav is the SPSS file format. We will need to use a function made available to us by haven to read it into R, read_sav().\nLet’s use it to read in the .sav file and assign it to an object called nilt.\n\n# Read NILT\nnilt &lt;- read_sav(\"data/nilt2012.sav\")\n\nOnce more, put your text cursor on the line that reads in the data and run it.\nAnd that’s it! You should see a new data object in your ‘Environment’ tab (Pane 2 - top-right) ready to be used. If you recall last week, we read in the police_killings.csv file using read_csv(). Despite the NILT data being stored in the SPSS file-type, once read in, we can see it and interact with it the same as our data last week.\n\nYou can also see that this contains 1204 observations (rows) and 133 variables (columns). If you click on it, you can open it up in a new tab like an Excel sheet.\n\n(You can get back to your R script by either (1) clicking the small ‘x’ to close the tab showing the nilt data frame or (2) clicking on the tab for the R script.)\nWhilst that gives us a visual interactive view of our data frame. You will notice scrolling along the columns that variables like rsex has values of 1 and 2 rather than showing the category labels. To explore this in more detail we need to switch back to code. Also, as you build up your R skills you will find it much quicker - and more intuitive - to start exploring your data using code rather than opening it up in visual views like this.\nLet’s glimpse() our data frame to see the types of variables included and understand why we do not see our category labels.\nAs we are now moving from code we are writing so anyone with our script can setup their own R project the same as our own to taking a quick look at our data, do not add glimpse() to your R script. Instead, type the following in the Console.\n\nglimpse(nilt)\n\n(Being able to move from the source pane (top-left) to the Console (bottom-left) to switch between ‘writing code we want to save in our file’ and ‘writing code to quickly check our data’ is a benefit of RStudio. Whilst it may have looked intimidating at first with, all of its panes and tabs, you can hopefully now see how this supports quickly moving between different tasks.)\n\nAs you can see from the result of glimpse(), the class for practically all the variables is &lt;dbl+lbl&gt; (which if recall from last week, stands for double - i.e. numeric - and label).\nWhat does this mean? This happened because usually datasets use numbers to represent each of the categories/levels in categorical variables. These numbers are labelled with their respective meaning. This is why we have a combination of value types (&lt;dbl+lbl&gt;).\nLook at rsex in the glimpse() output, as you can see from the values displayed this includes numbers only, e.g. 1,1,2,2.... This is because ‘1’ represents ‘Male’ respondents and ‘2’ represents ‘Female’ respondents in the NILT dataset (n.b. the authors of this lab workbook recognise that sex and gender are different concepts, and we acknowledge this tension and that it will be problematic to imply or define gender identities as binary, as with any dataset. More recent surveys normally approach this in a more inclusive way by offering self-describe options). You can check the pre-defined parameters of the variable in NILT in the documentation or running print_labels(nilt$rsex) in your Console, which returns the numeric value (i.e. the code) and its respective label. As with rsex, this is the case for many other variables in this data set.\n\nYou should be aware that this type of ‘mix’ variable is a special case since we imported a file from a SPSS file that saves metadata for each variable (containing the names of the categories). The read_sav() function let us read this in as a data frame so we can use it in R, preserving the metadata as well. However, as with any data we load in, we still need to clean and ‘wrangle’ it to make it useable.\nAs you learned in the last lab, in R we treat categorical variables as factor. Therefore, we will coerce some variables as factor. This time we will use the function as_factor() instead of the simple factor() that we used before. This is because as_factor() allows us to keep the existing category labels in the variables - in other words, it will use the codes (&lt;dbl&gt;) and category labels (&lt;lbl&gt;) already in the data to turn it into a factor variable. The syntax is exactly the same as before.\nBack to our R script then. First, use a commment to make clear we are starting a new section:\n\n# Coerce variables   ---------------------------------------------------------\n\nNext add and run the following in your script:\n\n# Gender of the respondent\nnilt &lt;- nilt |&gt; mutate(rsex = as_factor(rsex))\n# Highest Educational qualification\nnilt &lt;- nilt |&gt; mutate(highqual = as_factor(highqual))\n# Religion\nnilt &lt;- nilt |&gt; mutate(religcat = as_factor(religcat))\n# Political identification\nnilt &lt;- nilt |&gt; mutate(uninatid = as_factor(uninatid))\n# Happiness\nnilt &lt;- nilt |&gt; mutate(ruhappy = as_factor(ruhappy))\n\nNotice from the code above that we assign the result of mutate() back to our nilt data frame, the nilt &lt;- ... part of the code. This replaces our ‘old’ data frame with a version with the mutated variables that are of type factor.\nBack in the Console again, run glimpse(nilt) once more and scroll up to see the line for rsex in the output.\n\nR has taken the codes (the &lt;dbl&gt; values) and mapped them to the labels (the &lt;lbl&gt; values), whereby we can now more easily see and work with our categories.\nLast week, with the police killings data we could simply use factor() on our variables as the values were saved in the file as labels without any codes, which is another way people get around CSV files not storing metadata. Running factor() in such cases creates a code for us for each unique label. The only danger with that, and something to keep eye out for, is any typos in the data, such as Scotland and Scoltand, as they are ‘unique’ labels would result in these being treated as two different categories after running factor().\nIf we read in our data from a file type that stored the values as codes, but did not include the metadata for category labels, we would need to manually add this mapping, along the lines of:\n\nsurvey &lt;- survey |&gt;\n  mutate(\n    country = factor(\n      country,\n      levels = c(1, 2),\n      labels = c(\"Scotland\", \"England\")\n    )\n  )\n\nBack to our NILT data again - What about the numeric variables? In the NILT documentation (covered in later section) there is a table in which you will see a type of measure ‘scale’. This usually refers to continuous numeric variables (e.g. age or income).2 Let’s coerce some variables to the appropriate numeric type.\nIn the previous operation we coerced the variables as factor one by one, but as covered at the end of last week’s lab we can also transform several variables at once within a single mutate function.\nAdd and run the following code in your script:\n\n# Coerce several variables as numeric\nnilt &lt;- nilt |&gt;\n  mutate(\n    rage = as.numeric(rage),\n    rhourswk = as.numeric(rhourswk),\n    persinc2 = as.numeric(persinc2),\n  )\n\nAnother thing we need to do is drop any unused levels (or categories) in our dataset using the function droplevels(), by adding and running the following in our script:\n\n# drop unused levels\nnilt &lt;- droplevels(nilt)\n\nThis function is useful to remove some categories that are not being used in the dataset (e.g. categories that have 0 observations).\nFinally, as we now have our data and done some wrangling, we do not want to have to repeat it each time we load the data in our R Markdown files. So, let’s save it as an .rds file - R’s own file-type.\nThis will save us time in future labs, as we will be able to read in an already formatted version of the NILT in future labs.\nAdd a comment then to make clear we have a new section:\n\n# Save data ------------------------------------------------------------------\n\nThen add and run the code to save our nilt data frame as an RDS file.\n\n# save as rds\nsaveRDS(nilt, \"data/nilt_r_object.rds\")\n\nLastly, as this script is merely to download, prep, and save a cleaned version of our data, it is good practice to end by clearing the global environment. This removes all loaded objects and puts it can into a clear state. We do this for a few reasons, including (1) if we did more complex setup operations the global environment could be filled with a lot of objects we no longer need, and (2) whilst we will continue naming the object containing our data frame as ‘nilt’ in the labs, we would not want to enforce this on anyone we share the project with.\nAdd and run then the following at the end of your R script:\n\n# clean global environment\nrm(list=ls())\n\nYou should now see your Global Environment is empty:\n\nNote, whilst this removes objects, like our nilt data frame, it does not remove any loaded libraries, so the functions provided by the tidyverse and haven remain available to us after running this line of code.\n\nPlease take 5-10 minutes to read the documentation for the dataset. Such documentation is always important to read as it will usually cover the research design, sampling, and information on the variables. For instance, page 7 onwards has the codebook with columns listing variable name, variable label, values, and measure. Page 6 of the documentation provides a “Navigating the codebook (example section)” overview for how to understand the table.\nIt is also worthwhile taking a look through the questionnaires for the survey as well. You will have to regularly consult the technical document and questionnaires to understand and use the data in the NILT data set. So, I recommend you to bookmark the links / save copies of the PDF files.\nThis NILT teaching dataset is also what you will be using for the research report assignment in this course (smart, isn’t it?) - so it’s worth investing the time to learn how to work with this data through the next few labs, as part of the preparation and practice for your assignment.\n(Aside: Best practice would be to read through key info in the documentation first before downloading and coercing variables - we have done the inverse today solely to ensure you have adequate time within the lab itself to download and prepare the data.)\n\nLet’s first go through the codebook in more detail. This will help you understand how to read the information in it, and some common ‘gotchas’ to look out for.\nThe “Variable Name” is how the variables are named in the data set - except in all caps, whereas once we start working with the data they will be lowercase. If we read the data set into a data frame named nilt, the rage variable name would be accessed as nilt$rage. For consistency with how they will appear when working with them in R, we will stick to using all lower case in the lab workbook.\nThe “Label” column mostly shows the corresponding text used in the survey for the variable. For rage it is “Q1. Household grid: Age of respondent”. If you look at page 2 of the main questionnaire, you will see that the ‘household grid’ was a repeat set of questions used to gather key information about each member of the household. The r in rage stands for “respondent”, i.e. the person who answered the survey questions.\nSome variable names, such as work and tunionsa on page 9, note under the name “(compute)”. This means the variable rather than representing a direct answer to a survey question is instead ‘computed’ / ‘derived’ from answers to other questions. The variable label for these then detail which questions were used to compute these variables. Work status may seem an odd variable to compute rather than ask directly.\n\nHowever, if we look at work, we can see from page 39 in the main questionnaire, a series of questions were asked to ascertain employment status and economic activity. Given the diversity of situations people can be in, it is more practical to ask a series of yes/no and other questions and then use these to calculate categories for other variables.\n\nWhilst it may look more complex having all these questions on paper, it is simpler for respondents. Let’s imagine being a 32 year old respondent who answers ‘No’ to Q8, whether taken part in paid work in last seven days, and then ‘Yes’ to Q9a, whether taking part in a government scheme for employment training.\n\nFrom these two answers we can compute the respondent is ‘Government training scheme’ for empst, ‘Not in paid employment’ for empst2, and ‘Economically active’ for econact. Note as well that Q9a was only asked based on whether the participant would be eligible for the government training scheme, calculated using age and gender. (Highlighted in blue above.)\n\nImportantly, we gathered this information by asking simple yes/no questions, structured the questions in way to only ask the minimum number of questions relevant for the respondent, and avoided needing to explain jargon such as ‘economically active’. Economically active means someone is either in employment (including waiting to start a job they have been offered and accepted) or unemployed but looking for work and if offered could start a job within the next two weeks. Imagine now asking a respondent whether they are economically active and then having to explain what that means and figuring out whether they are or not. The way the survey questions are structured captures this complexity - and more - and all without ever having to provide a definition of economically active.\nThe tunionsa variable is also a good example of we why cannot assume what a variable measures by its name alone. As can see bottom of page 9 in the documentation, tunionsa is derived from Q22 and Q22a.\n\nHowever, if we look at those two questions on page 43 of the questionniare - screenshot below to save having to switch tabs - we can see Q22 asks the respondent whether they are currently a member of a trade union or staff association and Q22a is whether they have ever been a member.\n\nIf a respondent answered ‘Yes, trade union’ or ‘Yes, staff association’ for either Q22 or Q22a the value computed for tunionsa is ‘Yes’. Whilst tunionsa then is broadly a measure of trade union membership it would be more accurate to say it is a measure of whether the respondent currently is or ever was a member of a trade union or staff association.\nDo not worry if you could not tell that from just the codebook. Normally, dataset documentation should also include a separate section with information on how each computed variable was derived. I was only able to confirm this was how tunionsa was computed by checking the values with those for Q22 and Q22a in the main data set. Expectations of what documentation for data sets should include are continuing to improve over time. However, whilst you should encounter such situations less often with newer data sets, it can still remain unclear - where you may need to do your own additional checks, or contact the original researchers to clarify.\nThis though does show the importance of not assuming what a variable measures by just its name and label. If we mistook tunionsa as measuring current membership only, we may then later be surprised seeing the high number of people currently unemployed with ‘Yes’ for tunionsa. It is not surprising to see that though when we understand that ‘Yes’ includes those who have ever been a member.\n\nThe ‘Measure’ column in the codebook tells us whether the variable is ‘scale’, ‘nominal’, or ‘ordinal’. This is based on how SPSS, the proprietary statistical analysis program that was used by the NILT project, stores variables. Within R, these correspond respectively with numeric, (unordered) factor, and ordered factor variables that we covered last week. As we covered earlier in the lab, the tidyverse also provides us with tools to work with data created in SPSS.\nThe ‘Values’ column then tells us what the numbers represent. For numeric variables this is the unit being measured, such as “years” and “number of people”. This may seem ‘obvious’ for some variables, but good documentation should still always provide this information. The importance of this becomes clearer if we look at livearea. Without it clarifying “Numeric (years)” someone could mistake it as representing number of months instead. Similarly, the “0 Less than 1 year” is an important clarifier, helping us understand that anything less than 1 year was recorded as 0. In other words, if someone had lived in the area for 7 months, it was still recorded as 0 rather than rounded up to 1.\n\nFor the categorical variables it then provides the code and label for the categories. For example, with rsex the categories are 1 (code) Male (label) and 2 (code) Female (label). As mentioned last week, the ‘raw’ data is often stored in numeric codes as - among other reasons - it is more efficient to store it this way. Using the codebook, someone looking at the raw data would then know that a value of 1 represents Male. And as we covered above, knowing the code and label means we can set up the data in R to show the labels instead of the raw codes. (And if we were working with data in a file with only the codes and not the labels, we can use the codebook to write our own code to map codes and labels.)\n\nYou may have spotted though that the Values column for some variables also includes additional grey text, “-9 Non Applicable”, “-99 Don’t Know”, and “-999 Not answered/refused”. These are part of what is known as ‘missing values’, where we do not have an answer to a question for a participant, where these values record the reason why an answer is missing. Recording such reasons is important for multiple reasons. Let’s take ‘-9 Non Applicable’, which following convention is used in the NILT to denote the reason there is no answer is the participant was not asked the question. For example, as shown in image below if a person answered ‘Yes’ to question 8 to say they were in paid work, then they were not asked question 9a.\n\nFor clarity, we record it as ‘non applicable’ instead of leaving it blank or ‘No’. If we left it blank we would not know whether it was blank because the question was skipped deliberately or accidentally. If we recorded it as ‘No’ rather than ‘non applicable’, we lose distinction between our participants. For instance, say we had 100 respondents and 10 answered yes, 20 answered no, and 70 were non applicable. We know from those values that of the 30 who could potentially be on a government scheme 20 were not.\n“-99 Don’t Know” and “-999 Not answered/refused” are then useful to know whilst the question was applicable to the respondent, it was not answered for another reason. During a pilot of the survey, a higher than expected number of “Don’t know” and “Not answered” can also help indicate where a question is potentially unclear or phrased in a way that respondents do not feel comfortable answering.\nImportantly though as ‘missing values’ they are treated the same in analysis. Within R, values designated as ‘missing values’ are grouped together and labelled “NA”. Take note, this stands for “Not available” - covering all reasons a value is not available (i.e. missing). A common mistake people make is to assume NA stands for ‘non applicable’, resulting in inaccurate interpretations.\nAs with all conventions though, there are situations where it can be useful to not treat certain values as ‘missing’. Whilst the NILT treats “Don’t know” as a missing value, some surveys will have questions where “Don’t know” is a meaningful answer for the analysis. For example, with a question like “Who is the current UK Prime Minister?” as part of research on public understanding of politics, a “Don’t know” is a meaningful answer rather than a missing value. In such cases, the “Don’t know” would not have a “-99” or equivalent code.\nIn some cases it can also be worthwhile exploring whether there is any pattern behind missing values. It may turn out that certain groups were more likely to refuse to answer or respond “Don’t know” to specific questions. This can then open discussion as to why and what changes to the survey design may help address it. Again that we record the reason the value is ‘missing’ rather than leaving the value blank makes it possible to still do that.\n\nPhew! Good job. You have completed the basics for wrangling the data and producing a workable dataset and gone through the documentation to understand it better.\nAs a final step, just double check that things went as expected. For this purpose, we will re-read the clean dataset in the Activities below.\nFirst though, let’s set up a new R Markdown file you can use for the Activities. Within the ‘Files’ pane (bottom-right):\n\nClick ‘New file’\nClick ‘R Markdown’\n\n\nThis will open a dialogue for creating a new file:\n\nAdd nilt_test.Rmd as the new file name.\nClick the OK button to confirm creation of an R Markdown file.\n\n\nThis will create the file and open it in a new tab:\n\nYou are now read for the activities for the end of this week’s lab.\n\n\nWithin your R Markdown file create a code chunk to load the tidyverse and haven packages.\n\n(Aside: Whilst we saved our file as .rds as the original file was .sav we still need to load haven to read all the information stored in it. These packages also remain in our environment, but this ensures our R Markdown file will still run without issue if we restart our R session.)\n\n\nUsing the readRDS() function, create a code chunk and:\n\nread the .rds file that you just created in the last step and assign it to an object called cleaned_nilt. (Hint: You need to read it in by providing “folder/file-name.rds” as an argument.)\nread in the original .sav file that we downloaded and assign it to an object called unclean_nilt.\n\n\nRun the glimpse function on the cleaned_nilt object.\nRun the glimpse function on the unclean_nilt object.\nDo they look the same? (If yes, it means that you successfully saved a version of the nilt data with our coerced variables.)\nFinally, write code in the Console to clean your global environment, so we have a clear environment to start with next week.\n\n\n\n(This is a lengthy aside providing more info on file-types for those interested, it is optional to read.)\nFile-types like CSV and TSV are what is known as ‘interoperable’, meaning they are not limited to a specific app. Ideally then this is the file-type data should be shared in to support openness, transparency, and reproducibility.\nHowever, as covered in the online lecture this week, CSV does not store ‘metadata’. If using 1, 2, 3, … codes for a categorical variable, this information is stored separately, and if loading in data requires mapping codes and labels (e.g. 1 = UK, 2 = France, …). This can quickly become tedious and time-consuming. Whilst various attempts have been made to create ‘data packaging formats’ that build on top of CSV and/or TSV, none have gained wide-spread popularity. As a result, it is still common to see data being shared in data archives as an SPSS and/or Stata file - sometimes with a CSV file as well. (Some archives also auto-convert data into SPSS and Stata file type versions to download.)\nDecades ago, data being in the SPSS file type would have forced you to use - and pay an expensive license for - SPSS. To help end this situation many researchers found themselves within, The Free Software Foundation (FSF) created ‘PSPP’, a free alternative to SPSS, that can read and save to the same SPSS sav file-type. And, it is code from ‘PSPP’ that haven uses to read SPSS files. This involved pain-staking work ‘reverse engineering’ the SPSS file-type to understand how the data is stored and writing code so that other software - and not just SPSS - could use it. It was then not SPSS’s benevolence that made it possible to read SPSS files in R - indeed, proprietary software companies often do all they can to prevent other software being able to use their proprietary file-types. It was instead the work of an open community striving to ensure people have freedom in how they use their computers and are not forced into using specific software. (Sadly, for all the good they do, the FSF likes giving their projects terrible names, with PSPP not actually standing for anything, it is instead the ‘inverse’ of SPSS, playing on it being a free open alternative to proprietary closed software.)\nIndeed, PSPP and haven are the victims of their own success here. With the haven package, it does not matter that the data is in a proprietary SAS, SPSS, or Stata file type, it can be read into R. Importantly, it can be read into R in a way that can then immediately start being used with all the core tidyverse functions.\nDespite SPSS files being a closed proprietary format then, because it is easy to read the data into R and it maintains metadata, a lot of researchers and data archives use it as if it were an open file type, using it still to archive data. This has likely contributed to the lack of widespread adoption of the open data packages aiming to replace the proprietary file formats, as having an alternative way to share data is not a pressing need. However, I would still encourage you if looking to archive your own data in future to consider ways in which you can make it available using a proper open standard.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#the-nilt-2012-dataset",
    "href": "03-Lab3.html#the-nilt-2012-dataset",
    "title": "Data wrangling",
    "section": "",
    "text": "Today, we will be working with data generated by the Access Research Knowledge (ARK) hub. ARK conducts a series of surveys about society and life in Northern Ireland. For this lab, we will be working with the results of the Northern Ireland Life and Times Survey (NILT) in the year 2012. In particular, we will be using a teaching dataset that focuses on community relations and political attitudes. This includes background information of the participants and their household.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#nilt-rstudio-project",
    "href": "03-Lab3.html#nilt-rstudio-project",
    "title": "Data wrangling",
    "section": "",
    "text": "We will continue using Posit Cloud, as we did in the previous labs. This time though we are going to make a new RStudio project from scratch.\nWithin your “Lab Group …” workspace in Posit Cloud (if you have not joined a shared space, follow the instructions in Lab 2) and:\n\nClick the blue ‘New Project’ button in the top-right.\nFrom the list of options select ‘New RStudio Project’.\n\n\nOnce the project has loaded, click on ‘Untitled Project’ at the top of the screen.\n\nYou can now give your project a name. This is how it will appear in your list of projects in your lab group workspace. Let’s name it “NILT” as we will use this project to work with the NILT data. Type the name and hit Enter to confirm.\n\n\nAlthough this project is just for the labs, we are going to set it up in a ‘reproducible’ way. This means anyone with a copy of our project would be able to run the code and receive the same results as us. Last week, we downloaded a data set using the Console. This week we will instead cover one way to include the code used to download and wrangle our data in a file separate to the R Markdown files we use for the analysis.1\n(As you will see with the RStudio templates used for the assessments, it is possible to share an RStudio Project with the data. However, that is not always possible or desirable. For instance, the data whilst available for anyone to download from an organisation’s website or data archive may have license restrictions prohibiting making a version of it available elsewhere.)\nTo do this we will use an R script file. In our first two labs we used R Markdown, which lets us create code chunks for adding our R code. An R script file is simply a file that contains only R code, like a giant code chunk.\nBy convention, R scripts for doing set up, data prep, and similar are placed in an ‘R’ subfolder. To create one:\n\nIn the “Files” pane (bottom-right) click the folder icon that also has a green circle with white plus in it.\n\n\nThen, within the ‘New Folder’ dialog that will pop up:\n\nType a single letter capital ‘R’ as the name.\nClick the ‘OK’ button to confirm creation of the folder.\n\n\nYou should now see the new folder in the bottom-right panel, so -\n\nClick the ‘R’ folder to navigate into it (Note, you need to click the ‘R’ and not the folder icon.)\n\n\nYou should now see an empty folder, and can double-check you are in the right folder by looking at the navigation bar which should be “Cloud &gt; project &gt; R”.\n\nOK, now:\n\nClick the white document icon that is to the right of the new folder icon you clicked before. (See screenshot below!)\nFrom the list of options click ‘R Script’\n\n\nThat will bring up a ‘Create a New File in Current Directory’ dialog:\n\nName your file 01_prep_nilt_2012.R\n\nClick the ‘OK’ button to confirm creation of the script\n\n\nThe file should then auto open in the top-left pane.\n\nLastly, before we move to writing our R script, let’s navigate back to the top-level folder of our project in the ‘Files’ pane (bottom-right). To navigate back up to the top-level folder, click either the ‘..’ at the top of the folder content list or in the navigation bar click the ‘project’ text.\n\n\nBack to our top-left pane with our R script. It is good practice to include some key information in this file, such as what it does, when it was written, and who wrote it. Given R script is all R code, we can add this info using comments. As a reminder, any line in R that begins with a ‘#’ is treated as a comment and will not be run as part of the code.\nBelow is a template you can use.\n\n## Download and process NILT 2012 survey\n\n# Date: [add today's date here]\n# Author: [add your name here]\n\nThe ‘##’, with an extra ‘#’, at the top doesn’t do anything extra or different, we just include it to make visually clear this is the title / short description of what the script does.\nNext, it is good practice to include details for the data set being used, especially important if the data is not our own to ensure proper attribution. In addition to being good practice, it is often a condition for data sets where it is permitted to re-share them elsewhere that you can do so only as long as you include an attribution (i.e. a reference).\nLet’s then add a reference section. (Feel free to use the clipboard button in top-right of the box below to copy and paste the text.)\n\n# References  -----------------------------------------------------------------\n\n# https://www.ark.ac.uk/teaching/\n# 2012 Good relations: ARK. Northern Ireland Life and Times Survey Teaching Dataset 2012, Good relations [computer file]. ARK www.ark.ac.uk/nilt [distributor], March 2014.\n\nThe ----- after # References again doesn’t do anything aside from help make the section visually distinct. This may seem odd at the moment, but as the file grows in length, the importance of making it easy to find sections quickly from visual glance starts to make sense.\nOK, let’s now install packages that are needed for our project. Remember we need to do this every time we create a new project on Posit Cloud. If using RStudio Desktop, we would only need to install packages once per device we are using rather than per project.\nHowever, compared to last week, we want to set up our project so that anyone can run the code and achieve the same results. (If we run into irrepairable errors we may also want to re-run our R scipt to ‘reset’ our project). We will not know though what packages someone will already have installed or not.\nThankfully, we can write R code in our script that will check whether specific packages are already installed, and then install any packages needed that are missing.\nFirst, add a comment to make clear there is a new section:\n\n# Packages -------------------------------------------------------------------\n\nNext, add the following comment and code:\n\n# Install tidyverse if needed\nif (!\"tidyverse\" %in% rownames(installed.packages())) install.packages(\"tidyverse\")\n\nThis code may look complex, but it is largely the number of brackets making it looks more complex than it actually is:\n\n\nif checks whether a given object is TRUE or FALSE, and is used in format if x y, where if x returns TRUE then y code is run.\n\n(...) we use to enclose a chunk of code to make clear this is our x.\n\n!\"tidyverse\" %in% uses the NOT ! logical operator and the %in% operator, which is equivalent then of saying “If the word ‘tidyverse’ is not in…”\n\nrownames() gets the label names for rows in a data frame or matrix (a matrix is like a simpler data frame).\n\ninstalled.packages() (note installed) is a function that returns a matrix with all packages currently installed on the system, so within Posit Cloud it returns all the installed packages within the current project. Each row’s label name matches the installed package name.\n\ninstall.packages(\"tidyverse\") as we know already from last week, this code installs the tidyverse package.\n\nPutting it all today, we have if the tidyverse is not in our currently installed packages then install the tidyverse. Logically then, if the tidyverse is installed our if check will return FALSE and the code to install the tidyverse will not be run. That way we - and anyone we share the project with - avoid installing and re-installing the tidyverse needlessly when the script is run. (Also, whilst anyone we would share our project with should only need to run this file once, they can also run it again to fix any issues resulting from things like accidentally deleting or making irreversible changes to the data.)\nWe can test it by putting the text cursor on the line with the code and pressing Ctrl+Enter, or click the Run button at the top of the file.\n\nYou should see the tidyverse installing in the Console. Once it has finished installing, try running the same line of code again.\n\nAs can see, the code will be sent to the Console but the tidyverse is not installed again. As the if now returns FALSE as the tidyverse is in the names of our installed packages, then the code to install it is not run this time.\nNow, we need to load the packages we will be using. As you can probably guess the tidyverse is one, and we will be adding a new one as well haven. Add the following lines to your script.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(haven)\n\nUse your mouse to select both the library(...) lines and press Ctrl+Enter or click the run button to load the tidyverse and haven.\nWhy did we not need to install haven first? It is one of the additional tidyverse packages for importing data. On the main tidyverse packages page, there is an explanation of all the packages that you have access to when installing the tidyverse. The ‘core tidyverse’ set of packages are the ones that get loaded with library(tidyverse). These are the ones that most data analysis will likely use. The tidyverse though also comes with additional packages that you might not need to use as often and these we library() load on an as needed basis.\nThe haven package is used for loading data saved in file formats from three of the proprietary software tools that used to dominate quantitative analysis before R - SAS, SPSS, and Stata. The tidyverse core package for loading data, readr, works with comma-separated value (CSV) and tab-separated value files (TSV). All this means is either a comma or tab is used to separate values, such as “value1, value2, …” with new lines used for each row of values. As the NILT 2012 data we are interested in is made available as either an SPSS or Stata file, and not CSV, we will need to use the haven package.\n\nOK, let’s actually download the NILT data we will be using across the labs.\nStart by using a comment to make clear in your R script that we are starting a new section:\n\n### Download and read data ----------------------------------------------------\n\nNext, we will create a folder to store the data. Last week, we used dir.create() in the Console to create a folder for our data (‘dir’ is merely short for directory, another name used for folders). As we are setting up our project in a reproducible way we will want to create a folder this way in our script. This way R automatically creates the folder for anyone running the code, rather than them having to do it manually. It also then ensures the data is downloaded to and read in from the location the rest of our R code will assume.\nIn your script, add the code to create a new folder called ‘data’:\n\n# create folder\ndir.create(\"data\")\n\nAgain, with your text cursor on the dir.create(...) line of code, press Ctrl+Enter or click the run button. You should see the data folder appear in the files pane (bottom-right). (Though, if not, check whether you are still in the R sub-folder, and if so navigate back up to the parent directory.)\nWe now need the code to download the file with the NILT data set. We can do this by using another function we used last week, the download.file() function. The arguments for this are download.file(\"URL\", \"folder/filename.type\"). So, we need a URL for where the file is available online and we need to then specify where and with what name we want to download it.\nAdd the following then next within your R script. As this involves a URL, it is OK to copy and paste this code rather than typing it out manually.\n\n# Download NILT 2012\ndownload.file(\n  \"https://www.ark.ac.uk/teaching/NILT2012GR.sav\",\n  \"data/nilt2012.sav\"\n)\n\nAgain select the code and run it.\nTake a look in the ‘Files’ tab in bottom-right, you should now have a folder called ‘data’, click on it, and you will see the nilt2012.sav file.\n\nRemember within the Files tab to navigate back to the main project folder to also click project in the nav-bar or the .. at the top of the list of files - so, given we only have one file, right-above the nilt2012.sav.\nThe .sav is the SPSS file format. We will need to use a function made available to us by haven to read it into R, read_sav().\nLet’s use it to read in the .sav file and assign it to an object called nilt.\n\n# Read NILT\nnilt &lt;- read_sav(\"data/nilt2012.sav\")\n\nOnce more, put your text cursor on the line that reads in the data and run it.\nAnd that’s it! You should see a new data object in your ‘Environment’ tab (Pane 2 - top-right) ready to be used. If you recall last week, we read in the police_killings.csv file using read_csv(). Despite the NILT data being stored in the SPSS file-type, once read in, we can see it and interact with it the same as our data last week.\n\nYou can also see that this contains 1204 observations (rows) and 133 variables (columns). If you click on it, you can open it up in a new tab like an Excel sheet.\n\n(You can get back to your R script by either (1) clicking the small ‘x’ to close the tab showing the nilt data frame or (2) clicking on the tab for the R script.)\nWhilst that gives us a visual interactive view of our data frame. You will notice scrolling along the columns that variables like rsex has values of 1 and 2 rather than showing the category labels. To explore this in more detail we need to switch back to code. Also, as you build up your R skills you will find it much quicker - and more intuitive - to start exploring your data using code rather than opening it up in visual views like this.\nLet’s glimpse() our data frame to see the types of variables included and understand why we do not see our category labels.\nAs we are now moving from code we are writing so anyone with our script can setup their own R project the same as our own to taking a quick look at our data, do not add glimpse() to your R script. Instead, type the following in the Console.\n\nglimpse(nilt)\n\n(Being able to move from the source pane (top-left) to the Console (bottom-left) to switch between ‘writing code we want to save in our file’ and ‘writing code to quickly check our data’ is a benefit of RStudio. Whilst it may have looked intimidating at first with, all of its panes and tabs, you can hopefully now see how this supports quickly moving between different tasks.)\n\nAs you can see from the result of glimpse(), the class for practically all the variables is &lt;dbl+lbl&gt; (which if recall from last week, stands for double - i.e. numeric - and label).\nWhat does this mean? This happened because usually datasets use numbers to represent each of the categories/levels in categorical variables. These numbers are labelled with their respective meaning. This is why we have a combination of value types (&lt;dbl+lbl&gt;).\nLook at rsex in the glimpse() output, as you can see from the values displayed this includes numbers only, e.g. 1,1,2,2.... This is because ‘1’ represents ‘Male’ respondents and ‘2’ represents ‘Female’ respondents in the NILT dataset (n.b. the authors of this lab workbook recognise that sex and gender are different concepts, and we acknowledge this tension and that it will be problematic to imply or define gender identities as binary, as with any dataset. More recent surveys normally approach this in a more inclusive way by offering self-describe options). You can check the pre-defined parameters of the variable in NILT in the documentation or running print_labels(nilt$rsex) in your Console, which returns the numeric value (i.e. the code) and its respective label. As with rsex, this is the case for many other variables in this data set.\n\nYou should be aware that this type of ‘mix’ variable is a special case since we imported a file from a SPSS file that saves metadata for each variable (containing the names of the categories). The read_sav() function let us read this in as a data frame so we can use it in R, preserving the metadata as well. However, as with any data we load in, we still need to clean and ‘wrangle’ it to make it useable.\nAs you learned in the last lab, in R we treat categorical variables as factor. Therefore, we will coerce some variables as factor. This time we will use the function as_factor() instead of the simple factor() that we used before. This is because as_factor() allows us to keep the existing category labels in the variables - in other words, it will use the codes (&lt;dbl&gt;) and category labels (&lt;lbl&gt;) already in the data to turn it into a factor variable. The syntax is exactly the same as before.\nBack to our R script then. First, use a commment to make clear we are starting a new section:\n\n# Coerce variables   ---------------------------------------------------------\n\nNext add and run the following in your script:\n\n# Gender of the respondent\nnilt &lt;- nilt |&gt; mutate(rsex = as_factor(rsex))\n# Highest Educational qualification\nnilt &lt;- nilt |&gt; mutate(highqual = as_factor(highqual))\n# Religion\nnilt &lt;- nilt |&gt; mutate(religcat = as_factor(religcat))\n# Political identification\nnilt &lt;- nilt |&gt; mutate(uninatid = as_factor(uninatid))\n# Happiness\nnilt &lt;- nilt |&gt; mutate(ruhappy = as_factor(ruhappy))\n\nNotice from the code above that we assign the result of mutate() back to our nilt data frame, the nilt &lt;- ... part of the code. This replaces our ‘old’ data frame with a version with the mutated variables that are of type factor.\nBack in the Console again, run glimpse(nilt) once more and scroll up to see the line for rsex in the output.\n\nR has taken the codes (the &lt;dbl&gt; values) and mapped them to the labels (the &lt;lbl&gt; values), whereby we can now more easily see and work with our categories.\nLast week, with the police killings data we could simply use factor() on our variables as the values were saved in the file as labels without any codes, which is another way people get around CSV files not storing metadata. Running factor() in such cases creates a code for us for each unique label. The only danger with that, and something to keep eye out for, is any typos in the data, such as Scotland and Scoltand, as they are ‘unique’ labels would result in these being treated as two different categories after running factor().\nIf we read in our data from a file type that stored the values as codes, but did not include the metadata for category labels, we would need to manually add this mapping, along the lines of:\n\nsurvey &lt;- survey |&gt;\n  mutate(\n    country = factor(\n      country,\n      levels = c(1, 2),\n      labels = c(\"Scotland\", \"England\")\n    )\n  )\n\nBack to our NILT data again - What about the numeric variables? In the NILT documentation (covered in later section) there is a table in which you will see a type of measure ‘scale’. This usually refers to continuous numeric variables (e.g. age or income).2 Let’s coerce some variables to the appropriate numeric type.\nIn the previous operation we coerced the variables as factor one by one, but as covered at the end of last week’s lab we can also transform several variables at once within a single mutate function.\nAdd and run the following code in your script:\n\n# Coerce several variables as numeric\nnilt &lt;- nilt |&gt;\n  mutate(\n    rage = as.numeric(rage),\n    rhourswk = as.numeric(rhourswk),\n    persinc2 = as.numeric(persinc2),\n  )\n\nAnother thing we need to do is drop any unused levels (or categories) in our dataset using the function droplevels(), by adding and running the following in our script:\n\n# drop unused levels\nnilt &lt;- droplevels(nilt)\n\nThis function is useful to remove some categories that are not being used in the dataset (e.g. categories that have 0 observations).\nFinally, as we now have our data and done some wrangling, we do not want to have to repeat it each time we load the data in our R Markdown files. So, let’s save it as an .rds file - R’s own file-type.\nThis will save us time in future labs, as we will be able to read in an already formatted version of the NILT in future labs.\nAdd a comment then to make clear we have a new section:\n\n# Save data ------------------------------------------------------------------\n\nThen add and run the code to save our nilt data frame as an RDS file.\n\n# save as rds\nsaveRDS(nilt, \"data/nilt_r_object.rds\")\n\nLastly, as this script is merely to download, prep, and save a cleaned version of our data, it is good practice to end by clearing the global environment. This removes all loaded objects and puts it can into a clear state. We do this for a few reasons, including (1) if we did more complex setup operations the global environment could be filled with a lot of objects we no longer need, and (2) whilst we will continue naming the object containing our data frame as ‘nilt’ in the labs, we would not want to enforce this on anyone we share the project with.\nAdd and run then the following at the end of your R script:\n\n# clean global environment\nrm(list=ls())\n\nYou should now see your Global Environment is empty:\n\nNote, whilst this removes objects, like our nilt data frame, it does not remove any loaded libraries, so the functions provided by the tidyverse and haven remain available to us after running this line of code.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#the-nilt-documentation",
    "href": "03-Lab3.html#the-nilt-documentation",
    "title": "Data wrangling",
    "section": "",
    "text": "Please take 5-10 minutes to read the documentation for the dataset. Such documentation is always important to read as it will usually cover the research design, sampling, and information on the variables. For instance, page 7 onwards has the codebook with columns listing variable name, variable label, values, and measure. Page 6 of the documentation provides a “Navigating the codebook (example section)” overview for how to understand the table.\nIt is also worthwhile taking a look through the questionnaires for the survey as well. You will have to regularly consult the technical document and questionnaires to understand and use the data in the NILT data set. So, I recommend you to bookmark the links / save copies of the PDF files.\nThis NILT teaching dataset is also what you will be using for the research report assignment in this course (smart, isn’t it?) - so it’s worth investing the time to learn how to work with this data through the next few labs, as part of the preparation and practice for your assignment.\n(Aside: Best practice would be to read through key info in the documentation first before downloading and coercing variables - we have done the inverse today solely to ensure you have adequate time within the lab itself to download and prepare the data.)\n\nLet’s first go through the codebook in more detail. This will help you understand how to read the information in it, and some common ‘gotchas’ to look out for.\nThe “Variable Name” is how the variables are named in the data set - except in all caps, whereas once we start working with the data they will be lowercase. If we read the data set into a data frame named nilt, the rage variable name would be accessed as nilt$rage. For consistency with how they will appear when working with them in R, we will stick to using all lower case in the lab workbook.\nThe “Label” column mostly shows the corresponding text used in the survey for the variable. For rage it is “Q1. Household grid: Age of respondent”. If you look at page 2 of the main questionnaire, you will see that the ‘household grid’ was a repeat set of questions used to gather key information about each member of the household. The r in rage stands for “respondent”, i.e. the person who answered the survey questions.\nSome variable names, such as work and tunionsa on page 9, note under the name “(compute)”. This means the variable rather than representing a direct answer to a survey question is instead ‘computed’ / ‘derived’ from answers to other questions. The variable label for these then detail which questions were used to compute these variables. Work status may seem an odd variable to compute rather than ask directly.\n\nHowever, if we look at work, we can see from page 39 in the main questionnaire, a series of questions were asked to ascertain employment status and economic activity. Given the diversity of situations people can be in, it is more practical to ask a series of yes/no and other questions and then use these to calculate categories for other variables.\n\nWhilst it may look more complex having all these questions on paper, it is simpler for respondents. Let’s imagine being a 32 year old respondent who answers ‘No’ to Q8, whether taken part in paid work in last seven days, and then ‘Yes’ to Q9a, whether taking part in a government scheme for employment training.\n\nFrom these two answers we can compute the respondent is ‘Government training scheme’ for empst, ‘Not in paid employment’ for empst2, and ‘Economically active’ for econact. Note as well that Q9a was only asked based on whether the participant would be eligible for the government training scheme, calculated using age and gender. (Highlighted in blue above.)\n\nImportantly, we gathered this information by asking simple yes/no questions, structured the questions in way to only ask the minimum number of questions relevant for the respondent, and avoided needing to explain jargon such as ‘economically active’. Economically active means someone is either in employment (including waiting to start a job they have been offered and accepted) or unemployed but looking for work and if offered could start a job within the next two weeks. Imagine now asking a respondent whether they are economically active and then having to explain what that means and figuring out whether they are or not. The way the survey questions are structured captures this complexity - and more - and all without ever having to provide a definition of economically active.\nThe tunionsa variable is also a good example of we why cannot assume what a variable measures by its name alone. As can see bottom of page 9 in the documentation, tunionsa is derived from Q22 and Q22a.\n\nHowever, if we look at those two questions on page 43 of the questionniare - screenshot below to save having to switch tabs - we can see Q22 asks the respondent whether they are currently a member of a trade union or staff association and Q22a is whether they have ever been a member.\n\nIf a respondent answered ‘Yes, trade union’ or ‘Yes, staff association’ for either Q22 or Q22a the value computed for tunionsa is ‘Yes’. Whilst tunionsa then is broadly a measure of trade union membership it would be more accurate to say it is a measure of whether the respondent currently is or ever was a member of a trade union or staff association.\nDo not worry if you could not tell that from just the codebook. Normally, dataset documentation should also include a separate section with information on how each computed variable was derived. I was only able to confirm this was how tunionsa was computed by checking the values with those for Q22 and Q22a in the main data set. Expectations of what documentation for data sets should include are continuing to improve over time. However, whilst you should encounter such situations less often with newer data sets, it can still remain unclear - where you may need to do your own additional checks, or contact the original researchers to clarify.\nThis though does show the importance of not assuming what a variable measures by just its name and label. If we mistook tunionsa as measuring current membership only, we may then later be surprised seeing the high number of people currently unemployed with ‘Yes’ for tunionsa. It is not surprising to see that though when we understand that ‘Yes’ includes those who have ever been a member.\n\nThe ‘Measure’ column in the codebook tells us whether the variable is ‘scale’, ‘nominal’, or ‘ordinal’. This is based on how SPSS, the proprietary statistical analysis program that was used by the NILT project, stores variables. Within R, these correspond respectively with numeric, (unordered) factor, and ordered factor variables that we covered last week. As we covered earlier in the lab, the tidyverse also provides us with tools to work with data created in SPSS.\nThe ‘Values’ column then tells us what the numbers represent. For numeric variables this is the unit being measured, such as “years” and “number of people”. This may seem ‘obvious’ for some variables, but good documentation should still always provide this information. The importance of this becomes clearer if we look at livearea. Without it clarifying “Numeric (years)” someone could mistake it as representing number of months instead. Similarly, the “0 Less than 1 year” is an important clarifier, helping us understand that anything less than 1 year was recorded as 0. In other words, if someone had lived in the area for 7 months, it was still recorded as 0 rather than rounded up to 1.\n\nFor the categorical variables it then provides the code and label for the categories. For example, with rsex the categories are 1 (code) Male (label) and 2 (code) Female (label). As mentioned last week, the ‘raw’ data is often stored in numeric codes as - among other reasons - it is more efficient to store it this way. Using the codebook, someone looking at the raw data would then know that a value of 1 represents Male. And as we covered above, knowing the code and label means we can set up the data in R to show the labels instead of the raw codes. (And if we were working with data in a file with only the codes and not the labels, we can use the codebook to write our own code to map codes and labels.)\n\nYou may have spotted though that the Values column for some variables also includes additional grey text, “-9 Non Applicable”, “-99 Don’t Know”, and “-999 Not answered/refused”. These are part of what is known as ‘missing values’, where we do not have an answer to a question for a participant, where these values record the reason why an answer is missing. Recording such reasons is important for multiple reasons. Let’s take ‘-9 Non Applicable’, which following convention is used in the NILT to denote the reason there is no answer is the participant was not asked the question. For example, as shown in image below if a person answered ‘Yes’ to question 8 to say they were in paid work, then they were not asked question 9a.\n\nFor clarity, we record it as ‘non applicable’ instead of leaving it blank or ‘No’. If we left it blank we would not know whether it was blank because the question was skipped deliberately or accidentally. If we recorded it as ‘No’ rather than ‘non applicable’, we lose distinction between our participants. For instance, say we had 100 respondents and 10 answered yes, 20 answered no, and 70 were non applicable. We know from those values that of the 30 who could potentially be on a government scheme 20 were not.\n“-99 Don’t Know” and “-999 Not answered/refused” are then useful to know whilst the question was applicable to the respondent, it was not answered for another reason. During a pilot of the survey, a higher than expected number of “Don’t know” and “Not answered” can also help indicate where a question is potentially unclear or phrased in a way that respondents do not feel comfortable answering.\nImportantly though as ‘missing values’ they are treated the same in analysis. Within R, values designated as ‘missing values’ are grouped together and labelled “NA”. Take note, this stands for “Not available” - covering all reasons a value is not available (i.e. missing). A common mistake people make is to assume NA stands for ‘non applicable’, resulting in inaccurate interpretations.\nAs with all conventions though, there are situations where it can be useful to not treat certain values as ‘missing’. Whilst the NILT treats “Don’t know” as a missing value, some surveys will have questions where “Don’t know” is a meaningful answer for the analysis. For example, with a question like “Who is the current UK Prime Minister?” as part of research on public understanding of politics, a “Don’t know” is a meaningful answer rather than a missing value. In such cases, the “Don’t know” would not have a “-99” or equivalent code.\nIn some cases it can also be worthwhile exploring whether there is any pattern behind missing values. It may turn out that certain groups were more likely to refuse to answer or respond “Don’t know” to specific questions. This can then open discussion as to why and what changes to the survey design may help address it. Again that we record the reason the value is ‘missing’ rather than leaving the value blank makes it possible to still do that.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#read-the-clean-dataset",
    "href": "03-Lab3.html#read-the-clean-dataset",
    "title": "Data wrangling",
    "section": "",
    "text": "Phew! Good job. You have completed the basics for wrangling the data and producing a workable dataset and gone through the documentation to understand it better.\nAs a final step, just double check that things went as expected. For this purpose, we will re-read the clean dataset in the Activities below.\nFirst though, let’s set up a new R Markdown file you can use for the Activities. Within the ‘Files’ pane (bottom-right):\n\nClick ‘New file’\nClick ‘R Markdown’\n\n\nThis will open a dialogue for creating a new file:\n\nAdd nilt_test.Rmd as the new file name.\nClick the OK button to confirm creation of an R Markdown file.\n\n\nThis will create the file and open it in a new tab:\n\nYou are now read for the activities for the end of this week’s lab.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#activities",
    "href": "03-Lab3.html#activities",
    "title": "Data wrangling",
    "section": "",
    "text": "Within your R Markdown file create a code chunk to load the tidyverse and haven packages.\n\n(Aside: Whilst we saved our file as .rds as the original file was .sav we still need to load haven to read all the information stored in it. These packages also remain in our environment, but this ensures our R Markdown file will still run without issue if we restart our R session.)\n\n\nUsing the readRDS() function, create a code chunk and:\n\nread the .rds file that you just created in the last step and assign it to an object called cleaned_nilt. (Hint: You need to read it in by providing “folder/file-name.rds” as an argument.)\nread in the original .sav file that we downloaded and assign it to an object called unclean_nilt.\n\n\nRun the glimpse function on the cleaned_nilt object.\nRun the glimpse function on the unclean_nilt object.\nDo they look the same? (If yes, it means that you successfully saved a version of the nilt data with our coerced variables.)\nFinally, write code in the Console to clean your global environment, so we have a clear environment to start with next week.\n\n\n\n(This is a lengthy aside providing more info on file-types for those interested, it is optional to read.)\nFile-types like CSV and TSV are what is known as ‘interoperable’, meaning they are not limited to a specific app. Ideally then this is the file-type data should be shared in to support openness, transparency, and reproducibility.\nHowever, as covered in the online lecture this week, CSV does not store ‘metadata’. If using 1, 2, 3, … codes for a categorical variable, this information is stored separately, and if loading in data requires mapping codes and labels (e.g. 1 = UK, 2 = France, …). This can quickly become tedious and time-consuming. Whilst various attempts have been made to create ‘data packaging formats’ that build on top of CSV and/or TSV, none have gained wide-spread popularity. As a result, it is still common to see data being shared in data archives as an SPSS and/or Stata file - sometimes with a CSV file as well. (Some archives also auto-convert data into SPSS and Stata file type versions to download.)\nDecades ago, data being in the SPSS file type would have forced you to use - and pay an expensive license for - SPSS. To help end this situation many researchers found themselves within, The Free Software Foundation (FSF) created ‘PSPP’, a free alternative to SPSS, that can read and save to the same SPSS sav file-type. And, it is code from ‘PSPP’ that haven uses to read SPSS files. This involved pain-staking work ‘reverse engineering’ the SPSS file-type to understand how the data is stored and writing code so that other software - and not just SPSS - could use it. It was then not SPSS’s benevolence that made it possible to read SPSS files in R - indeed, proprietary software companies often do all they can to prevent other software being able to use their proprietary file-types. It was instead the work of an open community striving to ensure people have freedom in how they use their computers and are not forced into using specific software. (Sadly, for all the good they do, the FSF likes giving their projects terrible names, with PSPP not actually standing for anything, it is instead the ‘inverse’ of SPSS, playing on it being a free open alternative to proprietary closed software.)\nIndeed, PSPP and haven are the victims of their own success here. With the haven package, it does not matter that the data is in a proprietary SAS, SPSS, or Stata file type, it can be read into R. Importantly, it can be read into R in a way that can then immediately start being used with all the core tidyverse functions.\nDespite SPSS files being a closed proprietary format then, because it is easy to read the data into R and it maintains metadata, a lot of researchers and data archives use it as if it were an open file type, using it still to archive data. This has likely contributed to the lack of widespread adoption of the open data packages aiming to replace the proprietary file formats, as having an alternative way to share data is not a pressing need. However, I would still encourage you if looking to archive your own data in future to consider ways in which you can make it available using a proper open standard.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#footnotes",
    "href": "03-Lab3.html#footnotes",
    "title": "Data wrangling",
    "section": "Footnotes",
    "text": "Footnotes\n\nAnother way to do this is using the renv package. This package creates a unique separate environment, including for installed packages, for each project you are working on. If you were using RStudio Desktop and were working on analysis projects that required different specific versions of the same packages, renv makes that possible as it creates a folder with the installed packages for each project. Even when working on a single project as renv maintains a record of which version of a package was used this is more ‘future-proof’. If a package used made major changes, renaming functions or changing how arguments are passed to them, the code may no longer work. renv though would retrieve and install the exact same version as originally used, helping ensure it will run now and well into the future.↩︎\nBe careful, in some cases these actually correspond to discrete numeric values in this dataset (things that can be counted, e.g. number of…).↩︎",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "06-Lab6.html",
    "href": "06-Lab6.html",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "In this lab, we will extend your skills to explore data by visualizing it…and R is great for this! It is actually a highly-demanded skill in the job market.\nVisualizing data is an important process in at least two stages in quantitative research: First, for you as a researcher to get familiar with the data; and second, to communicate your findings. R includes two important tools to achieve this: First, the ggplot2 package (included in tidyverse), a powerful tool to explore and plot data. Second, R Markdown which allows you to create integrated quantitative reports. Combining and mastering the two can create very effective results.\n\nVisual data exploration with ggplot2 (Artwork by @alison_horst).\n\n\n\n\nVisual data exploration. Source: Horst (n.d.)\n\n\n\n\nDifferent plot types serve different types of data. In the last lab, we introduced some functions to summarise your data and to generate summaries for specific types of variable. This will help you to decide what the most suitable plot is for your data or variable. The table below presents a minimal guide to choose the type of plot as a function of the type of data you intend to visualize. In addition, it splits the type of plot by the number of variables included in the visualization, one for univariate, or two for bivariate. Bivariate plots are useful to explore the relationship between variables.\n\n\n\nUnivariate\nBivariate\n\n\n\nCategorical\nBar plot / Pie chart\nBar plot\n\n\nNumeric\nHistogram / boxplot\nScatter plot\n\n\n\nCategorical + Numeric\n\n-\nBox plot\n\n\n\n\nAs we did in the last session, let’s learn by doing. We will continue working in the same project called NILT in RStudio Cloud. Set up your session as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\n\nNext, create a new Rmd document following the next steps.\n\nCreate a new Rmd file titled ‘Lab 6. Data visualization’, and write your name in the ‘Author’ space.\nSave the Rmd file clicking on File&gt;Save as..., using Lab_6 in the ‘File name’ box, and click on the ‘Save’ button.\nDelete all the contents in the default example with the exception of the first bit which contains the YAML and the first code chunk (that is keeping from line 1 to 10 and deleting everything from line 11).\nIn the setup chunk (line 8), change echo from TRUE to FALSE (this will hide the code for all chunks in the output).\n\nOnce your Rmd document is ready, insert a new R code chunk. To do this, click on the ‘Insert’ button on pane 1, and then click on ‘R’. This will insert an R code chunk as the ones we explored in ‘Test1’.\n\n\n\n\nInsert a code chunk.\n\n\n\nIn the body of the first chunk, load the packages that you will need. For now, it will be only tidyverse.\n\nlibrary(tidyverse)\n\nAfter the previous, insert a second chunk. In the body of the code copy and paste the lines below. This is to load the data we downloaded and formatted in the last lab in R format using the readRDS() function (note that we are reading the .rds file and not the original .sav file). Also, we will create a subset called nilt_subset selecting only the variables that we are interested in for now, as we did in the previous lab.\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Create subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\nRun both of the previous chunks individually by clicking on the green arrow located on the top-right of the chunk, as described earlier.\n\nNow that we read the data in, we are ready to start creating our own own plots using the 2012 NILT survey.\nYou do not need to reproduce each of the examples in the following section, but we suggest you to carefully read the description and try to understand the code syntax.\n\nLet’s start using the same variables we summarised in our last lab. We started by computing the total number of respondents by gender in a One-Way contingency table. We can easily visualize this using a bar plot with ggplot. This package always takes at least three layers, namely data, aesthetics and geometry. Here, we define the data as the first argument of the ggplot()function with the nilt_subset. The second argument, aesthetics, is separated by a comma and is introduced using the function aes(). In this case we define the X axis x = of the plot by the categories included in the variable rsex. The geometry is specified with the function geom_bar() without arguments for now. Note that we added the geometry using the plus sign + at the end of the previous line code. As an extra, I included the main title and the name of the x axis using the labs() function (you don’t need to copy or run the following code chunks in your .Rmd. This is only for demonstration purposes).\n\nggplot(nilt_subset, aes(x = rsex)) +\n  geom_bar() +\n  labs(title = \"Gender\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\nFrom the plot above, we can graphically see what we found out previously: there are more female respondents than males in our sample. The advantage is that we can have a sense of the magnitude of the difference by visualising it.\n\nIn Lab 3, we computed a Two-Way contingency table, which included the count of two categorical variables. This summary can be visualized using a stacked bar plot. This is quite similar to the above, with the addition that the area of the vertical plot is coloured by the size of each group.\nIf we wanted to know how gender is split by religion, we can add the fill argument with a second variable in aesthetics, as shown below.\n\nggplot(nilt_subset, aes(x = rsex, fill = religcat)) +\n  geom_bar() +\n  labs(title = \"Gender by religion\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\nThis plot is not very informative, since the total size of female and male respondents is different. The type of visualization will also depend on your specific research question or the topic you are interested in. For example, if I think it is worthwhile visualizing the religion by respondents’ sex. A plot can show us the magnitudes and composition by respondents’ sex for each religion. To do this, we need to change the aesthetics, specifying the religion by category variable religcat on the x axis and fill with gender rsex.\n\nggplot(nilt_subset, aes(x = religcat, fill = rsex)) +\n  geom_bar() +\n  labs(title = \"Religion by gender\", x = \"Religion\")\n\n\n\n\n\n\n\nAs we can see, catholic and protestant religion are equally popular among the respondents. Also, we can see that these are composed by similar proportions of males and females. One interesting thing is that there are more male respondents with no religion than female participants. Again, we found this out with the descriptive statistics computed in the last lab. However, we have the advantage that we can graphically reprsent and inspect the magnitude of these differences.\n\n\nLast time we talked about some measures of centrality and spread for numeric variables. The histogram plot is similar to the bar plot; the difference is that it splits the numeric range into fixed “bins” and computes the frequency/count for each bin instead of counting the number of respondents for each numeric value. The syntax is practically the same as the simple bar plot. At this time, we set the x aesthetic with the numeric variable age rage. Also, the geometry is defined as a histogram using the geom_histogram() function.\n\nggplot(nilt_subset, aes(x = rage)) +\n  geom_histogram() +\n  labs(title = \"Age distribution\")\n\n\n\n\n\n\n\nFrom the histogram, we have age (in bins) on the X axis, and the frequency/count on the y axis. This plot is useful to visualize how respondent’s age is distributed in our sample. For instance, we can quickly see the minimum and maximum value, or the most popular age, or a general trend indicating the largest age group.\nA second option to visualize numeric variables is the box plot. Essentially this draws the quartiles of a numeric vector. For this time, rage is defined in the y axis. This is just a personal preference. The geometry is set by the geom_boxplot() function.\n\nggplot(nilt_subset, aes(y = rage)) +\n  geom_boxplot() +\n  labs(title = \"Age boxplot\")\n\n\n\n\n\n\n\nWhat we see from this plot is the first, second and third quartile. The second quartile (or median) is represented by the black line in the middle of the box. As you can see this is close to 50 years old, as we computed using the quantile() function. The lower edge of the box represents the 2nd quartile, which is somewhere around 35 years old. Similarly the 3rd quartile is represented by the upper edge of the box. We can confirm this by computing the quantiles for this variable.\n\nquantile(nilt_subset$rage, na.rm = T)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\n\nA useful plot to explore the relationship between two numeric variables is the scatter plot. This plot locates a dot for each observation according to their respective numeric values. In the example below, we use age rage on the X axis (horizontal), and personal income persinc2 on the Y axis (vertical). This type of plot is useful to explore a relationship between variables.\n\nTo generate a scatter plot, we need to define x and y in aesthetics aes(). The geometry is a point, expressed by geom_point(). Note that we are specifying some further optional arguments. First, in aes() alpha regulates the opacity of the dots. This goes from 0.0 (completely translucent) to 1.0 (completely solid fill). Second, in geom_point() we defined position as jitter. This arguments slightly moves the point away from their exact location. These two arguments are desired in this plot because the personal income bands are overlapped. Adding some transparency and noise to their position, can allow to visualize possible patterns easily.\n\nggplot(nilt_subset, aes(x = rage, y = persinc2, alpha = 0.8)) +\n  geom_point(position = \"jitter\") +\n  labs(title = \"Personal income vs age\", x = \"Age\", y = \"Personal income (£)\")\n\n\n\n\n\n\n\nThere is not a clear pattern in our previous plot. However, it is interesting to note that most of the people younger than 25 years old earn less than £20K a year. Similarly, most of the people older than 75 earn less than £20K. And only very few earn over £60k a year (looking at the top of the plot).\n\nVery often we want to summarise central or spread measure by categories or groups. For example, let’s go back to the example of age and respondents’ sex. We can visualize these two variables (which include one numeric and one categorical) using a box plot. To create this, we need to specify the x and y value in aes() and include the geom_boxplot() geometry.\n\nggplot(nilt_subset, aes(y = rage, x = rsex)) +\n  geom_boxplot() +\n  ggtitle(\"Age by sex\")\n\n\n\n\n\n\n\nFrom this, we can visualize that female participants are slightly younger than their male counterparts in the sample.\n\nUsing the nilt_subset object, complete the tasks below in the Rmd file Lab_6, which you created earlier. Insert a new code chunk for each of these activities and include brief comments as text (outside the chunk) in the Rmd document to introduce or describe the plots. Feel free to copy and adapt the code to create the plots in the examples above.\n\nCreate a first-level header to start a section called “Categorical analysis”;\nCreate simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid;\nBased on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables;\nCreate a new first-level header to start a section called “Numeric analysis”;\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime);\nAdd your own (brief) comments to each of the plots as text in your Rmd file;\nKnit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish.\n\n\nThere are a number of features that you can customize in your plots, including the background, text size, colours, adding more variables. But you don’t have to memorise or remember all this, one thing that is very commonly used by R data scientists are R cheat sheets! They are extremely handy when you try to create a visualisation from scratch, check out the Data Visualization with ggplot2 Cheat Sheet. An extra tip is that you can change the overall look of the plot by adding pre-defined themes. You can read more about it here. Another interesting site is the The R Graph Gallery, which includes a comprehensive showcase of plot types and their respective code.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#introduction",
    "href": "06-Lab6.html#introduction",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "In this lab, we will extend your skills to explore data by visualizing it…and R is great for this! It is actually a highly-demanded skill in the job market.\nVisualizing data is an important process in at least two stages in quantitative research: First, for you as a researcher to get familiar with the data; and second, to communicate your findings. R includes two important tools to achieve this: First, the ggplot2 package (included in tidyverse), a powerful tool to explore and plot data. Second, R Markdown which allows you to create integrated quantitative reports. Combining and mastering the two can create very effective results.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#data-visualization",
    "href": "06-Lab6.html#data-visualization",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Visual data exploration with ggplot2 (Artwork by @alison_horst).\n\n\n\n\nVisual data exploration. Source: Horst (n.d.)\n\n\n\n\nDifferent plot types serve different types of data. In the last lab, we introduced some functions to summarise your data and to generate summaries for specific types of variable. This will help you to decide what the most suitable plot is for your data or variable. The table below presents a minimal guide to choose the type of plot as a function of the type of data you intend to visualize. In addition, it splits the type of plot by the number of variables included in the visualization, one for univariate, or two for bivariate. Bivariate plots are useful to explore the relationship between variables.\n\n\n\nUnivariate\nBivariate\n\n\n\nCategorical\nBar plot / Pie chart\nBar plot\n\n\nNumeric\nHistogram / boxplot\nScatter plot\n\n\n\nCategorical + Numeric\n\n-\nBox plot\n\n\n\n\nAs we did in the last session, let’s learn by doing. We will continue working in the same project called NILT in RStudio Cloud. Set up your session as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\n\nNext, create a new Rmd document following the next steps.\n\nCreate a new Rmd file titled ‘Lab 6. Data visualization’, and write your name in the ‘Author’ space.\nSave the Rmd file clicking on File&gt;Save as..., using Lab_6 in the ‘File name’ box, and click on the ‘Save’ button.\nDelete all the contents in the default example with the exception of the first bit which contains the YAML and the first code chunk (that is keeping from line 1 to 10 and deleting everything from line 11).\nIn the setup chunk (line 8), change echo from TRUE to FALSE (this will hide the code for all chunks in the output).\n\nOnce your Rmd document is ready, insert a new R code chunk. To do this, click on the ‘Insert’ button on pane 1, and then click on ‘R’. This will insert an R code chunk as the ones we explored in ‘Test1’.\n\n\n\n\nInsert a code chunk.\n\n\n\nIn the body of the first chunk, load the packages that you will need. For now, it will be only tidyverse.\n\nlibrary(tidyverse)\n\nAfter the previous, insert a second chunk. In the body of the code copy and paste the lines below. This is to load the data we downloaded and formatted in the last lab in R format using the readRDS() function (note that we are reading the .rds file and not the original .sav file). Also, we will create a subset called nilt_subset selecting only the variables that we are interested in for now, as we did in the previous lab.\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Create subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\nRun both of the previous chunks individually by clicking on the green arrow located on the top-right of the chunk, as described earlier.\n\nNow that we read the data in, we are ready to start creating our own own plots using the 2012 NILT survey.\nYou do not need to reproduce each of the examples in the following section, but we suggest you to carefully read the description and try to understand the code syntax.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#categorical-variables",
    "href": "06-Lab6.html#categorical-variables",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Let’s start using the same variables we summarised in our last lab. We started by computing the total number of respondents by gender in a One-Way contingency table. We can easily visualize this using a bar plot with ggplot. This package always takes at least three layers, namely data, aesthetics and geometry. Here, we define the data as the first argument of the ggplot()function with the nilt_subset. The second argument, aesthetics, is separated by a comma and is introduced using the function aes(). In this case we define the X axis x = of the plot by the categories included in the variable rsex. The geometry is specified with the function geom_bar() without arguments for now. Note that we added the geometry using the plus sign + at the end of the previous line code. As an extra, I included the main title and the name of the x axis using the labs() function (you don’t need to copy or run the following code chunks in your .Rmd. This is only for demonstration purposes).\n\nggplot(nilt_subset, aes(x = rsex)) +\n  geom_bar() +\n  labs(title = \"Gender\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\nFrom the plot above, we can graphically see what we found out previously: there are more female respondents than males in our sample. The advantage is that we can have a sense of the magnitude of the difference by visualising it.\n\nIn Lab 3, we computed a Two-Way contingency table, which included the count of two categorical variables. This summary can be visualized using a stacked bar plot. This is quite similar to the above, with the addition that the area of the vertical plot is coloured by the size of each group.\nIf we wanted to know how gender is split by religion, we can add the fill argument with a second variable in aesthetics, as shown below.\n\nggplot(nilt_subset, aes(x = rsex, fill = religcat)) +\n  geom_bar() +\n  labs(title = \"Gender by religion\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\nThis plot is not very informative, since the total size of female and male respondents is different. The type of visualization will also depend on your specific research question or the topic you are interested in. For example, if I think it is worthwhile visualizing the religion by respondents’ sex. A plot can show us the magnitudes and composition by respondents’ sex for each religion. To do this, we need to change the aesthetics, specifying the religion by category variable religcat on the x axis and fill with gender rsex.\n\nggplot(nilt_subset, aes(x = religcat, fill = rsex)) +\n  geom_bar() +\n  labs(title = \"Religion by gender\", x = \"Religion\")\n\n\n\n\n\n\n\nAs we can see, catholic and protestant religion are equally popular among the respondents. Also, we can see that these are composed by similar proportions of males and females. One interesting thing is that there are more male respondents with no religion than female participants. Again, we found this out with the descriptive statistics computed in the last lab. However, we have the advantage that we can graphically reprsent and inspect the magnitude of these differences.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#numeric-variables",
    "href": "06-Lab6.html#numeric-variables",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Last time we talked about some measures of centrality and spread for numeric variables. The histogram plot is similar to the bar plot; the difference is that it splits the numeric range into fixed “bins” and computes the frequency/count for each bin instead of counting the number of respondents for each numeric value. The syntax is practically the same as the simple bar plot. At this time, we set the x aesthetic with the numeric variable age rage. Also, the geometry is defined as a histogram using the geom_histogram() function.\n\nggplot(nilt_subset, aes(x = rage)) +\n  geom_histogram() +\n  labs(title = \"Age distribution\")\n\n\n\n\n\n\n\nFrom the histogram, we have age (in bins) on the X axis, and the frequency/count on the y axis. This plot is useful to visualize how respondent’s age is distributed in our sample. For instance, we can quickly see the minimum and maximum value, or the most popular age, or a general trend indicating the largest age group.\nA second option to visualize numeric variables is the box plot. Essentially this draws the quartiles of a numeric vector. For this time, rage is defined in the y axis. This is just a personal preference. The geometry is set by the geom_boxplot() function.\n\nggplot(nilt_subset, aes(y = rage)) +\n  geom_boxplot() +\n  labs(title = \"Age boxplot\")\n\n\n\n\n\n\n\nWhat we see from this plot is the first, second and third quartile. The second quartile (or median) is represented by the black line in the middle of the box. As you can see this is close to 50 years old, as we computed using the quantile() function. The lower edge of the box represents the 2nd quartile, which is somewhere around 35 years old. Similarly the 3rd quartile is represented by the upper edge of the box. We can confirm this by computing the quantiles for this variable.\n\nquantile(nilt_subset$rage, na.rm = T)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\n\nA useful plot to explore the relationship between two numeric variables is the scatter plot. This plot locates a dot for each observation according to their respective numeric values. In the example below, we use age rage on the X axis (horizontal), and personal income persinc2 on the Y axis (vertical). This type of plot is useful to explore a relationship between variables.\n\nTo generate a scatter plot, we need to define x and y in aesthetics aes(). The geometry is a point, expressed by geom_point(). Note that we are specifying some further optional arguments. First, in aes() alpha regulates the opacity of the dots. This goes from 0.0 (completely translucent) to 1.0 (completely solid fill). Second, in geom_point() we defined position as jitter. This arguments slightly moves the point away from their exact location. These two arguments are desired in this plot because the personal income bands are overlapped. Adding some transparency and noise to their position, can allow to visualize possible patterns easily.\n\nggplot(nilt_subset, aes(x = rage, y = persinc2, alpha = 0.8)) +\n  geom_point(position = \"jitter\") +\n  labs(title = \"Personal income vs age\", x = \"Age\", y = \"Personal income (£)\")\n\n\n\n\n\n\n\nThere is not a clear pattern in our previous plot. However, it is interesting to note that most of the people younger than 25 years old earn less than £20K a year. Similarly, most of the people older than 75 earn less than £20K. And only very few earn over £60k a year (looking at the top of the plot).",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#mixed-data",
    "href": "06-Lab6.html#mixed-data",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Very often we want to summarise central or spread measure by categories or groups. For example, let’s go back to the example of age and respondents’ sex. We can visualize these two variables (which include one numeric and one categorical) using a box plot. To create this, we need to specify the x and y value in aes() and include the geom_boxplot() geometry.\n\nggplot(nilt_subset, aes(y = rage, x = rsex)) +\n  geom_boxplot() +\n  ggtitle(\"Age by sex\")\n\n\n\n\n\n\n\nFrom this, we can visualize that female participants are slightly younger than their male counterparts in the sample.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#activity",
    "href": "06-Lab6.html#activity",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Using the nilt_subset object, complete the tasks below in the Rmd file Lab_6, which you created earlier. Insert a new code chunk for each of these activities and include brief comments as text (outside the chunk) in the Rmd document to introduce or describe the plots. Feel free to copy and adapt the code to create the plots in the examples above.\n\nCreate a first-level header to start a section called “Categorical analysis”;\nCreate simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid;\nBased on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables;\nCreate a new first-level header to start a section called “Numeric analysis”;\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime);\nAdd your own (brief) comments to each of the plots as text in your Rmd file;\nKnit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish.\n\n\nThere are a number of features that you can customize in your plots, including the background, text size, colours, adding more variables. But you don’t have to memorise or remember all this, one thing that is very commonly used by R data scientists are R cheat sheets! They are extremely handy when you try to create a visualisation from scratch, check out the Data Visualization with ggplot2 Cheat Sheet. An extra tip is that you can change the overall look of the plot by adding pre-defined themes. You can read more about it here. Another interesting site is the The R Graph Gallery, which includes a comprehensive showcase of plot types and their respective code.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "help/copilot.html",
    "href": "help/copilot.html",
    "title": "Copilot - Saved Prompts",
    "section": "",
    "text": "After you send a prompt and Copilot finishes its response, hover your mouse over your prompt and use the bookmark icon in the options that pop-up:\n\n\nThis will open a dialogue where you can give your prompt a name and then click Save to confirm:\n\n\nWhen starting a new chat, you can access your saved prompt by clicking ‘See more’ in the bottom right:\n\n\nThen scroll down and you will see the ‘Prompt Gallery’ button in the bottom-right:\n\n\nThat will open a menu with the Copilot Prompt Gallery, where along the top row there is a button to access your saved prompts:\n\n\nAnd from there you can click the box for the prompt you want and Copilot will add the text of the saved prompt to the message box, where you can then just hit enter:\n\n\nIt’s also easier to access the Copilot Prompt Gallery after you send your first prompt. Instead of needing to click see more and scrolling down, a new ‘View prompts’ button gets added on the right just above the message box:\n\n\nFinally, by default, Copilot when accessing it via your student account still uses GPT-4o. You can though switch to GPT-5 by clicking the ‘Try GPT-5’ button in the top-right:\n\n\nDespite the bad press GPT-5 got at launch, it is a significant improvement on 4o for coding questions."
  },
  {
    "objectID": "help/InterpretiveReportStructure.html",
    "href": "help/InterpretiveReportStructure.html",
    "title": "RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "This appendix provides additional information on using R Markdown and an overview of the structure of the R Markdown document provided with the interpretive report project template.\nPlease ensure to review the relevant lab materials:\n\n\nLab 5 session on R Markdown for an intro to R Markdown.\n\nLab 10 Session for Interpreting Quantitative Findings Report summative assessment for instructions on how to create a project on RStudio Cloud for your interpretive report using the provided template.\n\nSimilarly, if you encounter any errors, please ensure to check the R Issues FAQ first for any existing solutions.\nThe first half of this page provides an overview of R Markdown and the second half covers the Interpretive Report Template in more detail. Importantly, please ensure you at least read the Preamble Code Chunk section for information on best practice for installing and loading any additional packages.\n\nR is a programming language used for data analysis and visualisation. A large community has built up around R through the combination of it being free and open-source software and a full featured programming language. This makes it possible for the community to contribute to and extend upon what it is possible to do with R. The Comprehensive R Archive Network, where packages are downloaded from when you install.packages(), helps illustrate the power that comes from an open-source community. It now lists 21,810 packages created by the R community.\nThese benefits are behind the rapid adoption of R within the social sciences, and academia and data analysis more broadly. However, compared to expensive closed source software where most analysis could be done through menus and mouse-clicks, using R requires learning the programming language. This is not the disadvantage it may at first seem. It does add an additional hurdle at the very start, but it also provides a far more powerful and flexible way to do data analysis. Menus are also not as simple and accessible as may first seem. A graph that can be created with a few lines of code in R may require going through multiple menus with 20+ mouse-clicks and entering text into multiple fields.\n\nRStudio is what is known as an ‘integrated development environment’, which is just a technical way of saying that it easier to write and run R code, manage files, view outputs, and so on within a (somewhat) user-friendly interface. R strictly speaking is merely the programming language. It comes bundled with a ‘command line console’ that runs the code, which is all you would see if you opened plain ‘R’ on your desktop.\nWithin RStudio the R console is placed in the bottom-left panel. The top-left is where ‘Sources’, such as R scripts and RMarkdown files, are opened.When you run a script, RMarkdown file, or individual code chunks, the lines of code they contain are sent by RStudio to the console. RStudio will then, as appropriate, add outputs from the console below code chunks or display them in the bottom-right Viewer panel.\nThe top-right panel includes the main R environment. The environment is the work space where all data and functions are stored during a session. When code that reads in a dataset is run, it stores the read dataset as a dataframe object in the environment. Loading a package similarly adds its functions as objects available in the environment.\n\nObjects are created in R by ‘assigning’ values/data to them. For example, the code below creates objects with the names a and b with numeric values 1 and 2 assigned to them respectively.\n\na &lt;- 1\nb &lt;- 2\n\nAfter running the above, the objects can be referred to by name in subsequent code, such as adding the values they contain together:\n\na + b\n\n[1] 3\n\n\nNote though that whilst R calculates the values of the two objects combined is 3, this output is not stored, as it is not being assigned to an object. If we wanted to store the value we could assign it to a new object:\n\nc &lt;- a + b # 1 + 2 = 3, so 'c' is assigned the value 3\n\nIn contrast, if we were to assign the a + b back to a, it is effectively saying “add a and b together and update a to store the combined value”.\n\na &lt;- a + b # 1 + 2 = 3, so 'a' is now assigned the value 3\na &lt;- a + b # 3 + 2 = 5, so 'a' is now assigned the value 5\na &lt;- a + b # 5 + 2 = 7, so 'a' is now assigned the value 7\na\n\n[1] 7\n\n\nThis is why it is important to ensure you create objects for values/data you want to store and take care that you do not accidentally assign changes to objects that you do not intend to make. If you receive an object not found error it is likely you have either mistyped the name of the object or have not assigned anything to it yet. If variables suddenly disappear or your graphs unexpectedly show different results when re-run, it is likely that you ran code that assigned a changed version of the data back to the object storing it.\n\ndataframe$variable\nrage\n\nreading in data sets\n\n\n\n\nRMarkdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with Code Chunks that can be converted to multiple file formats, such as HTML or PDF. This makes RMarkdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis.\nRMarkdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text (see the Formatting section below). Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. RMarkdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text and passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format.\n\nRMarkdown uses simple, human-readable syntax for basic formatting. Whilst different to how you would write text in a Word document, it is easy to learn. This simple transparent formatting also helps avoid the hidden complexities of Word document, where formatting issues often arise from inconsistent syntax that is hidden from the user.\nBelow is a screenshot from RStudio with text using the key basic syntax for formatting your main text:\n\nknitr::include_graphics(\"./images/markdownraw.png\")\n\nAnd, this is how it appears when knitted:\n\nknitr::include_graphics(\"./images/markdownknitted.png\")\n\nHeadings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level:\n\nknitr::include_graphics(\"./images/headingsraw.png\")\n\nAnd, how it appears when knitted:\n\nknitr::include_graphics(\"./images/headingsknitted.png\")\n\n\nThe main thing in RMarkdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between:\n\nEach paragraph\nBefore and after a list\nBefore and after a header\n\nHere’s some example text in an RMarkdown document, the first without line spacing, the second with:\n\nknitr::include_graphics(\"./images/linespacingraw.png\")\n\nAnd, how this looks when knitted:\n\nknitr::include_graphics(\"./images/linespacingknitted.png\")\n\nSide-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in RMarkdown.\n\nThere are three main ways to create a code chunk:\n\nClick the ‘Insert Code Chunk’ button and from the drop-down select ‘R’ at the top. (See gif below)\nPress Ctrl + Alt + I (Windows & Linux) or Cmd + Option + I (macOS)\nManually typing three back-ticks with ‘r’ in curly brackets before your code and adding three back-ticks after.\n\n\nknitr::include_graphics(\"./images/insertcodechunk.gif\")\n\nCurly brackets at the start of a code chunk are used to specify {programming-language optional name, options = values}. Since we are using R, all our code chunks have {r ...}. Within code chunks, the pound sign / hash, #, at the start of a line is used to add any comments:\n\nknitr::include_graphics(\"./images/codechunk.png\")\n\nSide-note, this may seem confusing given hash signs are used for headers in the main text. However, inside of a code chunk, all text is treated as code for the language specified in the curly brackets. In R, hash signs are used for comments, so they are treated as comments within the code chunk rather than as Markdown headers.\n\nAdding a label is useful as a quick way to remind yourself the purpose of each code chunk. The quick outline (button at bottom of Source panel) can be used to jump to specific sections and code chunks in your document. If you provide a label for your code chunk it will also appear here:\n\nknitr::include_graphics(\"./images/chunkaddingname.gif\")\n\n\nOptions are used to specify how code chunks are handled when run/knitted. Two key ones we have used in the labs and in the project template:\n\n\necho - whether the code chunk is displayed in knitted files.\n\ninclude - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files.\n\nBy default, the code chunk and the output are displayed in knitted files. For the interpretive report, you do not want to display the code chunks in your knitted HTML file, so ensure you add echo=FALSE to the options for any new code chunk you create. Alternatively, see the Setup Code Chunk section for information on how to set echo=FALSE as a ‘global option’.\nExamples when working in RMarkdown document:\n\nknitr::include_graphics(\"./images/echoincluderaw.png\")\n\nAnd when knitted:\n\nknitr::include_graphics(\"./images/echoincludeknitted.png\")\n\n\nThere are multiple ways to run your R code.\nWithin the top-right of each code chunk there are two buttons:\n\nknitr::include_graphics(\"./images/runchunk.png\")\n\n\nRun Current Chunk - which will run the code within that code chunks only.\nRun All Chunks Above - which will run all code chunks from the top of your document down to this code chunk.\n\nThe second is useful when a code chunk depends upon others being run first. Remember the R environment maintains the results of any previously run code, so you do not need to continuously run all previous chunks. However, this can be useful when debugging issues or if you restart your R environment, losing the results of previously run code, and needing to re-run everything before this chunk.\nFurther options can be found in the ‘Run’ drop-down menu, accessed from the top-right of the Source Panel:\n\nknitr::include_graphics(\"./images/runoptions.png\")\n\nUseful additional ones here are:\n\n\nRun Selected Line(s) - running all lines you have manually selected using the mouse / text cursor.\n\nRestart R and Run All Chunks - incredibly useful when you need to reset your R environment, this can be useful when debugging an error to figure out whether the issue stems from your current code or code you previously ran but now removed.\n\nNote as well that many of these options have keyboard shortcuts listed. Learning these pays off long-term as you will be able to write and run your code without needing to move your hands from the keyboard.\n\nKnitting a document runs all your R code from top to bottom, then combines the results from this with the Markdown text to convert these into different file formats. YAML is used to specify which file formats to convert to; see the YAML Block section for information on how this is set up within the project template.\nImportantly, when you knit an RMarkdown document, all code is run sequentially from top to bottom in a clean R environment. This ensures the document is reproducible. Anyone with a copy can knit it and produce the same results. This requires though that code to load any required packages are included within a code chunk in the document. Simply loading a package via the Console adds it to your current R environment and manually run code chunks will be able to access the package. However, when knitting, the clean R environment won’t have access to the package, and you’ll receive an error message when the knit process tries to run a function that requires the package. This is why it is important to include a code chunk at the top of your document that loads all required packages. See the Preamble Code Chunk for how to do this in the RMarkdown file provided in the interpretive report project template.\nAssuming you have YAML specifying which file format(s) to convert to, all you then need to do each time is simply click the ‘Knit’ button:\n\nknitr::include_graphics(\"./images/knitting.png\")\n\n\nIt is possible to access an RMarkdown cheat sheet (as well as ones for ggplot2 and dplyr!) from within RStudio:\n\nknitr::include_graphics(\"./images/markdowncheetsheet.gif\")\n\nNote, from the same ‘Help’ menu, just below ‘Cheat Sheets’ is an option for ‘Keyboard Shortcuts Help’. This displays a screen with most of the shortcuts available within RStudio. This also includes a link to ‘See All Shortcuts…’.\n\nknitr::include_graphics(\"./images/keyboard-shortcuts.gif\")\n\n\n\nThe top of the RMarkdown document includes a YAML block, enclosed with three dashes, ---, at the start and end. The YAML block must be at the very start of the document, so do not add any new lines before it.\nYAML is often used with Markdown as it provides a simple human-readable structure for configuring metadata, such as the document title, author, date, and desired output when knitting:\n\nknitr::include_graphics(\"./images/templateyaml.png\")\n\nPlease remember to add your student number - and not your name - to line 3.\nTo save having to remember to update the date before knitting and submitting your final version, you can use the following in the YAML block:\n\ndate: \"`r format(Sys.time(), '%d/%m/%y')`\"\n\nWhen knitted that code will automatically add the current date in dd/mm/yy format (e.g. 04/12/24):\n\nknitr::include_graphics(\"./images/knitteddate.png\")\n\n\nAfter the YAML Block, there is a ‘setup’ code chunk that sets global options for knitr. The include=FALSE in its options means that this chunk and its output are not displayed in your knitted files:\n\nknitr::include_graphics(\"./images/templatesetup.png\")\n\nGlobal options apply to all code chunks in the document. This means anything set in the global options does not have to be individually added to each code chunk. Note, you can set a global option and still set a different one for a specific code chunk where needed. For example, if you set option=FALSE globally, but needed it to be TRUE for a specific code chunk, all you need to do is add option=TRUE to its options to override the global default.\nThe global options set in the setup code chunk are:\n\n\nmessage=FALSE, which hides all non-warning messages when knitting code chunks. For example, when loading a library it will sometimes display non-warning messages.\n\nwarning=FALSE, which hides all warning messages when knitting code chunks. Again, making sure that no warnings messages displayed when running code is included in your knitted file.\n\nImportantly, you do not want to display your code chunks in your knitted file, only the tables and graphs they generate. To achieve this you could add echo=FALSE to the options for each individual code chunk. Or – you can simply add it as a global option in the setup code chunk!\n\nknitr::include_graphics(\"./images/templatesetupechofalse.png\")\n\n\nIt is best practice to install and load any required packages and read in any data being used at the top of the RMarkdown file. This keeps all package management and data reading in a single location at the start of the document, making your code simpler and clearer. It further ensures that when running/knitting your code that all required packages are loaded and dataframes created before the rest of your code is run.\nThe preamble code block within the template provides a structure following this practice.\n\nknitr::include_graphics(\"./images/templatepreamble.png\")\n\n\n\nInstall packages if missing, outlined in pink. This code may look complex, but all it does is create a list of packages that are going to be used, assigning it to the object list.of.packages. This list is checked against packages already installed, creating a new list new.packages containing only the names of packages not already installed. The final line then basically says “if the length of new.packages is 1 or more, then install all packages in the new.packages list”.\n\nWord count addin if missing, outlined in blue. There is not an R package for calculating word counts, but there is what is known as an ‘addin’ for RStudio. The code here checks if the addin is already installed and if not it downloads a copy of the addin from GitHub. Please see the Word Count Code section for further information on how this addin is used in the document.\n\nLoad packages, outlined in yellow. This is where all packages used in the analysis are loaded.\n\nRead data, outlined in red. The dataset used for the assignment in read in and assigned to the nilt dataframe object.\n\nImportant, as a dataset is provided with the project template and read in already for you, you do not need to download or read in any other dataset. The assignment uses a version of the NILT dataset that includes more variables than the version of the data we used in the lab sessions. Downloading and reading in the dataset used in the labs will result in variables disappearing from the regression results table. Please see the R Issues FAQ for more info.\nIf you require additional packages, the best way to add this would be:\n\nWithin the Install packages if missing section, add the package name at the end of the list that is being assigned to ‘list.of.packages’\nWithin the Load packages section, add a new library() line under the existing ones.\n\nFor example, if needing to install and load ‘vtable’ these are the changes that you would make:\n\nknitr::include_graphics(\"./images/addingvtable.png\")\n\nAs a gif:\n\nknitr::include_graphics(\"./images/addingpackages.gif\")\n\nNote, moving all code for installing and loading of packages into this preamble code chunk means you do not need, and can safely remove, any other install.package() and library() lines from the rest of your code chunks. This will reduce risk of encountering error messages and make it easier to debug them if any do occur.\n\nTables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. Note, you will need to knit your document each time you want to check the updated word count.\nThis is how the code to calculate the word count looks within the RMarkdown file:\n\nknitr::include_graphics(\"./images/wordcountinline.png\")\n\nSide-note, surrounding code with single back-ticks, (`), creates an “inline code chunk”, enabling you to add short snippets of code. The r at the start specifies that the code is R, similar to adding r in curly brackets for code chunks.\nThis is then how it looks when knitting the project template (assuming you haven’t added any additional text yet):\n\nknitr::include_graphics(\"./images/knittedwordcount.png\")\n\nWe use this addin as the built-in RStudio word count, accessed via the ‘Edit’ menu at the top of the screen, includes all code and comments. So, despite the addin calculating ‘0’, RStudio will provide:\n\nknitr::include_graphics(\"./images/rstudiowordcount.png\")\n\nImportant, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to wordcountaddin::word_count(\"Assignmet2-template.Rmd\") - 193.\n\nThe RMarkdown document provided in the project template includes headed sections you can use alongside suggested word count for each and brief summary reminder of what to include in each section. The Course Handbook, pages 26-30, provide a more detailed breakdown of what to include in each section.\nScreenshot of the outline and commented suggestions:\n\nknitr::include_graphics(\"./images/templateoutline.png\")\n\nImportant, within the “Results and discussion” section, ensure to add your interpretation after the code chunk that creates the regression results table:\n\nknitr::include_graphics(\"./images/regressioninterpretativeaftercode.png\")\n\n\nThe code chunk that runs the multiple linear regression model and creates the table with the regression results can be found in the ‘Results and discussion’ section:\n\nknitr::include_graphics(\"./images/templateresults.png\")\n\n\nFirst the model is run, outlined in pink, and assigned to the model object.\nNext a list with labels to use for the independent and control variables in the regression table is created, outlined in blue, and assigned to the cov_labels object.\nFinally, the stargazer package is used to create the regression results table, outlined in red. It is passed the model and cov_label objects, highlighted in yellow, alongside arguments for outputting the table in HTML, a title, and caption & label to use for the dependent variable.\n\nStargazer produces well-formatted regression tables, but with the downside that when running the code chunk in RStudio, you will only see the raw HTML syntax that is created:\n\nknitr::include_graphics(\"./images/templatehtmltable.png\")\n\nIn order to view the table, you will need to knit your document and view the outputted HTML file:\n\nknitr::include_graphics(\"./images/knitting.png\")\n\nYour knitted HTML file should then open in a new popup window. However, you might instead receive a dialogue window saying that a pop-up was prevented from opening, if so just click ‘Try again’:\n\nknitr::include_graphics(\"./images/tryagain.png\")\n\nAlternatively, you can open the last knitted version of your HTML file from the ‘Files’ panel. Click the HTML file and select ‘View in Web Browser’:\n\nknitr::include_graphics(\"./images/openknittedhtml.gif\")\n\nBelow is how your regression results table should look in the knitted HTML file:\n\nknitr::include_graphics(\"./images/knittedresultstable.png\")\n\nIf it doesn’t, please see the R Issues FAQ.",
    "crumbs": [
      "RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/InterpretiveReportStructure.html#introduction",
    "href": "help/InterpretiveReportStructure.html#introduction",
    "title": "RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "This appendix provides additional information on using R Markdown and an overview of the structure of the R Markdown document provided with the interpretive report project template.\nPlease ensure to review the relevant lab materials:\n\n\nLab 5 session on R Markdown for an intro to R Markdown.\n\nLab 10 Session for Interpreting Quantitative Findings Report summative assessment for instructions on how to create a project on RStudio Cloud for your interpretive report using the provided template.\n\nSimilarly, if you encounter any errors, please ensure to check the R Issues FAQ first for any existing solutions.\nThe first half of this page provides an overview of R Markdown and the second half covers the Interpretive Report Template in more detail. Importantly, please ensure you at least read the Preamble Code Chunk section for information on best practice for installing and loading any additional packages.",
    "crumbs": [
      "RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/InterpretiveReportStructure.html#r-basics",
    "href": "help/InterpretiveReportStructure.html#r-basics",
    "title": "RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "R is a programming language used for data analysis and visualisation. A large community has built up around R through the combination of it being free and open-source software and a full featured programming language. This makes it possible for the community to contribute to and extend upon what it is possible to do with R. The Comprehensive R Archive Network, where packages are downloaded from when you install.packages(), helps illustrate the power that comes from an open-source community. It now lists 21,810 packages created by the R community.\nThese benefits are behind the rapid adoption of R within the social sciences, and academia and data analysis more broadly. However, compared to expensive closed source software where most analysis could be done through menus and mouse-clicks, using R requires learning the programming language. This is not the disadvantage it may at first seem. It does add an additional hurdle at the very start, but it also provides a far more powerful and flexible way to do data analysis. Menus are also not as simple and accessible as may first seem. A graph that can be created with a few lines of code in R may require going through multiple menus with 20+ mouse-clicks and entering text into multiple fields.\n\nRStudio is what is known as an ‘integrated development environment’, which is just a technical way of saying that it easier to write and run R code, manage files, view outputs, and so on within a (somewhat) user-friendly interface. R strictly speaking is merely the programming language. It comes bundled with a ‘command line console’ that runs the code, which is all you would see if you opened plain ‘R’ on your desktop.\nWithin RStudio the R console is placed in the bottom-left panel. The top-left is where ‘Sources’, such as R scripts and RMarkdown files, are opened.When you run a script, RMarkdown file, or individual code chunks, the lines of code they contain are sent by RStudio to the console. RStudio will then, as appropriate, add outputs from the console below code chunks or display them in the bottom-right Viewer panel.\nThe top-right panel includes the main R environment. The environment is the work space where all data and functions are stored during a session. When code that reads in a dataset is run, it stores the read dataset as a dataframe object in the environment. Loading a package similarly adds its functions as objects available in the environment.\n\nObjects are created in R by ‘assigning’ values/data to them. For example, the code below creates objects with the names a and b with numeric values 1 and 2 assigned to them respectively.\n\na &lt;- 1\nb &lt;- 2\n\nAfter running the above, the objects can be referred to by name in subsequent code, such as adding the values they contain together:\n\na + b\n\n[1] 3\n\n\nNote though that whilst R calculates the values of the two objects combined is 3, this output is not stored, as it is not being assigned to an object. If we wanted to store the value we could assign it to a new object:\n\nc &lt;- a + b # 1 + 2 = 3, so 'c' is assigned the value 3\n\nIn contrast, if we were to assign the a + b back to a, it is effectively saying “add a and b together and update a to store the combined value”.\n\na &lt;- a + b # 1 + 2 = 3, so 'a' is now assigned the value 3\na &lt;- a + b # 3 + 2 = 5, so 'a' is now assigned the value 5\na &lt;- a + b # 5 + 2 = 7, so 'a' is now assigned the value 7\na\n\n[1] 7\n\n\nThis is why it is important to ensure you create objects for values/data you want to store and take care that you do not accidentally assign changes to objects that you do not intend to make. If you receive an object not found error it is likely you have either mistyped the name of the object or have not assigned anything to it yet. If variables suddenly disappear or your graphs unexpectedly show different results when re-run, it is likely that you ran code that assigned a changed version of the data back to the object storing it.\n\ndataframe$variable\nrage\n\nreading in data sets",
    "crumbs": [
      "RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/InterpretiveReportStructure.html#rmarkdown",
    "href": "help/InterpretiveReportStructure.html#rmarkdown",
    "title": "RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "RMarkdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with Code Chunks that can be converted to multiple file formats, such as HTML or PDF. This makes RMarkdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis.\nRMarkdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text (see the Formatting section below). Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. RMarkdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text and passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format.\n\nRMarkdown uses simple, human-readable syntax for basic formatting. Whilst different to how you would write text in a Word document, it is easy to learn. This simple transparent formatting also helps avoid the hidden complexities of Word document, where formatting issues often arise from inconsistent syntax that is hidden from the user.\nBelow is a screenshot from RStudio with text using the key basic syntax for formatting your main text:\n\nknitr::include_graphics(\"./images/markdownraw.png\")\n\nAnd, this is how it appears when knitted:\n\nknitr::include_graphics(\"./images/markdownknitted.png\")\n\nHeadings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level:\n\nknitr::include_graphics(\"./images/headingsraw.png\")\n\nAnd, how it appears when knitted:\n\nknitr::include_graphics(\"./images/headingsknitted.png\")\n\n\nThe main thing in RMarkdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between:\n\nEach paragraph\nBefore and after a list\nBefore and after a header\n\nHere’s some example text in an RMarkdown document, the first without line spacing, the second with:\n\nknitr::include_graphics(\"./images/linespacingraw.png\")\n\nAnd, how this looks when knitted:\n\nknitr::include_graphics(\"./images/linespacingknitted.png\")\n\nSide-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in RMarkdown.\n\nThere are three main ways to create a code chunk:\n\nClick the ‘Insert Code Chunk’ button and from the drop-down select ‘R’ at the top. (See gif below)\nPress Ctrl + Alt + I (Windows & Linux) or Cmd + Option + I (macOS)\nManually typing three back-ticks with ‘r’ in curly brackets before your code and adding three back-ticks after.\n\n\nknitr::include_graphics(\"./images/insertcodechunk.gif\")\n\nCurly brackets at the start of a code chunk are used to specify {programming-language optional name, options = values}. Since we are using R, all our code chunks have {r ...}. Within code chunks, the pound sign / hash, #, at the start of a line is used to add any comments:\n\nknitr::include_graphics(\"./images/codechunk.png\")\n\nSide-note, this may seem confusing given hash signs are used for headers in the main text. However, inside of a code chunk, all text is treated as code for the language specified in the curly brackets. In R, hash signs are used for comments, so they are treated as comments within the code chunk rather than as Markdown headers.\n\nAdding a label is useful as a quick way to remind yourself the purpose of each code chunk. The quick outline (button at bottom of Source panel) can be used to jump to specific sections and code chunks in your document. If you provide a label for your code chunk it will also appear here:\n\nknitr::include_graphics(\"./images/chunkaddingname.gif\")\n\n\nOptions are used to specify how code chunks are handled when run/knitted. Two key ones we have used in the labs and in the project template:\n\n\necho - whether the code chunk is displayed in knitted files.\n\ninclude - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files.\n\nBy default, the code chunk and the output are displayed in knitted files. For the interpretive report, you do not want to display the code chunks in your knitted HTML file, so ensure you add echo=FALSE to the options for any new code chunk you create. Alternatively, see the Setup Code Chunk section for information on how to set echo=FALSE as a ‘global option’.\nExamples when working in RMarkdown document:\n\nknitr::include_graphics(\"./images/echoincluderaw.png\")\n\nAnd when knitted:\n\nknitr::include_graphics(\"./images/echoincludeknitted.png\")\n\n\nThere are multiple ways to run your R code.\nWithin the top-right of each code chunk there are two buttons:\n\nknitr::include_graphics(\"./images/runchunk.png\")\n\n\nRun Current Chunk - which will run the code within that code chunks only.\nRun All Chunks Above - which will run all code chunks from the top of your document down to this code chunk.\n\nThe second is useful when a code chunk depends upon others being run first. Remember the R environment maintains the results of any previously run code, so you do not need to continuously run all previous chunks. However, this can be useful when debugging issues or if you restart your R environment, losing the results of previously run code, and needing to re-run everything before this chunk.\nFurther options can be found in the ‘Run’ drop-down menu, accessed from the top-right of the Source Panel:\n\nknitr::include_graphics(\"./images/runoptions.png\")\n\nUseful additional ones here are:\n\n\nRun Selected Line(s) - running all lines you have manually selected using the mouse / text cursor.\n\nRestart R and Run All Chunks - incredibly useful when you need to reset your R environment, this can be useful when debugging an error to figure out whether the issue stems from your current code or code you previously ran but now removed.\n\nNote as well that many of these options have keyboard shortcuts listed. Learning these pays off long-term as you will be able to write and run your code without needing to move your hands from the keyboard.\n\nKnitting a document runs all your R code from top to bottom, then combines the results from this with the Markdown text to convert these into different file formats. YAML is used to specify which file formats to convert to; see the YAML Block section for information on how this is set up within the project template.\nImportantly, when you knit an RMarkdown document, all code is run sequentially from top to bottom in a clean R environment. This ensures the document is reproducible. Anyone with a copy can knit it and produce the same results. This requires though that code to load any required packages are included within a code chunk in the document. Simply loading a package via the Console adds it to your current R environment and manually run code chunks will be able to access the package. However, when knitting, the clean R environment won’t have access to the package, and you’ll receive an error message when the knit process tries to run a function that requires the package. This is why it is important to include a code chunk at the top of your document that loads all required packages. See the Preamble Code Chunk for how to do this in the RMarkdown file provided in the interpretive report project template.\nAssuming you have YAML specifying which file format(s) to convert to, all you then need to do each time is simply click the ‘Knit’ button:\n\nknitr::include_graphics(\"./images/knitting.png\")\n\n\nIt is possible to access an RMarkdown cheat sheet (as well as ones for ggplot2 and dplyr!) from within RStudio:\n\nknitr::include_graphics(\"./images/markdowncheetsheet.gif\")\n\nNote, from the same ‘Help’ menu, just below ‘Cheat Sheets’ is an option for ‘Keyboard Shortcuts Help’. This displays a screen with most of the shortcuts available within RStudio. This also includes a link to ‘See All Shortcuts…’.\n\nknitr::include_graphics(\"./images/keyboard-shortcuts.gif\")",
    "crumbs": [
      "RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/InterpretiveReportStructure.html#interpretive-report-template",
    "href": "help/InterpretiveReportStructure.html#interpretive-report-template",
    "title": "RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "The top of the RMarkdown document includes a YAML block, enclosed with three dashes, ---, at the start and end. The YAML block must be at the very start of the document, so do not add any new lines before it.\nYAML is often used with Markdown as it provides a simple human-readable structure for configuring metadata, such as the document title, author, date, and desired output when knitting:\n\nknitr::include_graphics(\"./images/templateyaml.png\")\n\nPlease remember to add your student number - and not your name - to line 3.\nTo save having to remember to update the date before knitting and submitting your final version, you can use the following in the YAML block:\n\ndate: \"`r format(Sys.time(), '%d/%m/%y')`\"\n\nWhen knitted that code will automatically add the current date in dd/mm/yy format (e.g. 04/12/24):\n\nknitr::include_graphics(\"./images/knitteddate.png\")\n\n\nAfter the YAML Block, there is a ‘setup’ code chunk that sets global options for knitr. The include=FALSE in its options means that this chunk and its output are not displayed in your knitted files:\n\nknitr::include_graphics(\"./images/templatesetup.png\")\n\nGlobal options apply to all code chunks in the document. This means anything set in the global options does not have to be individually added to each code chunk. Note, you can set a global option and still set a different one for a specific code chunk where needed. For example, if you set option=FALSE globally, but needed it to be TRUE for a specific code chunk, all you need to do is add option=TRUE to its options to override the global default.\nThe global options set in the setup code chunk are:\n\n\nmessage=FALSE, which hides all non-warning messages when knitting code chunks. For example, when loading a library it will sometimes display non-warning messages.\n\nwarning=FALSE, which hides all warning messages when knitting code chunks. Again, making sure that no warnings messages displayed when running code is included in your knitted file.\n\nImportantly, you do not want to display your code chunks in your knitted file, only the tables and graphs they generate. To achieve this you could add echo=FALSE to the options for each individual code chunk. Or – you can simply add it as a global option in the setup code chunk!\n\nknitr::include_graphics(\"./images/templatesetupechofalse.png\")\n\n\nIt is best practice to install and load any required packages and read in any data being used at the top of the RMarkdown file. This keeps all package management and data reading in a single location at the start of the document, making your code simpler and clearer. It further ensures that when running/knitting your code that all required packages are loaded and dataframes created before the rest of your code is run.\nThe preamble code block within the template provides a structure following this practice.\n\nknitr::include_graphics(\"./images/templatepreamble.png\")\n\n\n\nInstall packages if missing, outlined in pink. This code may look complex, but all it does is create a list of packages that are going to be used, assigning it to the object list.of.packages. This list is checked against packages already installed, creating a new list new.packages containing only the names of packages not already installed. The final line then basically says “if the length of new.packages is 1 or more, then install all packages in the new.packages list”.\n\nWord count addin if missing, outlined in blue. There is not an R package for calculating word counts, but there is what is known as an ‘addin’ for RStudio. The code here checks if the addin is already installed and if not it downloads a copy of the addin from GitHub. Please see the Word Count Code section for further information on how this addin is used in the document.\n\nLoad packages, outlined in yellow. This is where all packages used in the analysis are loaded.\n\nRead data, outlined in red. The dataset used for the assignment in read in and assigned to the nilt dataframe object.\n\nImportant, as a dataset is provided with the project template and read in already for you, you do not need to download or read in any other dataset. The assignment uses a version of the NILT dataset that includes more variables than the version of the data we used in the lab sessions. Downloading and reading in the dataset used in the labs will result in variables disappearing from the regression results table. Please see the R Issues FAQ for more info.\nIf you require additional packages, the best way to add this would be:\n\nWithin the Install packages if missing section, add the package name at the end of the list that is being assigned to ‘list.of.packages’\nWithin the Load packages section, add a new library() line under the existing ones.\n\nFor example, if needing to install and load ‘vtable’ these are the changes that you would make:\n\nknitr::include_graphics(\"./images/addingvtable.png\")\n\nAs a gif:\n\nknitr::include_graphics(\"./images/addingpackages.gif\")\n\nNote, moving all code for installing and loading of packages into this preamble code chunk means you do not need, and can safely remove, any other install.package() and library() lines from the rest of your code chunks. This will reduce risk of encountering error messages and make it easier to debug them if any do occur.\n\nTables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. Note, you will need to knit your document each time you want to check the updated word count.\nThis is how the code to calculate the word count looks within the RMarkdown file:\n\nknitr::include_graphics(\"./images/wordcountinline.png\")\n\nSide-note, surrounding code with single back-ticks, (`), creates an “inline code chunk”, enabling you to add short snippets of code. The r at the start specifies that the code is R, similar to adding r in curly brackets for code chunks.\nThis is then how it looks when knitting the project template (assuming you haven’t added any additional text yet):\n\nknitr::include_graphics(\"./images/knittedwordcount.png\")\n\nWe use this addin as the built-in RStudio word count, accessed via the ‘Edit’ menu at the top of the screen, includes all code and comments. So, despite the addin calculating ‘0’, RStudio will provide:\n\nknitr::include_graphics(\"./images/rstudiowordcount.png\")\n\nImportant, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to wordcountaddin::word_count(\"Assignmet2-template.Rmd\") - 193.\n\nThe RMarkdown document provided in the project template includes headed sections you can use alongside suggested word count for each and brief summary reminder of what to include in each section. The Course Handbook, pages 26-30, provide a more detailed breakdown of what to include in each section.\nScreenshot of the outline and commented suggestions:\n\nknitr::include_graphics(\"./images/templateoutline.png\")\n\nImportant, within the “Results and discussion” section, ensure to add your interpretation after the code chunk that creates the regression results table:\n\nknitr::include_graphics(\"./images/regressioninterpretativeaftercode.png\")\n\n\nThe code chunk that runs the multiple linear regression model and creates the table with the regression results can be found in the ‘Results and discussion’ section:\n\nknitr::include_graphics(\"./images/templateresults.png\")\n\n\nFirst the model is run, outlined in pink, and assigned to the model object.\nNext a list with labels to use for the independent and control variables in the regression table is created, outlined in blue, and assigned to the cov_labels object.\nFinally, the stargazer package is used to create the regression results table, outlined in red. It is passed the model and cov_label objects, highlighted in yellow, alongside arguments for outputting the table in HTML, a title, and caption & label to use for the dependent variable.\n\nStargazer produces well-formatted regression tables, but with the downside that when running the code chunk in RStudio, you will only see the raw HTML syntax that is created:\n\nknitr::include_graphics(\"./images/templatehtmltable.png\")\n\nIn order to view the table, you will need to knit your document and view the outputted HTML file:\n\nknitr::include_graphics(\"./images/knitting.png\")\n\nYour knitted HTML file should then open in a new popup window. However, you might instead receive a dialogue window saying that a pop-up was prevented from opening, if so just click ‘Try again’:\n\nknitr::include_graphics(\"./images/tryagain.png\")\n\nAlternatively, you can open the last knitted version of your HTML file from the ‘Files’ panel. Click the HTML file and select ‘View in Web Browser’:\n\nknitr::include_graphics(\"./images/openknittedhtml.gif\")\n\nBelow is how your regression results table should look in the knitted HTML file:\n\nknitr::include_graphics(\"./images/knittedresultstable.png\")\n\nIf it doesn’t, please see the R Issues FAQ.",
    "crumbs": [
      "RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/RFAQ.html",
    "href": "help/RFAQ.html",
    "title": "R FAQ",
    "section": "",
    "text": "This appendix contains information for solving common issues students have reported when working on their interpretive findings report.\nUpdates will continue being added to this appendix with information for any new issues being reported. So, if you experience an issue, this page will be a good first port of call to check for any existing solution.",
    "crumbs": [
      "R FAQ"
    ]
  },
  {
    "objectID": "help/RFAQ.html#introduction",
    "href": "help/RFAQ.html#introduction",
    "title": "R FAQ",
    "section": "",
    "text": "This appendix contains information for solving common issues students have reported when working on their interpretive findings report.\nUpdates will continue being added to this appendix with information for any new issues being reported. So, if you experience an issue, this page will be a good first port of call to check for any existing solution.",
    "crumbs": [
      "R FAQ"
    ]
  },
  {
    "objectID": "help/RFAQ.html#reporting-r-issues",
    "href": "help/RFAQ.html#reporting-r-issues",
    "title": "R FAQ",
    "section": "Reporting R Issues",
    "text": "Reporting R Issues\nWhen making a Moodle post or emailing about an issue it is vital to provide sufficient context to enable others to help find a solution. This includes:\n\nThe full text of any error message that you are receiving.\nAny relevant code that is producing the error.\n\nError messages will often flag the relevant code chunk where the error occurred:\n\nknitr::include_graphics(\"./images/errorlines.png\")\n\nSo, based on the above error message you would include the code within the code block at lines 49-50.\nAdditionally, when emailing, please also include:\n\nYour lab group number.\nIf already made a Moodle post about the issue, a link to the post.",
    "crumbs": [
      "R FAQ"
    ]
  },
  {
    "objectID": "help/RFAQ.html#regression-results-table-issues",
    "href": "help/RFAQ.html#regression-results-table-issues",
    "title": "R FAQ",
    "section": "Regression Results Table Issues",
    "text": "Regression Results Table Issues\nIf variables go missing from your regression results table or the regression results are different to how they originally appeared when you knitted the template, you may have unintentionally replaced the data included with the template.\nThe preamble code block at the top of the RMarkdown file provide with the template reads in the ‘fullnilt_2012.rds’ dataset and assigns it to the ‘nilt’ data frame object.\n\nknitr::include_graphics(\"./images/preamble_readRDS.png\")\n\nYou therefore do not need to download and read in anther dataset, this is all setup for you already within the preamble code block. To resolve the issue, remove all code where you download and assign another dataset to the nilt data frame.\nFor example, these would be lines to remove:\n\nknitr::include_graphics(\"./images/removelines.png\")\n\nThe ‘NILT2012GR’ that we used in the labs has a different set of the NILT variables, and does not contain all the variables in the ‘fullnilt_2012’ dataset provided in the template. The above code then would result in variables disappearing from your regression results table.\nSimilarly, this would also be code to remove:\n\nknitr::include_graphics(\"./images/removelines2.png\")\n\nThe above code replaces the original nilt data frame with a version that only includes the 5 variables listed within the selection function. Again, resulting in variables disappearing from the regression results table.\nThe select() function keeps variables passed to it and drops the others. The first argument is always the dataframe being selected from, select(dataframe, ...), with variables to keep from it listed after, select(dataframe, variable1, variable2, ...). Assigning, &lt;-, select() to the same dataframe, dataframe &lt;- select(dataframe, ..), is effectively saying, ‘from this dataframe keep these variables and permanently remove the others’. Unless your whole analysis is only going to use that selection, always assign it to a new dataframe instead, dataframe_subset &lt;- select(dataframe, ...).\nIf you need to create a subset of the data, ensure to assign it to a new data frame:\n\nknitr::include_graphics(\"./images/subsetcorrect.png\")\n\nNote, always consider whether you actually need to create a subset. Most functions have arguments for defining which variables to use, so you do not necessarily need to provide a subset to them.",
    "crumbs": [
      "R FAQ"
    ]
  },
  {
    "objectID": "help/RFAQ.html#object-nilt_subset-not-found",
    "href": "help/RFAQ.html#object-nilt_subset-not-found",
    "title": "R FAQ",
    "section": "Object ‘nilt_subset’ Not Found",
    "text": "Object ‘nilt_subset’ Not Found\n‘Object not found’ errors often arise when no data has been assigned to a data frame object. For example, if you ran code using ‘nilt_subset’, such as sumtable(nilt_subset, ...) without first assigning data to it nilt_subset &lt;- ..., you would receive the following error message when running/knitting your code:\n\nknitr::include_graphics(\"./images/objectnotfound.png\")\n\nWhen working with a subset of the data, ensure before the lines identified in the error message that you included code that assigns data to it (dataframe &lt;- ...). For example, if you need to work with a subset of data that used filter() or select():\n\nknitr::include_graphics(\"./images/subsetexamples.png\")\n\nIf you already have code assigning data to the object and do so before the code that is producing the error message, try running the lines of code where you assigning the data again, or ‘Run All’. If you still receive an error message, double-check for typos and capitalisation, nilt_subset, NILT_subset, and ni1t_subset will be treated as different objects.\nIf the code was previously working, it may be the case that you have accidentally deleted code blocks / lines that assigned data to the object, added new code above the lines where you assigned data, or moved code where the lines assigning data to the object and calling it within a function are now in the wrong order.\nNote, always consider whether you actually need to create a subset. Most functions have arguments for defining which variables to use, so you do not necessarily need to provide a subset to them.",
    "crumbs": [
      "R FAQ"
    ]
  },
  {
    "objectID": "help/RFAQ.html#wrong-word-count",
    "href": "help/RFAQ.html#wrong-word-count",
    "title": "R FAQ",
    "section": "Wrong Word Count",
    "text": "Wrong Word Count\nIf the word count displayed at the top of your knitted HTML is wrong, check whether the line of code that calculates the word count refers to the correct RMarkdown file you are using.\nWithin the template RMarkdown document, we include an inline code block that uses a word count addin to calculate your word count within the knitted HTML file:\n\nknitr::include_graphics(\"./images/wordcountinline.png\")\n\nWhich when knitted will look as follows in the HTML file:\n\nknitr::include_graphics(\"./images/knittedwordcount.png\")\n\nHowever, the code to calculate the word count refers to a specific file “Assignmet2-template.Rmd”. if you created a new RMarkdown file, such as one that included your student number ‘Assignment2-255615q.Rmd’, you also need to update the file name in the code:\n\nknitr::include_graphics(\"./images/wordcountchange.png\")\n\nNote, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to “wordcountaddin::word_count(”Assignmet2-template.Rmd”) - 193”.",
    "crumbs": [
      "R FAQ"
    ]
  },
  {
    "objectID": "help/RFAQ.html#unable-to-knit",
    "href": "help/RFAQ.html#unable-to-knit",
    "title": "R FAQ",
    "section": "Unable to Knit",
    "text": "Unable to Knit\nPackage could not be loaded\nIf receiving a “package ‘…’ could not be loaded” error message, check the YAML code block at the top of your RMarkdown file. Where this error message has occurred previously, it is due to using the ‘vtable’ package and the YAML code block includes pdf as an output.\nYou should have either of the following for output: ...:\n\nknitr::include_graphics(\"./images/outputhtml1.png\")\n\n\nknitr::include_graphics(\"./images/outputhtml2.png\")\n\nBut, will probably have the something like the following if receiving the error message:\n\nknitr::include_graphics(\"./images/outputhtmlpdf.png\")\n\nThis can happen unintentionally as RStudio can automatically add a line for PDF outline, such as if accidentally clicked ‘Knit to PDF’:\n\nknitr::include_graphics(\"./images/accidentalpdf.gif\")\n\nAnnoyingly, even if you click ‘Knit to HTML’ from the options, if your YAML block includes pdf_document in its outputs, R will still try to create a PDF document and keep running into the error message.\nSo, if you are receiving the error message and have the two output lines, you can easily fix it by removing the pdf_document line:\n\nknitr::include_graphics(\"./images/deletepdfoutput.gif\")\n\nNote, as the YAML block is already specifying to create an HTML output, you can just click the ‘Knit’ button directly each time rather than clicking the wee down arrow to select ‘Knit to HTML’ specifically.\nExecution Halted\nThe most common cause of this error is from renaming the RMarkdown file, but not updating the line of code that calculates the word count. See Wrong Word Count section for general info.\nFor example, if you renamed the RMarkdwon file from ‘Assignmet2-template.Rmd’ to ‘Assignment2-255615q.Rmd’, you will receive an ‘Execution halted’ error when trying to knit:\n\nknitr::include_graphics(\"./images/exechalted.png\")\n\nTo fix it, just update the line of code that calculates the word count so it refers to the new name you have given the file:\n\nknitr::include_graphics(\"./images/wordcountchange.png\")",
    "crumbs": [
      "R FAQ"
    ]
  },
  {
    "objectID": "help/RFAQ.html#turnitin-unable-to-open-html-file",
    "href": "help/RFAQ.html#turnitin-unable-to-open-html-file",
    "title": "R FAQ",
    "section": "Turnitin Unable to Open HTML File",
    "text": "Turnitin Unable to Open HTML File\nTo avoid such potential issues, please ensure to submit an exported version of your knitted HTML file rather than one saved through your browser.\nIf Turnitin is providing an error message that it is unable to open your submitted HTML file, it may be due to having saved the file through your browser (e.g. right-clicking and ‘Save as…’) rather than exporting it from RStudio. Whilst HTML files saved from the browser and exported from RStudio will look absolutely identical when opened, some browsers - and browser extensions - will add additional code within the HTML file. This additional code does not change how the file looks when opened, but creates issues for Turnitin.\nTo export your knitted HTML file - 1. Check the box next to your knitted HTML file in the ‘Files’ panel. By default, the Files panel is on the bottom right of the screen.\n\nknitr::include_graphics(\"./images/exporthtml1.png\")\n\n\nClick ‘More’ from the toolbar at the top of the Files panel.\n\n\nknitr::include_graphics(\"./images/exporthtml2.png\")\n\n\nClick ‘Export’ from the menu options that pop-up.\n\n\nknitr::include_graphics(\"./images/exporthtml3.png\")\n\n\nThis will pop open an ‘Export Files’ dialogue. Here, you can rename the file to include your student number. Just make sure the “.html” at the end remains included.\n\n\nknitr::include_graphics(\"./images/exporthtml4.png\")\n\n\nAfter naming the file, click ‘Download’. This will open a dialogue to select which location to save the file.\n\n\nknitr::include_graphics(\"./images/exporthtml5.png\")\n\nHere’s a short gif running through all the steps together.\n\nknitr::include_graphics(\"./images/exporthtml.gif\")\n\nPlease note: Don’t panic if Turnitin is unable to open your file. We are aware of the issue and you will not be penalised if you submit a file on time, but then have to resubmit after the deadline due to Turnitin being unable to open your original submission.",
    "crumbs": [
      "R FAQ"
    ]
  },
  {
    "objectID": "help/rstudio_helper.html",
    "href": "help/rstudio_helper.html",
    "title": "RStudio Helper GPT",
    "section": "",
    "text": "RStudio Helper is designed to address common issues when using GenAI to learn quantitative data analysis with R.\nYou can access it as a Copilot Saved Prompt and a ChatGPT Custom GPT:\nLink to Copilot Saved Prompt\nLink to ChatGPT Custom GPT\nOn this page, you will also find:\nTo be clear, your GenAI use on this course is not limited to RStudio Helper. Instead, it is made available to help you get more useful responses from GenAI. The instructions with explanations are provided on this page, so you can see how to achieve similar when writing your own prompts. Overtime as you build your quants skills and awareness of common issues in GenAI responses, you will want to experiment with writing prompts tailored to your needs and use-cases.\nWhilst this GPT is also set up with instructions to mitigate GenAI’s predilection towards undermining learning and disregard for academic integrity, due to the inherent limitations with GenAI care still needs to be taken when using it. Please ensure to read the information about permitted GenAI use for the assessments and the SLD’s Quick Guidance for Students page."
  },
  {
    "objectID": "help/rstudio_helper.html#what-problems-does-this-gpt-address",
    "href": "help/rstudio_helper.html#what-problems-does-this-gpt-address",
    "title": "RStudio Helper GPT",
    "section": "What problems does this GPT address?",
    "text": "What problems does this GPT address?\nA key issue with GenAI models - such as ChatGPT, Gemini, and Claude - is that by default they are over-eager to do as much work as possible for you. The default response structure is “short opener, work done on behalf of user, suggestions for what else it can do”. An easy way to see this is to prompt GenAI “I need help improving the clarity of this paragraph -” along with a paragraph of text. More often than not, it will response with “Here’s a clearer and more concise version of your paragraph” and end with overly terse explanation for the changes. Sometimes it won’t even bother to offer an explanation.\nGenAI is also nowhere near as ‘intelligent’ as AI companies and online AI boosters claim. This is especially the case when using GenAI for R. GenAI models can generate over-convulted code, sometimes providing 10+ lines of code for something that can be done in 1-4 lines instead. Without including additional relevant information, responses tend towards being abstract and assuming a reasonable degree of prior knowledge. This can result in misleading and confusing responses as key information useful for beginners gets left out. Responses can also include fabricated data, whilst claiming the code it is spitting out is based on the dataset you provided it."
  },
  {
    "objectID": "help/rstudio_helper.html#instructions",
    "href": "help/rstudio_helper.html#instructions",
    "title": "RStudio Helper GPT",
    "section": "Instructions",
    "text": "Instructions\nBelow is a copy of the instructions used to create RStudio Helper GPT. If you hover your mouse over the gray box, you’ll see a clipboard appear in the top-right of it, that you can clip to copy the instructions in full to the clipboard.\n\n\n## Role and General Interaction\n\nRStudio Helper assists users learn R for quantitative social science. Users are honours-level undergraduate social science students. They are new to quantitative methods, statistics, and R. They are using RStudio via Posit Cloud, the tidyverse package, and R Markdown. Users are based in the UK, so use UK measurements.\n\nYou provide actionable advice through textbook style explanations and code chunks with detailed accessible documentation that breaks down and explains the code bit by bit. When users provide their own code with an error message or ask about writing code for a specific dataset, you always continue using textbook style examples and accessible documentation to guide users in learning how to debug error messages and write code themselves. Where appropriate include information relevant for data analysis and interpretation within the social sciences rather than making abstract simplistic statements about 'good' sample sizes and model fit results. \n\nYou have an ardent indefatigable desire to aid students learn quantitative analysis, doing so by giving detailed beginner-friendly explanations in a formal but friendly tone that ALWAYS follow the 'Golden Rules' below.\n\n## Golden Rules\n\nRule one: Support students in their learning, NEVER do the work for them. Academic integrity must always be maintained. Under no circumstances do you ever directly fix code provided to you, write code for specific datasets that can be copy and pasted, nor interpret statistical results on behalf of the user.\n\nRule two: Across all forms of response, NEVER use the exact dataset, variables, and values if these are provided by the user. You can use analogous examples, but keep it general. If a user is asking about a categorical variable on 'religcat' that stores value of respondents' religion, give a textbook example with another categorical variable such as employment. If they mention a variable for number of children, use an example for number of jobs. Never use an overly similar example, such as using 'annual income' in your example if the user mentioned 'income' or 'monthly income'.\n\nRule three: NEVER interpret statistical results for the user. If they provide a copy of a plot, table, or similar, NEVER interpret these for them. Instead give a general textbook explanation for how the type of graph, table, and so on can be interpreted, avoiding all specifics of what was provided to you. Within your explanation follow rule two and NEVER use the same variables and statistical results as provided by the user. For example, if their prompt mentioned employment status, use a different categorical variable for your explanation. Similarly, use different values and statistical results to the ones provided by the user.\n\nRule four: NEVER assume information about variables mentioned by the user. If a user mentions a variable for 'age' do not write a full response assuming it is interval or categorical. Instead first ask the user to clarify, with details for how they can check using R. Only once you have this information should you provide a full response.\n\n## Contextual Responses\n\nAdapt responses to the context of the learning environment. Write accessible explanations for social science students who are new to quantitative data analysis, RStudio, R, tidyverse, and R Markdown. The structure of R Markdown files and code chunks should follow best data analysis and coding practices.\n\n- Always use the tidyverse, 'tidyverse friendly' packages, and vtable or modelsummary for table packages.\n- Prefer `|&gt;` over `%&gt;%` for pipes.\n- Load the tidyverse rather than any specific individual package from it - installing it if have not done so already for the current project. For example, if needing ggplot2, load the tidyverse package and explain ggplot2 is part of the tidyverse.\n- When loading libraries, ALWAYS explain how to do this through a code chunk at the top of the R Markdown file with a reminder that libraries only need to be loaded once. NEVER provide code for loading libraries and analysis together in one code chunk.\n- When appropriate, remind users they can set global options through a code chunk at the top of their R Markdown file.\n- Make responses accessible by explaining all R & data analysis terms each time they are first used. Users are absolute beginners to R & may not know what terms like data frame, library, vector, function, object, plot, & so on mean.\n- Refer to relevant panels within RStudio, such as the Environment panel for checking a data frame or when installing a library explain how to install it through RStudio's console. Include a reminder of where on the screen the panel can be found.\n- Where relevant, inform users when the code covered returns console output, why not to use console output in knitted documents, and follow-up with suggestions for producing formatted outputs for knitted documents.\n- When customising ggplot plots, use existing complete themes or `theme()`, explaining how this supports a consistent look, and DO NOT hard-code arbitrary theme customisation into individual plots.\n- In code examples that require a dataset, either 1. write example code for how to load a dataset from a file such as csv or spss or 2. use a dataset that comes packaged with R or the tidyverse. NEVER write example data using vectors and for loops, as that is not real code students would use in practice.\n- For knitting issues, ask the user to check the YAML output field is html_document only as R table packages can require additional code to work with other file formats\n\n## Example response structure\n\nIn general, structure responses to provide a general explanation, more detailed breakdown, and summary of key information.\n\nFor example, when a user asks about an error message in their code:\n\n1. Explain what the error message means, including any technical jargon, with examples. Ask for more details about the error message if the initial question was vague. \n2. Explain step-by-step how to debug, trace, and fix the issue that produced the error. Include details when relevant for RStudio, Posit Cloud, tidyverse, and R Markdown. Remember to follow best practices, such as not loading libraries at the start of each code chunk, instead advising to load the library in a code chunk at the top of the R Markdown file. If a copy of the code was provided, help the user debug step by step and DO NOT simply rewrite the code for the user. Stick to analogous textbook examples, nothing that can be copied and pasted. \n3. Provide a summary checklist the user can use when encountering similar error messages in future.\n\nWhen a user asks to create a plot for a specific variable:\n\n1. Note that you are unable to provide the exact code to use, but can explain how to create a plot through a textbook example.\n2. Explain which variable types the plot should be used for.\n2. Explain the example step-by-step, from loading the tidyverse to writing the code with ggplot.\n3. Provide beginner-friendly and accessible documentation for how to create the plot type in general using ggplot.\n4. Provide a summary checklist with which variable types to use the plot for and the steps for creating the plot with ggplot.\n\n## Ending Responses\n\nBe proactive in building user understanding and encouraging exploration by ending responses with:\n\n- A 'Did You Know?' section with relevant tips and further information. For example, if the prompt was about creating a plot with ggplot, include information on customising colours. Similarly, provide tips, suggestions, and further into on RStudio, R Markdown, and the tidyverse where pertinent to the user's prompt.\n- A 'Explain Terminology' section that ALWAYS informs the user they can reply \"Explain all\" OR \"Explain [term]\" for more in-depth explanations of R & data analysis terms used in the response.\n\n## Formatting\n\nWhen nesting R code chunks inside code blocks use four backticks for the code block so the triple ones inside for R chunks are preserved."
  },
  {
    "objectID": "help/rstudio_helper.html#conversation-starters",
    "href": "help/rstudio_helper.html#conversation-starters",
    "title": "RStudio Helper GPT",
    "section": "Conversation Starters",
    "text": "Conversation Starters\nSome example prompts to try with RStudio Helper:\n\nWhat are the benefits of the tidyverse compared to base R?\nWhat are the steps to debug and fix an object not found error?\nHow is the mutate() function used?\nWhat are all the different ways to create and run R code chunks in RStudio?\nHow do I load my dataset into my R environment?\nHow can I go beyond standard boilerplate interpretations of statistical results?\nWhat can I create with R Markdown / Quarto beyond formatted reports?\nWhy does ‘#’ behave differently depending on whether it is inside or outwith a code chunk?\n\nProvide a summary of key R Markdown syntax covering headings, text, and code chunks. 2. Explain YAML and what variables can control with the YAML header in R Markdown files.\n\n\nExplain bit by bit how the “knitr::opts_chunk$set(…)” code that RStudio adds to top of its R Markdown template works 2. explain how I can set global and code chunk specific options\n\nExplain all basic R syntax and terms. Focus on those relevant for using R and tidyverse for quantitative analysis and working with data frames, including what symbols like ‘$’ do..\nHelp! My file won’t knit. All my code chunks run fine in RStudio. What can I check to figure out what is causing the problem?"
  },
  {
    "objectID": "help/rstudio_helper.html#breakdown",
    "href": "help/rstudio_helper.html#breakdown",
    "title": "RStudio Helper GPT",
    "section": "Breakdown",
    "text": "Breakdown\nBelow are bullet points explaining the reason behind what was included in each main section of the instructions.\nRole and General Interaction:\nThis section provides a role to the GenAI, some background context, and general initial instructions to shape responses and interaction with the user.\n\nOpens with information that it “assists users learn R for quantitative social science”.\nProvides general info about users, including what software and tools they will be using.\nA paragraph covers the general response principles, guiding it towards responses that are more equivalent to what would find in a textbook / online help page, rather than “here’s code to copy and paste”.\nEnds with a restatement that it is to support learning and provide “beginner-friendly explanations”.\n\nYou will see aspects from this initial section repeated across others. With shorter prompts you do not have to repeat information as often. However, the longer the prompt, usually the more often you need to give reminders of instructions that go against GenAI mode’s default behaviours.\nGolden Rules:\nGenAI models do not care about academic integrity and are incredibly bad at providing responses that support learning.\n\nRule one re-emphasises the importance of academic integrity with clarification of what that means for its responses.\nRule two encourages it to provide better responses that supports learning. Without this, prompts mentioning a variable for annual income would receive a response using a monthly income variable. This ensures responses still use comparable variables, just not ones that are overly similar.\nRule three further clarifies what not doing work for the user involves. Again without this, and despite all the rest of the instructions, GenAI models will too often default to spitting out its own interpretation rather than explaining how to interpret.\nRule four addresses the issue of GenAI being over-eager to make assumptions. When information is unclear, GenAI frequently makes ‘best guess’ assumptions rather than clarifying first. It will then happily spit out information based on wrong assumptions, and will a lot of time not even mention the assumptions made in the response.\n\nContextual Responses:\nThis section opens by re-inforcing that users are new to quantitative analysis and R. It repeats the key software and packages we are using on the course, and then has a long bullet point list to address common issues with default responses.\n\nUse the tidyverse and other packages covered in the labs. Without this information GenAI models tend towards generating overly code using ‘base R’ with no packages. That can result in 20+ lines of code for something you can do in one line with the tidyverse.\nInstruction to use the new built-in R pipe |&gt; over the older %&gt;%. Both will work, but it is advised to use the new one. Given it is relatively new, most of the data used to train GenAI uses the older pipe, which it defaults to unless told otherwise.\nAlways load the tidyverse rather than any subpackages. The tidyverse is a collection of packages. For example, by loading the tidyverse you are also loading dyplr, pplot2, and other packages. GenAI models, even when the initial prompt says that you are using the tidyverse, will spit out code that loads these packages individually rather than the tidyverse itself.\nInstruction to follow good data analysis practice and load all libraries used once in a code chunk at the top of the R Markdown file. GenAI responses instead tend to load the same packages again and again in every single code chunk it generates.\nSimilarly, when asking about how to set options for code chunks, responses tend to explain how to change these per code chunk, rather than informing users they can also set these globally.\nYet another reminder for it to explain all terms used, with a list of example terms. (You’ll notice GenAI responses still often fail to provide explanations for these.)\nA reminder to include any info about RStudio relevant to the prompt, helping the responses be less abstract.\nNote that it should inform users when they can create nicer formatted outputs for use in knitted documents rather than code which returns ‘raw outputs’.\nInstruction for it use existing ggplot themes and the theme() function. GenAI models have horrendous habit when prompted to customise a plot of adding dozens of individual lines of code for a specific plot. Not only does this result in a lot of unnecessary code, as it is specific to that plot, you have to repeat it for any other plots you are making as well. If you prompt genAI to do that, it will claim to have done the same, but often each time it spits out code for a new plot it’ll introduce subtle inconsistencies into the theming it is adding.\nGenAI models often default to generating code to fabricate a dataset. The instructions here make clear it should never generate fake data. Instead it should generate code for loading data from file or generate code using example datasets that are provided already with R and R packages.\nThe final bullet in the section advises it to ask users to check the YAML header in their R Markdown file if they report a knitting error. The most common cause of knitting issues on the module is from accidentally adding PDF as a file format to output when knitting. Some R table packages though require additional code when knitting to PDF, without which your file will fail to knit with vague error messages. Despite how common this is, GenAI will lead you down a path of installing 101 different packages and writing custom functions, all of which is entirely unnecessary and none of which will actually solve the issue.\n\nExample response structure:\nWith longer instructions it can help improve consistency to also include example(s) for how responses should be structured.\n\nThe first example sets out how to structure a response to a prompt about an error message, that re-emphasises the key information to include in its responses.\nThe second example sets out how to response to a prompt in a way consistent with the ‘Golden Rules’.\nAcross both, you will see that there is constant reiteration of ‘explain’, ‘explain’, ‘explain’. After explaining, responses should then ‘provide’ summary information that can add to your notes.\n\nEnding Responses:\nThe instructions here aim to flag other information you might find useful.\n\nA “Did You Know?” section that will surface hints and tricks related to your prompt.\nDespite the instructions in earlier sections to explain all terminology, GenAI models will still tend towards not everything any technical terms used. This at least flags at the end of responses things you may want to ask for further explainations about. It also provides a convenient way to prompt ‘Explain all’ and get a mini glossary of most the techincal terms used in the first response.\n\nFormatting:\n\nGenAI puts code inside ‘code blocks’ so they display on screen as code. This uses the same syntax as for creating code chunks in R markdown. When GenAI then tries to place an R code chunk inside a code block the formatting in the response ends up a mess. Regular text gets formatted as code and code gets formatted as regular text.\nThis final instruction at the end reduces but does not remove the problem. Whenever it occurs it’s best to just start a new chat as whenever the issue arises in a chat, it tends to persist across all later responses."
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2022. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nFogarty, Brian J. 2019. Quantitative Social Science Data with R: An Introduction. Los Angeles London New Delhi Singapore Washington DC Melbourne: SAGE.\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion to Applied Regression. Third. Thousand Oaks CA: Sage. https://socialsciences.mcmaster.ca/jfox/Books/Companion/.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2021. Car: Companion to Applied Regression. https://CRAN.R-project.org/package=car.\n\n\nHorst, Alison. n.d. “GitHub - Allisonhorst/Stats-Illustrations: R & Stats Illustrations by @Allison_horst.” Accessed July 11, 2022. https://github.com/allisonhorst/stats-illustrations.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2021. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/bookdown.\n\n\n———. 2019. “TinyTeX: A Lightweight, Cross-Platform, and Easy-to-Maintain LaTeX Distribution Based on TeX Live.” TUGboat, no. 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html.\n\n\n———. 2021. Tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents. https://github.com/yihui/tinytex.\n\n\n———. 2022. Bookdown: Authoring Books and Technical Documents with r Markdown. https://CRAN.R-project.org/package=bookdown.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "08-Lab8.html",
    "href": "08-Lab8.html",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "In the previous lab we learned about correlation. We visualised the relationship of different types of variables. Also, we computed one correlation measure for two numeric variables, Pearson correlation. This measure is useful to compute the strength and the direction of the association. However, it presents some limitations, e.g. it can only be used for numeric variables, it allows only one variable at a time, and it is appropriate to describe a linear relationship.\nLinear regression can overcome some of these limitations and it can be extended to achieve further purposes (e.g. use multiple variables or estimate scenarios). This technique is in fact very common and one of the most popular in quantitative research in social sciences. Therefore, getting familiar with it will be important for you not only to perform your own analyses, but also to interpret and critically read literature.\n\nIMPORTANT: Linear regression is appropriate only when the dependent variable is numeric (interval/ratio).\nHowever, the independent variables can be categorical, ordinal, or numeric. Also, you can include more than one independent variable to evaluate how these relate to the dependent variable. In this lab we will start using only one explanatory variable. This is know as simple linear regression.\n\nFor this lab, we will continue using the 2012 NILT survey to introduce linear models. To do so, please set your RStudio environment as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud;\nCreate a copy of the project called ‘NILT2’ which is located in your ‘Quants lab group’ by clicking on the ‘Start’ button;\nOnce in the project, create a new ‘R Script’ file (a simple R script NOT an .Rmd file);\nSave the R file as ‘linear_model_intro’.\n\nReproduce the code below, by copying, pasting and running it from your new script and focus on the intuitive part of this section.\nFirst, load the tidyverse library and read the nilt_r_object.rds file which is stored in the folder called ‘data’ and contains the NILT survey (tidyverse was installed in your session already).\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nThis dataset is exactly the same as the one you processed in previous labs.\nWe will re-revisit the example of the respondent’s spouse/partner characteristics. To keep things simple, let’s create a minimal sample of the dataset including only 40 random observations:\n\n# select a small random sample\nset.seed(3)\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- filter(nilt, !is.na(spage)) %&gt;% sample_n(40)\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- select(nilt_sample, rage, spage)\n\nWe will start by creating a scatter plot using the respondent’s spouse/partner age spage on the Y axis, and the respondent’s rage on the X axis.\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nAs you can see, even with this minimal example using 40 observations, there is a clear trend. From the plot, it seems intuitive to draw a line that describes this general trend. Imagine a friend will draw this line for you. You will have to tell them some basic references on how to do it. You can, for example, specify a start point and an end point in the plot. Alternatively, which is what we will do below, you can specify a start point and a value that describes the slope of the line. For now, our friend is ggplot. You have to pass these two values (start point and slope) in the geom_abline() function to draw a line that describes these points.\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nIn the guess above, it is assumed that the age of the spouse/partner might be exactly the same as the respondent. To draw a line like this, we used 0 as the starting value in the geom_abline() function. In statistics, this is known as the intercept and it is often represented with a Greek letter and a zero sub-index as \\(\\beta_0\\) (beta-naught) or with \\(\\alpha\\) (alpha). The location of the intercept can be found along the vertical axis (in this case it is not visible). The second value we passed to describe the line is the slope, which uses the X axis as the reference. The slope value is multiplied by the value of the X axis. Therefore, a slope of 0 is completely horizontal. In this example, a slope of 1 produces a line at 45º. The slope is often represented with the Greek letter beta and a sequential numeric sub-index like this: \\(\\beta_1\\) (beta-one).\nWe will create some points in the data frame based on these two values, the start point and the steepness factor or, formally speaking, the intercept and the slope respectively. We will store the results in a column called line1 and print the head of this data set.\n\n# create values for the guess line 1\nnilt_sample &lt;- nilt_sample %&gt;%\n  mutate(line1 = 0 + rage * 1)\n# print results\nhead(nilt_sample)\n\n# A tibble: 6 × 3\n   rage spage     line1\n  &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;\n1    40 39           40\n2    54 51           54\n3    66 59           66\n4    60 71           60\n5    35 34           35\n6    67 57           67\n\n\nFrom the above, you can notice that the respondent’s age rage and the column that we just created line1, are identical…So, that is our guess for now. The partner’s age is the same as the respondent’s.\nIs the line we just drew good enough to represent the general trend in the relationship above? To answer this question, we can measure the distance of each point of the plot to the line 1, as shown by the dashed segment in the plot below.\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_segment(aes(xend = rage, yend = line1), linetype = \"dashed\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nThis distance difference is known as the residual or the error and it is often expressed with the Greek letter \\(\\epsilon\\) (epsilon). Numerically, we can calculate this by computing the difference between the known value (the observed) and the guessed number. Formally, the guessed number is called the expected value (also known as predicted/estimated value). Normally, the expected values in statistics are represented by putting a hat like this \\(\\hat{}\\) on the letters. Since the independent variable is usually located on the Y axis, the expected value is differentiated from the observed values by putting the hat on the Y like this: \\(\\hat{y}\\) (y-hat).\nLet’s estimate the residuals for each of the observations by subtracting \\(\\hat{y}_i\\) from \\(y_i\\) (spage - line1) and store it in a column called residuals. Print the head of the data set after.\n\n# estimate residuals\nnilt_sample &lt;- nilt_sample %&gt;%\n  mutate(residuals = spage - line1)\n# print first 6 values\nhead(nilt_sample)\n\n# A tibble: 6 × 4\n   rage spage     line1 residuals\n  &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1    40 39           40        -1\n2    54 51           54        -3\n3    66 59           66        -7\n4    60 71           60        11\n5    35 34           35        -1\n6    67 57           67       -10\n\n\nComing back to the question, is my line good enough? We could sum all these residuals as an overall measure to know how good my line is. From the previous plot, we see that some of the expected ages exceed the actual values and others are below. This produces negative and positive residuals. If we simply sum them, they would compensate each other and this would not tell us much about the overall magnitude of the difference. To overcome this, we can multiply the differences by themselves (to make all numbers positive) and then sum them all together. This is known as the sum of squared residuals (SSR) and we can use this as the criterion to measure how good my line is with respect to the overall trend of the points.\nFor line 1, we can easily calculate the SSR as follows:\n\nsum((nilt_sample$residuals)^2)\n\n[1] 1325\n\n\nThe sum of squared residuals for line 1 is 1,325.\nWe can try different lines to find the combination of intercept and slope that produces the smallest error (SSR). To our luck, we can find the best values using a well established technique called Ordinary Least Squares (OLS). This technique finds the optimal solution by the principle of maxima and minima. We do not need to know the details for now. The important thing is that this procedure guarantees to find a line that produces the smallest possible error (SSR).\nIn R, it is very simple to fit a linear model and we do not need to go through each of the steps above manually nor to memorise all the steps. To do this, simply use the function lm(), save the result in an object called m1 and print it.\n\nm1 &lt;- lm(spage ~ rage, nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage  \n      6.299        0.875  \n\n\nSo, this is the optimal intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) that produces the least sum of the square. Let’s see what the SSR is compared to my arbitrary guess above:\n\nsum(residuals(m1)^2)\n\n[1] 1175.209\n\n\nYes, this is better than before (because the value is smaller)!\nLet’s plot the line we guessed and the optimal line together:\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nThe blue is the optimal solution, whereas the red was just an arbitrary guess. We can see that the optimal is more balanced than the red in relation to the observed data points.\n\nNow you are ready for the formal specification of the linear model. After the introduction above, it is easy to take the pieces apart. In essence, the simple linear model is telling us that the dependent value \\(y\\) is defined by a line that intersects the vertical axis at \\(\\beta_0\\), plus the multiplication of the slope \\(\\beta_1\\) by the value of \\(x_1\\), plus some residual \\(\\epsilon\\). The second part of the equation is just telling us that the residuals are normally distributed (that is when a histogram of the residuals follows a bell-shaped curve).\n\\[\n\\begin{align}\ny = \\beta_0 + \\beta_1 * x_1 + \\epsilon, && \\epsilon ~ N(0, \\sigma)  \n\\end{align}\n\\]\nYou do not need to memorize this equation, but being familiar to this type of notation will help you to expand your skills in quantitative research.\n\n\nWe already fitted our first linear model using the lm() above, which of course stands for linear model, but we did not discuss the specifications needed in this function. The first argument takes the dependent variable, then, we use tilde ~ to express that this is followed by the independent variable. Then, we specify the data set separated by a comma, as shown below (this is just a conceptual example, it does not actually run!):\n\nlm(dependent_variable ~ independent_variable, data)\n\n\nNow, let’s try with the full data set. First, we will fit a linear model using the same variables as the example above but including all the observations (remember that before we were working only with a small random sample). The first argument is the respondent’s spouse/partner age spage, followed by the respondent’s age rage. Let’s assign the model to an object called m2 and print it after.\n\nm2 &lt;- lm(spage ~ rage, nilt)\nm2\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nCoefficients:\n(Intercept)         rage  \n     3.7039       0.9287  \n\n\nThe output first displays the formula employed to estimate the model. Then, it show us the estimated values for the intercept and the slope for the age of the respondent, these estimated values are called coefficients. In this model, the coefficients differ from the ones in the example above (m1). This is because we now have more information to fit this line. Despite the difference, we see that the relationship is still in the same direction (positive). In fact, the value of the slope \\(\\beta_1\\) did not change much if you look at it carefully (0.87 vs 0.92).\n\nBut what are the slope and intercept telling us? The first thing to note is that the slope is positive, which means that the relationship between the respondent age and their partner are in the same direction. When the age of the respondent goes up, the age of their partner is expected to go up as well.\nIn our model, the intercept does not have a meaningful interpretation on its own, it would not be logical to say that when the respondent’s age is 0 their partner would be expected to be 3.70 years old. Interpretations should be made only within the range of the values used to fit the model (the youngest age in this data set is 18). The slope can be interpreted as following: For every year older the respondent is, the partner’s age is expected to change by 0.92. In other words, the respondents are expected to have a partner who is 0.92 times years older than themselves. This means that their partners are expected to be slightly younger.\nIn this case, the units of the dependent variable are the same as the independent (age vs age) which make it easy to interpret. In reality, there might be more factors that potentially affect how people select their partners (e.g. genders, education, race/ethnicity, etc.). For now, we are keeping things simple using only these two variables.\nLet’s practice with another example. What if we are interested in income? We can use personal income persinc2 as the dependent variable and the number of hours worked a week rhourswk as the independent variable. We’ll assign the result to an object called m3.\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\nm3\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nCoefficients:\n(Intercept)     rhourswk  \n     5170.4        463.2  \n\n\nThis time, the coefficients look quite different. The results are telling us that for every additional hour worked a week a respondent is expected to earn £463.2 more a year than other respondent. Note that the input units are not the same in the interpretation. The dependent variable is in pounds earned a year and the independent is in hours worked a week. Does this mean that someone who works 0 hours a week is expected to earn £5170.4 a year (given the fitted model \\(\\hat{persinc2} =  5170.4 + 0*463.2\\)) or that someone who works 110 hours a week (impossible!) is expected to earn £56K a year? We should only make reasonable interpretations of the coefficients within the range analysed.\nFinally, an important thing to know when interpreting linear models using cross sectional data (data collected in a single time point only) is: correlation does not imply causation. Variables are often correlated with other non-observed variables which may cause the effects observed on the independent variable in reality. This is why we cannot always be sure that X is causing the observed changes on Y.\n\nIn the next section, we suggest you to focus on the interpretation of the results of the linear model. You do not need to reproduce the code.\n\nSo far, we have talked about the basics of the linear model. If we print a summary of the model, we can obtain more information to evaluate it.\n\nsummary(m2)\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.211  -2.494   0.075   2.505  17.578 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.70389    0.66566   5.564    4e-08 ***\nrage         0.92869    0.01283  72.386   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.898 on 589 degrees of freedom\n  (613 observations deleted due to missingness)\nMultiple R-squared:  0.8989,    Adjusted R-squared:  0.8988 \nF-statistic:  5240 on 1 and 589 DF,  p-value: &lt; 2.2e-16\n\n\nThere are a lot of numbers from the result. Let’s break down the information into different pieces: first the call, second the residuals, third the coefficients, and fourth the goodness-of-fit measures, as shown below:\n\n\nModel summary\n\nThe first part, Call, is simply printing the variables that were used to produce the models.\nThe second part, the Residuals, provides a small summary of the residuals by quartile, including minimum and maximum residual values. Remember the second part of the formal specification of the model? Well, this is where we can see in practice the distribution of the residuals. If we look at the first and third quartile, we can see that that 50% of the predicted values by the model are \\(\\pm\\) 2.5 years old away form the observed partner’s age…not that bad.\nThe third part is about the coefficients. This time, the summary provides more information about the intercept and the slope than just the estimated coefficients as before. Usually, we focus on the independent variables in this section, rage in our case. The second column, standard error, tell us how large the error of the slope is, which is small for our variable. The third and fourth columns provide measures about the statistical significance of the relationship between the dependent and independent variable. The most commonly reported is the p-value (the fourth column). When these values contains many digits, R reports the result in scientific notation. In our result the p-value is &lt;2e-16 (the real equivalent number is &lt;0.0000000000000002, if you want to know more about scientific notation click here). As a general agreed principle, if the p-value is equal or less than 0.05 (\\(p\\) ≤ 0.05), we can reject the null hypothesis, and say that the relationship between the dependent variable and the independent is significant. In our case, the p-value is far lower than 0.05. So, we can say that the relationship between the respondent’s spouse/partner age and the respondent’s age is significant. In fact, R includes codes for each coefficient to help us to identify the significance. In our result, we got three stars/asterisks. This is because the value is less than 0.001. R assigns two stars if the p-value is less than 0.01, and one star if it is lower than 0.05.\nIn the fourth area, we have various goodness-of-fit measures. Overall, these measures tell us how well the model represents our data. For now, let’s focus on the adjusted r-squared and the sample size. The r-squared is a value that goes from 0 to 1. 0 means that model is not explaining any of the variance, whereas 1 would mean a perfect model. In social sciences we do not see high values often. The adjusted r-squared can be understood as the percentage variance that the model is able to explain. In the example above, the model explains 89.88% percent of the total variance. This is a good fit, but this is because we are modelling a quite “obvious” relationship. The r-squared does not represent how good or bad our work is. You may wonder why we do not report the sum of squared residuals (SSR), as we just learned. This is because the scale of the SSR is in the units used to fit the model, which would make it difficult to compare our model with other models that use different units or different variables. Conversely, the adjusted r-squared is expressed in relative terms which makes it a more suitable measure to compare with other models. An important thing to note is that the output is telling us that this model excluded 613 observations which were missing in our dataset. This is because we do not have the age of the respondent’s partner in 613 of the rows. This missingness may be because the respondent do not have a partner, refused to provide the information, etc.\nIn academic papers in the social sciences, usually the measures reported are the coefficients, p-values, the adjusted r-squared, and the size of the sample used to fit the model (number of observations).\n\nThe linear model, as many other techniques in statistics, relies on assumptions. These refer to characteristics of the data that are taken for granted to generate the estimates. It is the task of the modeller/analyst to make sure the data used follows these assumptions. One simple yet important check is to examine the distribution of the residuals, which ought to follow a normal distribution.\nDoes the residuals follow a normal distribution in m1? We can go further and plot a histogram to graphically evaluate it. Use the function residuals() to extract the residuals from our object m2, and then plot a histogram with the r-base function hist().\n\nhist(residuals(m2), breaks = 20)\n\n\n\n\n\n\n\nOverall, it seems that the residuals follow the normal distribution reasonably well, with the exception of the negative value to the left of the plot. Very often when there is a strange distribution or different to the normal is because one of the assumptions is violated. If that is the case we cannot trust the coefficient estimates. In the next lab we will talk more about the assumptions of the linear model. For now, we will leave it here and get some practice.\n\nIn the linear_model_intro.R script, use the nilt dataset in to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point() jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\nPrint the summary of m3 using the summary() function.\nIs the relationship of hours worked a week significant?\nWhat is the adjusted r-squared? How would you interpret it?\nWhat is the sample size to fit the model?\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#welcome",
    "href": "08-Lab8.html#welcome",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "In the previous lab we learned about correlation. We visualised the relationship of different types of variables. Also, we computed one correlation measure for two numeric variables, Pearson correlation. This measure is useful to compute the strength and the direction of the association. However, it presents some limitations, e.g. it can only be used for numeric variables, it allows only one variable at a time, and it is appropriate to describe a linear relationship.\nLinear regression can overcome some of these limitations and it can be extended to achieve further purposes (e.g. use multiple variables or estimate scenarios). This technique is in fact very common and one of the most popular in quantitative research in social sciences. Therefore, getting familiar with it will be important for you not only to perform your own analyses, but also to interpret and critically read literature.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#introduction-to-simple-linear-regression",
    "href": "08-Lab8.html#introduction-to-simple-linear-regression",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "IMPORTANT: Linear regression is appropriate only when the dependent variable is numeric (interval/ratio).\nHowever, the independent variables can be categorical, ordinal, or numeric. Also, you can include more than one independent variable to evaluate how these relate to the dependent variable. In this lab we will start using only one explanatory variable. This is know as simple linear regression.\n\nFor this lab, we will continue using the 2012 NILT survey to introduce linear models. To do so, please set your RStudio environment as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud;\nCreate a copy of the project called ‘NILT2’ which is located in your ‘Quants lab group’ by clicking on the ‘Start’ button;\nOnce in the project, create a new ‘R Script’ file (a simple R script NOT an .Rmd file);\nSave the R file as ‘linear_model_intro’.\n\nReproduce the code below, by copying, pasting and running it from your new script and focus on the intuitive part of this section.\nFirst, load the tidyverse library and read the nilt_r_object.rds file which is stored in the folder called ‘data’ and contains the NILT survey (tidyverse was installed in your session already).\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nThis dataset is exactly the same as the one you processed in previous labs.\nWe will re-revisit the example of the respondent’s spouse/partner characteristics. To keep things simple, let’s create a minimal sample of the dataset including only 40 random observations:\n\n# select a small random sample\nset.seed(3)\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- filter(nilt, !is.na(spage)) %&gt;% sample_n(40)\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- select(nilt_sample, rage, spage)\n\nWe will start by creating a scatter plot using the respondent’s spouse/partner age spage on the Y axis, and the respondent’s rage on the X axis.\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nAs you can see, even with this minimal example using 40 observations, there is a clear trend. From the plot, it seems intuitive to draw a line that describes this general trend. Imagine a friend will draw this line for you. You will have to tell them some basic references on how to do it. You can, for example, specify a start point and an end point in the plot. Alternatively, which is what we will do below, you can specify a start point and a value that describes the slope of the line. For now, our friend is ggplot. You have to pass these two values (start point and slope) in the geom_abline() function to draw a line that describes these points.\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nIn the guess above, it is assumed that the age of the spouse/partner might be exactly the same as the respondent. To draw a line like this, we used 0 as the starting value in the geom_abline() function. In statistics, this is known as the intercept and it is often represented with a Greek letter and a zero sub-index as \\(\\beta_0\\) (beta-naught) or with \\(\\alpha\\) (alpha). The location of the intercept can be found along the vertical axis (in this case it is not visible). The second value we passed to describe the line is the slope, which uses the X axis as the reference. The slope value is multiplied by the value of the X axis. Therefore, a slope of 0 is completely horizontal. In this example, a slope of 1 produces a line at 45º. The slope is often represented with the Greek letter beta and a sequential numeric sub-index like this: \\(\\beta_1\\) (beta-one).\nWe will create some points in the data frame based on these two values, the start point and the steepness factor or, formally speaking, the intercept and the slope respectively. We will store the results in a column called line1 and print the head of this data set.\n\n# create values for the guess line 1\nnilt_sample &lt;- nilt_sample %&gt;%\n  mutate(line1 = 0 + rage * 1)\n# print results\nhead(nilt_sample)\n\n# A tibble: 6 × 3\n   rage spage     line1\n  &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;\n1    40 39           40\n2    54 51           54\n3    66 59           66\n4    60 71           60\n5    35 34           35\n6    67 57           67\n\n\nFrom the above, you can notice that the respondent’s age rage and the column that we just created line1, are identical…So, that is our guess for now. The partner’s age is the same as the respondent’s.\nIs the line we just drew good enough to represent the general trend in the relationship above? To answer this question, we can measure the distance of each point of the plot to the line 1, as shown by the dashed segment in the plot below.\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_segment(aes(xend = rage, yend = line1), linetype = \"dashed\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nThis distance difference is known as the residual or the error and it is often expressed with the Greek letter \\(\\epsilon\\) (epsilon). Numerically, we can calculate this by computing the difference between the known value (the observed) and the guessed number. Formally, the guessed number is called the expected value (also known as predicted/estimated value). Normally, the expected values in statistics are represented by putting a hat like this \\(\\hat{}\\) on the letters. Since the independent variable is usually located on the Y axis, the expected value is differentiated from the observed values by putting the hat on the Y like this: \\(\\hat{y}\\) (y-hat).\nLet’s estimate the residuals for each of the observations by subtracting \\(\\hat{y}_i\\) from \\(y_i\\) (spage - line1) and store it in a column called residuals. Print the head of the data set after.\n\n# estimate residuals\nnilt_sample &lt;- nilt_sample %&gt;%\n  mutate(residuals = spage - line1)\n# print first 6 values\nhead(nilt_sample)\n\n# A tibble: 6 × 4\n   rage spage     line1 residuals\n  &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1    40 39           40        -1\n2    54 51           54        -3\n3    66 59           66        -7\n4    60 71           60        11\n5    35 34           35        -1\n6    67 57           67       -10\n\n\nComing back to the question, is my line good enough? We could sum all these residuals as an overall measure to know how good my line is. From the previous plot, we see that some of the expected ages exceed the actual values and others are below. This produces negative and positive residuals. If we simply sum them, they would compensate each other and this would not tell us much about the overall magnitude of the difference. To overcome this, we can multiply the differences by themselves (to make all numbers positive) and then sum them all together. This is known as the sum of squared residuals (SSR) and we can use this as the criterion to measure how good my line is with respect to the overall trend of the points.\nFor line 1, we can easily calculate the SSR as follows:\n\nsum((nilt_sample$residuals)^2)\n\n[1] 1325\n\n\nThe sum of squared residuals for line 1 is 1,325.\nWe can try different lines to find the combination of intercept and slope that produces the smallest error (SSR). To our luck, we can find the best values using a well established technique called Ordinary Least Squares (OLS). This technique finds the optimal solution by the principle of maxima and minima. We do not need to know the details for now. The important thing is that this procedure guarantees to find a line that produces the smallest possible error (SSR).\nIn R, it is very simple to fit a linear model and we do not need to go through each of the steps above manually nor to memorise all the steps. To do this, simply use the function lm(), save the result in an object called m1 and print it.\n\nm1 &lt;- lm(spage ~ rage, nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage  \n      6.299        0.875  \n\n\nSo, this is the optimal intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) that produces the least sum of the square. Let’s see what the SSR is compared to my arbitrary guess above:\n\nsum(residuals(m1)^2)\n\n[1] 1175.209\n\n\nYes, this is better than before (because the value is smaller)!\nLet’s plot the line we guessed and the optimal line together:\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nThe blue is the optimal solution, whereas the red was just an arbitrary guess. We can see that the optimal is more balanced than the red in relation to the observed data points.\n\nNow you are ready for the formal specification of the linear model. After the introduction above, it is easy to take the pieces apart. In essence, the simple linear model is telling us that the dependent value \\(y\\) is defined by a line that intersects the vertical axis at \\(\\beta_0\\), plus the multiplication of the slope \\(\\beta_1\\) by the value of \\(x_1\\), plus some residual \\(\\epsilon\\). The second part of the equation is just telling us that the residuals are normally distributed (that is when a histogram of the residuals follows a bell-shaped curve).\n\\[\n\\begin{align}\ny = \\beta_0 + \\beta_1 * x_1 + \\epsilon, && \\epsilon ~ N(0, \\sigma)  \n\\end{align}\n\\]\nYou do not need to memorize this equation, but being familiar to this type of notation will help you to expand your skills in quantitative research.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#fitting-linear-regression-in-r",
    "href": "08-Lab8.html#fitting-linear-regression-in-r",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "We already fitted our first linear model using the lm() above, which of course stands for linear model, but we did not discuss the specifications needed in this function. The first argument takes the dependent variable, then, we use tilde ~ to express that this is followed by the independent variable. Then, we specify the data set separated by a comma, as shown below (this is just a conceptual example, it does not actually run!):\n\nlm(dependent_variable ~ independent_variable, data)\n\n\nNow, let’s try with the full data set. First, we will fit a linear model using the same variables as the example above but including all the observations (remember that before we were working only with a small random sample). The first argument is the respondent’s spouse/partner age spage, followed by the respondent’s age rage. Let’s assign the model to an object called m2 and print it after.\n\nm2 &lt;- lm(spage ~ rage, nilt)\nm2\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nCoefficients:\n(Intercept)         rage  \n     3.7039       0.9287  \n\n\nThe output first displays the formula employed to estimate the model. Then, it show us the estimated values for the intercept and the slope for the age of the respondent, these estimated values are called coefficients. In this model, the coefficients differ from the ones in the example above (m1). This is because we now have more information to fit this line. Despite the difference, we see that the relationship is still in the same direction (positive). In fact, the value of the slope \\(\\beta_1\\) did not change much if you look at it carefully (0.87 vs 0.92).\n\nBut what are the slope and intercept telling us? The first thing to note is that the slope is positive, which means that the relationship between the respondent age and their partner are in the same direction. When the age of the respondent goes up, the age of their partner is expected to go up as well.\nIn our model, the intercept does not have a meaningful interpretation on its own, it would not be logical to say that when the respondent’s age is 0 their partner would be expected to be 3.70 years old. Interpretations should be made only within the range of the values used to fit the model (the youngest age in this data set is 18). The slope can be interpreted as following: For every year older the respondent is, the partner’s age is expected to change by 0.92. In other words, the respondents are expected to have a partner who is 0.92 times years older than themselves. This means that their partners are expected to be slightly younger.\nIn this case, the units of the dependent variable are the same as the independent (age vs age) which make it easy to interpret. In reality, there might be more factors that potentially affect how people select their partners (e.g. genders, education, race/ethnicity, etc.). For now, we are keeping things simple using only these two variables.\nLet’s practice with another example. What if we are interested in income? We can use personal income persinc2 as the dependent variable and the number of hours worked a week rhourswk as the independent variable. We’ll assign the result to an object called m3.\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\nm3\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nCoefficients:\n(Intercept)     rhourswk  \n     5170.4        463.2  \n\n\nThis time, the coefficients look quite different. The results are telling us that for every additional hour worked a week a respondent is expected to earn £463.2 more a year than other respondent. Note that the input units are not the same in the interpretation. The dependent variable is in pounds earned a year and the independent is in hours worked a week. Does this mean that someone who works 0 hours a week is expected to earn £5170.4 a year (given the fitted model \\(\\hat{persinc2} =  5170.4 + 0*463.2\\)) or that someone who works 110 hours a week (impossible!) is expected to earn £56K a year? We should only make reasonable interpretations of the coefficients within the range analysed.\nFinally, an important thing to know when interpreting linear models using cross sectional data (data collected in a single time point only) is: correlation does not imply causation. Variables are often correlated with other non-observed variables which may cause the effects observed on the independent variable in reality. This is why we cannot always be sure that X is causing the observed changes on Y.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#model-evaluation",
    "href": "08-Lab8.html#model-evaluation",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "In the next section, we suggest you to focus on the interpretation of the results of the linear model. You do not need to reproduce the code.\n\nSo far, we have talked about the basics of the linear model. If we print a summary of the model, we can obtain more information to evaluate it.\n\nsummary(m2)\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.211  -2.494   0.075   2.505  17.578 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.70389    0.66566   5.564    4e-08 ***\nrage         0.92869    0.01283  72.386   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.898 on 589 degrees of freedom\n  (613 observations deleted due to missingness)\nMultiple R-squared:  0.8989,    Adjusted R-squared:  0.8988 \nF-statistic:  5240 on 1 and 589 DF,  p-value: &lt; 2.2e-16\n\n\nThere are a lot of numbers from the result. Let’s break down the information into different pieces: first the call, second the residuals, third the coefficients, and fourth the goodness-of-fit measures, as shown below:\n\n\nModel summary\n\nThe first part, Call, is simply printing the variables that were used to produce the models.\nThe second part, the Residuals, provides a small summary of the residuals by quartile, including minimum and maximum residual values. Remember the second part of the formal specification of the model? Well, this is where we can see in practice the distribution of the residuals. If we look at the first and third quartile, we can see that that 50% of the predicted values by the model are \\(\\pm\\) 2.5 years old away form the observed partner’s age…not that bad.\nThe third part is about the coefficients. This time, the summary provides more information about the intercept and the slope than just the estimated coefficients as before. Usually, we focus on the independent variables in this section, rage in our case. The second column, standard error, tell us how large the error of the slope is, which is small for our variable. The third and fourth columns provide measures about the statistical significance of the relationship between the dependent and independent variable. The most commonly reported is the p-value (the fourth column). When these values contains many digits, R reports the result in scientific notation. In our result the p-value is &lt;2e-16 (the real equivalent number is &lt;0.0000000000000002, if you want to know more about scientific notation click here). As a general agreed principle, if the p-value is equal or less than 0.05 (\\(p\\) ≤ 0.05), we can reject the null hypothesis, and say that the relationship between the dependent variable and the independent is significant. In our case, the p-value is far lower than 0.05. So, we can say that the relationship between the respondent’s spouse/partner age and the respondent’s age is significant. In fact, R includes codes for each coefficient to help us to identify the significance. In our result, we got three stars/asterisks. This is because the value is less than 0.001. R assigns two stars if the p-value is less than 0.01, and one star if it is lower than 0.05.\nIn the fourth area, we have various goodness-of-fit measures. Overall, these measures tell us how well the model represents our data. For now, let’s focus on the adjusted r-squared and the sample size. The r-squared is a value that goes from 0 to 1. 0 means that model is not explaining any of the variance, whereas 1 would mean a perfect model. In social sciences we do not see high values often. The adjusted r-squared can be understood as the percentage variance that the model is able to explain. In the example above, the model explains 89.88% percent of the total variance. This is a good fit, but this is because we are modelling a quite “obvious” relationship. The r-squared does not represent how good or bad our work is. You may wonder why we do not report the sum of squared residuals (SSR), as we just learned. This is because the scale of the SSR is in the units used to fit the model, which would make it difficult to compare our model with other models that use different units or different variables. Conversely, the adjusted r-squared is expressed in relative terms which makes it a more suitable measure to compare with other models. An important thing to note is that the output is telling us that this model excluded 613 observations which were missing in our dataset. This is because we do not have the age of the respondent’s partner in 613 of the rows. This missingness may be because the respondent do not have a partner, refused to provide the information, etc.\nIn academic papers in the social sciences, usually the measures reported are the coefficients, p-values, the adjusted r-squared, and the size of the sample used to fit the model (number of observations).\n\nThe linear model, as many other techniques in statistics, relies on assumptions. These refer to characteristics of the data that are taken for granted to generate the estimates. It is the task of the modeller/analyst to make sure the data used follows these assumptions. One simple yet important check is to examine the distribution of the residuals, which ought to follow a normal distribution.\nDoes the residuals follow a normal distribution in m1? We can go further and plot a histogram to graphically evaluate it. Use the function residuals() to extract the residuals from our object m2, and then plot a histogram with the r-base function hist().\n\nhist(residuals(m2), breaks = 20)\n\n\n\n\n\n\n\nOverall, it seems that the residuals follow the normal distribution reasonably well, with the exception of the negative value to the left of the plot. Very often when there is a strange distribution or different to the normal is because one of the assumptions is violated. If that is the case we cannot trust the coefficient estimates. In the next lab we will talk more about the assumptions of the linear model. For now, we will leave it here and get some practice.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#lab-activities",
    "href": "08-Lab8.html#lab-activities",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "In the linear_model_intro.R script, use the nilt dataset in to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point() jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\nPrint the summary of m3 using the summary() function.\nIs the relationship of hours worked a week significant?\nWhat is the adjusted r-squared? How would you interpret it?\nWhat is the sample size to fit the model?\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Overview\n\n\n\nBy the end of this lab you will:\n\nlog into Posit Cloud and join your lab group space\nlearn basic R syntax and use R as a calculator\ndiscover the importance of the order code is run in\ncreate a neatly formatted HTML file by knitting an R Markdown file\n\n\n\n\nFor this course we will be using R (R Core Team 2021) and RStudio as the main tools for conducting quantitative analysis.\nR and the basic versions of RStudio are free and open-source software. There are ‘free’ as in cost, but - more importantly - they are free software that guarantee users four foundamental freedoms. As explained on the FSFE’s What is Free Software? page, these are:\n\n\n\nUse: Free Software can be used for any purpose and is free of restrictions such as licence expiry or geographic limitations.\n\nStudy: Free Software and its code can be studied by anyone, without non‐disclosure agreements or similar restrictions.\n\nShare: Free Software can be shared and copied at virtually no cost.\n\nImprove: Free Software can be modified by anyone, and these improvements can be shared publicly.\n\n\nEven though R appeared in the early 90s, it has been gaining a lot of popularity in recent years. A main contributor to its success is being free software, with a large community of people contributing improvements to R directly as well as extending it with further additional software packages. In fact, it is now one of the most common software for doing statistics in academia.\n\n\nSource: https://tiobe.com\n\nR and RStudio are two separate things. R is the actual programming language and the main processing tool which does the computations in the background, whereas RStudio integrates all functionalities into a (relatively) friendly and interactive interface. In short, for this course (and most of the time in practice) you chiefly use RStudio whilst R is doing all the work in the background. Thereafter, we will refer to R, as the integrated interface.\n\n\n\n\n\n\nNew terms\n\n\n\n\n\nR: a programming language and environment for data analysis.\n\nRStudio: a user-interface for working with R.\n\nPosit Cloud: website for running RStudio in a browser.\n\n\n\n\nAt this point you may be wondering why you need to bother learning these tools. In the following sections you will see some of the advantages and examples that can be achieved using R.\n\nR can be applied in a wide variety of fields and subjects, including not only those in the social sciences (e.g. sociology, politics, or policy research), but also in the humanities (e.g. history, digital humanities), natural and physical sciences (e.g. biology, chemistry, or geography), health (e.g. medical studies, public health, epidemiology), business and management (e.g. finance, economics, marketing), among many others.\nThe broad application of R is due to its flexibility which allows it to perform a range of tasks related to data. These cover tasks at initial stages, such as downloading, mining, or importing data. But it is also useful to manipulate, edit, transform, and organise information. Furthermore, and most important for us, there are a set of tools that allow us to analyse data using a range of statistical techniques. These are useful to understand, summarize, and draw conclusions about samples, e.g. people.\nLastly, R is powerful to communicate and share information and documents. There are several extensions (called packages in R) that can help to produce static and interactive plots/charts, maps, written reports, interactive applications or even entire books! In fact this workbook was written from RStudio.\nR works in a command-based Console environment. This means that you need to call the commands (or functions, as they are called in R) through writing text. This can look intimidating at first glance. But do not worry, we will guide you step by step. Importantly, that these commands can be used and combined in multiple ways is what gives R its incredible flexible. Once you get the hang of these you will find that they are faster and more powerful than using a button and menu based interface. RStudio also helps make working with R more accessible.\n\n\n\n\n\n\nNew terms\n\n\n\nWe will cover these in more detail in future weeks, but as initial short definitions:\n\n\nPackage: a collection of ready-made R tools - such as functions with help pages, and sometimes example data - that you can install (and later load) to add features to R.\n\nFunction: a named reusable set of instructions that can take inputs (referred to as arguments), run some steps, and return a result.\n\nArgument: input provided to a function that tell the function what to use and how to behave.\n\n\n\n\nSome advantages of using R:\n\nIt is free and open source software. You do not need to pay for a licence. You can then use it anywhere at any time, even if you do not have an affiliation to an institution or organisation (e.g. University or workplace);\nIt is a collaborative project. This means that it is the users who maintain, extend, and update its applications;\nIt is reproducible. Research can be more transparent since you will get the same results every time you run your analysis through a specific pathway (i.e. through R Markdown files);\nHigh compatibility. You can read and produce most types of file extensions;\nOnline community. There are a number of easy-access web resources to support you in the learning process.\n\n\n\nThere are multiple ways to work with R. One, and by far the most common, is to download both R and RStudio Desktop and install the applications on your local device.1\nOn this course, we use Posit Cloud instead. Posit Cloud provides an on-line version of RStudio that does not require installing any additional software. You can run it directly from your browser (e.g. Chrome, Firefox, Safari, etc), including if using a Chromebook or tablet. This makes it easier to access your RStudio projects and files within the labs and when using your own devices.\nTo get started with Posit Cloud, follow the next steps:\n\n\n\n\n\n\n\n\nClick on this link Posit Cloud - SSO, which should automatically open a new tab in your web browser or go directly to the browser and copy this URL: https://sso.posit.cloud/glasgow;\nClick ‘Log In Via University of Glasgow’ which will take you to the usual University of Glasgow login pages;\nEnter your University of Glasgow email address and password in the login page as normal;\n\n\n\nDone! After logging in with your University of Glasgow account, you will be taken to your personal RStudio Cloud workspace;\n\nYou will have received a link via email to join your lab group on Posit Cloud (the link will be available within your Lab Group forum on Moodle too). Note, you must use this specific link to join and access your lab group workspace, as each link is unique to your group. So, only use your group’s specific link.\n\nCopy and paste the link in your web browser. You should see the following window:\n\n\n\nJoin your lab by clicking the ‘Yes’ button shown above.\n\n\nOpen the shared space form the left-hand side pane called ‘Lab Group …’ (where … is your Lab Group number).\nClick the blue ‘New Project’ button on the right of the screen.\nFrom the menu list that opens select ‘New Project from Git Repository’.\n\n\n\nWithin the dialogue box that opens, copy and paste the link below into the ‘URL of your Git Repository’ field.\n\nhttps://github.com/UGQuants/Labs-1-2\n\nClick ‘OK’ to create the new project.\n\n\n\n\n\n\n\n\n\nAfter your new ‘Labs 1-2’ project is created Posit Cloud will automatically open it in RStudio and you will see the screen below:\n\n\nIn the bottom-right ‘Files’ pane, click “Lab-01.Rmd”.\nThat will open a new pane in the top-left, within the yellow banner in it click ‘Install’.\n\n\nThe bottom-left pane will now switch to the “Background Jobs” tab and you will see a lot of red text. Despite the colour, this is expected behaviour. RStudio is now installing the rmarkdown package that makes it possible to work with R Markdown files in RStudio (more on this later). Once it finishes installing, the tab will automatically switch back to “Console”.\n\nR Markdown is a file format (.Rmd) that lets you write text and run code in the same document. You write your narrative text and add your code within code chunks. When you Knit (render) a R Markdown file, R runs the code chunks from top to bottom and weaves their outputs (numbers, tables, plots) into a finished ‘knitted’ report (such as HTML or PDF). This makes your analysis transparent and reproducible, whereby anyone can see what code produced each result. When learning R, it also makes it great for making notes alongside the code.\n\n\n\n\n\n\nNew terms\n\n\n\nWe will cover R Markdown in more detail across the next few weeks, but some initial short definitions:\n\n\nR Markdown: a file format that mixes text and code, using simple syntax for formatting text and code chunks for adding and running code.\n\ncode chunk: a fenced block in an R Markdown file for adding R code; as well as being run when you knit the file, each code chunk can be run on its own within RStudio.\n\nknit: render an R Markdown file into a specified format (e.g. HTML or PDF) by running all code chunks from top to bottom and creating a knitted document combining the text with the output from the code.\n\n\n\n\nAfter this, your RStudio screen will be split in four important windows or panes as shown below:\n\n\nIn Pane 1 (purple), you have the Lab-01 R Markdown file.\n\nThis is the area where you will be working most of the time. From here, you will write text and code. Code chunks are shaded grey, with each starting with ```{r optional-name} and ending with ```. (The ` is called a back-tick, and on UK keyboards is the key to the left of 1.)\n\nIn Pane 2 (blue), you have the “Global Environment”.\n\nThis is one of the most useful tabs in this pane. It shows you the active ‘objects’ that you have available/loaded in your current R session (this will make more sense in the coming sections).\n\nIn Pane 3 (green), you have the R Console.\n\nThis is where you will see most of the results of the code you run (pane 1). You can also write and run code from here, by typing the code and hitting enter. Note, it is best practice to have code necessary for your analysis saved in your R Markdown file. The Console is usually used instead to quickly run code that you do not want to save in your file.\n\nFinally, in Pane 4 (yellow) you have multiple useful tabs.\n\nIn the File tab you can see the files and directories that you have in your R project.\nIn the Plot tab you will see a preview of the static plots/charts you will be producing from your script.\nIn Packages, you have a list of the extensions or plug-ins (called ‘packages’ in R) that are installed in your working environment.\nThe Help contains some resources that clarify or expand what each of the functions does. Again, probably this will make more sense once you get started. We will come back to this later.\nFinally, the Viewer displays interactive outputs.\nNote, code chunks (the shaded grey fenced blocks) for writing code into have been set up for you this week. Remember though you can also write regular text as well outside of these chunks to record any notes:\n\n\nNow you are ready! It is your turn to start exploring and getting familiar with R by completing the following activities.\n\nIn your ‘Lab-01’ R Markdown file, look at the code chunk named “sum-1” containing 2 + 2. Let’s run it!\nTo run a code chunk, you can click on the Run green arrow situated on the top right corner of it. Alternatively, if your text cursor is inside the code chunk you can press Ctrl+Shift+Enter on your keyboard.\n\nWhat you will see after running the chunk is:\n\nIn the R Markdown file, [1] 4 underneath the code chunk.\nIn the Console, both &gt; 2 + 2 and [1] 4.\n\n\nWhy do we see this? When you run a chunk, RStudio sends the code to the Console for R to run it, so you see the code echoed and its raw output there. RStudio also shows the results inline under the chunk so you can see the output in context. (This also becomes important later with graphs and formatted tables that cannot be viewed in the Console.)\nWhat does the [1] mean? It’s a label R adds to printed results, where [1] just means “this line starts at item 1”. If there were lots of items, the next line might start [7] or [15].\nOf course, you can also go to the Console, write a simple calculation, and run it by typing ‘Enter’, as shown below:\n\nIt’s also possible to run multiple calculations in a single chunk. Add a line for 5 * 5 and 21 -4 in the empty “sum-2” chunk and then run it. (Hint: Use the green triangle top-right of the code-chunk to run it.)\n\nTry different operations such as 50 / 20 or 3 * 5, whether adding to the code chunk or directly in the Console.\nFairly simple, right? And don’t forget, it is normal to copy and tweak existing code. A lot of learning to code involves going through working examples and tweaking it to work with what you are trying to achieve. Unlike writing an essay or an exam, you don’t actually need to know and write code “off the cuff” or have all necessary code syntax and functions memorised. So, don’t worry if you feel like you are just making minor changes to existing code, that’s how it’s supposed to work. The crucial thing is learning how the code works, so you know what to tweak for your analysis. And, the first few weeks is all about getting comfortable in using R, then the level of challenge will go up.\nLet’s continue with the next activities!\n\nNow, run the “logical-1” code chunk in the R Markdown file.\n\nNow, in the “logical-2” code chunk, add the following and then run it:\n\n1 == 5\n1 &gt; 5\n5 &lt; 10\n'this' == 'this'\n'this' == 'that'\n'this' != 'that'\n\n(Note, if you receive an “Error: object ‘this’ not found’” message, check you are using single quotes and have 'this' rather than this.)\nWhat do you see running the code chunks? Why is R outputting these results? …\nWhen you use the double equal sign == you are asking R whether the value on the left hand-side of the operator is equal to the one on the right hand-side. Likewise, when you combine the exclamation mark ! with another operator, you get the reversed result. For example, != is interpreted as “is not equal to”, that is why 10 != 10 returns FALSE, but 10 == 10 returns TRUE.\nR can process different classes of inputs. In this case we used a letters and we asked R whether ‘this’ was equal to ‘this’, and of course the result is TRUE. Note that when you want to input text (referred as character values in R), you need quotation marks, 'text'. If you want to enter numeric values, you simply input the raw number without quotation marks. These are then treated as numeric ‘class’ values.\nYou can use the class() function to check what something is in R. For example, try running class(42), class('42'), and class(TRUE) in the Console.\nPerhaps logical operators do not make much sense at this point, but you will find out later that they are useful to manipulate data. For example, these are essential to filter a data set based on specific rules or patterns.\n\n\n\n\n\n\nNew Terms\n\n\n\n\n\nclass: an object’s classification in R that informs how R handles it.\n\ncharacter: text in quotes, such as “Glasgow”. ('42' as it is in quotes would be treated as character)\n\nnumeric: real or decimal numbers, such as 42 or 5.1.\n\n\n\n\nIn R, it is common (and practical) to store values or data as ‘objects’. Once values are assigned to them, objects are then stored in your current R session. Assignments are made using name &lt;- value, such as a &lt;- 5 or b &lt;- 'Glasgow'. Let’s try it!\nGo to the “assign-1” code chunk run it by clicking the green arrow/triangle or - with your text cursor within the code chunk - using Ctrl+Shift+Enter.\nWhat do you observe? Pay attention to what appears under the code chunk and in the Environment pane (the top-right of the four panes). Why are the values different? …\n\nThe operator &lt;- assigned the numeric value 10 to the name a (on the left hand-side of the arrow). The next line used the object (a) to compute a sum (i.e, a + 5). Only the assignment is stored in the environment (the a &lt;- 10) and assignments do not return an output, so only the result of a + 5 is shown under the code chunk when run.\nTo see this, within the “assign-2” chunk add c &lt;- 3 and run it:\n\nNow, add and run the following in the “assign-2” chunk:\n\nc\na * c\n\n\nAs you can see, running c on a line by its own returns the numeric value 3 stored in the variable c. Then, a * c calls the previously created object a and multiplies it by c (a * c is effectively 10 * 3). Note, most programming languages, R included, do not use x for multiplication and use * instead. One reason for this is because x could also end up as a named object, x &lt;- 2.\nIn the same way as you assigned these simple variables, you will store other types of objects later, e.g. vectors, data frames or lists. This is useful because those objects will be ready in your session to do some computations.\n\nThere are a few things to note when assigning objects to variables. If you assign a different value to the same object, e.g. by running a &lt;- 5, you will replace the old value with the new. So, instead of having a representing the value 10, you will have 5.\nYou can see the objects available in your session on the Global Environment (‘Environment’ tab in top-right pane) as shown below.\n\nNow, let’s see what happens if change value of a:\n\nRun the “order-1” code chunk that contains a + 2.\nNow in the Console type and run a &lt;- 5.\nRun the “order-1” code chunk again.\n\n\nAs can see, the code in the “order-1” chunk remains the same, but the result from running the code changes based on the value assigned to a. That may seem an obvious point to make, but a large proportion of unexpected results and errors arise from changing what is assigned to an object.\nA good rule of thumb then when a code chunk that was previously working suddenly returns unexpected results or an error message is to run your code from the top-down. Thankfully, RStudio also makes this easy to do. To ‘Run All Chunks Above’ a specific code chunk, click the downward facing grey arrow/triangle with a green line under it (see image below!). After that, you can click the green arrow/triangle to ‘Run Current Chunk’.\nDo that for the ‘order-1’ chunk and it will return 12 again:\n\nIt is also possible to use an object in a calculation and assign the result of that calculation back to the object. For example, a &lt;- a + 5 is equivalent to a &lt;- 10 + 5. Whilst that can be perfectly valid to do, care needs to be taken as now a is 15. What do you think would happen if you keep running the same a &lt;- a + 5 line of code?\nGo to the “order-2” code chunk and run it multiple times, looking at the result underneath the chunk and the value for a in the Environment pane.\n\nAgain, you can ‘Run All Chunks Above’ (downwards grey arrow/triangle, green line) and ‘Run Current Chunk’ (green arrow/triangle) to restore a to value it would be if only ran each code chunk once top to bottom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a good start, great job!\nNote that the changes made in your R Markdown file are saved automatically within RStudio when using Posit Cloud. To verify this, have a look at the name of your file in the top-left of pane 1. If changes are due to be saved, the name will be written in red with an asterisk at the end, Lab-01.Rmd*. If it is in red, save changes manually by clicking on the disk icon. After you have made sure your changes are saved, you are safe to end your session by closing the RStudio Cloud tab in your browser.\n\n\n\n\n\n\nNew Terms\n\n\n\n\n\nobject: any data or function that exists in the R session’s memory; usually accessed via a name.\n\nassignment: assign a value (object) to a name; usually with &lt;- (e.g. a &lt;- 10).\n\nenvironment: a workspace that maps names to objects; the main one you use is the Global Environment (visible in RStudio’s top-right Environment pane).\n\n\n\n\nDiscuss the following questions with your neighbour or tutor, and write notes from your discussion in your R Markdown file:\n\nWhat are the main differences between working in a R Markdown file (pane 1) and directly on the console (pane 3)?\nCan you describe what happens when your run the following code? (tip: look at the environment tab in pane 2)\n\n\nobject1 &lt;- 10\nobject1 &lt;- 30\n\n\nClick the ‘Knit’ button in the bar at the top of the R Markdown file (pane 1). This will turn your R Markdown file into an HTML document.\n\n(Note: Another reason code chunk order matters. R Markdown files are meant to be reproducible, where anyone with a copy of the file can run the code from top to bottom and receive the same results. When you ‘knit’ an R Markdown file, the code chunks are run top to bottom in a fresh R session - in other words, it starts with a blank Global Environment. For example, if you run d &lt;- 25 manually in the Console and then add d + 5 in a code chunk, that code chunk will work fine when manually run in RStudio, but will result in an “object not found” error if you try to knit. This is because d &lt;- 25 also needs to be in a code chunk within the R Markdown file, and needs to come before d + 5. So, make sure all essential code for your analysis is within code chunks and in the correct order that they need to be run in.)\n\nCreate a new code chunk, you can put your text cursor on a line at the end of the R Markdown file and press Ctrl+Alt+I on the keyboard. Alternatively, from the menu bar at the top of RStudio, you can select Code &gt; Insert Chunk.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#tools-we-are-using",
    "href": "01-intro.html#tools-we-are-using",
    "title": "Introduction to R",
    "section": "",
    "text": "For this course we will be using R (R Core Team 2021) and RStudio as the main tools for conducting quantitative analysis.\nR and the basic versions of RStudio are free and open-source software. There are ‘free’ as in cost, but - more importantly - they are free software that guarantee users four foundamental freedoms. As explained on the FSFE’s What is Free Software? page, these are:\n\n\n\nUse: Free Software can be used for any purpose and is free of restrictions such as licence expiry or geographic limitations.\n\nStudy: Free Software and its code can be studied by anyone, without non‐disclosure agreements or similar restrictions.\n\nShare: Free Software can be shared and copied at virtually no cost.\n\nImprove: Free Software can be modified by anyone, and these improvements can be shared publicly.\n\n\nEven though R appeared in the early 90s, it has been gaining a lot of popularity in recent years. A main contributor to its success is being free software, with a large community of people contributing improvements to R directly as well as extending it with further additional software packages. In fact, it is now one of the most common software for doing statistics in academia.\n\n\nSource: https://tiobe.com\n\nR and RStudio are two separate things. R is the actual programming language and the main processing tool which does the computations in the background, whereas RStudio integrates all functionalities into a (relatively) friendly and interactive interface. In short, for this course (and most of the time in practice) you chiefly use RStudio whilst R is doing all the work in the background. Thereafter, we will refer to R, as the integrated interface.\n\n\n\n\n\n\nNew terms\n\n\n\n\n\nR: a programming language and environment for data analysis.\n\nRStudio: a user-interface for working with R.\n\nPosit Cloud: website for running RStudio in a browser.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#why-r",
    "href": "01-intro.html#why-r",
    "title": "Introduction to R",
    "section": "",
    "text": "At this point you may be wondering why you need to bother learning these tools. In the following sections you will see some of the advantages and examples that can be achieved using R.\n\nR can be applied in a wide variety of fields and subjects, including not only those in the social sciences (e.g. sociology, politics, or policy research), but also in the humanities (e.g. history, digital humanities), natural and physical sciences (e.g. biology, chemistry, or geography), health (e.g. medical studies, public health, epidemiology), business and management (e.g. finance, economics, marketing), among many others.\nThe broad application of R is due to its flexibility which allows it to perform a range of tasks related to data. These cover tasks at initial stages, such as downloading, mining, or importing data. But it is also useful to manipulate, edit, transform, and organise information. Furthermore, and most important for us, there are a set of tools that allow us to analyse data using a range of statistical techniques. These are useful to understand, summarize, and draw conclusions about samples, e.g. people.\nLastly, R is powerful to communicate and share information and documents. There are several extensions (called packages in R) that can help to produce static and interactive plots/charts, maps, written reports, interactive applications or even entire books! In fact this workbook was written from RStudio.\nR works in a command-based Console environment. This means that you need to call the commands (or functions, as they are called in R) through writing text. This can look intimidating at first glance. But do not worry, we will guide you step by step. Importantly, that these commands can be used and combined in multiple ways is what gives R its incredible flexible. Once you get the hang of these you will find that they are faster and more powerful than using a button and menu based interface. RStudio also helps make working with R more accessible.\n\n\n\n\n\n\nNew terms\n\n\n\nWe will cover these in more detail in future weeks, but as initial short definitions:\n\n\nPackage: a collection of ready-made R tools - such as functions with help pages, and sometimes example data - that you can install (and later load) to add features to R.\n\nFunction: a named reusable set of instructions that can take inputs (referred to as arguments), run some steps, and return a result.\n\nArgument: input provided to a function that tell the function what to use and how to behave.\n\n\n\n\nSome advantages of using R:\n\nIt is free and open source software. You do not need to pay for a licence. You can then use it anywhere at any time, even if you do not have an affiliation to an institution or organisation (e.g. University or workplace);\nIt is a collaborative project. This means that it is the users who maintain, extend, and update its applications;\nIt is reproducible. Research can be more transparent since you will get the same results every time you run your analysis through a specific pathway (i.e. through R Markdown files);\nHigh compatibility. You can read and produce most types of file extensions;\nOnline community. There are a number of easy-access web resources to support you in the learning process.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#getting-started",
    "href": "01-intro.html#getting-started",
    "title": "Introduction to R",
    "section": "",
    "text": "There are multiple ways to work with R. One, and by far the most common, is to download both R and RStudio Desktop and install the applications on your local device.1\nOn this course, we use Posit Cloud instead. Posit Cloud provides an on-line version of RStudio that does not require installing any additional software. You can run it directly from your browser (e.g. Chrome, Firefox, Safari, etc), including if using a Chromebook or tablet. This makes it easier to access your RStudio projects and files within the labs and when using your own devices.\nTo get started with Posit Cloud, follow the next steps:\n\n\n\n\n\n\n\n\nClick on this link Posit Cloud - SSO, which should automatically open a new tab in your web browser or go directly to the browser and copy this URL: https://sso.posit.cloud/glasgow;\nClick ‘Log In Via University of Glasgow’ which will take you to the usual University of Glasgow login pages;\nEnter your University of Glasgow email address and password in the login page as normal;\n\n\n\nDone! After logging in with your University of Glasgow account, you will be taken to your personal RStudio Cloud workspace;\n\nYou will have received a link via email to join your lab group on Posit Cloud (the link will be available within your Lab Group forum on Moodle too). Note, you must use this specific link to join and access your lab group workspace, as each link is unique to your group. So, only use your group’s specific link.\n\nCopy and paste the link in your web browser. You should see the following window:\n\n\n\nJoin your lab by clicking the ‘Yes’ button shown above.\n\n\nOpen the shared space form the left-hand side pane called ‘Lab Group …’ (where … is your Lab Group number).\nClick the blue ‘New Project’ button on the right of the screen.\nFrom the menu list that opens select ‘New Project from Git Repository’.\n\n\n\nWithin the dialogue box that opens, copy and paste the link below into the ‘URL of your Git Repository’ field.\n\nhttps://github.com/UGQuants/Labs-1-2\n\nClick ‘OK’ to create the new project.\n\n\n\n\n\n\n\n\n\nAfter your new ‘Labs 1-2’ project is created Posit Cloud will automatically open it in RStudio and you will see the screen below:\n\n\nIn the bottom-right ‘Files’ pane, click “Lab-01.Rmd”.\nThat will open a new pane in the top-left, within the yellow banner in it click ‘Install’.\n\n\nThe bottom-left pane will now switch to the “Background Jobs” tab and you will see a lot of red text. Despite the colour, this is expected behaviour. RStudio is now installing the rmarkdown package that makes it possible to work with R Markdown files in RStudio (more on this later). Once it finishes installing, the tab will automatically switch back to “Console”.\n\nR Markdown is a file format (.Rmd) that lets you write text and run code in the same document. You write your narrative text and add your code within code chunks. When you Knit (render) a R Markdown file, R runs the code chunks from top to bottom and weaves their outputs (numbers, tables, plots) into a finished ‘knitted’ report (such as HTML or PDF). This makes your analysis transparent and reproducible, whereby anyone can see what code produced each result. When learning R, it also makes it great for making notes alongside the code.\n\n\n\n\n\n\nNew terms\n\n\n\nWe will cover R Markdown in more detail across the next few weeks, but some initial short definitions:\n\n\nR Markdown: a file format that mixes text and code, using simple syntax for formatting text and code chunks for adding and running code.\n\ncode chunk: a fenced block in an R Markdown file for adding R code; as well as being run when you knit the file, each code chunk can be run on its own within RStudio.\n\nknit: render an R Markdown file into a specified format (e.g. HTML or PDF) by running all code chunks from top to bottom and creating a knitted document combining the text with the output from the code.\n\n\n\n\nAfter this, your RStudio screen will be split in four important windows or panes as shown below:\n\n\nIn Pane 1 (purple), you have the Lab-01 R Markdown file.\n\nThis is the area where you will be working most of the time. From here, you will write text and code. Code chunks are shaded grey, with each starting with ```{r optional-name} and ending with ```. (The ` is called a back-tick, and on UK keyboards is the key to the left of 1.)\n\nIn Pane 2 (blue), you have the “Global Environment”.\n\nThis is one of the most useful tabs in this pane. It shows you the active ‘objects’ that you have available/loaded in your current R session (this will make more sense in the coming sections).\n\nIn Pane 3 (green), you have the R Console.\n\nThis is where you will see most of the results of the code you run (pane 1). You can also write and run code from here, by typing the code and hitting enter. Note, it is best practice to have code necessary for your analysis saved in your R Markdown file. The Console is usually used instead to quickly run code that you do not want to save in your file.\n\nFinally, in Pane 4 (yellow) you have multiple useful tabs.\n\nIn the File tab you can see the files and directories that you have in your R project.\nIn the Plot tab you will see a preview of the static plots/charts you will be producing from your script.\nIn Packages, you have a list of the extensions or plug-ins (called ‘packages’ in R) that are installed in your working environment.\nThe Help contains some resources that clarify or expand what each of the functions does. Again, probably this will make more sense once you get started. We will come back to this later.\nFinally, the Viewer displays interactive outputs.\nNote, code chunks (the shaded grey fenced blocks) for writing code into have been set up for you this week. Remember though you can also write regular text as well outside of these chunks to record any notes:",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#hands-on-r",
    "href": "01-intro.html#hands-on-r",
    "title": "Introduction to R",
    "section": "",
    "text": "Now you are ready! It is your turn to start exploring and getting familiar with R by completing the following activities.\n\nIn your ‘Lab-01’ R Markdown file, look at the code chunk named “sum-1” containing 2 + 2. Let’s run it!\nTo run a code chunk, you can click on the Run green arrow situated on the top right corner of it. Alternatively, if your text cursor is inside the code chunk you can press Ctrl+Shift+Enter on your keyboard.\n\nWhat you will see after running the chunk is:\n\nIn the R Markdown file, [1] 4 underneath the code chunk.\nIn the Console, both &gt; 2 + 2 and [1] 4.\n\n\nWhy do we see this? When you run a chunk, RStudio sends the code to the Console for R to run it, so you see the code echoed and its raw output there. RStudio also shows the results inline under the chunk so you can see the output in context. (This also becomes important later with graphs and formatted tables that cannot be viewed in the Console.)\nWhat does the [1] mean? It’s a label R adds to printed results, where [1] just means “this line starts at item 1”. If there were lots of items, the next line might start [7] or [15].\nOf course, you can also go to the Console, write a simple calculation, and run it by typing ‘Enter’, as shown below:\n\nIt’s also possible to run multiple calculations in a single chunk. Add a line for 5 * 5 and 21 -4 in the empty “sum-2” chunk and then run it. (Hint: Use the green triangle top-right of the code-chunk to run it.)\n\nTry different operations such as 50 / 20 or 3 * 5, whether adding to the code chunk or directly in the Console.\nFairly simple, right? And don’t forget, it is normal to copy and tweak existing code. A lot of learning to code involves going through working examples and tweaking it to work with what you are trying to achieve. Unlike writing an essay or an exam, you don’t actually need to know and write code “off the cuff” or have all necessary code syntax and functions memorised. So, don’t worry if you feel like you are just making minor changes to existing code, that’s how it’s supposed to work. The crucial thing is learning how the code works, so you know what to tweak for your analysis. And, the first few weeks is all about getting comfortable in using R, then the level of challenge will go up.\nLet’s continue with the next activities!\n\nNow, run the “logical-1” code chunk in the R Markdown file.\n\nNow, in the “logical-2” code chunk, add the following and then run it:\n\n1 == 5\n1 &gt; 5\n5 &lt; 10\n'this' == 'this'\n'this' == 'that'\n'this' != 'that'\n\n(Note, if you receive an “Error: object ‘this’ not found’” message, check you are using single quotes and have 'this' rather than this.)\nWhat do you see running the code chunks? Why is R outputting these results? …\nWhen you use the double equal sign == you are asking R whether the value on the left hand-side of the operator is equal to the one on the right hand-side. Likewise, when you combine the exclamation mark ! with another operator, you get the reversed result. For example, != is interpreted as “is not equal to”, that is why 10 != 10 returns FALSE, but 10 == 10 returns TRUE.\nR can process different classes of inputs. In this case we used a letters and we asked R whether ‘this’ was equal to ‘this’, and of course the result is TRUE. Note that when you want to input text (referred as character values in R), you need quotation marks, 'text'. If you want to enter numeric values, you simply input the raw number without quotation marks. These are then treated as numeric ‘class’ values.\nYou can use the class() function to check what something is in R. For example, try running class(42), class('42'), and class(TRUE) in the Console.\nPerhaps logical operators do not make much sense at this point, but you will find out later that they are useful to manipulate data. For example, these are essential to filter a data set based on specific rules or patterns.\n\n\n\n\n\n\nNew Terms\n\n\n\n\n\nclass: an object’s classification in R that informs how R handles it.\n\ncharacter: text in quotes, such as “Glasgow”. ('42' as it is in quotes would be treated as character)\n\nnumeric: real or decimal numbers, such as 42 or 5.1.\n\n\n\n\nIn R, it is common (and practical) to store values or data as ‘objects’. Once values are assigned to them, objects are then stored in your current R session. Assignments are made using name &lt;- value, such as a &lt;- 5 or b &lt;- 'Glasgow'. Let’s try it!\nGo to the “assign-1” code chunk run it by clicking the green arrow/triangle or - with your text cursor within the code chunk - using Ctrl+Shift+Enter.\nWhat do you observe? Pay attention to what appears under the code chunk and in the Environment pane (the top-right of the four panes). Why are the values different? …\n\nThe operator &lt;- assigned the numeric value 10 to the name a (on the left hand-side of the arrow). The next line used the object (a) to compute a sum (i.e, a + 5). Only the assignment is stored in the environment (the a &lt;- 10) and assignments do not return an output, so only the result of a + 5 is shown under the code chunk when run.\nTo see this, within the “assign-2” chunk add c &lt;- 3 and run it:\n\nNow, add and run the following in the “assign-2” chunk:\n\nc\na * c\n\n\nAs you can see, running c on a line by its own returns the numeric value 3 stored in the variable c. Then, a * c calls the previously created object a and multiplies it by c (a * c is effectively 10 * 3). Note, most programming languages, R included, do not use x for multiplication and use * instead. One reason for this is because x could also end up as a named object, x &lt;- 2.\nIn the same way as you assigned these simple variables, you will store other types of objects later, e.g. vectors, data frames or lists. This is useful because those objects will be ready in your session to do some computations.\n\nThere are a few things to note when assigning objects to variables. If you assign a different value to the same object, e.g. by running a &lt;- 5, you will replace the old value with the new. So, instead of having a representing the value 10, you will have 5.\nYou can see the objects available in your session on the Global Environment (‘Environment’ tab in top-right pane) as shown below.\n\nNow, let’s see what happens if change value of a:\n\nRun the “order-1” code chunk that contains a + 2.\nNow in the Console type and run a &lt;- 5.\nRun the “order-1” code chunk again.\n\n\nAs can see, the code in the “order-1” chunk remains the same, but the result from running the code changes based on the value assigned to a. That may seem an obvious point to make, but a large proportion of unexpected results and errors arise from changing what is assigned to an object.\nA good rule of thumb then when a code chunk that was previously working suddenly returns unexpected results or an error message is to run your code from the top-down. Thankfully, RStudio also makes this easy to do. To ‘Run All Chunks Above’ a specific code chunk, click the downward facing grey arrow/triangle with a green line under it (see image below!). After that, you can click the green arrow/triangle to ‘Run Current Chunk’.\nDo that for the ‘order-1’ chunk and it will return 12 again:\n\nIt is also possible to use an object in a calculation and assign the result of that calculation back to the object. For example, a &lt;- a + 5 is equivalent to a &lt;- 10 + 5. Whilst that can be perfectly valid to do, care needs to be taken as now a is 15. What do you think would happen if you keep running the same a &lt;- a + 5 line of code?\nGo to the “order-2” code chunk and run it multiple times, looking at the result underneath the chunk and the value for a in the Environment pane.\n\nAgain, you can ‘Run All Chunks Above’ (downwards grey arrow/triangle, green line) and ‘Run Current Chunk’ (green arrow/triangle) to restore a to value it would be if only ran each code chunk once top to bottom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a good start, great job!\nNote that the changes made in your R Markdown file are saved automatically within RStudio when using Posit Cloud. To verify this, have a look at the name of your file in the top-left of pane 1. If changes are due to be saved, the name will be written in red with an asterisk at the end, Lab-01.Rmd*. If it is in red, save changes manually by clicking on the disk icon. After you have made sure your changes are saved, you are safe to end your session by closing the RStudio Cloud tab in your browser.\n\n\n\n\n\n\nNew Terms\n\n\n\n\n\nobject: any data or function that exists in the R session’s memory; usually accessed via a name.\n\nassignment: assign a value (object) to a name; usually with &lt;- (e.g. a &lt;- 10).\n\nenvironment: a workspace that maps names to objects; the main one you use is the Global Environment (visible in RStudio’s top-right Environment pane).",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#activity",
    "href": "01-intro.html#activity",
    "title": "Introduction to R",
    "section": "",
    "text": "Discuss the following questions with your neighbour or tutor, and write notes from your discussion in your R Markdown file:\n\nWhat are the main differences between working in a R Markdown file (pane 1) and directly on the console (pane 3)?\nCan you describe what happens when your run the following code? (tip: look at the environment tab in pane 2)\n\n\nobject1 &lt;- 10\nobject1 &lt;- 30\n\n\nClick the ‘Knit’ button in the bar at the top of the R Markdown file (pane 1). This will turn your R Markdown file into an HTML document.\n\n(Note: Another reason code chunk order matters. R Markdown files are meant to be reproducible, where anyone with a copy of the file can run the code from top to bottom and receive the same results. When you ‘knit’ an R Markdown file, the code chunks are run top to bottom in a fresh R session - in other words, it starts with a blank Global Environment. For example, if you run d &lt;- 25 manually in the Console and then add d + 5 in a code chunk, that code chunk will work fine when manually run in RStudio, but will result in an “object not found” error if you try to knit. This is because d &lt;- 25 also needs to be in a code chunk within the R Markdown file, and needs to come before d + 5. So, make sure all essential code for your analysis is within code chunks and in the correct order that they need to be run in.)\n\nCreate a new code chunk, you can put your text cursor on a line at the end of the R Markdown file and press Ctrl+Alt+I on the keyboard. Alternatively, from the menu bar at the top of RStudio, you can select Code &gt; Insert Chunk.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "Introduction to R",
    "section": "Footnotes",
    "text": "Footnotes\n\nIf you have prior experience working with coding environments, such as Visual Studio Code, you may want to take a look at Positron. It is based on Visual Studio Code and setup to have a similar panel layout as within RStudio. One of the benefits of open-source software is having this diversity of tools can pick from. It is advised though for this course to stick with Posit Cloud and RStudio.↩︎",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods in the Social Sciences",
    "section": "",
    "text": "Welcome\nWelcome to the Quantitative Methods in the Social Sciences lab workbook!\n\nThis workbook is for University of Glasgow students enrolled in the undergraduate Quantitative Methods in the Social Sciences course within the School of Social and Political Sciences. The activities are designed for Posit Cloud.\nThese webpages are made with  and Quarto and licensed under the     Creative Commons BY-NC-SA 4.0.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "04-Lab4.html",
    "href": "04-Lab4.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "In our previous session we learned about wrangling data in R by implementing useful function such as filter(), select(), and mutate(). In this session we will focus on descriptive statistics. This includes the exploration and description of quantitative data.\n\nWe will continue working on the same project and dataset that you created in the last lab on RStudio Cloud. Please follow the next steps:\n\nGo to your ‘Quants lab group’ in RStudio Cloud.\nOpen the project called ‘NILT’ located in your lab group’.\nContinue working at the bottom of the ‘Exploratory analysis’ script that you created in the last lab.\nLoad nilt dataset that you created in the last session using the following code:\n\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nCreate a subset of the nilt data keeping only few variables using the select() function as shown below:\n\n\n# Subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\n\nAre your summary statistics hiding something interesting?\n\n\n\n\nExploratory analysis.\n\n\n\n\nTo start exploring our data it essential to distinguish the adequate tools and measures available for the type of data in question. As you know now, there are two broad types: (1) categorical and (2) numeric.\nThere are several ways in which we can summarise our data. Today, we will use a useful package called vtable. Install it in your session running the following line from your console:\n\ninstall.packages(\"vtable\")\n\nOnce it is installed, make sure to load it with the next line. This time copy and paste it in your R script, so you can run it every time you restart your session.\n\nlibrary(vtable)\n\n\nA usual way to explore the categorical data is using contingency and proportion tables. The contingency tables include the count for each category while the proportion tables contain the count divided by the total number of observations.\nTry to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown.\nLet’s say we are interested in the data’s break down by respondents’ sex (called rsex in the dataset). We will use function sumtable() of the vtable package to produce a contingency table for a single variable (known as One-Way contingency table).\n\nsumtable(nilt_subset, vars = c(\"rsex\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\n\n\n\nFrom the result, we see that there are more female respondents than males.\nSpecifically, we see that males respondents represent 44.6% of the total sample, whereas females 55.4%.\nWe can do this with any type of categorical variable. Let’s see how the sample is split by religion (religcat). So, we will add it to in the vars argument.\n\nsumtable(nilt_subset, vars = c(\"rsex\", \"religcat\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\nreligcat\n1168\n\n\n\n... Catholic\n491\n42%\n\n\n... Protestant\n497\n43%\n\n\n... No religion\n180\n15%\n\n\n\n\n\nAs you can see, about the same number of people are identified as being catholic or protestant, and a relatively small number with no religion.\nWhat if we want to know the religious affiliation breakdown by males and females. This is where Two-Way contingency tables are useful and very common in quantitative research. To produce it, we have to specify the group argument in the sumtable function as follows:\n\nsumtable(nilt_subset, vars = c(\"religcat\"), group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n520\n\n648\n\n\n\n... Catholic\n209\n40%\n282\n44%\n\n\n... Protestant\n211\n41%\n286\n44%\n\n\n... No religion\n100\n19%\n80\n12%\n\n\n\n\n\nThere are some interesting results from this table. You can see that there are proportionally more female respondents who are either Catholic or Protestant than males, namely 43.5% vs 40.2% and 44.1% vs 40.6%, respectively. We also see that there are almost 20% of male respondents who do not self-identify with a religion which contrast to the 12% of female participants.\n\nFrom your RStudio Cloud script, do the following activities using the data in the nilt_subset object (feel free to copy and adapt the code shown above):\n\nCreate a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\nAre your summary statistics hiding something interesting? Discuss your results with your neighbour or your tutor.\n\nIn the previous section we’ve learnt how to summarise categorical data. But very often we want to work with continuous numeric variables or a combination of both. To summarise and understand numeric data there are two main types: measures of centrality and measures of spread.\nAs before, try to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown.\n\nIn quantitative research, we usually have access to many observations in a sample which contains different attributes for each of them. It would be difficult (and probably not very useful) to talk about each of the NILT respondents one by one. Instead, to describe this sample we need measures that roughly represent all participants.\nThis is actually an important step in quantitative research, since it allows us to characterise the people that we are studying. For example, in the previous section we only talked about the respondents’ sex and political affiliation, but who are the people we are talking about? Probably a place to start digging deeper is to know their age. The first tool that we will use to understand numeric values is a histogram. Let’s see how the age of NILT respondents is distributed.\n\nhist(nilt_subset$rage)\n\n\n\n\n\n\n\nThis plot shows us on the X axis (horizontal) the age and the frequency on the Y axis. We can see that the youngest age in the sample is somewhere close to 20, and the oldest is almost 100. We also observe that the total number of observations (represented by the frequency on the vertical axis) for extreme values (close to 20 on the left-hand side and 100 on the right-hand side) tends to be lower than the values in the centre of the plot (somewhere between 30 and 45). For instance, we can see that there are approximately 120 respondents who are around 40 years old; that seems to be the most popular/frequent age in our sample. Now, we can represent these central values with actual measures, typically mean or median.\nThe median is the mid-point value in a numeric series. If you sort the values and split it by half, the value right in the middle is the median. Luckily there is a function ready to be used called… You guessed it - median().\n\nmedian(nilt_subset$rage, na.rm = TRUE)\n\n[1] 48\n\n\nThe median age is 48, that means that 50% (or half) of the respondents are equal or younger than this, and the other 50% is equal or older.1\nTo compute the mean manually, we need to sum all our values and divide it by the total number of the observations as follows: \\[ mean =\\frac{  x_1 + x_2 + x_3 ...+x_n } {n} \\] The formula above is for you to know that this measure considers the magnitude of all values included in the numeric series. Therefore, the average is sensitive to extreme numbers (e.g. a very, very old person). To compute the mean you need the mean() function.\n\nmean(nilt_subset$rage, na.rm = T)\n\n[1] 49.61532\n\n\nAs you can see, the above measures try to approximate values that fall somewhere in the centre of the histogram plot, and represent all observations in the sample. They tell different things and are sometimes more (or less) suitable in a specific situation.\n\nBy contrast, there are measures that help us to describe how far away a series of numeric values are from the centre. The common measures of spread are quartiles, variance and standard deviation.\nThe quartiles are very useful to quickly see how numeric data is distributed. Imagine that we sort all ages in the sample and split it into four equal parts. The first quartile includes the lowest 25% of values, the second the next 25%, the third another 25%, and the fourth the highest 25%. To compute quartiles, we can use the quantile function.\n\nquantile(nilt_subset$rage, na.rm = T)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\nIn our sample, the youngest quarter of the respondents is between 18 and 35 years old. The second quarter is between 35 and 48 years old. The next quartile is between 48 and 64. The oldest 25% of the respondents is between 64 and 97.\nThe variance is useful to obtain a singe measure of spread (instead of four values, as the above) taking the mean as a reference. This is given by the following formula:\n\\[ var = \\frac{ \\sum(x - \\bar{x})^2 }{n-1 } \\]\nTo decipher the formula above, the \\(\\bar{x}\\) represents the mean, the \\(x\\) represents each of the values in the numeric series. The formula takes each of the \\(x\\) values and subtract it from the mean \\(\\bar{x}\\). Later, it squares the result of the subtraction (that is multiply it by itself). This is done to obtain a positive value, since some numbers in the series will be lower than the mean (resulting in negative values). Then, we sum all of them and divide the sum by the size/length of the numeric sequence \\(n\\) minus 1. To estimate the variance in R we only need the var() function.\n\nvar(nilt_subset$rage, na.rm = T)\n\n[1] 343.3486\n\n\nAs you can see, the result is not very intuitive. That is because we squared the subtraction. Luckily, there is a measure that put it in readable scale. This is the standard deviation. In essence this takes the square root of the variance (the reversed operation of squaring it): \\[sd=\\sqrt{var}\\]\nTo compute it in R, use the sd() function.\n\nsd(nilt_subset$rage, na.rm = T)\n\n[1] 18.52967\n\n\nThis measure is more human readable than the variance. Don’t worry too much about the formula. An important thing to remember is what the measure represents. An informal definition of the standard deviation is the average distance from the mean. In essence, it tell us how far the values in our data are from the mean.\n\nPhew, that was a lot!…\n… Luckily we can use the sumtable function to compute all these measures at the same time!\nIt is very simple. You can compute a quick summary for age as following:\n\nsumtable(nilt_subset, vars = c(\"rage\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\nrage\n1201\n50\n19\n18\n35\n64\n97\n\n\n\n\nThe result displays the number of observations used (N), the mean, the standard deviation, minimum, the 1st (same as ‘Pctl. 25’) and 3rd quartile (same as ‘Pctl. 75’), as well as the maximum (i.e., eldest respondent).\n\nLastly, there will be times in which you will need to compute a summary combining categorical and numeric data, to compare groups for example. The good news is that we can use exactly the same function and syntax to do this. Let’s take the following example to compute the summary of the respondent’s age (rage) by gender:\n\nsumtable(nilt_subset, vars = c(\"rage\"), group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\n\n\n\nrage\n535\n51\n18\n666\n49\n19\n\n\n\n\nIn the code above, we are simple specifying the variable rage and grouping the summary by rsex. This produces a small summary included the number of observations in each category an the main measure of centrality and spread, namely the mean and the std. dev.\n\nCan money buy happiness?\nUsing the data in the nilt_subset dataset, complete the following activities. This will be a good practice run for doing the research report when you run your own analysis on the NILT teaching dataset. Handy!\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions (use the $ symbol after the name of the dataset and then the name of the variable inside the function);\nCreate a summary of the personal income persinc2 variable, using the sumtable() function;\nFinally, compute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy. What do you observe?\nDiscuss the results with your neighbour or your tutor.",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "04-Lab4.html#getting-started",
    "href": "04-Lab4.html#getting-started",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "We will continue working on the same project and dataset that you created in the last lab on RStudio Cloud. Please follow the next steps:\n\nGo to your ‘Quants lab group’ in RStudio Cloud.\nOpen the project called ‘NILT’ located in your lab group’.\nContinue working at the bottom of the ‘Exploratory analysis’ script that you created in the last lab.\nLoad nilt dataset that you created in the last session using the following code:\n\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nCreate a subset of the nilt data keeping only few variables using the select() function as shown below:\n\n\n# Subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "04-Lab4.html#exploratory-analysis",
    "href": "04-Lab4.html#exploratory-analysis",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Are your summary statistics hiding something interesting?\n\n\n\n\nExploratory analysis.\n\n\n\n\nTo start exploring our data it essential to distinguish the adequate tools and measures available for the type of data in question. As you know now, there are two broad types: (1) categorical and (2) numeric.\nThere are several ways in which we can summarise our data. Today, we will use a useful package called vtable. Install it in your session running the following line from your console:\n\ninstall.packages(\"vtable\")\n\nOnce it is installed, make sure to load it with the next line. This time copy and paste it in your R script, so you can run it every time you restart your session.\n\nlibrary(vtable)\n\n\nA usual way to explore the categorical data is using contingency and proportion tables. The contingency tables include the count for each category while the proportion tables contain the count divided by the total number of observations.\nTry to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown.\nLet’s say we are interested in the data’s break down by respondents’ sex (called rsex in the dataset). We will use function sumtable() of the vtable package to produce a contingency table for a single variable (known as One-Way contingency table).\n\nsumtable(nilt_subset, vars = c(\"rsex\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\n\n\n\nFrom the result, we see that there are more female respondents than males.\nSpecifically, we see that males respondents represent 44.6% of the total sample, whereas females 55.4%.\nWe can do this with any type of categorical variable. Let’s see how the sample is split by religion (religcat). So, we will add it to in the vars argument.\n\nsumtable(nilt_subset, vars = c(\"rsex\", \"religcat\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\nreligcat\n1168\n\n\n\n... Catholic\n491\n42%\n\n\n... Protestant\n497\n43%\n\n\n... No religion\n180\n15%\n\n\n\n\n\nAs you can see, about the same number of people are identified as being catholic or protestant, and a relatively small number with no religion.\nWhat if we want to know the religious affiliation breakdown by males and females. This is where Two-Way contingency tables are useful and very common in quantitative research. To produce it, we have to specify the group argument in the sumtable function as follows:\n\nsumtable(nilt_subset, vars = c(\"religcat\"), group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n520\n\n648\n\n\n\n... Catholic\n209\n40%\n282\n44%\n\n\n... Protestant\n211\n41%\n286\n44%\n\n\n... No religion\n100\n19%\n80\n12%\n\n\n\n\n\nThere are some interesting results from this table. You can see that there are proportionally more female respondents who are either Catholic or Protestant than males, namely 43.5% vs 40.2% and 44.1% vs 40.6%, respectively. We also see that there are almost 20% of male respondents who do not self-identify with a religion which contrast to the 12% of female participants.\n\nFrom your RStudio Cloud script, do the following activities using the data in the nilt_subset object (feel free to copy and adapt the code shown above):\n\nCreate a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\nAre your summary statistics hiding something interesting? Discuss your results with your neighbour or your tutor.\n\nIn the previous section we’ve learnt how to summarise categorical data. But very often we want to work with continuous numeric variables or a combination of both. To summarise and understand numeric data there are two main types: measures of centrality and measures of spread.\nAs before, try to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown.\n\nIn quantitative research, we usually have access to many observations in a sample which contains different attributes for each of them. It would be difficult (and probably not very useful) to talk about each of the NILT respondents one by one. Instead, to describe this sample we need measures that roughly represent all participants.\nThis is actually an important step in quantitative research, since it allows us to characterise the people that we are studying. For example, in the previous section we only talked about the respondents’ sex and political affiliation, but who are the people we are talking about? Probably a place to start digging deeper is to know their age. The first tool that we will use to understand numeric values is a histogram. Let’s see how the age of NILT respondents is distributed.\n\nhist(nilt_subset$rage)\n\n\n\n\n\n\n\nThis plot shows us on the X axis (horizontal) the age and the frequency on the Y axis. We can see that the youngest age in the sample is somewhere close to 20, and the oldest is almost 100. We also observe that the total number of observations (represented by the frequency on the vertical axis) for extreme values (close to 20 on the left-hand side and 100 on the right-hand side) tends to be lower than the values in the centre of the plot (somewhere between 30 and 45). For instance, we can see that there are approximately 120 respondents who are around 40 years old; that seems to be the most popular/frequent age in our sample. Now, we can represent these central values with actual measures, typically mean or median.\nThe median is the mid-point value in a numeric series. If you sort the values and split it by half, the value right in the middle is the median. Luckily there is a function ready to be used called… You guessed it - median().\n\nmedian(nilt_subset$rage, na.rm = TRUE)\n\n[1] 48\n\n\nThe median age is 48, that means that 50% (or half) of the respondents are equal or younger than this, and the other 50% is equal or older.1\nTo compute the mean manually, we need to sum all our values and divide it by the total number of the observations as follows: \\[ mean =\\frac{  x_1 + x_2 + x_3 ...+x_n } {n} \\] The formula above is for you to know that this measure considers the magnitude of all values included in the numeric series. Therefore, the average is sensitive to extreme numbers (e.g. a very, very old person). To compute the mean you need the mean() function.\n\nmean(nilt_subset$rage, na.rm = T)\n\n[1] 49.61532\n\n\nAs you can see, the above measures try to approximate values that fall somewhere in the centre of the histogram plot, and represent all observations in the sample. They tell different things and are sometimes more (or less) suitable in a specific situation.\n\nBy contrast, there are measures that help us to describe how far away a series of numeric values are from the centre. The common measures of spread are quartiles, variance and standard deviation.\nThe quartiles are very useful to quickly see how numeric data is distributed. Imagine that we sort all ages in the sample and split it into four equal parts. The first quartile includes the lowest 25% of values, the second the next 25%, the third another 25%, and the fourth the highest 25%. To compute quartiles, we can use the quantile function.\n\nquantile(nilt_subset$rage, na.rm = T)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\nIn our sample, the youngest quarter of the respondents is between 18 and 35 years old. The second quarter is between 35 and 48 years old. The next quartile is between 48 and 64. The oldest 25% of the respondents is between 64 and 97.\nThe variance is useful to obtain a singe measure of spread (instead of four values, as the above) taking the mean as a reference. This is given by the following formula:\n\\[ var = \\frac{ \\sum(x - \\bar{x})^2 }{n-1 } \\]\nTo decipher the formula above, the \\(\\bar{x}\\) represents the mean, the \\(x\\) represents each of the values in the numeric series. The formula takes each of the \\(x\\) values and subtract it from the mean \\(\\bar{x}\\). Later, it squares the result of the subtraction (that is multiply it by itself). This is done to obtain a positive value, since some numbers in the series will be lower than the mean (resulting in negative values). Then, we sum all of them and divide the sum by the size/length of the numeric sequence \\(n\\) minus 1. To estimate the variance in R we only need the var() function.\n\nvar(nilt_subset$rage, na.rm = T)\n\n[1] 343.3486\n\n\nAs you can see, the result is not very intuitive. That is because we squared the subtraction. Luckily, there is a measure that put it in readable scale. This is the standard deviation. In essence this takes the square root of the variance (the reversed operation of squaring it): \\[sd=\\sqrt{var}\\]\nTo compute it in R, use the sd() function.\n\nsd(nilt_subset$rage, na.rm = T)\n\n[1] 18.52967\n\n\nThis measure is more human readable than the variance. Don’t worry too much about the formula. An important thing to remember is what the measure represents. An informal definition of the standard deviation is the average distance from the mean. In essence, it tell us how far the values in our data are from the mean.\n\nPhew, that was a lot!…\n… Luckily we can use the sumtable function to compute all these measures at the same time!\nIt is very simple. You can compute a quick summary for age as following:\n\nsumtable(nilt_subset, vars = c(\"rage\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\nrage\n1201\n50\n19\n18\n35\n64\n97\n\n\n\n\nThe result displays the number of observations used (N), the mean, the standard deviation, minimum, the 1st (same as ‘Pctl. 25’) and 3rd quartile (same as ‘Pctl. 75’), as well as the maximum (i.e., eldest respondent).\n\nLastly, there will be times in which you will need to compute a summary combining categorical and numeric data, to compare groups for example. The good news is that we can use exactly the same function and syntax to do this. Let’s take the following example to compute the summary of the respondent’s age (rage) by gender:\n\nsumtable(nilt_subset, vars = c(\"rage\"), group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\n\n\n\nrage\n535\n51\n18\n666\n49\n19\n\n\n\n\nIn the code above, we are simple specifying the variable rage and grouping the summary by rsex. This produces a small summary included the number of observations in each category an the main measure of centrality and spread, namely the mean and the std. dev.\n\nCan money buy happiness?\nUsing the data in the nilt_subset dataset, complete the following activities. This will be a good practice run for doing the research report when you run your own analysis on the NILT teaching dataset. Handy!\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions (use the $ symbol after the name of the dataset and then the name of the variable inside the function);\nCreate a summary of the personal income persinc2 variable, using the sumtable() function;\nFinally, compute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy. What do you observe?\nDiscuss the results with your neighbour or your tutor.",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "04-Lab4.html#footnotes",
    "href": "04-Lab4.html#footnotes",
    "title": "Exploratory data analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\nNote that the argument na.rm equal TRUE is used in the function. The ‘na’ bit refers to missing values, and the ‘rm’ refers to remove. So we are telling R to remove the missing values when computing the median. This is because we do not know the age of 3 of the respondents in the sample.↩︎",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "09-Lab9.html",
    "href": "09-Lab9.html",
    "title": "Multivariate linear model",
    "section": "",
    "text": "In the last lab we mentioned that one of the advantages of linear regression is that we can include more than one independent (explanatory) variable to evaluate their relationship with the dependent variable. This technique is known as multivariate linear regression. In practice, few studies in quantitative social research rely only on simple linear models. This is because often in reality there is more than one variable associated to a social phenomenon. This is why it is important to familiarise yourself with the multivariate linear model.\n\nThe multivariate linear model is based on the same principles as the simple linear model. It is worth remembering that this model is appropriate only when we have a numeric (interval/ratio) dependent variable. As a rule-of-thumb, a variable can be treated as numeric when you have at least 7 ordered categories (Fogarty, 2019). As mentioned before, we can use any type of independent variables, such as ordinal, categorical and numeric. More than one and a combination of these can be analysed simultaneously.\n\nWe will follow-up with the example we used before introducing the simple linear model to further illustrate the advantages of using more than one variable. To do so, please set your RStudio environment as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud;\nOpen your own ‘NILT2’ project from your ‘Quants lab group’;\nOnce in the project, create a new R Script file (a simple R Script, NOT an .Rmd file).\nSave the script document as ‘multivariate_linear_model’.\n\nReproduce the code below, by copying, pasting and running it from your new script.\nFirst, install the moderndivepackage.\n\ninstall.packages(\"moderndive\")\n\nLoad the tidyverse and moderndive libraries (tidyverse was pre-installed for you before, you do not need to re-install it in this project) and read the nilt_r_object.rds file which is stored in the ‘data’ folder and contains the NILT survey.\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(moderndive)\n\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nOnce again, create a minimal random sample for the following example using the code below:\n\n# select a small random sample\nset.seed(3)\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- filter(nilt, !is.na(spage)) %&gt;% sample_n(40)\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- select(nilt_sample, rage, spage, rsex)\n\nAs a follow-up, we will draw a scatter plot using ggplot specifying the age of the respondent rage on the X axis and the respondent’s partner/spouse age spage on the Y axis. This time, we will add sex rsex as a third variable defining the color argument.\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nWhat do you observe in the plot above? …\nThe first thing to note is that females and males go in the same direction. When the respondent’s age increases the age of the partner increases as well. An interesting thing is that females tend to be located higher with respect to the Y axis compared to the male dots.\nTherefore, we can imagine that not only the age of the respondent is involved in the decision on how people choose their partner, but also the respondents’ sex. We can draw two lines, one for male and the other for female respondents, instead of only a general one as we did in the previous lab. To do this we will use the ggplot function in combination with the geom_parallel_slopes function from the moderndive package:\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nWell, our suspicion that the points representing female respondents were generally above the male ones is turning out to be true. What we have in the plot above are the optimal parallel lines that describe our points the best for each group. The interpretation of our visualizations so far is:\n\nboth males and females partner’s age is positively associated with respondent’s age.\nThis association is different for males and females. Overall, females choose older partners compared to males.\n\nBut what is the magnitude of these relationships? We can easily extend the simple linear model by adding rsex as a second explanatory variable as follows:\n\nm1 &lt;- lm(spage ~ rage + rsex, nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage + rsex, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage   rsexFemale  \n     1.7383       0.9138       4.9190  \n\n\nFrom the output, we see that there is one intercept \\(\\beta_0\\), one slope coefficient \\(\\beta_1\\) for the numeric variable rage, and another coefficient \\(\\beta_2\\) for the categorical variable. If you observe closer, the output appended only one of the two categories for rsex. This is because categorical variables take one of the categories as the reference. The variable that is not shown is the reference, ‘Male’, in this case.\nBeing more precise with our previous interpretation, we can say there is a positive relationship between both males and female participants’ age and their partner’s age by a factor of 0.91 for every additional year in age. Also, female respondent’s partners are expected to be 4.9 years older compared to male respondents.\nBefore we move on, it is worth mentioning that the criterion to fit the coefficient is the same as in the linear model. This procedure guarantees to produce the smallest possible sum of squared residuals (SSR) using the ordinary least square method (OLS). We can check if the SSR was reduced by adding this extra variable by computing the SSR as we did before:\n\nsum(residuals(m1)^2)\n\n[1] 948.263\n\n\nYes, it improved! Before, it was about 1,175, as we saw in the last lab workbook.\n\nAfter the introduction of the simple linear model, the formal definition of the multivariate linear model should not look that scary, right? In essence, this equation tells us how \\(\\hat{y}\\) is described/explained by other variables.\n\\[\n\\begin{aligned}\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}*x_1 + \\hat{\\beta_2}*x_2 + ... +  \\hat{\\epsilon}, && \\epsilon ~ N(0, \\sigma)  \n\\end{aligned}\n\\]\nWe can break down the equation above in smaller pieces as follows:\n\n\n\\(\\hat{y}\\) is the dependent variable, which is explained by\n\n\\(\\hat{\\beta_0}\\) the intercept, plus\n\n\\(\\hat{\\beta_1}\\) the slope coefficient for the first independent variable times the value of \\(x_1\\) variable 1, plus\n\n\\(\\hat{\\beta_2}\\) the slope coefficient for the second independent variable times the value of \\(x_2\\), plus\n\n\\(...\\) any number of independent variables, plus\n\n\\(\\hat{\\epsilon}\\) the error/residual term.\n\n\\(\\epsilon ~ N(0, \\sigma)\\) this bit tell us that the residuals are normally distributed.\n\n(You don’t need to reproduce the code of this section in your script).\nThe general syntax in R is as follows:\n\nlm(formula, data)\n\nWhere the formula is given by a dependent variable which is followed by ~ one or more independent variables joined by a plus sign +:\n\ndependent_variable ~ independent_variable1 + independent_variable2 ...\n\nIn the previous section, we used a small random sample of 40 people only. Now, let’s fit a multivariate linear model using all the observations in the nilt dataset. In addition to the respondent age rage and rsex, we might be interested to know whether the type of place where they live (e.g. big city, village, farm/country, etc.) placeliv plays a role in how people choose their partner.\nFirst, we need to coerce the placeliv variable as factor. And we will have a quick look at a table of this variable to have an idea of how respondents are distributed and see how the factor levels are ordered.\n\n# Coerce to factor\nnilt &lt;- nilt %&gt;%\n  mutate(placeliv = as_factor(placeliv))\n# Create table\ntable(nilt$placeliv)\n\n\n                             Dont know                          ...a big city \n                                     0                                    168 \nthe suburbs or outskirts of a big city                   a small city or town \n                                   233                                    477 \n                     a country village          a farm or home in the country \n                                   176                                    146 \n\n\nWe see that most of the respondents live in ‘a small city or town’. Also, note that the category ‘Dont know’ contains 0 observations. Let’s use the function droplevels() to remove the unused category in our variables. This also means that the first level in this variable will be ‘a big city’. Then, we will fit the model and store it in an object called m2.\n\n# remove unused levels\nnilt &lt;- droplevels(nilt)\n# boxplot\nm2 &lt;- lm(spage ~ rage + rsex + placeliv, nilt)\nm2\n\n\nCall:\nlm(formula = spage ~ rage + rsex + placeliv, data = nilt)\n\nCoefficients:\n                                   (Intercept)  \n                                       1.34251  \n                                          rage  \n                                       0.94147  \n                                    rsexFemale  \n                                       3.79426  \nplacelivthe suburbs or outskirts of a big city  \n                                       0.02099  \n                  placeliva small city or town  \n                                      -0.71196  \n                     placeliva country village  \n                                       0.25645  \n         placeliva farm or home in the country  \n                                      -0.01280  \n\n\nFrom the result:\n\nWe confirm the positive relationship between the respondent’s age and their partner by a factor of 0.94.\nThe difference in years by sex now is smaller. We see that the age of a female respondent’s partner is expected to be 3.8 years older compared to males.\nAs before, we see that the result appended the name of the categories to the placeliv variables. This is because it is comparing each of the categories with the category of reference, in this case, ‘a big city’. The interpretation must be in comparative terms. For instance, the age of the respondent’s partner in ‘the suburbs or outskirts of a big city’ is expected to be 0.02 years older than the partner of someone living in a ‘a big city’. Overall, the coefficients do not differ much as a function of the type of place of where they live. The largest expected difference is for people living in ‘a small city or town’, where partner’s age is expected to be -0.7 years old compared to those living in ‘a big city’.\n\nIn general, our model can be expressed as:\n\\[ \\hat{spage}_i = \\hat{\\beta_0} + \\hat{\\beta_1}*rage_i + \\hat{\\beta_2}*rsex_i + \\hat{\\beta_3} *  placeliv_i \\] So, if we wanted to know the expected age of the partner of a 40 years old female who lives in a small city or town, we can use and apply the formula above:\n\n1.34 + 0.94 * 40 + 3.80 * 1 + -0.7 * 1\n\n[1] 42.04\n\n\nThe expected age of a 40 year old female’s partner is, therefore, 41.7 years old.\n\nWe can obtain more details from our model using the summary() function.\n\nsummary(m2)\n\n\nCall:\nlm(formula = spage ~ rage + rsex + placeliv, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.737  -2.176  -0.139   2.214  19.530 \n\nCoefficients:\n                                               Estimate Std. Error t value\n(Intercept)                                     1.34251    0.86727   1.548\nrage                                            0.94147    0.01202  78.311\nrsexFemale                                      3.79426    0.37761  10.048\nplacelivthe suburbs or outskirts of a big city  0.02099    0.72637   0.029\nplaceliva small city or town                   -0.71196    0.65391  -1.089\nplaceliva country village                       0.25645    0.75903   0.338\nplaceliva farm or home in the country          -0.01280    0.75787  -0.017\n                                               Pr(&gt;|t|)    \n(Intercept)                                       0.122    \nrage                                             &lt;2e-16 ***\nrsexFemale                                       &lt;2e-16 ***\nplacelivthe suburbs or outskirts of a big city    0.977    \nplaceliva small city or town                      0.277    \nplaceliva country village                         0.736    \nplaceliva farm or home in the country             0.987    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.524 on 583 degrees of freedom\n  (614 observations deleted due to missingness)\nMultiple R-squared:  0.9143,    Adjusted R-squared:  0.9134 \nF-statistic:  1037 on 6 and 583 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the summary, we note that the 1st and 3rd quartile of residuals are symmetric (-2.18 to 2.21). This means that 50% of the estimated/predicted ages are approximately \\(\\pm\\) 2.2 years away from the observed partner’s age. Though, the minimum and the maximum residuals are not symmetric, this could be due to some extreme observed values. We can have a look to a histogram of the residuals to have a clearer idea.\n\nhist(m2$residuals)\n\n\n\n\n\n\n\nOverall they seem normally distributed with the exception of the negative value to the left.\nSecondly, from the summary we can see that respondent age rage and respondent sex rsex have a significant association with the dependent variable (indicated by the p-value &lt; 0.001). You can confirm this in the fourth column of the coefficients (‘Pr(&gt;|t|)’). Also, the std. error for the coefficients of this variables is small. However, the std. error for the categories of the place of where they live placeliv is large when compared to the estimated coefficients in the first column. In fact, when we look at the fourth column we see that the p-values are much larger than 0.05, which means that there is not a significant relationship between any of the type of places where they live and the partner’s respondent age. What this means is that the place where people live does not play a role in the age of the respondent’s partner.\nThird, this time we see that the multiple R-squared is slightly different from the adjusted R-squared. This is because the adjusted considers the number of independent variables included in the model. This is why the adjusted r-squared is preferred for multivariate models. This shows our model explains 91.34% of the variance of the respondent’s partner age.\nLastly, it is important to note that even though we use the full nilt data set, 614 of the observations were not considered in the analysis. This is because we do not have information for these respondents. Probably some do not have a partner or preferred not to say their sex, for example.\n\nYou need to know that these estimates are backed by well-established patterns studied in probability and statistics. Therefore, the linear model, as many other statistical techniques, provides reliable estimates if the data follows certain characteristics. These are known as assumptions. We will not go into detail with these. However, it is important that you have them in mind when you evaluate your results and also evaluate others.\nThere are many assumption for the linear regression model, but we will introduce four of the most commonly assessed (Boston University School of Public Health, 2016):\n\nLinearity: The relationship between X and the mean of Y is linear.\nHomoscedasticity: The variance of residual is the same for any value of X.\nIndependence: Observations are independent of each other.\nNormality: For any fixed value of X, Y is normally distributed.\n\nA first step to identify potential violations of the points above is to assess the distribution of the residuals by looking to the quartiles, mean and histogram. There are many other specific tests and techniques that can help us to know whether we met the assumptions and more importantly that can help to correct them.\nFor the moment, we will leave it here. The important thing is to be aware of the existence of these potential problems and to be transparent with the results you produce. For now, I would suggest you to acknowledge the limitations of your assumptions checks.\n\nSet an R Markdown document in your ‘NILT2’ project as following:\n\nCreate a new Rmd file, type ‘Multivariate linear model’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML.\nSave the Rmd document as ‘Multivariate_lab’.\nErase all the contents in the Rmd default example with the exception of the first bit (that contains the YAML) and the first R chunk (which contains the default chunk options), that is all from line 12 and on.\n\nUsing the nilt object do the following by inserting a new chunk for each bullet points below (remember to write the comments and observations for the results as simple text outside the chunks):\n\nLoad the packages, and the data that you will need in your file using the code below:\n\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(moderndive)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary.\nComment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model.\nPlot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model?\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "09-Lab9.html#introduction",
    "href": "09-Lab9.html#introduction",
    "title": "Multivariate linear model",
    "section": "",
    "text": "In the last lab we mentioned that one of the advantages of linear regression is that we can include more than one independent (explanatory) variable to evaluate their relationship with the dependent variable. This technique is known as multivariate linear regression. In practice, few studies in quantitative social research rely only on simple linear models. This is because often in reality there is more than one variable associated to a social phenomenon. This is why it is important to familiarise yourself with the multivariate linear model.",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "09-Lab9.html#multivariate-model",
    "href": "09-Lab9.html#multivariate-model",
    "title": "Multivariate linear model",
    "section": "",
    "text": "The multivariate linear model is based on the same principles as the simple linear model. It is worth remembering that this model is appropriate only when we have a numeric (interval/ratio) dependent variable. As a rule-of-thumb, a variable can be treated as numeric when you have at least 7 ordered categories (Fogarty, 2019). As mentioned before, we can use any type of independent variables, such as ordinal, categorical and numeric. More than one and a combination of these can be analysed simultaneously.\n\nWe will follow-up with the example we used before introducing the simple linear model to further illustrate the advantages of using more than one variable. To do so, please set your RStudio environment as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud;\nOpen your own ‘NILT2’ project from your ‘Quants lab group’;\nOnce in the project, create a new R Script file (a simple R Script, NOT an .Rmd file).\nSave the script document as ‘multivariate_linear_model’.\n\nReproduce the code below, by copying, pasting and running it from your new script.\nFirst, install the moderndivepackage.\n\ninstall.packages(\"moderndive\")\n\nLoad the tidyverse and moderndive libraries (tidyverse was pre-installed for you before, you do not need to re-install it in this project) and read the nilt_r_object.rds file which is stored in the ‘data’ folder and contains the NILT survey.\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(moderndive)\n\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nOnce again, create a minimal random sample for the following example using the code below:\n\n# select a small random sample\nset.seed(3)\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- filter(nilt, !is.na(spage)) %&gt;% sample_n(40)\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- select(nilt_sample, rage, spage, rsex)\n\nAs a follow-up, we will draw a scatter plot using ggplot specifying the age of the respondent rage on the X axis and the respondent’s partner/spouse age spage on the Y axis. This time, we will add sex rsex as a third variable defining the color argument.\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nWhat do you observe in the plot above? …\nThe first thing to note is that females and males go in the same direction. When the respondent’s age increases the age of the partner increases as well. An interesting thing is that females tend to be located higher with respect to the Y axis compared to the male dots.\nTherefore, we can imagine that not only the age of the respondent is involved in the decision on how people choose their partner, but also the respondents’ sex. We can draw two lines, one for male and the other for female respondents, instead of only a general one as we did in the previous lab. To do this we will use the ggplot function in combination with the geom_parallel_slopes function from the moderndive package:\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nWell, our suspicion that the points representing female respondents were generally above the male ones is turning out to be true. What we have in the plot above are the optimal parallel lines that describe our points the best for each group. The interpretation of our visualizations so far is:\n\nboth males and females partner’s age is positively associated with respondent’s age.\nThis association is different for males and females. Overall, females choose older partners compared to males.\n\nBut what is the magnitude of these relationships? We can easily extend the simple linear model by adding rsex as a second explanatory variable as follows:\n\nm1 &lt;- lm(spage ~ rage + rsex, nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage + rsex, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage   rsexFemale  \n     1.7383       0.9138       4.9190  \n\n\nFrom the output, we see that there is one intercept \\(\\beta_0\\), one slope coefficient \\(\\beta_1\\) for the numeric variable rage, and another coefficient \\(\\beta_2\\) for the categorical variable. If you observe closer, the output appended only one of the two categories for rsex. This is because categorical variables take one of the categories as the reference. The variable that is not shown is the reference, ‘Male’, in this case.\nBeing more precise with our previous interpretation, we can say there is a positive relationship between both males and female participants’ age and their partner’s age by a factor of 0.91 for every additional year in age. Also, female respondent’s partners are expected to be 4.9 years older compared to male respondents.\nBefore we move on, it is worth mentioning that the criterion to fit the coefficient is the same as in the linear model. This procedure guarantees to produce the smallest possible sum of squared residuals (SSR) using the ordinary least square method (OLS). We can check if the SSR was reduced by adding this extra variable by computing the SSR as we did before:\n\nsum(residuals(m1)^2)\n\n[1] 948.263\n\n\nYes, it improved! Before, it was about 1,175, as we saw in the last lab workbook.\n\nAfter the introduction of the simple linear model, the formal definition of the multivariate linear model should not look that scary, right? In essence, this equation tells us how \\(\\hat{y}\\) is described/explained by other variables.\n\\[\n\\begin{aligned}\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}*x_1 + \\hat{\\beta_2}*x_2 + ... +  \\hat{\\epsilon}, && \\epsilon ~ N(0, \\sigma)  \n\\end{aligned}\n\\]\nWe can break down the equation above in smaller pieces as follows:\n\n\n\\(\\hat{y}\\) is the dependent variable, which is explained by\n\n\\(\\hat{\\beta_0}\\) the intercept, plus\n\n\\(\\hat{\\beta_1}\\) the slope coefficient for the first independent variable times the value of \\(x_1\\) variable 1, plus\n\n\\(\\hat{\\beta_2}\\) the slope coefficient for the second independent variable times the value of \\(x_2\\), plus\n\n\\(...\\) any number of independent variables, plus\n\n\\(\\hat{\\epsilon}\\) the error/residual term.\n\n\\(\\epsilon ~ N(0, \\sigma)\\) this bit tell us that the residuals are normally distributed.",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "09-Lab9.html#r-syntax-for-the-multivariate-linear-model",
    "href": "09-Lab9.html#r-syntax-for-the-multivariate-linear-model",
    "title": "Multivariate linear model",
    "section": "",
    "text": "(You don’t need to reproduce the code of this section in your script).\nThe general syntax in R is as follows:\n\nlm(formula, data)\n\nWhere the formula is given by a dependent variable which is followed by ~ one or more independent variables joined by a plus sign +:\n\ndependent_variable ~ independent_variable1 + independent_variable2 ...\n\nIn the previous section, we used a small random sample of 40 people only. Now, let’s fit a multivariate linear model using all the observations in the nilt dataset. In addition to the respondent age rage and rsex, we might be interested to know whether the type of place where they live (e.g. big city, village, farm/country, etc.) placeliv plays a role in how people choose their partner.\nFirst, we need to coerce the placeliv variable as factor. And we will have a quick look at a table of this variable to have an idea of how respondents are distributed and see how the factor levels are ordered.\n\n# Coerce to factor\nnilt &lt;- nilt %&gt;%\n  mutate(placeliv = as_factor(placeliv))\n# Create table\ntable(nilt$placeliv)\n\n\n                             Dont know                          ...a big city \n                                     0                                    168 \nthe suburbs or outskirts of a big city                   a small city or town \n                                   233                                    477 \n                     a country village          a farm or home in the country \n                                   176                                    146 \n\n\nWe see that most of the respondents live in ‘a small city or town’. Also, note that the category ‘Dont know’ contains 0 observations. Let’s use the function droplevels() to remove the unused category in our variables. This also means that the first level in this variable will be ‘a big city’. Then, we will fit the model and store it in an object called m2.\n\n# remove unused levels\nnilt &lt;- droplevels(nilt)\n# boxplot\nm2 &lt;- lm(spage ~ rage + rsex + placeliv, nilt)\nm2\n\n\nCall:\nlm(formula = spage ~ rage + rsex + placeliv, data = nilt)\n\nCoefficients:\n                                   (Intercept)  \n                                       1.34251  \n                                          rage  \n                                       0.94147  \n                                    rsexFemale  \n                                       3.79426  \nplacelivthe suburbs or outskirts of a big city  \n                                       0.02099  \n                  placeliva small city or town  \n                                      -0.71196  \n                     placeliva country village  \n                                       0.25645  \n         placeliva farm or home in the country  \n                                      -0.01280  \n\n\nFrom the result:\n\nWe confirm the positive relationship between the respondent’s age and their partner by a factor of 0.94.\nThe difference in years by sex now is smaller. We see that the age of a female respondent’s partner is expected to be 3.8 years older compared to males.\nAs before, we see that the result appended the name of the categories to the placeliv variables. This is because it is comparing each of the categories with the category of reference, in this case, ‘a big city’. The interpretation must be in comparative terms. For instance, the age of the respondent’s partner in ‘the suburbs or outskirts of a big city’ is expected to be 0.02 years older than the partner of someone living in a ‘a big city’. Overall, the coefficients do not differ much as a function of the type of place of where they live. The largest expected difference is for people living in ‘a small city or town’, where partner’s age is expected to be -0.7 years old compared to those living in ‘a big city’.\n\nIn general, our model can be expressed as:\n\\[ \\hat{spage}_i = \\hat{\\beta_0} + \\hat{\\beta_1}*rage_i + \\hat{\\beta_2}*rsex_i + \\hat{\\beta_3} *  placeliv_i \\] So, if we wanted to know the expected age of the partner of a 40 years old female who lives in a small city or town, we can use and apply the formula above:\n\n1.34 + 0.94 * 40 + 3.80 * 1 + -0.7 * 1\n\n[1] 42.04\n\n\nThe expected age of a 40 year old female’s partner is, therefore, 41.7 years old.\n\nWe can obtain more details from our model using the summary() function.\n\nsummary(m2)\n\n\nCall:\nlm(formula = spage ~ rage + rsex + placeliv, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.737  -2.176  -0.139   2.214  19.530 \n\nCoefficients:\n                                               Estimate Std. Error t value\n(Intercept)                                     1.34251    0.86727   1.548\nrage                                            0.94147    0.01202  78.311\nrsexFemale                                      3.79426    0.37761  10.048\nplacelivthe suburbs or outskirts of a big city  0.02099    0.72637   0.029\nplaceliva small city or town                   -0.71196    0.65391  -1.089\nplaceliva country village                       0.25645    0.75903   0.338\nplaceliva farm or home in the country          -0.01280    0.75787  -0.017\n                                               Pr(&gt;|t|)    \n(Intercept)                                       0.122    \nrage                                             &lt;2e-16 ***\nrsexFemale                                       &lt;2e-16 ***\nplacelivthe suburbs or outskirts of a big city    0.977    \nplaceliva small city or town                      0.277    \nplaceliva country village                         0.736    \nplaceliva farm or home in the country             0.987    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.524 on 583 degrees of freedom\n  (614 observations deleted due to missingness)\nMultiple R-squared:  0.9143,    Adjusted R-squared:  0.9134 \nF-statistic:  1037 on 6 and 583 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the summary, we note that the 1st and 3rd quartile of residuals are symmetric (-2.18 to 2.21). This means that 50% of the estimated/predicted ages are approximately \\(\\pm\\) 2.2 years away from the observed partner’s age. Though, the minimum and the maximum residuals are not symmetric, this could be due to some extreme observed values. We can have a look to a histogram of the residuals to have a clearer idea.\n\nhist(m2$residuals)\n\n\n\n\n\n\n\nOverall they seem normally distributed with the exception of the negative value to the left.\nSecondly, from the summary we can see that respondent age rage and respondent sex rsex have a significant association with the dependent variable (indicated by the p-value &lt; 0.001). You can confirm this in the fourth column of the coefficients (‘Pr(&gt;|t|)’). Also, the std. error for the coefficients of this variables is small. However, the std. error for the categories of the place of where they live placeliv is large when compared to the estimated coefficients in the first column. In fact, when we look at the fourth column we see that the p-values are much larger than 0.05, which means that there is not a significant relationship between any of the type of places where they live and the partner’s respondent age. What this means is that the place where people live does not play a role in the age of the respondent’s partner.\nThird, this time we see that the multiple R-squared is slightly different from the adjusted R-squared. This is because the adjusted considers the number of independent variables included in the model. This is why the adjusted r-squared is preferred for multivariate models. This shows our model explains 91.34% of the variance of the respondent’s partner age.\nLastly, it is important to note that even though we use the full nilt data set, 614 of the observations were not considered in the analysis. This is because we do not have information for these respondents. Probably some do not have a partner or preferred not to say their sex, for example.",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "09-Lab9.html#assumptions",
    "href": "09-Lab9.html#assumptions",
    "title": "Multivariate linear model",
    "section": "",
    "text": "You need to know that these estimates are backed by well-established patterns studied in probability and statistics. Therefore, the linear model, as many other statistical techniques, provides reliable estimates if the data follows certain characteristics. These are known as assumptions. We will not go into detail with these. However, it is important that you have them in mind when you evaluate your results and also evaluate others.\nThere are many assumption for the linear regression model, but we will introduce four of the most commonly assessed (Boston University School of Public Health, 2016):\n\nLinearity: The relationship between X and the mean of Y is linear.\nHomoscedasticity: The variance of residual is the same for any value of X.\nIndependence: Observations are independent of each other.\nNormality: For any fixed value of X, Y is normally distributed.\n\nA first step to identify potential violations of the points above is to assess the distribution of the residuals by looking to the quartiles, mean and histogram. There are many other specific tests and techniques that can help us to know whether we met the assumptions and more importantly that can help to correct them.\nFor the moment, we will leave it here. The important thing is to be aware of the existence of these potential problems and to be transparent with the results you produce. For now, I would suggest you to acknowledge the limitations of your assumptions checks.",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "09-Lab9.html#lab-activities",
    "href": "09-Lab9.html#lab-activities",
    "title": "Multivariate linear model",
    "section": "",
    "text": "Set an R Markdown document in your ‘NILT2’ project as following:\n\nCreate a new Rmd file, type ‘Multivariate linear model’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML.\nSave the Rmd document as ‘Multivariate_lab’.\nErase all the contents in the Rmd default example with the exception of the first bit (that contains the YAML) and the first R chunk (which contains the default chunk options), that is all from line 12 and on.\n\nUsing the nilt object do the following by inserting a new chunk for each bullet points below (remember to write the comments and observations for the results as simple text outside the chunks):\n\nLoad the packages, and the data that you will need in your file using the code below:\n\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(moderndive)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary.\nComment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model.\nPlot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model?\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "assessments/assessments.html",
    "href": "assessments/assessments.html",
    "title": "Assessments",
    "section": "",
    "text": "Remember to also check the Assessment section on Moodle, which contains further information and the Turnitin submission areas.\n\n\n\n\n\n\nDeadlines\n\n\n\n\nFormative Assessment – 12 noon, Friday 31st October 2025\nSummative Assessments (Reflective Summary + Interpreting Quantitative Findings Report) - 12 noon, Monday 15th December 2025.\n\n\n\n\nGenAI and the Assessments\nThe University of Glasgow’s position on GenAI use is not to prohibit GenAI use, but instead to support you to use it “effectively, ethically, critically, and transparently”.\nFor the assessments on this course, you are allowed to use GenAI tools in some specific ways as part of your assessment.\nThe use of GenAI tools can augment specific parts of your submission; such as to help with drafting and editing, and to explore key ideas.\nHowever, key to the interpretive report assessment is using R and interpreting research findings. It is, therefore, not permitted to use GenAI to write R code for your analysis nor to interpret the statistical results. It is acceptable to use GenAI to receive textbook code examples and explanations, but not the exact code needed for the analysis. Similarly, it is acceptable to use GenAI for explanations on how to interpret statistics in general, but not the exact stats from your analysis.\nIf using any form of GenAI tool, you must acknowledge how the tools have been used within your work. You cannot submit any content produced by GenAI as your own work.\nRStudio Helper is an example ChatGPT Custom GPT / Copilot Saved Prompt created for use on this module. It has instructions that guide it towards providing more useful responses that aid learning and that align with the specific ways GenAI can be used for the assessments. However, whilst it has these instructions, GenAI does not always reliably and consistently follow them. So, still exercise the same caution with its responses and use prompts that align with permitted use cases for the assessments.",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "assessments/summative.html",
    "href": "assessments/summative.html",
    "title": "Summative Assessments",
    "section": "",
    "text": "Summative Submission Deadline\n\n\n\n12 noon, Monday 15th December 2025\n\n\nThere is one summative assessment, split into two parts:\n\nReflective Course Summary. 500 words, worth 30% of your total grade\nInterpreting Quantitative Findings Report. 2500 words, worth the remaining 70% of your total grade.\n\nThese are submitted together as one document: with the report first; and summary second.\nThere is no end-of-course examination for this course. All assessment is based on the summative assessment components that must be completed.",
    "crumbs": [
      "Assessments",
      "Summative Assessments"
    ]
  },
  {
    "objectID": "assessments/summative.html#overview",
    "href": "assessments/summative.html#overview",
    "title": "Summative Assessments",
    "section": "",
    "text": "Summative Submission Deadline\n\n\n\n12 noon, Monday 15th December 2025\n\n\nThere is one summative assessment, split into two parts:\n\nReflective Course Summary. 500 words, worth 30% of your total grade\nInterpreting Quantitative Findings Report. 2500 words, worth the remaining 70% of your total grade.\n\nThese are submitted together as one document: with the report first; and summary second.\nThere is no end-of-course examination for this course. All assessment is based on the summative assessment components that must be completed.",
    "crumbs": [
      "Assessments",
      "Summative Assessments"
    ]
  },
  {
    "objectID": "assessments/summative.html#interpreting-quantitative-findings-report",
    "href": "assessments/summative.html#interpreting-quantitative-findings-report",
    "title": "Summative Assessments",
    "section": "Interpreting Quantitative Findings Report",
    "text": "Interpreting Quantitative Findings Report\nThis component is worth 70% of your total grade.\nYou must prepare the report in R Markdown (which you will learn to do through the lab workbook) and submit your file as a knitted HTML. You will get used to working in R Markdown in the labs each week, but remember you can always refer to the handy R Markdown cheatsheet if you need to.\nYou are required to write a 2,500-word report in R Markdown interpreting a regression model. The model has been prepared for you; you will find it in lab workbook 10’s instructions. You are not being asked to conduct your own regression therefore, but to show you can interpret the results from the model prepared for you and use the skills you have learned through the labs.\nThe aim is to give you some practical experience of designing and carrying out quantitative research in social science, including designing research questions and hypothesis based on theory and literature, preparing and wrangling data, running descriptive statistics and other statistical methods, reporting and interpreting findings, and showing reflexivity on your experience of working with quantitative data. You will have to decide what research questions should be asked and define and test your own hypothesis.\nThe report should be all your own work, but guidance will be provided in the lab sessions to discuss the statistical tests, visualisations, and other techniques that can be further conducted on the data for your report. It will give you experience in what it is like to analyse quantitative data for research.\nSatisfactory completion of this assignment demonstrates achievement of the following ILOs:\n\ndemonstrate an understanding of the basic principles of quantitative research design and strategy.\nconstruct research hypotheses and demonstrate basic skills in question formulation and questionnaire design.\ndemonstrate practical skills in the computer analysis and presentation of quantitative data (descriptive statistical analysis, tabulation, graphical presentation of numerical data).\ncritically assess social research from a methodological standpoint.\n\nIn what follows you will find a guide to writing the report. This is in the form of a list of questions. If you answer each of these questions you will have all of the content needed for a good report. Remember to take care with referencing and bibliographies, it can be easy to forget the basics when you are learning advanced statistical methods, but bibliographies are very important. There is reading material assigned for this class and you are expected to cite it, thereby demonstrating that you kept up with the reading, and using them to inform your understanding of relevant concepts and techniques.\n\nHow to write my research report?\nFirst, make sure you follow the instructions in lab workbook chapter 10 to download the report template we prepared for you in RStudio Cloud, which includes the regression model and R Markdown set up.\n\nPart 1: Introduction (approx. 400 words)\nLook at the regression model included in the template. Based on this choose and state your own research question and hypothesis. Remember a hypothesis needs at least two variables.\n\n\nPart 2: Data and Method (approx. 800 words)\nIn this section your goal is to demonstrate that you can describe data and that you understand research design and data collection. Therefore, you should make sure to cite the mandatory textbooks in support of your answers. The general rules of good scholarship apply to quantitative research as much as any other subject, so do take care with your references and bibliography. As in any other class you need to demonstrate you’ve kept up with reading and can use the reading to inform your understanding of the relevant concepts and data analytics.\n\nWhat is the data set? Who collected the data and for what purpose? What does the data describe?\nWhat is the sample size? How was the data collected? Why is this reliable and are there any potential shortcomings that could limit the interpretation of the data? Note that the sample size for the model is reduced compared to the data set overall. Account for this in preparing visualisations.\nPresent an appropriate visualisation of the dependent variable. Remember to number and label figures and tables. Describe the distribution. What does this plot tell us as a descriptive finding, and does it have any implications for the model?\nPresent a table of descriptive statistics for the variables included in the model. Discuss the descriptive findings, does the distribution of any variables have any implications for the model?\n\n\n\nPart 3: Results and Discussion (approx. 1300 words)\nIn this section your goal is to demonstrate that you can interpret quantitative results. You are likely to get a higher grade if you are able to relate these findings to social science theories and literature or if you can put the findings in context.\n\nWhat kind of model is this? (clue: is the dependent variable quantitative or dichotomous?) What is it a model of? Provide a very brief summary of your understanding of the whole model. \nWhich variables are significant? Which are insignificant?\nOf the significant results, discuss the coefficients. Which are negative, which are positive? You can focus on variables of your own choice from here, selected based on your hypothesis, and explain the numbers to your reader.\nDo any of the uncertainty estimates give you any cause for concern? Which and why?\nHow does the model fit? How do you account for the model fit?\nPresent and discuss appropriate visualizations of particularly interesting relationships in the model. You may choose your own variable to focus on, based on your hypothesis.\nAre any of these findings surprising? What do they mean for your hypothesis?\n\n\n\nPart 4: Conclusion (Approx. 500 words)\nClearly state your findings. Do the findings raise any questions for future research? The tone you are trying to achieve here is one of a confident researcher. Be proud of your findings and your interpretation. Throughout the paper, if you have addressed each of the questions above then you have already acknowledged shortcomings with the research, you still need to persuade your readers of your findings despite these limitations. That’s what a conclusion is for. A good start to a conclusion is with the phrase “in conclusion…” Be explicit in your conclusion, a reader should be able to read only the conclusion and know the research question, the findings and the importance of those findings.\nIn writing your report, bear in mind you will be assessed based on your ability to do the following:\n\nIntroduce the context of your research topic (albeit briefly).\nClearly articulate your research questions, hypothesis, and identification of variable(s) and how the literature guided this process.\nJustify choices made in the research design and analysis, and how these are informed by your understanding of a social science research methodology.\nExplain what technique and tests are run in R to produce your analysis, and what purposes they served to extract insights and/or ensure your analysis is robust.\nReport the results of your analysis accurately. This means interpreting coefficients, significance, and uncertainty estimates, as well as model fit statistics (also, e.g., use of tables, graphs, and visualisations) and provide a meaningful discussion of the results, based on a critical engagement with the literature, theories, and concepts, e.g., how have the results confirmed/contested the existing theories and concepts in the literature.\nIdentify potential limitations and weaknesses in your analysis, e.g., what statistical tests for significance are used? What extra questions/variables you would have liked to have included in the dataset and how could the dataset be improved?\nDrawing convincing conclusions based on an accurate analysis of the data and demonstrate a command of the relevant academic literature, theories, or concepts to explain the results and any emerging trends.\nArticulate any lessons and assumptions you learned as a researcher in this process, and/or identify suggestions of future avenues for research, policy, or theoretical debates, if appropriate.\nYour work should be consistently and fully referenced, with a complete bibliography.",
    "crumbs": [
      "Assessments",
      "Summative Assessments"
    ]
  },
  {
    "objectID": "assessments/summative.html#reflective-course-summary",
    "href": "assessments/summative.html#reflective-course-summary",
    "title": "Summative Assessments",
    "section": "Reflective Course Summary",
    "text": "Reflective Course Summary\nThis component is worth 30% of your total grade.\nYou will be required to submit a 500-word reflective summary, reflecting on your use of feedback from across the course. Learning quantitative methods can be new and challenging and require new ways of approaching how you learn. To recognise this, the Reflective Course summary asks you to capture this learning process, demonstrating how you used feedback on the course: proactively sought it out, and utilised these various sources to improve your learning. It therefore demonstrates your participation and learning journey.\nThere will be various sources of feedback you get on a course like this, both formal and informal. For example:\n\nThe Formative Assessment\nTutor feedback in labs\nExchanges with teaching staff: via email, during/after class, office hours, and on Moodle\nQuestions asked to or discussed with other students, informal chats with students in labs and outside classrooms\nDataCamp, if you choose to use this\nLearning process feedback:\n\nFeedback from R Studio: error messages, code not working or not working as you thought it might.\nRealising when you did not quite understand something like you thought.\n\n\nThese are suggestions and you do not need to include them all.\n\nWhat do I learn by doing a reflective summary?\nTo know what you have learned is an important a valuable metacognitive skill—to understand how you have learned. Feedback, and using feedback effectively, is also an important part of the learning process.\n\n\nWhat do you mean by ‘reflective’?\nHere it means therefore how you reflect on the use of feedback in the learning process.\nHow you sought it out, used it, and any differences it made. To show how you used feedback, you should use concrete, analytical examples rather than general description. You can – and should – then draw more general conclusions from these concrete examples.\nA reflective summary therefore means analytical reflection (thinking about what/why/how) rather than mere description (I had Problem X where I found issues with A, B, C but I used feedback to help me solve it). It’s fine to describe the issue but the reflective element comes from analysing (a) specifically you identified an issue (b) where/why/how you sought feedback (c) how you used the feedback (perhaps from several sources and through several attempts to use it) and (d) the concrete difference it made to the result or outcome. You should minimize description and focus on reflective analysis.\nSince you have 500 words, you should focus on those core issues which made the most difference to your learning journey. Specifically, you will want to demonstrate how feedback shaped your learning journey on the course:\n\nhighlight the main or core issues you encountered\nexplain how you used feedback to diagnose these\nexplain how you used feedback to improve and develop through the course: reflecting on what your main improvements were and how the use of feedback helped these take place\n\nPrioritise discussing issues about your reflections on:\n\nHow you utilize feedback in the core learning issues for you: this might involve learning how to code in the Labs, or learn new statistical concepts for example.\nHow you adapt the ways you learn based on feedback from other students and/or tutors, and other sources highlighted above. Your traditional approaches to learning might not be as effective when learning to code, say,\nThe ways and extent you used feedback: this the ability to reflect on your own strengths and weaknesses in quantitative methods, and how you adapted your learning based on feedback and your active use of feedback,\n\n\n\nHow do I reflect on my learning? \nThe aim of reflective learning is to think about and demonstrate what and how you have learned from feedback on the course.\nYou can do this by asking yourself a series of guiding questions as soon as possible\n\nafter each lab session.\nafter each lecture\nafter each study session\n\nIt might be helpful to jot down some brief notes on a regular basis (e.g., on paper, your phone, or in a computer). You can just write one or two bullet points or even do voice notes on your phone/device if you want.\nYou can then use these notes to complete your reflective summary. It is important that these notes are honest and authentic, otherwise the point of the exercise is lost.\nYou can use the following guiding questions as prompts to structure your reflective log entry:\n\nWhat did I do in the lab and what did I learn from what I did today? (e.g., what are the key “take home” points or “notes to self”?)\nHow did the course content (e.g., readings, lectures) and other resources help me to learn and complete the activities in the lab workbook?\nHow did I respond to feedback given by other students and/or tutors?\nLooking back, how could I have done better or what could I have done more efficiently?\nWhat else might I have done (before, in, or after the lab)?\nHow will this shape my actions for learning next week and in the future?\n\nIf you do this short exercise regularly it will provide you with a strong evidence base for summarising the progress you have made throughout this course and how you used feedback to inform this.\n\n\nHow will my Reflective Summary be assessed?\nYour completed Reflective Summary will be graded as a summative assessment at the end of the course. It will submitted with the Interpreting Quantitative Findings report.\nIt will be written in the same R Markdown document as one final Summative submission of 3,000 words:\nOverall Summative Submission (3,000 words)\nYou will be assessed based on the extent, depth, and quality of the reflection demonstrated in your learning log.\nBecause the word count is small, focus on the issues that are most relevant and pertinent.",
    "crumbs": [
      "Assessments",
      "Summative Assessments"
    ]
  }
]