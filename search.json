[
  {
    "objectID": "02-Lab2.html",
    "href": "02-Lab2.html",
    "title": "Data in R",
    "section": "",
    "text": "In our previous lab, we set up an RStudio Cloud session and we got familiar with the RStudio environment and some of the purpose and contents of its panes. In this Lab we will learn about R packages, how to install them and load them. Also, we will use different types of data. You will have the chance to practice with additional R operators. Lastly, we will load a real-world data set and put in practice your new skills.\n\nAs mentioned in our last lab, R (R Core Team 2021) is a collaborative project. This means that R users are developing, maintaining, and extending the functionalities constantly. When you set up R and RStudio for the first time, as we did it last week, it comes only with the ‘basic’ functionalities by default. However, there are literally thousands of extensions that are developed by other users. In R, these non-default extensions are called packages.\nMost of the times, we use packages because they simplify our work in R or they allow us to extend the capabilites of base R.\n\nLet’s put hands-on to install and load some useful packages. We will start with tidyverse (Wickham 2021).1\n\nPart 1. Access your lab group in RStudio Cloud\n\nMake sure you have a free, institutional-subscription RStudio Cloud account (in case you have not created one yet, please follow the guidance provided in Lab 1);\nYou will receive a link from your tutor to join your lab group in a shared space. Copy and paste it in your web browser (log in if necessary). If you already joined your lab group in RStudio Cloud, simply access the ‘Lab 2’ project and omit steps 3 to 5. Otherwise, continue with steps 3, 4 and 5.\nIf you did not join your lab group yet, you should see the following window:\n\n\n\n\n\nJoin Space\n\n\n\n\nClick on the ‘Join space’ button shown above.\nOpen the shared space form the left-hand side pane called ‘Quants Lab Group..’ and start the Lab 2 project by clicking on the ‘Start’ button as shown below:\n\n\n\n\n\nStart Lab 2.\n\n\n\nPart 2. Working on your script\n\nOnce you have accessed the ‘Lab 2’ project, write or copy the following line in your script (pane 1) and run it:\n\n\ninstall.packages(\"tidyverse\")\n\n\nWait until you get the message ‘The downloaded source packages are in…’. The installing process can take up to a couple of minutes to finish.\nOnce the package is installed, you need to load it using the library() function. Please, copy and paste the following line, and run it:\n\n\nlibrary(tidyverse)\n\nAnd that’s it, tidyverse is ready to be used in your current session!\nThere are couple of things you should know. First, the packages need to be installed only per project in RStudio Cloud (and only once if you are working in RStudio Desktop version). However, packages must be loaded using the library() function every time you restart an R session.\nAnother thing to notice is that when you install a package you need to use quotation marks, whereas in library() you only need to write the plain package name within brackets. Usually, you will load the packages at the beginning of your script.\n\nR can handle many classes of data. It is crucial that you can distinguish the main ones. Broadly speaking there are two types of variables,\n\n\ncategorical and;\n\nnumeric (formally know as interval or ratio).\n\nCategorical variables are distinctive because they are limited in the number of categories it can take, e.g., country, name, political party, or gender. Ordinal data is a sub-type of the categorical, and it is used when the categories can be ranked and their order is meaningful, e.g., education level or level of satisfaction. Numeric values can be continuous (these are usually measured and can take infinite values, e.g. speed or time).2\nIn R, the basic types of data are known as ‘atomic vectors’ and there are 6 of them (logical, integer, double, character, complex and raw). In the social sciences, we often use the following: numeric, factor and character. Numeric vectors are used to represent continuous numerical data.3 On the other hand, factor vectors are used to represent categorical and ordinal data.\nIn R, there are couple of functions that will help us to identify the type of data. First, we have glimpse(). This prints some of the main characteristics of a data set, namely its overall dimension, name of each variable (column), the first values for each variable, and the type of the variable. Second we have the function class(), that will help us to determine the overall class(type) of on R object.\n\nWe are now going to use some datasets that are pre-loaded in the R session by default. Please go to your ‘Lab_2’ project in RStudio Cloud and do the following:\n\nWe will start with a classic dataset example in R called iris. This contains measurements of various flowers species (for more info type ?iris in your console). Please go to your console and type the line below.\n\n\nglimpse(iris)\n\n\nWhat do you observe from the output?… First, it tells you the number of rows and the columns on the top. Later, it lists the name of each variable. Additionally, it tells you the type of the variable between these symbols &lt; &gt;. The first five variables in this dataset are of type &lt;dbl&gt; which is a type of numeric variable. The last, Species, is a factor &lt;fct&gt;. In sum, there is information of the species and four types of continuous measures associated to each flower in this dataset.\nNow you know that each flower belongs to a species, but what are the specific categories in this data set? To find out, type the following in your console.\n\n\nlevels(iris$Species)\n\n\nAs you can see, there are three categories, which are three types of flower species. In R the categories in factor vectors are called levels.\n\nNote the syntax above. Inside the function, we used the name of the data set followed by the dollar sign ($), which is needed to access the specific column/variable Species.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s get serious and explore Star Wars. Yes, the famous film series!\nThe starwars data set from the dplyr package contains information about the characters, including height, hair colour, and sex (to get more information type ?starwars in your console). At this time we will use a reduced version of the full data set. Please complete the following activities from your R script (pane 1).\n\nFirst, we will run the next couple of lines to reduce the data set, and then we will glimpse the Star Wars characters:\n\n\nstarwars2 &lt;- starwars[, 1:11]\nglimpse(starwars2)\n\n\nWhat do you observe this time? … It seems that the data type is not consistent with their content. For example, the variables species, gender, and hair_color are of type &lt;chr&gt; (that is character), when according to what we just learnt they should be a factor. To transform them, we will use the function ´factor()´. This process is known as coercing a variable, that is when you change from one type to another.\n\n\nLet’s coerce the species variable from character to factor and assign the result to the same column in the dataset.\n\n\nstarwars2$species &lt;- factor(starwars2$species)\n\n\nLet’s check if the type of variable really changed by glimpsing the data and checking the levels of species.\n\n\nglimpse(starwars2)\nlevels(starwars2$species)\n\nThe glimpse result now is telling us that species is a &lt;fct&gt;, as expected. Furthermore, the levels() function reveals that there are 37 types of species, including Human, Ewok, Droid, and more.\nHopefully, these examples will help you to identify the main vector types and, more importantly, to coerce them into an appropriate type. Be aware that many data sets represent categories with numeric values, for example, using ‘0’ for males and ‘1’ for females. Usually, large data sets are accompanied by extra information in a code book or documentation file, which specifies the values of the numeric code and their respective meaning. It’s important to read the code book/documentation of every data set as the conventions and meanings can vary.\n\n\n\nA useful operator is the pipe %&gt;%. This is part of the tidyverse package. So, it is ready for you to use. This operator passes the result of one operation to the next. Check the results of the following operations in your console:\n\n1 %&gt;% +1\n1 %&gt;% +1 %&gt;% +5\n\nObserve what happened…The result from the first line was 2. This is because this line can be read as: ‘take 1, THEN sum 1’. Therefore, the result is 2.\nSimilarly, the second line follows this process: ‘take 1, THEN sum 1, take the result of this (which is ’2’) and THEN sum 5’. Therefore, the result is 7. This can sound a bit abstract at this point, but we will practice with some data in the next section.\n\nIn this section we will work with data originally collected by The Guardian in 2015, for more information click here. The data set we will use today is an extended version which was openly shared in GitHub by the American news website FiveThirtyEight. This data set contains information about the people that were killed by police or other law enforcement bodies in the US, such as age, gender, race/ethnicity, etc. Additionally, it includes information about the city or region where the event happened. For more information click here.\n\nFor the following excercices, please make sure that your are working in your R script.\nFirst, we will create a new folder in our project directory to store the data. To do it from R, run this line in your script (Don’t worry if you get a warning. This appears because you already have a folder with this name):\n\ndir.create(\"data\")\n\nNote that in the ‘Files’ tab of Pane 4, there is a new folder called data.\nNow, download the data from the GitHub repository using the function download.file(). This function takes two arguments separated by a comma: (1) the URL and (2) the destination (including the directory, file name, and file extension), as shown below. Also, since the file we downloaded is wrapped in a .zip file, we will need to unzip it using unzip(). Copy, paste in your script, the following lines:\n\ndownload.file(\n  \"https://projects.fivethirtyeight.com/data-webpage-data/datasets/police-killings.zip\",\n  \"data/police-killings.zip\"\n)\nunzip(\"data/police-killings.zip\", exdir = \"data\")\n\nAfter following the previous steps, we are ready to read the data. As you can see in the ‘File’ tab, the data comes as a .csv file. Thus, we can use the read_csv() function included in the tidyverse package (make sure you the package is loaded in your session as explained in a previous section). We will assign the data in an object called police.\n\npolice &lt;- read_csv(\"data/police-killings/police_killings.csv\")\n\n\nIf you look at your ‘Environment’ tab in pane 2, you will see there is a new object called police, which has 467 observations and 34 variables (or columns). To start exploring the contents, we will glimpse the police data as following:\n\nglimpse(police)\n\nAs you can see, there are several variables included in the dataset, such as age, gender, law enforcement agency (lawenforcementagency), or whether the victim was armed (armed). You will see some of these variables are not in the appropriate type. For instance, some are categorical and should be type &lt;fct&gt; instead of &lt;chr&gt;.\n\nBefore coercing these variables, we will create a smaller subset selecting only the variables that we are interested in. To do so, we can use the select() function. The select function takes the name of the data first and then the name of the variables we want to keep (no quotation marks needed). We will select a few variables and assign the result to a new object called police_2.\n\npolice_2 &lt;- select(police, age, gender, raceethnicity, lawenforcementagency, armed)\n\nIf you look again to the ‘Environment’ tab, there is a second data set with the same number of observations but only 5 variables. You can glimpse this object to have a better idea of its contents.\n\nglimpse(police_2)\n\nHaving a closer look at the reduced version, we can see that in fact all the variables are of type &lt;chr&gt;, including age.\nLet’s coerce the variables in to their correct type. We will start with age, from character to numeric:\n\npolice_2 &lt;- police_2 %&gt;% mutate(age = as.numeric(age))\n\nAge is not known for some cases. Thus, it is recorded as ‘Unknown’ in the dataset. Since this is not recognized as a numeric value in the coercion process, R automatically sets it as a missing value, NA. This is why it will give you a warning message.\nWe can continue coercing raceethnicity and gender from character to a factor:\n\npolice_2 &lt;- police_2 %&gt;% mutate(raceethnicity = factor(raceethnicity))\npolice_2 &lt;- police_2 %&gt;% mutate(gender = factor(gender))\n\nLet’s run a summary of your data. This shows the number of observations in each category or a summary of a numeric variable:\n\nsummary(police_2)\n\nThere are some interesting figures coming out from the summary. For instance, in age you can see that the youngest is… 16 years old(?!), and the oldest 87 years old. Also, the vast majority are male individuals (445 vs 22). In relation to race/ethnicity, roughly half of them is ‘White’, whereas ‘Black’ individuals represent an important share. One may question about the proportion of people killed in terms of race/ethnicity compared to the composition of the total population (considering Black is a minority group in the US).\nLet’s suppose that we only want observations in which race/ethnicity is not unknown. To ‘remove’ undesired observation we can use the filter() function. We will assign the result of filter in a variable called police_2.\n\npolice_2 &lt;- police_2 %&gt;% filter(raceethnicity != \"Unknown\")\n\nSo, what just happened in the code above? First, the pipe operator, %&gt;%: What we are doing verbally is take the object police_2, THEN filter raceethnicity based on a condition. Later, what is happening inside filter? Lets have a look at what R does in the background for us (Artwork by @alison_horst):\n\n\n\n\nFilter. Source: Artwork by Horst (n.d.).\n\n\n\nIn the example above, we are keeping the observations in raceethnicity that are NOT EQUAL to ‘Unknown’. Finally, when we assigned the result to an object named as the same as our previous object, we replaced the old dataset with the filtered version.\n\n\n\n\n\n\n\n\n\nDiscuss the following questions with your neighbour or tutor:\n\nWhat is the main purpose of the functions select() and filter?\nWhat does coerce mean in the context of R? and Why do we need to coerce some variables?\nWhat is the mutate() function useful for?\n\nUsing the police_2 dataset:\n\nFilter how many observations are ‘White’ in raceethnicity? How may rows/observations are left?\nHow many ‘Latino/Hispanic’ are there in the dataset?\nUsing the example of Figure 2.3, could you filter how many were killed that were (a) ‘Black’ and (b) killed by firearm (‘firearm’)?\nWhat about ‘White’ and ‘firearm’?\n\nExtra activities: 1. Why did you have to use quotes in the following: filter(police, raceethnicity==”White” & raceethnicity==”firearm”)? 2. What do you have to repeat the variable raceethnicity twice?\nThis is the end of Lab 2. Again, the changes in your script should be saved automatically in RStudio Cloud. However, make sure this is the case as you were taught in Lab 1. After this, you can close the tab in your web browser. Hope you had fun!",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#welcome-back",
    "href": "02-Lab2.html#welcome-back",
    "title": "Data in R",
    "section": "",
    "text": "In our previous lab, we set up an RStudio Cloud session and we got familiar with the RStudio environment and some of the purpose and contents of its panes. In this Lab we will learn about R packages, how to install them and load them. Also, we will use different types of data. You will have the chance to practice with additional R operators. Lastly, we will load a real-world data set and put in practice your new skills.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#r-packages",
    "href": "02-Lab2.html#r-packages",
    "title": "Data in R",
    "section": "",
    "text": "As mentioned in our last lab, R (R Core Team 2021) is a collaborative project. This means that R users are developing, maintaining, and extending the functionalities constantly. When you set up R and RStudio for the first time, as we did it last week, it comes only with the ‘basic’ functionalities by default. However, there are literally thousands of extensions that are developed by other users. In R, these non-default extensions are called packages.\nMost of the times, we use packages because they simplify our work in R or they allow us to extend the capabilites of base R.\n\nLet’s put hands-on to install and load some useful packages. We will start with tidyverse (Wickham 2021).1\n\nPart 1. Access your lab group in RStudio Cloud\n\nMake sure you have a free, institutional-subscription RStudio Cloud account (in case you have not created one yet, please follow the guidance provided in Lab 1);\nYou will receive a link from your tutor to join your lab group in a shared space. Copy and paste it in your web browser (log in if necessary). If you already joined your lab group in RStudio Cloud, simply access the ‘Lab 2’ project and omit steps 3 to 5. Otherwise, continue with steps 3, 4 and 5.\nIf you did not join your lab group yet, you should see the following window:\n\n\n\n\n\nJoin Space\n\n\n\n\nClick on the ‘Join space’ button shown above.\nOpen the shared space form the left-hand side pane called ‘Quants Lab Group..’ and start the Lab 2 project by clicking on the ‘Start’ button as shown below:\n\n\n\n\n\nStart Lab 2.\n\n\n\nPart 2. Working on your script\n\nOnce you have accessed the ‘Lab 2’ project, write or copy the following line in your script (pane 1) and run it:\n\n\ninstall.packages(\"tidyverse\")\n\n\nWait until you get the message ‘The downloaded source packages are in…’. The installing process can take up to a couple of minutes to finish.\nOnce the package is installed, you need to load it using the library() function. Please, copy and paste the following line, and run it:\n\n\nlibrary(tidyverse)\n\nAnd that’s it, tidyverse is ready to be used in your current session!\nThere are couple of things you should know. First, the packages need to be installed only per project in RStudio Cloud (and only once if you are working in RStudio Desktop version). However, packages must be loaded using the library() function every time you restart an R session.\nAnother thing to notice is that when you install a package you need to use quotation marks, whereas in library() you only need to write the plain package name within brackets. Usually, you will load the packages at the beginning of your script.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#types-of-variables",
    "href": "02-Lab2.html#types-of-variables",
    "title": "Data in R",
    "section": "",
    "text": "R can handle many classes of data. It is crucial that you can distinguish the main ones. Broadly speaking there are two types of variables,\n\n\ncategorical and;\n\nnumeric (formally know as interval or ratio).\n\nCategorical variables are distinctive because they are limited in the number of categories it can take, e.g., country, name, political party, or gender. Ordinal data is a sub-type of the categorical, and it is used when the categories can be ranked and their order is meaningful, e.g., education level or level of satisfaction. Numeric values can be continuous (these are usually measured and can take infinite values, e.g. speed or time).2\nIn R, the basic types of data are known as ‘atomic vectors’ and there are 6 of them (logical, integer, double, character, complex and raw). In the social sciences, we often use the following: numeric, factor and character. Numeric vectors are used to represent continuous numerical data.3 On the other hand, factor vectors are used to represent categorical and ordinal data.\nIn R, there are couple of functions that will help us to identify the type of data. First, we have glimpse(). This prints some of the main characteristics of a data set, namely its overall dimension, name of each variable (column), the first values for each variable, and the type of the variable. Second we have the function class(), that will help us to determine the overall class(type) of on R object.\n\nWe are now going to use some datasets that are pre-loaded in the R session by default. Please go to your ‘Lab_2’ project in RStudio Cloud and do the following:\n\nWe will start with a classic dataset example in R called iris. This contains measurements of various flowers species (for more info type ?iris in your console). Please go to your console and type the line below.\n\n\nglimpse(iris)\n\n\nWhat do you observe from the output?… First, it tells you the number of rows and the columns on the top. Later, it lists the name of each variable. Additionally, it tells you the type of the variable between these symbols &lt; &gt;. The first five variables in this dataset are of type &lt;dbl&gt; which is a type of numeric variable. The last, Species, is a factor &lt;fct&gt;. In sum, there is information of the species and four types of continuous measures associated to each flower in this dataset.\nNow you know that each flower belongs to a species, but what are the specific categories in this data set? To find out, type the following in your console.\n\n\nlevels(iris$Species)\n\n\nAs you can see, there are three categories, which are three types of flower species. In R the categories in factor vectors are called levels.\n\nNote the syntax above. Inside the function, we used the name of the data set followed by the dollar sign ($), which is needed to access the specific column/variable Species.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s get serious and explore Star Wars. Yes, the famous film series!\nThe starwars data set from the dplyr package contains information about the characters, including height, hair colour, and sex (to get more information type ?starwars in your console). At this time we will use a reduced version of the full data set. Please complete the following activities from your R script (pane 1).\n\nFirst, we will run the next couple of lines to reduce the data set, and then we will glimpse the Star Wars characters:\n\n\nstarwars2 &lt;- starwars[, 1:11]\nglimpse(starwars2)\n\n\nWhat do you observe this time? … It seems that the data type is not consistent with their content. For example, the variables species, gender, and hair_color are of type &lt;chr&gt; (that is character), when according to what we just learnt they should be a factor. To transform them, we will use the function ´factor()´. This process is known as coercing a variable, that is when you change from one type to another.\n\n\nLet’s coerce the species variable from character to factor and assign the result to the same column in the dataset.\n\n\nstarwars2$species &lt;- factor(starwars2$species)\n\n\nLet’s check if the type of variable really changed by glimpsing the data and checking the levels of species.\n\n\nglimpse(starwars2)\nlevels(starwars2$species)\n\nThe glimpse result now is telling us that species is a &lt;fct&gt;, as expected. Furthermore, the levels() function reveals that there are 37 types of species, including Human, Ewok, Droid, and more.\nHopefully, these examples will help you to identify the main vector types and, more importantly, to coerce them into an appropriate type. Be aware that many data sets represent categories with numeric values, for example, using ‘0’ for males and ‘1’ for females. Usually, large data sets are accompanied by extra information in a code book or documentation file, which specifies the values of the numeric code and their respective meaning. It’s important to read the code book/documentation of every data set as the conventions and meanings can vary.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#more-operators-and-some-essential-symbols",
    "href": "02-Lab2.html#more-operators-and-some-essential-symbols",
    "title": "Data in R",
    "section": "",
    "text": "A useful operator is the pipe %&gt;%. This is part of the tidyverse package. So, it is ready for you to use. This operator passes the result of one operation to the next. Check the results of the following operations in your console:\n\n1 %&gt;% +1\n1 %&gt;% +1 %&gt;% +5\n\nObserve what happened…The result from the first line was 2. This is because this line can be read as: ‘take 1, THEN sum 1’. Therefore, the result is 2.\nSimilarly, the second line follows this process: ‘take 1, THEN sum 1, take the result of this (which is ’2’) and THEN sum 5’. Therefore, the result is 7. This can sound a bit abstract at this point, but we will practice with some data in the next section.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#black-lives-matter",
    "href": "02-Lab2.html#black-lives-matter",
    "title": "Data in R",
    "section": "",
    "text": "In this section we will work with data originally collected by The Guardian in 2015, for more information click here. The data set we will use today is an extended version which was openly shared in GitHub by the American news website FiveThirtyEight. This data set contains information about the people that were killed by police or other law enforcement bodies in the US, such as age, gender, race/ethnicity, etc. Additionally, it includes information about the city or region where the event happened. For more information click here.\n\nFor the following excercices, please make sure that your are working in your R script.\nFirst, we will create a new folder in our project directory to store the data. To do it from R, run this line in your script (Don’t worry if you get a warning. This appears because you already have a folder with this name):\n\ndir.create(\"data\")\n\nNote that in the ‘Files’ tab of Pane 4, there is a new folder called data.\nNow, download the data from the GitHub repository using the function download.file(). This function takes two arguments separated by a comma: (1) the URL and (2) the destination (including the directory, file name, and file extension), as shown below. Also, since the file we downloaded is wrapped in a .zip file, we will need to unzip it using unzip(). Copy, paste in your script, the following lines:\n\ndownload.file(\n  \"https://projects.fivethirtyeight.com/data-webpage-data/datasets/police-killings.zip\",\n  \"data/police-killings.zip\"\n)\nunzip(\"data/police-killings.zip\", exdir = \"data\")\n\nAfter following the previous steps, we are ready to read the data. As you can see in the ‘File’ tab, the data comes as a .csv file. Thus, we can use the read_csv() function included in the tidyverse package (make sure you the package is loaded in your session as explained in a previous section). We will assign the data in an object called police.\n\npolice &lt;- read_csv(\"data/police-killings/police_killings.csv\")\n\n\nIf you look at your ‘Environment’ tab in pane 2, you will see there is a new object called police, which has 467 observations and 34 variables (or columns). To start exploring the contents, we will glimpse the police data as following:\n\nglimpse(police)\n\nAs you can see, there are several variables included in the dataset, such as age, gender, law enforcement agency (lawenforcementagency), or whether the victim was armed (armed). You will see some of these variables are not in the appropriate type. For instance, some are categorical and should be type &lt;fct&gt; instead of &lt;chr&gt;.\n\nBefore coercing these variables, we will create a smaller subset selecting only the variables that we are interested in. To do so, we can use the select() function. The select function takes the name of the data first and then the name of the variables we want to keep (no quotation marks needed). We will select a few variables and assign the result to a new object called police_2.\n\npolice_2 &lt;- select(police, age, gender, raceethnicity, lawenforcementagency, armed)\n\nIf you look again to the ‘Environment’ tab, there is a second data set with the same number of observations but only 5 variables. You can glimpse this object to have a better idea of its contents.\n\nglimpse(police_2)\n\nHaving a closer look at the reduced version, we can see that in fact all the variables are of type &lt;chr&gt;, including age.\nLet’s coerce the variables in to their correct type. We will start with age, from character to numeric:\n\npolice_2 &lt;- police_2 %&gt;% mutate(age = as.numeric(age))\n\nAge is not known for some cases. Thus, it is recorded as ‘Unknown’ in the dataset. Since this is not recognized as a numeric value in the coercion process, R automatically sets it as a missing value, NA. This is why it will give you a warning message.\nWe can continue coercing raceethnicity and gender from character to a factor:\n\npolice_2 &lt;- police_2 %&gt;% mutate(raceethnicity = factor(raceethnicity))\npolice_2 &lt;- police_2 %&gt;% mutate(gender = factor(gender))\n\nLet’s run a summary of your data. This shows the number of observations in each category or a summary of a numeric variable:\n\nsummary(police_2)\n\nThere are some interesting figures coming out from the summary. For instance, in age you can see that the youngest is… 16 years old(?!), and the oldest 87 years old. Also, the vast majority are male individuals (445 vs 22). In relation to race/ethnicity, roughly half of them is ‘White’, whereas ‘Black’ individuals represent an important share. One may question about the proportion of people killed in terms of race/ethnicity compared to the composition of the total population (considering Black is a minority group in the US).\nLet’s suppose that we only want observations in which race/ethnicity is not unknown. To ‘remove’ undesired observation we can use the filter() function. We will assign the result of filter in a variable called police_2.\n\npolice_2 &lt;- police_2 %&gt;% filter(raceethnicity != \"Unknown\")\n\nSo, what just happened in the code above? First, the pipe operator, %&gt;%: What we are doing verbally is take the object police_2, THEN filter raceethnicity based on a condition. Later, what is happening inside filter? Lets have a look at what R does in the background for us (Artwork by @alison_horst):\n\n\n\n\nFilter. Source: Artwork by Horst (n.d.).\n\n\n\nIn the example above, we are keeping the observations in raceethnicity that are NOT EQUAL to ‘Unknown’. Finally, when we assigned the result to an object named as the same as our previous object, we replaced the old dataset with the filtered version.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#activities",
    "href": "02-Lab2.html#activities",
    "title": "Data in R",
    "section": "",
    "text": "Discuss the following questions with your neighbour or tutor:\n\nWhat is the main purpose of the functions select() and filter?\nWhat does coerce mean in the context of R? and Why do we need to coerce some variables?\nWhat is the mutate() function useful for?\n\nUsing the police_2 dataset:\n\nFilter how many observations are ‘White’ in raceethnicity? How may rows/observations are left?\nHow many ‘Latino/Hispanic’ are there in the dataset?\nUsing the example of Figure 2.3, could you filter how many were killed that were (a) ‘Black’ and (b) killed by firearm (‘firearm’)?\nWhat about ‘White’ and ‘firearm’?\n\nExtra activities: 1. Why did you have to use quotes in the following: filter(police, raceethnicity==”White” & raceethnicity==”firearm”)? 2. What do you have to repeat the variable raceethnicity twice?\nThis is the end of Lab 2. Again, the changes in your script should be saved automatically in RStudio Cloud. However, make sure this is the case as you were taught in Lab 1. After this, you can close the tab in your web browser. Hope you had fun!",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#footnotes",
    "href": "02-Lab2.html#footnotes",
    "title": "Data in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.tidyverse.org/↩︎\nFor more details, please refer to the DataCamp module Introduction to Data in R.↩︎\nNotice that numeric vectors can be represented as integeror double in R, their difference is of little relevance for now.↩︎",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "05-Lab5.html",
    "href": "05-Lab5.html",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "In the previous labs you were exploring the 2012 Northern Ireland Life and Times Survey (NILT). You’ve learnt how to download, read and format the data. Also, you’ve learnt how to explore categorical and numeric data and a mix of them. In this lab, you will learn about how to efficiently report quantitative results directly from R, using R Markdown, which is used by many academics and professionals in a workplace setting to communicate quantitative findings to a wider audience. R Markdown is also what you will use to write your research report assignment for this course. So, Let’s dive in and learn more!\n\nR Markdown (Rmd) is a different type of file included in RStudio (and it is actually a different markup language). This allows you to generate reports in common file types, such as .html (the same one used for this lab workbook you’re reading right now), .pdf, or Word (.doc). The interesting thing is that the Rmd file allows you to integrate text, code, and plots directly into your report (so you do not have to copy and paste tables or graphs into a Word document, for example, which is often very messy and time-consuming). You have already seen how well this works in the lab workbooks so far, which are written entirely in R Markdown.\nThe basic components of an Rmd file are: the code, text and metadata. The code is integrated by blocks called ‘chunks’, and the metadata contains information to format the report. We believe the best way to learn is by doing it. So, let’s create your first Rmd document!\n\nWe will continue working in the same project called NILT in RStudio Cloud.\n\nPlease go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\nCreate a new Rmd file, this is similar as creating and R Script, from the ‘File’ tab on the top-left: File&gt;New File&gt;R Markdown... (Rstudio may ask to install some packages, click ‘Yes’);\nType ‘Test1’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML. Then, click ‘OK’.\nSave the Rmd file clicking on File&gt;Save as..., type Test1 in the ‘File name’ box, and click on the Save button.\n\nAfter completing the previous steps, your screen should look like this (You can minimize the console to expand pane 1):\n\n\n\n\nRmd file.\n\n\n\nNote that now you have two files open in Pane 1, one tab includes the R script that we created in the last lab (called Lab_3.R), and the other is the Rmd document that you just created.\nThe Rmd document Test1 contains an example by default. The first bit on the top enclosed by the dashes ---, contains the general metadata to format the output, as shown in the Figure @ref(fig:yaml-chunk). This bit is called YAML. In the default example, it contains the title, name of the author, data and the type of output (html). You can adjust this information directly by typing the relevant info (e.g. date or name).\n\n\n\n\nYAML and code chunk.\n\n\n\nBelow the YAML shown in Figure @ref(fig:yaml-chunk), there is another box. This is an R code ‘chunk’. To run a chunk of code individually (that is to visualize a partial result of an Rmd document), you can click on the green arrow pointed on the top-left of the first chunk.\nIn line 12, you have a second-level header, which contains the name of a section in the document. As you can see this is preceded by double hash tag ##. If you want a first-level header section, you would require only one hash tag like this #, and three for a third-level header. Finally the ‘Knit’ word is enclosed by double asterisk **. This is to format the characters enclosed in bold.\nIn line 26, you will see a chunk including a basic plot. Let’s check the results that this example in ‘Test1’ produce.\nTo render the document from Rmd to HTML, we need to Knit it by clicking on the icon shown below. Try it!\n\n\nKnit button\n\nRStudio may ask you if you want to update some packages, click ‘Yes’.\nAfter you knit the document, a window with the output will pop up automatically. As you can see, this document contains the main title, followed by your name and date, as specified in the YAML. Afterwards, there is a second-level header which includes the first section of this example document. Also, the word ‘Knit’ is shown in bold, as it was wrapped by double asterisk **.\nAn interesting thing is that we can integrate the result of our code in the output as we did with the second chunk (which starts in line 18). Here, we can use a summary of the data set cars (which is an in-built data set in R that contains only two variables).\n\nSimilarly, you can create a .pdf file. To do this, instead of clicking on the Knit icon directly, click on the black arrow next to it. Then click ‘Knit to PDF’. Try it and see the result.\nWhen you knitted the document, RStudio actually created a new .html or .pdf file with the same name as your Rmd document. You can confirm this in the ‘Files’ tab in Pane 4. You can download this file from the cloud to your local drive by clicking on the box of the output file. Then, click More &gt; Export... in the same pane (or click on the gear icon in Pane 4).\n\n\nIMPORTANT: Rmd files are different from simple R scripts. While everything you write in an R script is interpreted as code, only the bits within the code chunks will be interpreted as code in Rmd files. Everything else not within chunks in an Rmd file is interpreted as text.\n\nIn the Test1.Rmd file that you just created, do the following:\n\nChange the title of the document in the YAML to ‘My first R Markdown document’.\nIn the code chunk in line 21, replace the existing line (summary(car)) with the following: glimpse(iris).\nIn the code chunk called ‘pressure’, change echo=FALSE to echo=TRUE.\nAt the very bottom of the script, create a new paragraph and write one or two lines briefly describing how you think quantitative methods are improving your discipline (e.g. politics, sociology, social and public policy, or central and eastern European studies).\nKnit the document in html format.\nDownload the newly edited version of the Test1.html document to your machine.\nDiscuss how each of the edits suggested above modify the output with your neighbour or your tutor.\n\n\nMake sure you’ve got the basics of R Markdown, since this is the tool which you will use to write your final assignment (i.e. research report). If there is something not very clear, or you are curious about, feel free to ask your tutor. They will be happy to answer your questions.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#introduction",
    "href": "05-Lab5.html#introduction",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "In the previous labs you were exploring the 2012 Northern Ireland Life and Times Survey (NILT). You’ve learnt how to download, read and format the data. Also, you’ve learnt how to explore categorical and numeric data and a mix of them. In this lab, you will learn about how to efficiently report quantitative results directly from R, using R Markdown, which is used by many academics and professionals in a workplace setting to communicate quantitative findings to a wider audience. R Markdown is also what you will use to write your research report assignment for this course. So, Let’s dive in and learn more!",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#r-markdown",
    "href": "05-Lab5.html#r-markdown",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "R Markdown (Rmd) is a different type of file included in RStudio (and it is actually a different markup language). This allows you to generate reports in common file types, such as .html (the same one used for this lab workbook you’re reading right now), .pdf, or Word (.doc). The interesting thing is that the Rmd file allows you to integrate text, code, and plots directly into your report (so you do not have to copy and paste tables or graphs into a Word document, for example, which is often very messy and time-consuming). You have already seen how well this works in the lab workbooks so far, which are written entirely in R Markdown.\nThe basic components of an Rmd file are: the code, text and metadata. The code is integrated by blocks called ‘chunks’, and the metadata contains information to format the report. We believe the best way to learn is by doing it. So, let’s create your first Rmd document!\n\nWe will continue working in the same project called NILT in RStudio Cloud.\n\nPlease go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\nCreate a new Rmd file, this is similar as creating and R Script, from the ‘File’ tab on the top-left: File&gt;New File&gt;R Markdown... (Rstudio may ask to install some packages, click ‘Yes’);\nType ‘Test1’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML. Then, click ‘OK’.\nSave the Rmd file clicking on File&gt;Save as..., type Test1 in the ‘File name’ box, and click on the Save button.\n\nAfter completing the previous steps, your screen should look like this (You can minimize the console to expand pane 1):\n\n\n\n\nRmd file.\n\n\n\nNote that now you have two files open in Pane 1, one tab includes the R script that we created in the last lab (called Lab_3.R), and the other is the Rmd document that you just created.\nThe Rmd document Test1 contains an example by default. The first bit on the top enclosed by the dashes ---, contains the general metadata to format the output, as shown in the Figure @ref(fig:yaml-chunk). This bit is called YAML. In the default example, it contains the title, name of the author, data and the type of output (html). You can adjust this information directly by typing the relevant info (e.g. date or name).\n\n\n\n\nYAML and code chunk.\n\n\n\nBelow the YAML shown in Figure @ref(fig:yaml-chunk), there is another box. This is an R code ‘chunk’. To run a chunk of code individually (that is to visualize a partial result of an Rmd document), you can click on the green arrow pointed on the top-left of the first chunk.\nIn line 12, you have a second-level header, which contains the name of a section in the document. As you can see this is preceded by double hash tag ##. If you want a first-level header section, you would require only one hash tag like this #, and three for a third-level header. Finally the ‘Knit’ word is enclosed by double asterisk **. This is to format the characters enclosed in bold.\nIn line 26, you will see a chunk including a basic plot. Let’s check the results that this example in ‘Test1’ produce.\nTo render the document from Rmd to HTML, we need to Knit it by clicking on the icon shown below. Try it!\n\n\nKnit button\n\nRStudio may ask you if you want to update some packages, click ‘Yes’.\nAfter you knit the document, a window with the output will pop up automatically. As you can see, this document contains the main title, followed by your name and date, as specified in the YAML. Afterwards, there is a second-level header which includes the first section of this example document. Also, the word ‘Knit’ is shown in bold, as it was wrapped by double asterisk **.\nAn interesting thing is that we can integrate the result of our code in the output as we did with the second chunk (which starts in line 18). Here, we can use a summary of the data set cars (which is an in-built data set in R that contains only two variables).\n\nSimilarly, you can create a .pdf file. To do this, instead of clicking on the Knit icon directly, click on the black arrow next to it. Then click ‘Knit to PDF’. Try it and see the result.\nWhen you knitted the document, RStudio actually created a new .html or .pdf file with the same name as your Rmd document. You can confirm this in the ‘Files’ tab in Pane 4. You can download this file from the cloud to your local drive by clicking on the box of the output file. Then, click More &gt; Export... in the same pane (or click on the gear icon in Pane 4).\n\n\nIMPORTANT: Rmd files are different from simple R scripts. While everything you write in an R script is interpreted as code, only the bits within the code chunks will be interpreted as code in Rmd files. Everything else not within chunks in an Rmd file is interpreted as text.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#activity",
    "href": "05-Lab5.html#activity",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "In the Test1.Rmd file that you just created, do the following:\n\nChange the title of the document in the YAML to ‘My first R Markdown document’.\nIn the code chunk in line 21, replace the existing line (summary(car)) with the following: glimpse(iris).\nIn the code chunk called ‘pressure’, change echo=FALSE to echo=TRUE.\nAt the very bottom of the script, create a new paragraph and write one or two lines briefly describing how you think quantitative methods are improving your discipline (e.g. politics, sociology, social and public policy, or central and eastern European studies).\nKnit the document in html format.\nDownload the newly edited version of the Test1.html document to your machine.\nDiscuss how each of the edits suggested above modify the output with your neighbour or your tutor.\n\n\nMake sure you’ve got the basics of R Markdown, since this is the tool which you will use to write your final assignment (i.e. research report). If there is something not very clear, or you are curious about, feel free to ask your tutor. They will be happy to answer your questions.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "07-Lab7.html",
    "href": "07-Lab7.html",
    "title": "Correlation",
    "section": "",
    "text": "When conducting empirical research, we are often interested in associations between two variables, for example, personal income and attitudes towards migrants. In this lab we will focus on visualizing relationship between variables and how to measure it. In quantitative research, the main variable of interest in an analysis is called the dependent or response variable, and the second is known as the independent or explanatory. In the example above, we can think of personal income as the independent variable and attitudes as the dependent.\nThe relationship between variables can be positive, negative or non-existent. The figure below shows these type of relationships to different extents. The association is positive when one of the variables increases and the second variable tends to go in the same direction (that is increasing as well). The first plot on the left-hand side shows a strong positive relationship. As you can see, the points are closely clustered around the straight line. The next plot also shows a positive relationship. This time the relationship is moderate. Therefore, the points are more dispersed in relation to the line compared to the previous one.\n\n\n\n\n Types of correlation.\n\n\n\nThe plot in the middle, shows two variables that are not correlated. The location of the points is not following any pattern and the line is flat. By contrast, the last two plots on the right hand-side show a negative relationship. When the values on the X axis increase, the values on the Y axis tend to decrease.\n\nWe will continue working on the same RStudio Cloud project as in the previous session and using the 2012 Northern Ireland Life and Times Survey (NILT) data. To set the R environment please follow the next steps:\n\nPlease go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\nCreate a new Rmd file, type ‘Correlation analysis’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML.\nSave the Rmd document under the name ‘Lab7_correlation’.\nDelete all the contents in the Rmd default example with the exception of the first bit which contains the YAML and the first chunk, which contains the default chunk options (that is all from line 12 and on).\nIn the setup chunk, change echo from TRUE to FALSE in line 9 (this will hide the code for all chunks in your final document).\nWithin the first chunk, copy and paste the following code below line 9 knitr::opts_chunk$set(message = FALSE, warning = FALSE). This will hide the warnings and messages when you load the packages.\n\nIn the Rmd document insert a new a chunk, copy and paste the following code. Then, run the individual chunk by clicking on the green arrow on the top-right of the chunk.\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Load the data from the .rds file we created in lab 3\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nThis time we will use new variables from the survey. Therefore, we need to coerce them into their appropriate type first. Insert a second chunk, copy and paste the code below. Then, run the individual chunk.\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n# Migration\nnilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric)\n\nAlso, we will create a new variable called mig_per by summing the respondent’s opinion in relation to migration using the following variables: mil10yrs, miecono and micultur (see the documentation p. 14 here to know more about these variables). Again, insert a new chunk, copy and paste the code below, and run the individual chunk.\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) %&gt;%\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\n\nVisualizing two or more variables can help to uncover or understand the relationship between these variables. As briefly introduced in the previous session, different types of plots are appropriate for different types of variables. Therefore, we split the following sections according to the type of data to be analysed.\nYou do not need to run or reproduce the examples shown in the following sections in your R session with the exception of exercises that are under the activity headers.\n\nTo illustrate this type of correlation, let’s start with a relatively obvious but useful example. Suppose we are interested in how people choose their spouse or partner. The first characteristic that we might look at is age. We might suspect that there is a correlation between the nilt respondents’ own age and their partner’s age. Since both ages are numeric variables, a scatter plot is appropriate to visualise the correlation. To do this, let’s use the functions ggplot() and geom_point(). In aesthetics aes(), define the respondent’s age rage on the X axis and the respondent’s spouse/partner age spage on the Y axis. As a general convention in quantitative research, the response/dependent variable is visualised on the Y axis and the independent on the X axis (you do not need to copy and reproduce the example below).\n\nggplot(nilt, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nNote that in this plot the function geom_smooth() was used. This is to plot a straight line which describes the best all the points in the graph.\nFrom the plot above, we see that there is a strong positive correlation between the respondent’s age and their partner’s age. We see that for some individuals their partner’s age is older, whereas others is younger. Also, there are some dots that are far away from the straight line. For example, in one case the respondent is around 60 years old and the age of their partner is around 30 years old (can you find that dot on the plot?). These extreme values are known as outliers.\nWe may also suspect that the respondents’ sex is playing a role in this relationship. We can include this as a third variable in the plot by colouring the dots by the respondents’ sex. To do this, let’s specify the colour argument in aesthetics aes() with a categorical variable rsex.\n\nggplot(nilt, aes(x = rage, y = spage, colour = rsex)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"gray20\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nIn the previous plot, we included a line that describes what it would look like if the partner’s age were exactly the same as the respondent’s age. We observe a clear pattern in which female participants are on one side of the line and males on the other. As we can see, most female respondents tend to choose/have partners who are older, whereas males choose/have younger partners.\n\nIn the Lab7_correlation file, use the nilt data object to visualise the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text (outside the code chunks).\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function set the method argument to 'lm'.\nWhat type of relationship do you observe? Comment as text in the Rmd the overall result of the plot and whether this is in line with your previous expectation.\n\n\nAs briefly introduced in the last lab, correlations often occur between categorical and numeric data. A good way to observe the relationship between these type of variables is using a box plot. Which essentially shows the distribution of the numeric values by category/group.\nLet’s say we are interested in the relationship between education level and perception of migration. The variable highqual contains the respondent’s highest education qualification. Using ggplot(), we can situate mig_per on the X axis and highqual on the Y axis, and plot it with the geom_boxplot() function. Note that before passing the dataset to ggplot, we can filter out two categories of the variable highqual where education level is unknown (i.e. “Other, level unknown” or “Unclassified”).\n\nnilt %&gt;%\n  filter(highqual != \"Other, level unknown\" & highqual != \"Unclassified\") %&gt;%\n  ggplot(aes(x = mig_per, y = highqual)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFrom the plot above, we see that respondents with higher education level (on the bottom) appear to have more positive opinion on migration when compared to respondents with lower education level or no qualifications (on the top). Overall, the data shows a pattern that the lower one’s education level is, the worse their opinion towards migration is likely to be. Since education level is an ordinal variable, we can say this is a positive relationship.\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you can observe from the results as text in the Rmd file to introduce the plot.\n\nCreate a boxplot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the political party which the respondent identify with uninatid. Use ggplot() in combination with geom_boxplot(). Make sure to specify mig_per on the Y axis and uninatid on the X axis in aes().\nDo you think the opinion towards migration differs among the groups in the plot? Comment on the overall results in the Rmd document.\n\nSo far we have examined correlation by visualizing variables only. A useful practice in quantitative research is to actually measure the magnitude of the relationship between these variables. One common measure is the Pearson correlation coefficient. This measure results in a number that goes from -1 to 1. A coefficient below 0 implies a negative correlation whereas a coefficient over 0 a positive one. When the coefficient is close to positive one (1) or negative one (-1), it implies that the relationship is strong. By contrast, coefficients close to 0 indicate a weak relationship. This technique is appropriate to measure linear numeric relationships, which is when we have numeric variables with a normal distribution, e.g. age in our dataset.\nLet’s start measuring the relationship between the respondent’s age and their partner’s age. To do this in R, we should use the cor() function. In the R syntax, first we specify the variables separated by a comma. We need to be explicit by specifying the object name, the dollar sign, and the name of the variable, as shown below. Also, I set the use argument as 'pairwise.complete.obs'. This is because one or both of the variables contain more than one missing value. Therefore, we are telling R to use complete observations only.\n\ncor(nilt$rage, nilt$spage, use = \"pairwise.complete.obs\")\n\n[1] 0.9481297\n\n\nThe correlation coefficient between this variables is 0.95. This is close to positive 1. Therefore, it is a strong positive correlation. The result is completely in line with the plot above, since we saw how the dots were close to the straight line.\nWhat about the relationship between age and mig_per that you plotted earlier?\n\ncor(nilt$rage, nilt$mig_per, use = \"pairwise.complete.obs\")\n\n[1] -0.05680918\n\n\nThe coefficient is very close to 0, which means that the correlation is practically non-existent. The absence of correlation is also interesting in research. For instance, one might expect that younger people would be more open to migration. However, it seems that age does not play a role on people’s opinion about migration in NI according to this data.\nLet’s say that we are interested in the correlation between mig_per and all other numeric variables in the dataset. Instead of continuing computing the correlation one by one, we can run a correlation matrix. The code syntax can be read as follows: from the nilt data select these variables, then compute the correlation coefficient using complete cases, and then round the result to 3 decimals.\n\nnilt %&gt;%\n  select(mig_per, rage, spage, rhourswk, persinc2) %&gt;%\n  cor(use = \"pairwise.complete.obs\") %&gt;%\n  round(3)\n\n         mig_per   rage  spage rhourswk persinc2\nmig_per    1.000 -0.057 -0.132    0.082    0.228\nrage      -0.057  1.000  0.948   -0.013   -0.036\nspage     -0.132  0.948  1.000   -0.182   -0.090\nrhourswk   0.082 -0.013 -0.182    1.000    0.383\npersinc2   0.228 -0.036 -0.090    0.383    1.000\n\n\nFrom the result above, we have a correlation matrix that computes the Person correlation coefficient for the selected variables. In the first row we have migration perception. You will notice that the first value is 1.00, this is because it is measuring the correlation against the same variable (i.e. itself). The next value in the first row is age, which is nearly 0. The next variables also result in low coefficients, with the exception of the personal income, where we see a moderate/low positive correlation. This can be interpreted that respondents with high income are associated with more positive opinion towards migration compared to low-income respondents.\n\n\nInsert a new chunk in your Rmd file;\nUsing the nilt data object, compute a correlation matrix using the following variables: rage, persinc2, mil10yrs, miecono and micultur, setting the use argument to 'pairwise.complete.obs' and rounding the result to 3 decimals;\nRun the chunk individually and comment whether personal income or age is correlated with the perception of migrants in relation to the specific aspects asked in the variables measured (consult the documentation in p. 14 to get a description of these variables);\nKnit the Lab7_correlation Rmd document to .html or .pdf. The output document will automatically be saved in your project.\nDiscuss your previous results with your neighbour or tutor.",
    "crumbs": [
      "**Lab 7** Correlation"
    ]
  },
  {
    "objectID": "07-Lab7.html#what-is-correlation",
    "href": "07-Lab7.html#what-is-correlation",
    "title": "Correlation",
    "section": "",
    "text": "When conducting empirical research, we are often interested in associations between two variables, for example, personal income and attitudes towards migrants. In this lab we will focus on visualizing relationship between variables and how to measure it. In quantitative research, the main variable of interest in an analysis is called the dependent or response variable, and the second is known as the independent or explanatory. In the example above, we can think of personal income as the independent variable and attitudes as the dependent.\nThe relationship between variables can be positive, negative or non-existent. The figure below shows these type of relationships to different extents. The association is positive when one of the variables increases and the second variable tends to go in the same direction (that is increasing as well). The first plot on the left-hand side shows a strong positive relationship. As you can see, the points are closely clustered around the straight line. The next plot also shows a positive relationship. This time the relationship is moderate. Therefore, the points are more dispersed in relation to the line compared to the previous one.\n\n\n\n\n Types of correlation.\n\n\n\nThe plot in the middle, shows two variables that are not correlated. The location of the points is not following any pattern and the line is flat. By contrast, the last two plots on the right hand-side show a negative relationship. When the values on the X axis increase, the values on the Y axis tend to decrease.\n\nWe will continue working on the same RStudio Cloud project as in the previous session and using the 2012 Northern Ireland Life and Times Survey (NILT) data. To set the R environment please follow the next steps:\n\nPlease go to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\nCreate a new Rmd file, type ‘Correlation analysis’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML.\nSave the Rmd document under the name ‘Lab7_correlation’.\nDelete all the contents in the Rmd default example with the exception of the first bit which contains the YAML and the first chunk, which contains the default chunk options (that is all from line 12 and on).\nIn the setup chunk, change echo from TRUE to FALSE in line 9 (this will hide the code for all chunks in your final document).\nWithin the first chunk, copy and paste the following code below line 9 knitr::opts_chunk$set(message = FALSE, warning = FALSE). This will hide the warnings and messages when you load the packages.\n\nIn the Rmd document insert a new a chunk, copy and paste the following code. Then, run the individual chunk by clicking on the green arrow on the top-right of the chunk.\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Load the data from the .rds file we created in lab 3\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nThis time we will use new variables from the survey. Therefore, we need to coerce them into their appropriate type first. Insert a second chunk, copy and paste the code below. Then, run the individual chunk.\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n# Migration\nnilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric)\n\nAlso, we will create a new variable called mig_per by summing the respondent’s opinion in relation to migration using the following variables: mil10yrs, miecono and micultur (see the documentation p. 14 here to know more about these variables). Again, insert a new chunk, copy and paste the code below, and run the individual chunk.\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) %&gt;%\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\n\nVisualizing two or more variables can help to uncover or understand the relationship between these variables. As briefly introduced in the previous session, different types of plots are appropriate for different types of variables. Therefore, we split the following sections according to the type of data to be analysed.\nYou do not need to run or reproduce the examples shown in the following sections in your R session with the exception of exercises that are under the activity headers.\n\nTo illustrate this type of correlation, let’s start with a relatively obvious but useful example. Suppose we are interested in how people choose their spouse or partner. The first characteristic that we might look at is age. We might suspect that there is a correlation between the nilt respondents’ own age and their partner’s age. Since both ages are numeric variables, a scatter plot is appropriate to visualise the correlation. To do this, let’s use the functions ggplot() and geom_point(). In aesthetics aes(), define the respondent’s age rage on the X axis and the respondent’s spouse/partner age spage on the Y axis. As a general convention in quantitative research, the response/dependent variable is visualised on the Y axis and the independent on the X axis (you do not need to copy and reproduce the example below).\n\nggplot(nilt, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nNote that in this plot the function geom_smooth() was used. This is to plot a straight line which describes the best all the points in the graph.\nFrom the plot above, we see that there is a strong positive correlation between the respondent’s age and their partner’s age. We see that for some individuals their partner’s age is older, whereas others is younger. Also, there are some dots that are far away from the straight line. For example, in one case the respondent is around 60 years old and the age of their partner is around 30 years old (can you find that dot on the plot?). These extreme values are known as outliers.\nWe may also suspect that the respondents’ sex is playing a role in this relationship. We can include this as a third variable in the plot by colouring the dots by the respondents’ sex. To do this, let’s specify the colour argument in aesthetics aes() with a categorical variable rsex.\n\nggplot(nilt, aes(x = rage, y = spage, colour = rsex)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"gray20\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nIn the previous plot, we included a line that describes what it would look like if the partner’s age were exactly the same as the respondent’s age. We observe a clear pattern in which female participants are on one side of the line and males on the other. As we can see, most female respondents tend to choose/have partners who are older, whereas males choose/have younger partners.\n\nIn the Lab7_correlation file, use the nilt data object to visualise the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text (outside the code chunks).\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function set the method argument to 'lm'.\nWhat type of relationship do you observe? Comment as text in the Rmd the overall result of the plot and whether this is in line with your previous expectation.\n\n\nAs briefly introduced in the last lab, correlations often occur between categorical and numeric data. A good way to observe the relationship between these type of variables is using a box plot. Which essentially shows the distribution of the numeric values by category/group.\nLet’s say we are interested in the relationship between education level and perception of migration. The variable highqual contains the respondent’s highest education qualification. Using ggplot(), we can situate mig_per on the X axis and highqual on the Y axis, and plot it with the geom_boxplot() function. Note that before passing the dataset to ggplot, we can filter out two categories of the variable highqual where education level is unknown (i.e. “Other, level unknown” or “Unclassified”).\n\nnilt %&gt;%\n  filter(highqual != \"Other, level unknown\" & highqual != \"Unclassified\") %&gt;%\n  ggplot(aes(x = mig_per, y = highqual)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFrom the plot above, we see that respondents with higher education level (on the bottom) appear to have more positive opinion on migration when compared to respondents with lower education level or no qualifications (on the top). Overall, the data shows a pattern that the lower one’s education level is, the worse their opinion towards migration is likely to be. Since education level is an ordinal variable, we can say this is a positive relationship.\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you can observe from the results as text in the Rmd file to introduce the plot.\n\nCreate a boxplot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the political party which the respondent identify with uninatid. Use ggplot() in combination with geom_boxplot(). Make sure to specify mig_per on the Y axis and uninatid on the X axis in aes().\nDo you think the opinion towards migration differs among the groups in the plot? Comment on the overall results in the Rmd document.",
    "crumbs": [
      "**Lab 7** Correlation"
    ]
  },
  {
    "objectID": "07-Lab7.html#measuring-correlation",
    "href": "07-Lab7.html#measuring-correlation",
    "title": "Correlation",
    "section": "",
    "text": "So far we have examined correlation by visualizing variables only. A useful practice in quantitative research is to actually measure the magnitude of the relationship between these variables. One common measure is the Pearson correlation coefficient. This measure results in a number that goes from -1 to 1. A coefficient below 0 implies a negative correlation whereas a coefficient over 0 a positive one. When the coefficient is close to positive one (1) or negative one (-1), it implies that the relationship is strong. By contrast, coefficients close to 0 indicate a weak relationship. This technique is appropriate to measure linear numeric relationships, which is when we have numeric variables with a normal distribution, e.g. age in our dataset.\nLet’s start measuring the relationship between the respondent’s age and their partner’s age. To do this in R, we should use the cor() function. In the R syntax, first we specify the variables separated by a comma. We need to be explicit by specifying the object name, the dollar sign, and the name of the variable, as shown below. Also, I set the use argument as 'pairwise.complete.obs'. This is because one or both of the variables contain more than one missing value. Therefore, we are telling R to use complete observations only.\n\ncor(nilt$rage, nilt$spage, use = \"pairwise.complete.obs\")\n\n[1] 0.9481297\n\n\nThe correlation coefficient between this variables is 0.95. This is close to positive 1. Therefore, it is a strong positive correlation. The result is completely in line with the plot above, since we saw how the dots were close to the straight line.\nWhat about the relationship between age and mig_per that you plotted earlier?\n\ncor(nilt$rage, nilt$mig_per, use = \"pairwise.complete.obs\")\n\n[1] -0.05680918\n\n\nThe coefficient is very close to 0, which means that the correlation is practically non-existent. The absence of correlation is also interesting in research. For instance, one might expect that younger people would be more open to migration. However, it seems that age does not play a role on people’s opinion about migration in NI according to this data.\nLet’s say that we are interested in the correlation between mig_per and all other numeric variables in the dataset. Instead of continuing computing the correlation one by one, we can run a correlation matrix. The code syntax can be read as follows: from the nilt data select these variables, then compute the correlation coefficient using complete cases, and then round the result to 3 decimals.\n\nnilt %&gt;%\n  select(mig_per, rage, spage, rhourswk, persinc2) %&gt;%\n  cor(use = \"pairwise.complete.obs\") %&gt;%\n  round(3)\n\n         mig_per   rage  spage rhourswk persinc2\nmig_per    1.000 -0.057 -0.132    0.082    0.228\nrage      -0.057  1.000  0.948   -0.013   -0.036\nspage     -0.132  0.948  1.000   -0.182   -0.090\nrhourswk   0.082 -0.013 -0.182    1.000    0.383\npersinc2   0.228 -0.036 -0.090    0.383    1.000\n\n\nFrom the result above, we have a correlation matrix that computes the Person correlation coefficient for the selected variables. In the first row we have migration perception. You will notice that the first value is 1.00, this is because it is measuring the correlation against the same variable (i.e. itself). The next value in the first row is age, which is nearly 0. The next variables also result in low coefficients, with the exception of the personal income, where we see a moderate/low positive correlation. This can be interpreted that respondents with high income are associated with more positive opinion towards migration compared to low-income respondents.\n\n\nInsert a new chunk in your Rmd file;\nUsing the nilt data object, compute a correlation matrix using the following variables: rage, persinc2, mil10yrs, miecono and micultur, setting the use argument to 'pairwise.complete.obs' and rounding the result to 3 decimals;\nRun the chunk individually and comment whether personal income or age is correlated with the perception of migrants in relation to the specific aspects asked in the variables measured (consult the documentation in p. 14 to get a description of these variables);\nKnit the Lab7_correlation Rmd document to .html or .pdf. The output document will automatically be saved in your project.\nDiscuss your previous results with your neighbour or tutor.",
    "crumbs": [
      "**Lab 7** Correlation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods in the Social Sciences",
    "section": "",
    "text": "Welcome\n\nWelcome to the Quantitative Methods in the Social Sciences lab workbook!\nThis workbook is for University of Glasgow students enrolled in the undergraduate Quantitative Methods in the Social Sciences course within the School of Social and Political Sciences. The activities are designed for RStudio Cloud.\nThese webpages are made with  and Quarto and licensed under the     Creative Commons BY-NC-SA 4.0.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "For this course we will be using R (R Core Team 2021) and RStudio as the main tools for conducting quantitative analysis. R and the basic versions of RStudio are open-source and thus free software. Even though R appeared in the early 90s, it has been gaining a lot of popularity in recent years. In fact, it is now one of the most common software for doing statistics in academia.\nR and RStudio are two separate things. R is the actual programming language and the main processing tool which does the computations in the background, whereas RStudio integrates all functionalities in a friendly and interactive interface. In short, for this course (and most of the time in practice) you chiefly use RStudio whilst R is silently doing all the work in the background. Thereafter, we will refer to R, as the integrated interface.\nR works in a command-based line environment. This means that you need to call the commands (or functions, as called in R) through text. This can look intimidating at first glance. But do not worry, we will guide you step by step.\nAt this point you may be wondering why you need to bother learning these tools. In the next section you will see some of the advantages and examples that can be achieved using R.\n\n\nR can be applied in a wide variety of fields and subjects, including not only those in the social sciences (e.g. sociology, politics or policy research), but also in humanities (e.g. history, digital humanities), natural and physical sciences (e.g. biology, chemistry or geography), health (e.g. medical studies, public health, epidemiology), business and management (e.g. finance, economics, marketing), among many others.\nThe broad application of R is due to its flexibility which allows to perform a range of tasks related to data. These cover tasks at initial stages, such as downloading, mining, or importing data. But it is also useful to manipulate, edit, transform, and organize information. Furthermore and most important for us, there are a set of tools that allow us to analyse data using a range of statistical techniques. These are useful to understand, summarize and draw conclusions about samples, e.g. people. Lastly, R is powerful to communicate and share information and documents. There are several extensions (called packages in R) that can help to produce static and interactive plots/charts, maps, written reports, interactive applications or even entire books! In fact this workbook was written from RStudio.\n\nSome of the advantages of using R are the following:\n\nIt is free and open source. You do not need to pay for a licence. Thus you can use it anywhere at any time, even if you do not have an affiliation to an institution or organisation (e.g. University or workplace);\nIt is a collaborative project. This means that it is the users who maintain, extend and update its applications;\nIt is reproducible. Research can be more transparent since you will get the same results every time you run your analysis through a specific pathway (i.e. through scripts);\nHigh compatibility. You can read and produce most types of file extensions;\nThere are a number of easy-access web resources to support you in the learning process.\n\n\n\nAt this point, you need to know that there are at least two alternatives to start using RStudio. One, and by far the most common, is to download both R and RStudio and install the applications in your local drive. The other option is RStudio Cloud. This is an on-line version of RStudio that does not require installing any additional software. You can run it directly from your browser (e.g. GoogleChrome, Safari, Firefox, etc). For now, we will use the cloud version.\nTo get started, follow the next steps:\n\n\n\n\n\n\n\n\nClick on this link RStudio Cloud - SSO, which should automatically open a new tab in your web browser or go directly to the browser and copy this URL: https://sso.rstudio.cloud/glasgow;\nEnter your University of Glasgow email address in the login page as normal;\nThen it gets linked to the SSO sign-in page, which you input your GUID and password (same page as if you’re logging into the library portal/e-reading list);\n\n\n\n\n\nSSO Login\n\n\n\n\nDone! You will be taken you into your own Rstudio Cloud work space.\n\n\nYou will receive a link from your tutor to join your lab group on RStudio Cloud (the link will be posted on Moodle too). N.b. you must use this specific link to join and access your lab group workspace, as each link is unique to your group. So only use your group’s specific link. Copy and paste the link in your web browser. You should see the following window:\n\n\n\n\n\nJoin Space.\n\n\n\n\nJoin your lab by clicking on the ‘Join space’ button shown above.\nOpen the shared space form the left-hand side pane called ‘Quants Lab Group..’ and start the Lab 1 project by clicking on the ‘Start’ button as shown below:\n\n\n\n\n\nStart project.\n\n\n\n\n\n\n\n\n\n\nOnce you have started ‘Lab 1’ you will see the screen below:\n\n\n\n\nProject name.\n\n\n\nNow, go to the “File” tab and create a R Script as follows File &gt; New file &gt; R Script\n\n\n\n\nNew R Script.\n\n\n\nOnce you have created your first R Script, save it by clicking on File &gt; Save as.. &gt; [write the name of your file].\nAfter this, your RStudio screen will be split in four important windows or panes as shown below:\n\n\n\n\nRStudio panes.\n\n\n\n\nIn Pane 1, you have your newly created R script. This is the area where you will be working most of the time. From here, you will write functions. To run an R script line, you can click on the Run green arrow situated on the top of pane 1 or more commonly you can run a code line by typing alt + enter. The things you write in this section will be saved in your R script file.\n\n\n\nRun button\n\n\nIn Pane 2, you have the “Global Environment”, this is one of the most useful tabs in this pane. It shows you the active ‘objects’ that you have available/loaded in your current session (this will probably make more sense in the coming sections).\nIn Pane 3, you have the R Console, this is where you will see most of the results of the functions you run from your script (pane 1). You can also write and run functions from here, by typing the function and hitting enter. NOTE that what you do here will NOT be saved, this is usually used to quickly call functions that you do not want to save in your script.\n\n\n\n\n\nConsole.\n\n\n\n\nFinally, in Pane 4 you have multiple useful tabs. In the File tab you can see the files and directories that you have in your R project. In the Plot tab you will see a preview of the static plots/charts you will be producing from your script. In Packages, you have a list of the extensions or plug-ins (called ‘packages’ in R) that are installed in your working environment. The Help contains some resources that clarify or expand what each of the functions does. Again, probably this will make more sense once you get started. We will come back to this later. Finally, the Viewer displays interactive outputs.\n\nNow you are ready! It is your turn to start exploring and getting familiar with R by completing the following activities.\n\nGo to your console (pane 3, bottom-left pane), write some simple calculations and run them by typing ‘enter’ after each of them, as shown below.\n\n\n\n\nR Console as calculator.\n\n\n\n Try different operations such as 50 / 20 or 3 * 5.\nFairly simple, right? And don’t forget, it is entirely normal to copy/paste and tweak any existing codes. Unlike writing an essay or an exam, you don’t actually need to know and write codes “off the cuff” or recite/memorise any syntax. You are only expected to know how to run the codes and tweak them as you go along, there is a huge amount of trial and error when you work in R. So don’t worry if you feel like you are just making minor changes to the codes, that’s how it’s supposed to work, and the first few weeks is all about getting comfortable in using R, then the level of challenge will go up. Let’s continue with the next activities!\n\nNow, write and run the following lines in your console (pane 3) and take some time to observe the result in detail for each of them:\n\n10 == 10\n10 != 10\n1 == 5\n1 &gt; 5\n'a' == 'a'\n'a' == 'b'\n\nWhat do you see? …\n…That’s it! When you use the double equal sign == you are asking R whether the value on the left hand-side of the operator is equal to the one on the right hand-side. Likewise, when you combine the exclamation mark ! with other operator, you get the reversed result. In the past exercises you used !=, this was interpreted as “is not equal to”, that is why 10 != 10 returns FALSE, but 10 == 10 returns TRUE.\nR can process different classes of inputs. In this case we used letters and we asked R whether ‘a’ was equal to ‘a’, and of course the result is TRUE. Note that when you want to input text (referred as character values in R), you need quotation marks '. If you want to enter numeric values, you simply input the raw number. These are different ‘class’ values.\nPerhaps logical operators do not make much sense at this point, but you will find out later that they are useful to manipulate data. For example, these are essential to filter a data set based on specific rules or patterns.\n\nIn R, it is very common (and practical) to store values or data as ‘objects’. These are temporally stored in your current session. Let’s try it!\nNow, we will work in the R script file (Pane 1, top-left pane), write the following and run it by clicking the green arrow or using alt + enter:\n\na &lt;- 10\na + 5\n\nWhat do you observe?…\n…That’s right! The operator &lt;- assigned the numeric value 10 to the object a (on the left hand-side of the arrow). Later, you used the object (a) to compute a sum (i.e, a + 5).\nNow, write and run the following in your R script (Pane 1)\n\nc &lt;- 3\na * c\n\nAs you can see, you stored the numeric value 3 in the variable c. Then, you called the previously created object a in a multiplication.\nIn the same way as you assigned these simple variables, you will store other types of objects later, e.g. vectors, data frames or lists. This is useful because those objects will be ready in your session to do some computations.\nThere are a few things to note when assigning objects to variables. If you use a different value to the same variable, e.g. by typing a &lt;- 5, you will replace the old value with the new. So, instead of having a representing the value 10, you will have 5. You can see the objects available in your session on the Global Environment (‘Environment’ tab in Pane 2) as shown below.\n\n\n\n\n\n‘Environment’ tab.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a very good start, great job!\nNote that the changes made in your script are saved automatically in RStudio Cloud. To verify this, have a look at the name of your script in the top-left of pane 1. If changes are due to be saved, the name will be written in red. If it is in red, save changes manually by clicking on the disk icon. After you have made sure your changes are saved, end your session simply by closing the RStudio Cloud tab in your browser.\n\nDiscuss the following questions with your neighbour or tutor:\n\nWhat are the main differences between working on a R script file (pane 1) and directly on the console (pane 3)?\nCan you describe what happens when your run the following code? (tip: look at the environment tab in pane 2)\n\n\nobject1 &lt;- 10\nobject1 &lt;- 30",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#why-r",
    "href": "01-intro.html#why-r",
    "title": "Introduction to R",
    "section": "",
    "text": "R can be applied in a wide variety of fields and subjects, including not only those in the social sciences (e.g. sociology, politics or policy research), but also in humanities (e.g. history, digital humanities), natural and physical sciences (e.g. biology, chemistry or geography), health (e.g. medical studies, public health, epidemiology), business and management (e.g. finance, economics, marketing), among many others.\nThe broad application of R is due to its flexibility which allows to perform a range of tasks related to data. These cover tasks at initial stages, such as downloading, mining, or importing data. But it is also useful to manipulate, edit, transform, and organize information. Furthermore and most important for us, there are a set of tools that allow us to analyse data using a range of statistical techniques. These are useful to understand, summarize and draw conclusions about samples, e.g. people. Lastly, R is powerful to communicate and share information and documents. There are several extensions (called packages in R) that can help to produce static and interactive plots/charts, maps, written reports, interactive applications or even entire books! In fact this workbook was written from RStudio.\n\nSome of the advantages of using R are the following:\n\nIt is free and open source. You do not need to pay for a licence. Thus you can use it anywhere at any time, even if you do not have an affiliation to an institution or organisation (e.g. University or workplace);\nIt is a collaborative project. This means that it is the users who maintain, extend and update its applications;\nIt is reproducible. Research can be more transparent since you will get the same results every time you run your analysis through a specific pathway (i.e. through scripts);\nHigh compatibility. You can read and produce most types of file extensions;\nThere are a number of easy-access web resources to support you in the learning process.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#getting-started",
    "href": "01-intro.html#getting-started",
    "title": "Introduction to R",
    "section": "",
    "text": "At this point, you need to know that there are at least two alternatives to start using RStudio. One, and by far the most common, is to download both R and RStudio and install the applications in your local drive. The other option is RStudio Cloud. This is an on-line version of RStudio that does not require installing any additional software. You can run it directly from your browser (e.g. GoogleChrome, Safari, Firefox, etc). For now, we will use the cloud version.\nTo get started, follow the next steps:\n\n\n\n\n\n\n\n\nClick on this link RStudio Cloud - SSO, which should automatically open a new tab in your web browser or go directly to the browser and copy this URL: https://sso.rstudio.cloud/glasgow;\nEnter your University of Glasgow email address in the login page as normal;\nThen it gets linked to the SSO sign-in page, which you input your GUID and password (same page as if you’re logging into the library portal/e-reading list);\n\n\n\n\n\nSSO Login\n\n\n\n\nDone! You will be taken you into your own Rstudio Cloud work space.\n\n\nYou will receive a link from your tutor to join your lab group on RStudio Cloud (the link will be posted on Moodle too). N.b. you must use this specific link to join and access your lab group workspace, as each link is unique to your group. So only use your group’s specific link. Copy and paste the link in your web browser. You should see the following window:\n\n\n\n\n\nJoin Space.\n\n\n\n\nJoin your lab by clicking on the ‘Join space’ button shown above.\nOpen the shared space form the left-hand side pane called ‘Quants Lab Group..’ and start the Lab 1 project by clicking on the ‘Start’ button as shown below:\n\n\n\n\n\nStart project.\n\n\n\n\n\n\n\n\n\n\nOnce you have started ‘Lab 1’ you will see the screen below:\n\n\n\n\nProject name.\n\n\n\nNow, go to the “File” tab and create a R Script as follows File &gt; New file &gt; R Script\n\n\n\n\nNew R Script.\n\n\n\nOnce you have created your first R Script, save it by clicking on File &gt; Save as.. &gt; [write the name of your file].\nAfter this, your RStudio screen will be split in four important windows or panes as shown below:\n\n\n\n\nRStudio panes.\n\n\n\n\nIn Pane 1, you have your newly created R script. This is the area where you will be working most of the time. From here, you will write functions. To run an R script line, you can click on the Run green arrow situated on the top of pane 1 or more commonly you can run a code line by typing alt + enter. The things you write in this section will be saved in your R script file.\n\n\n\nRun button\n\n\nIn Pane 2, you have the “Global Environment”, this is one of the most useful tabs in this pane. It shows you the active ‘objects’ that you have available/loaded in your current session (this will probably make more sense in the coming sections).\nIn Pane 3, you have the R Console, this is where you will see most of the results of the functions you run from your script (pane 1). You can also write and run functions from here, by typing the function and hitting enter. NOTE that what you do here will NOT be saved, this is usually used to quickly call functions that you do not want to save in your script.\n\n\n\n\n\nConsole.\n\n\n\n\nFinally, in Pane 4 you have multiple useful tabs. In the File tab you can see the files and directories that you have in your R project. In the Plot tab you will see a preview of the static plots/charts you will be producing from your script. In Packages, you have a list of the extensions or plug-ins (called ‘packages’ in R) that are installed in your working environment. The Help contains some resources that clarify or expand what each of the functions does. Again, probably this will make more sense once you get started. We will come back to this later. Finally, the Viewer displays interactive outputs.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#hands-on-r",
    "href": "01-intro.html#hands-on-r",
    "title": "Introduction to R",
    "section": "",
    "text": "Now you are ready! It is your turn to start exploring and getting familiar with R by completing the following activities.\n\nGo to your console (pane 3, bottom-left pane), write some simple calculations and run them by typing ‘enter’ after each of them, as shown below.\n\n\n\n\nR Console as calculator.\n\n\n\n Try different operations such as 50 / 20 or 3 * 5.\nFairly simple, right? And don’t forget, it is entirely normal to copy/paste and tweak any existing codes. Unlike writing an essay or an exam, you don’t actually need to know and write codes “off the cuff” or recite/memorise any syntax. You are only expected to know how to run the codes and tweak them as you go along, there is a huge amount of trial and error when you work in R. So don’t worry if you feel like you are just making minor changes to the codes, that’s how it’s supposed to work, and the first few weeks is all about getting comfortable in using R, then the level of challenge will go up. Let’s continue with the next activities!\n\nNow, write and run the following lines in your console (pane 3) and take some time to observe the result in detail for each of them:\n\n10 == 10\n10 != 10\n1 == 5\n1 &gt; 5\n'a' == 'a'\n'a' == 'b'\n\nWhat do you see? …\n…That’s it! When you use the double equal sign == you are asking R whether the value on the left hand-side of the operator is equal to the one on the right hand-side. Likewise, when you combine the exclamation mark ! with other operator, you get the reversed result. In the past exercises you used !=, this was interpreted as “is not equal to”, that is why 10 != 10 returns FALSE, but 10 == 10 returns TRUE.\nR can process different classes of inputs. In this case we used letters and we asked R whether ‘a’ was equal to ‘a’, and of course the result is TRUE. Note that when you want to input text (referred as character values in R), you need quotation marks '. If you want to enter numeric values, you simply input the raw number. These are different ‘class’ values.\nPerhaps logical operators do not make much sense at this point, but you will find out later that they are useful to manipulate data. For example, these are essential to filter a data set based on specific rules or patterns.\n\nIn R, it is very common (and practical) to store values or data as ‘objects’. These are temporally stored in your current session. Let’s try it!\nNow, we will work in the R script file (Pane 1, top-left pane), write the following and run it by clicking the green arrow or using alt + enter:\n\na &lt;- 10\na + 5\n\nWhat do you observe?…\n…That’s right! The operator &lt;- assigned the numeric value 10 to the object a (on the left hand-side of the arrow). Later, you used the object (a) to compute a sum (i.e, a + 5).\nNow, write and run the following in your R script (Pane 1)\n\nc &lt;- 3\na * c\n\nAs you can see, you stored the numeric value 3 in the variable c. Then, you called the previously created object a in a multiplication.\nIn the same way as you assigned these simple variables, you will store other types of objects later, e.g. vectors, data frames or lists. This is useful because those objects will be ready in your session to do some computations.\nThere are a few things to note when assigning objects to variables. If you use a different value to the same variable, e.g. by typing a &lt;- 5, you will replace the old value with the new. So, instead of having a representing the value 10, you will have 5. You can see the objects available in your session on the Global Environment (‘Environment’ tab in Pane 2) as shown below.\n\n\n\n\n\n‘Environment’ tab.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a very good start, great job!\nNote that the changes made in your script are saved automatically in RStudio Cloud. To verify this, have a look at the name of your script in the top-left of pane 1. If changes are due to be saved, the name will be written in red. If it is in red, save changes manually by clicking on the disk icon. After you have made sure your changes are saved, end your session simply by closing the RStudio Cloud tab in your browser.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#activity",
    "href": "01-intro.html#activity",
    "title": "Introduction to R",
    "section": "",
    "text": "Discuss the following questions with your neighbour or tutor:\n\nWhat are the main differences between working on a R script file (pane 1) and directly on the console (pane 3)?\nCan you describe what happens when your run the following code? (tip: look at the environment tab in pane 2)\n\n\nobject1 &lt;- 10\nobject1 &lt;- 30",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "03-Lab3.html",
    "href": "03-Lab3.html",
    "title": "Data wrangling",
    "section": "",
    "text": "Welcome to Lab 3!\nIn our previous session we learned about R packages, including how to install and load them. We talked about the main types of data used in social science research and how to represent them in R. Also, we played around with some datasets using some key functions, such as: filter(), select(), and mutate(). In this session we will learn how to import data in R, clean and format the data using a real-world dataset. This is a common and important phase in quantitative research.\n\nToday, we will be working with data generated by the Access Research Knowledge (ARK) hub. ARK conducts a series of surveys about society and life in Northern Ireland. For this lab, we will be working with the results of the Northern Ireland Life and Times Survey (NILT) in the year 2012. In particular, we will be using a teaching dataset that focuses on community relations and political attitudes. This includes background information of the participants and their household. Please take 5-10 minutes to read the documentation of this dataset (click here to access the documentation). p.s. You will have to regularly consult this document to understand and use the data in NILT. So, I recommend you to save the PDF file in your local drive if you can. This NILT teaching dataset is also what you will be using for the research report assignment in this course (smart, isn’t it?) - so it’s worth investing the time to learn how to work with this data through the next few labs, as part of the preparation and practice for your assignemnt.\n\nWe will continue using RStudio Cloud, as we did in our previous labs. Please follow the next steps:\n\nGo to your ‘Quants lab group’ in RStudio Cloud (if you have not joined a shared space, follow the instructions in Lab 2).\nStart the project called ‘NILT’ located in your lab group.\nOnce you have initialized the project, generate a new R scrip file, and save it as ‘Exploratory analysis’.\nLoad the tidyverse and haven packages. This last package is useful to import data from SPSS (the tidyverse package was pre-installed in your session). You can copy, paste, and run the following functions from your script:\n\n\nlibrary(tidyverse)\nlibrary(haven)\n\nNext, we will create a folder to store the data. Then, download and read the NILT data set, following the next steps:\n\nFrom your script, create a new folder called ‘data’:\n\n\ndir.create(\"data\")\n\n\nDownload the data using the download.file() function. Remember that you have to specify the URL first, and the destination of the file second (including the folder).\n\n\ndownload.file(\n  \"https://www.ark.ac.uk/teaching/NILT2012GR.sav\",\n  \"data/nilt2012.sav\"\n)\n\n\nTake a look to the ‘Files’ tab in pane 3, you will see a folder called ‘data’, click on it, and you will see the nilt2012.sav file.\n\n\n\n\n\nCloud files.\n\n\n\n\nTo read this type of file use the read_sav() function. Read the .sav file and assign it to an object called nilt.\n\n\nnilt &lt;- read_sav(\"data/nilt2012.sav\")\n\n\nAnd that’s it! You should see a new data object in your ‘Environment’ tab (Pane 2) ready to be used. You can also see that this contains 1204 observations (rows) and 133 variables (columns). Lets glimpse our newly imported data and see the type of variables included.\n\nglimpse(nilt)\n\n\nAs you can see from the result of glimpse, the class for practically all the variables is &lt;dbl+lbl&gt;. What does this mean? This happened because usually datasets use numbers to represent each of the categories/levels in categorical variables. These numbers are labelled with their respective meaning. This is why we have a combination of value types (&lt;dbl+lbl&gt;). Take the example of the variable called rsex, as you can see from the values displayed using glimpse(), this includes numbers only, e.g. 1,1,2,2.... This is because ‘1’ represents ‘Male’ respondents and ‘2’ represents ‘Female’ respondents in the NILT dataset (n.b. the authors of this lab workbook recognise that sex and gender are different concepts, and we acknowledge this tension and that it will be problematic to imply or define gender identities as binary, as with any dataset. More recent surveys normally approach this in a more inclusive way by offering self-describe options). You can check the pre-defined parameters of the variable in NILT in the documentation or running print_labels(nilt$rsex) in your console, which returns the numeric value and its respective label. As with rsex, this is the case for many other variables in this data set.\nYou should be aware that this type of ‘mix’ variable is a special case since we imported a file from a foreign file that saves metadata for each variable (containing the names of the categories). As you learned in the last lab, in R we treat categorical variables as factor. Therefore, we will coerce some variables as factor. This time we will use the function as_factor() instead of the simple factor() that we used before. This is because as_factor() allows us to keep the names of each category in the variables. The syntax is exactly the same as before. Copy and run the following from your script:\n\n# Gender of the respondent\nnilt &lt;- nilt %&gt;% mutate(rsex = as_factor(rsex))\n# Highest Educational qualification\nnilt &lt;- nilt %&gt;% mutate(highqual = as_factor(highqual))\n# Religion\nnilt &lt;- nilt %&gt;% mutate(religcat = as_factor(religcat))\n# Political identification\nnilt &lt;- nilt %&gt;% mutate(uninatid = as_factor(uninatid))\n# Happiness\nnilt &lt;- nilt %&gt;% mutate(ruhappy = as_factor(ruhappy))\n\nNotice from the code above that we are replacing the ‘old’ dataset with the result of the mutated variables that are of type factor. This is why we assigned the result with the assigning operator &lt;-.\nWhat about the numeric variables? In the documentation file there is a table in which you will see a type of measure ‘scale’. This usually refers to continuous numeric variables (e.g. age or income).1 Let’s coerce some variables to the appropriate type.\nIn the previous operation we coerced the variables as factor one by one, but we can transform several variables at once within the mutate function. As we did before, copy and run the following code in your script:\n\n# Coerce several variables as numeric\nnilt &lt;- nilt %&gt;%\n  mutate(\n    rage = as.numeric(rage),\n    rhourswk = as.numeric(rhourswk),\n    persinc2 = as.numeric(persinc2),\n  )\n\nBefore doing some analyses, we will drop unused levels (or categories) in our dataset using the function droplevels(), as following:\n\n# drop unused levels\nnilt &lt;- droplevels(nilt)\n\nThe previous function is useful to remove some categories that are not being used in the dataset (e.g. categories including 0 observations).\nFinally, save the NILT survey in an .rds file (this is the R format). We will not use this file now, but this will save us time formatting the dataset in next labs (So, we do not have to repeat the steps above every time).\n\nsaveRDS(nilt, \"data/nilt_r_object.rds\")\n\n\nPhew! Good job. You have completed the basics for wrangling the data and producing a workable dataset.\nAs a final step, just double check that things went as expected. For this purpose, we will re-read the clean dataset.\n\n\nUsing the readRDS() function, read the .rds file that you just created in the last step and assign it to an object called cleaned_data. Remember to include the full directory of the file using quotation marks inside the function.\nRun the glimpse function on the cleaned_data object.\nRun the glimpse function on the nilt object.\nDo they look the same? If yes, it means that you successfully saved your work.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#importing-and-data-wrangling",
    "href": "03-Lab3.html#importing-and-data-wrangling",
    "title": "Data wrangling",
    "section": "",
    "text": "Today, we will be working with data generated by the Access Research Knowledge (ARK) hub. ARK conducts a series of surveys about society and life in Northern Ireland. For this lab, we will be working with the results of the Northern Ireland Life and Times Survey (NILT) in the year 2012. In particular, we will be using a teaching dataset that focuses on community relations and political attitudes. This includes background information of the participants and their household. Please take 5-10 minutes to read the documentation of this dataset (click here to access the documentation). p.s. You will have to regularly consult this document to understand and use the data in NILT. So, I recommend you to save the PDF file in your local drive if you can. This NILT teaching dataset is also what you will be using for the research report assignment in this course (smart, isn’t it?) - so it’s worth investing the time to learn how to work with this data through the next few labs, as part of the preparation and practice for your assignemnt.\n\nWe will continue using RStudio Cloud, as we did in our previous labs. Please follow the next steps:\n\nGo to your ‘Quants lab group’ in RStudio Cloud (if you have not joined a shared space, follow the instructions in Lab 2).\nStart the project called ‘NILT’ located in your lab group.\nOnce you have initialized the project, generate a new R scrip file, and save it as ‘Exploratory analysis’.\nLoad the tidyverse and haven packages. This last package is useful to import data from SPSS (the tidyverse package was pre-installed in your session). You can copy, paste, and run the following functions from your script:\n\n\nlibrary(tidyverse)\nlibrary(haven)\n\nNext, we will create a folder to store the data. Then, download and read the NILT data set, following the next steps:\n\nFrom your script, create a new folder called ‘data’:\n\n\ndir.create(\"data\")\n\n\nDownload the data using the download.file() function. Remember that you have to specify the URL first, and the destination of the file second (including the folder).\n\n\ndownload.file(\n  \"https://www.ark.ac.uk/teaching/NILT2012GR.sav\",\n  \"data/nilt2012.sav\"\n)\n\n\nTake a look to the ‘Files’ tab in pane 3, you will see a folder called ‘data’, click on it, and you will see the nilt2012.sav file.\n\n\n\n\n\nCloud files.\n\n\n\n\nTo read this type of file use the read_sav() function. Read the .sav file and assign it to an object called nilt.\n\n\nnilt &lt;- read_sav(\"data/nilt2012.sav\")\n\n\nAnd that’s it! You should see a new data object in your ‘Environment’ tab (Pane 2) ready to be used. You can also see that this contains 1204 observations (rows) and 133 variables (columns). Lets glimpse our newly imported data and see the type of variables included.\n\nglimpse(nilt)",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#data-wrangling-1",
    "href": "03-Lab3.html#data-wrangling-1",
    "title": "Data wrangling",
    "section": "",
    "text": "As you can see from the result of glimpse, the class for practically all the variables is &lt;dbl+lbl&gt;. What does this mean? This happened because usually datasets use numbers to represent each of the categories/levels in categorical variables. These numbers are labelled with their respective meaning. This is why we have a combination of value types (&lt;dbl+lbl&gt;). Take the example of the variable called rsex, as you can see from the values displayed using glimpse(), this includes numbers only, e.g. 1,1,2,2.... This is because ‘1’ represents ‘Male’ respondents and ‘2’ represents ‘Female’ respondents in the NILT dataset (n.b. the authors of this lab workbook recognise that sex and gender are different concepts, and we acknowledge this tension and that it will be problematic to imply or define gender identities as binary, as with any dataset. More recent surveys normally approach this in a more inclusive way by offering self-describe options). You can check the pre-defined parameters of the variable in NILT in the documentation or running print_labels(nilt$rsex) in your console, which returns the numeric value and its respective label. As with rsex, this is the case for many other variables in this data set.\nYou should be aware that this type of ‘mix’ variable is a special case since we imported a file from a foreign file that saves metadata for each variable (containing the names of the categories). As you learned in the last lab, in R we treat categorical variables as factor. Therefore, we will coerce some variables as factor. This time we will use the function as_factor() instead of the simple factor() that we used before. This is because as_factor() allows us to keep the names of each category in the variables. The syntax is exactly the same as before. Copy and run the following from your script:\n\n# Gender of the respondent\nnilt &lt;- nilt %&gt;% mutate(rsex = as_factor(rsex))\n# Highest Educational qualification\nnilt &lt;- nilt %&gt;% mutate(highqual = as_factor(highqual))\n# Religion\nnilt &lt;- nilt %&gt;% mutate(religcat = as_factor(religcat))\n# Political identification\nnilt &lt;- nilt %&gt;% mutate(uninatid = as_factor(uninatid))\n# Happiness\nnilt &lt;- nilt %&gt;% mutate(ruhappy = as_factor(ruhappy))\n\nNotice from the code above that we are replacing the ‘old’ dataset with the result of the mutated variables that are of type factor. This is why we assigned the result with the assigning operator &lt;-.\nWhat about the numeric variables? In the documentation file there is a table in which you will see a type of measure ‘scale’. This usually refers to continuous numeric variables (e.g. age or income).1 Let’s coerce some variables to the appropriate type.\nIn the previous operation we coerced the variables as factor one by one, but we can transform several variables at once within the mutate function. As we did before, copy and run the following code in your script:\n\n# Coerce several variables as numeric\nnilt &lt;- nilt %&gt;%\n  mutate(\n    rage = as.numeric(rage),\n    rhourswk = as.numeric(rhourswk),\n    persinc2 = as.numeric(persinc2),\n  )\n\nBefore doing some analyses, we will drop unused levels (or categories) in our dataset using the function droplevels(), as following:\n\n# drop unused levels\nnilt &lt;- droplevels(nilt)\n\nThe previous function is useful to remove some categories that are not being used in the dataset (e.g. categories including 0 observations).\nFinally, save the NILT survey in an .rds file (this is the R format). We will not use this file now, but this will save us time formatting the dataset in next labs (So, we do not have to repeat the steps above every time).\n\nsaveRDS(nilt, \"data/nilt_r_object.rds\")",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#read-the-clean-dataset",
    "href": "03-Lab3.html#read-the-clean-dataset",
    "title": "Data wrangling",
    "section": "",
    "text": "Phew! Good job. You have completed the basics for wrangling the data and producing a workable dataset.\nAs a final step, just double check that things went as expected. For this purpose, we will re-read the clean dataset.\n\n\nUsing the readRDS() function, read the .rds file that you just created in the last step and assign it to an object called cleaned_data. Remember to include the full directory of the file using quotation marks inside the function.\nRun the glimpse function on the cleaned_data object.\nRun the glimpse function on the nilt object.\nDo they look the same? If yes, it means that you successfully saved your work.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#footnotes",
    "href": "03-Lab3.html#footnotes",
    "title": "Data wrangling",
    "section": "Footnotes",
    "text": "Footnotes\n\nBe careful, in some cases these actually correspond to discrete numeric values in this dataset (things that can be counted, e.g. number of…).↩︎",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "06-Lab6.html",
    "href": "06-Lab6.html",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "In this lab, we will extend your skills to explore data by visualizing it…and R is great for this! It is actually a highly-demanded skill in the job market.\nVisualizing data is an important process in at least two stages in quantitative research: First, for you as a researcher to get familiar with the data; and second, to communicate your findings. R includes two important tools to achieve this: First, the ggplot2 package (included in tidyverse), a powerful tool to explore and plot data. Second, R Markdown which allows you to create integrated quantitative reports. Combining and mastering the two can create very effective results.\n\nVisual data exploration with ggplot2 (Artwork by @alison_horst).\n\n\n\n\nVisual data exploration. Source: Horst (n.d.)\n\n\n\n\nDifferent plot types serve different types of data. In the last lab, we introduced some functions to summarise your data and to generate summaries for specific types of variable. This will help you to decide what the most suitable plot is for your data or variable. The table below presents a minimal guide to choose the type of plot as a function of the type of data you intend to visualize. In addition, it splits the type of plot by the number of variables included in the visualization, one for univariate, or two for bivariate. Bivariate plots are useful to explore the relationship between variables.\n\n\n\nUnivariate\nBivariate\n\n\n\nCategorical\nBar plot / Pie chart\nBar plot\n\n\nNumeric\nHistogram / boxplot\nScatter plot\n\n\n\nCategorical + Numeric\n\n-\nBox plot\n\n\n\n\nAs we did in the last session, let’s learn by doing. We will continue working in the same project called NILT in RStudio Cloud. Set up your session as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\n\nNext, create a new Rmd document following the next steps.\n\nCreate a new Rmd file titled ‘Lab 6. Data visualization’, and write your name in the ‘Author’ space.\nSave the Rmd file clicking on File&gt;Save as..., using Lab_6 in the ‘File name’ box, and click on the ‘Save’ button.\nDelete all the contents in the default example with the exception of the first bit which contains the YAML and the first code chunk (that is keeping from line 1 to 10 and deleting everything from line 11).\nIn the setup chunk (line 8), change echo from TRUE to FALSE (this will hide the code for all chunks in the output).\n\nOnce your Rmd document is ready, insert a new R code chunk. To do this, click on the ‘Insert’ button on pane 1, and then click on ‘R’. This will insert an R code chunk as the ones we explored in ‘Test1’.\n\n\n\n\nInsert a code chunk.\n\n\n\nIn the body of the first chunk, load the packages that you will need. For now, it will be only tidyverse.\n\nlibrary(tidyverse)\n\nAfter the previous, insert a second chunk. In the body of the code copy and paste the lines below. This is to load the data we downloaded and formatted in the last lab in R format using the readRDS() function (note that we are reading the .rds file and not the original .sav file). Also, we will create a subset called nilt_subset selecting only the variables that we are interested in for now, as we did in the previous lab.\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Create subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\nRun both of the previous chunks individually by clicking on the green arrow located on the top-right of the chunk, as described earlier.\n\nNow that we read the data in, we are ready to start creating our own own plots using the 2012 NILT survey.\nYou do not need to reproduce each of the examples in the following section, but we suggest you to carefully read the description and try to understand the code syntax.\n\nLet’s start using the same variables we summarised in our last lab. We started by computing the total number of respondents by gender in a One-Way contingency table. We can easily visualize this using a bar plot with ggplot. This package always takes at least three layers, namely data, aesthetics and geometry. Here, we define the data as the first argument of the ggplot()function with the nilt_subset. The second argument, aesthetics, is separated by a comma and is introduced using the function aes(). In this case we define the X axis x = of the plot by the categories included in the variable rsex. The geometry is specified with the function geom_bar() without arguments for now. Note that we added the geometry using the plus sign + at the end of the previous line code. As an extra, I included the main title and the name of the x axis using the labs() function (you don’t need to copy or run the following code chunks in your .Rmd. This is only for demonstration purposes).\n\nggplot(nilt_subset, aes(x = rsex)) +\n  geom_bar() +\n  labs(title = \"Gender\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\nFrom the plot above, we can graphically see what we found out previously: there are more female respondents than males in our sample. The advantage is that we can have a sense of the magnitude of the difference by visualising it.\n\nIn Lab 3, we computed a Two-Way contingency table, which included the count of two categorical variables. This summary can be visualized using a stacked bar plot. This is quite similar to the above, with the addition that the area of the vertical plot is coloured by the size of each group.\nIf we wanted to know how gender is split by religion, we can add the fill argument with a second variable in aesthetics, as shown below.\n\nggplot(nilt_subset, aes(x = rsex, fill = religcat)) +\n  geom_bar() +\n  labs(title = \"Gender by religion\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\nThis plot is not very informative, since the total size of female and male respondents is different. The type of visualization will also depend on your specific research question or the topic you are interested in. For example, if I think it is worthwhile visualizing the religion by respondents’ sex. A plot can show us the magnitudes and composition by respondents’ sex for each religion. To do this, we need to change the aesthetics, specifying the religion by category variable religcat on the x axis and fill with gender rsex.\n\nggplot(nilt_subset, aes(x = religcat, fill = rsex)) +\n  geom_bar() +\n  labs(title = \"Religion by gender\", x = \"Religion\")\n\n\n\n\n\n\n\nAs we can see, catholic and protestant religion are equally popular among the respondents. Also, we can see that these are composed by similar proportions of males and females. One interesting thing is that there are more male respondents with no religion than female participants. Again, we found this out with the descriptive statistics computed in the last lab. However, we have the advantage that we can graphically reprsent and inspect the magnitude of these differences.\n\n\nLast time we talked about some measures of centrality and spread for numeric variables. The histogram plot is similar to the bar plot; the difference is that it splits the numeric range into fixed “bins” and computes the frequency/count for each bin instead of counting the number of respondents for each numeric value. The syntax is practically the same as the simple bar plot. At this time, we set the x aesthetic with the numeric variable age rage. Also, the geometry is defined as a histogram using the geom_histogram() function.\n\nggplot(nilt_subset, aes(x = rage)) +\n  geom_histogram() +\n  labs(title = \"Age distribution\")\n\n\n\n\n\n\n\nFrom the histogram, we have age (in bins) on the X axis, and the frequency/count on the y axis. This plot is useful to visualize how respondent’s age is distributed in our sample. For instance, we can quickly see the minimum and maximum value, or the most popular age, or a general trend indicating the largest age group.\nA second option to visualize numeric variables is the box plot. Essentially this draws the quartiles of a numeric vector. For this time, rage is defined in the y axis. This is just a personal preference. The geometry is set by the geom_boxplot() function.\n\nggplot(nilt_subset, aes(y = rage)) +\n  geom_boxplot() +\n  labs(title = \"Age boxplot\")\n\n\n\n\n\n\n\nWhat we see from this plot is the first, second and third quartile. The second quartile (or median) is represented by the black line in the middle of the box. As you can see this is close to 50 years old, as we computed using the quantile() function. The lower edge of the box represents the 2nd quartile, which is somewhere around 35 years old. Similarly the 3rd quartile is represented by the upper edge of the box. We can confirm this by computing the quantiles for this variable.\n\nquantile(nilt_subset$rage, na.rm = T)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\n\nA useful plot to explore the relationship between two numeric variables is the scatter plot. This plot locates a dot for each observation according to their respective numeric values. In the example below, we use age rage on the X axis (horizontal), and personal income persinc2 on the Y axis (vertical). This type of plot is useful to explore a relationship between variables.\n\nTo generate a scatter plot, we need to define x and y in aesthetics aes(). The geometry is a point, expressed by geom_point(). Note that we are specifying some further optional arguments. First, in aes() alpha regulates the opacity of the dots. This goes from 0.0 (completely translucent) to 1.0 (completely solid fill). Second, in geom_point() we defined position as jitter. This arguments slightly moves the point away from their exact location. These two arguments are desired in this plot because the personal income bands are overlapped. Adding some transparency and noise to their position, can allow to visualize possible patterns easily.\n\nggplot(nilt_subset, aes(x = rage, y = persinc2, alpha = 0.8)) +\n  geom_point(position = \"jitter\") +\n  labs(title = \"Personal income vs age\", x = \"Age\", y = \"Personal income (£)\")\n\n\n\n\n\n\n\nThere is not a clear pattern in our previous plot. However, it is interesting to note that most of the people younger than 25 years old earn less than £20K a year. Similarly, most of the people older than 75 earn less than £20K. And only very few earn over £60k a year (looking at the top of the plot).\n\nVery often we want to summarise central or spread measure by categories or groups. For example, let’s go back to the example of age and respondents’ sex. We can visualize these two variables (which include one numeric and one categorical) using a box plot. To create this, we need to specify the x and y value in aes() and include the geom_boxplot() geometry.\n\nggplot(nilt_subset, aes(y = rage, x = rsex)) +\n  geom_boxplot() +\n  ggtitle(\"Age by sex\")\n\n\n\n\n\n\n\nFrom this, we can visualize that female participants are slightly younger than their male counterparts in the sample.\n\nUsing the nilt_subset object, complete the tasks below in the Rmd file Lab_6, which you created earlier. Insert a new code chunk for each of these activities and include brief comments as text (outside the chunk) in the Rmd document to introduce or describe the plots. Feel free to copy and adapt the code to create the plots in the examples above.\n\nCreate a first-level header to start a section called “Categorical analysis”;\nCreate simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid;\nBased on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables;\nCreate a new first-level header to start a section called “Numeric analysis”;\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime);\nAdd your own (brief) comments to each of the plots as text in your Rmd file;\nKnit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish.\n\n\nThere are a number of features that you can customize in your plots, including the background, text size, colours, adding more variables. But you don’t have to memorise or remember all this, one thing that is very commonly used by R data scientists are R cheat sheets! They are extremely handy when you try to create a visualisation from scratch, check out the Data Visualization with ggplot2 Cheat Sheet. An extra tip is that you can change the overall look of the plot by adding pre-defined themes. You can read more about it here. Another interesting site is the The R Graph Gallery, which includes a comprehensive showcase of plot types and their respective code.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#introduction",
    "href": "06-Lab6.html#introduction",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "In this lab, we will extend your skills to explore data by visualizing it…and R is great for this! It is actually a highly-demanded skill in the job market.\nVisualizing data is an important process in at least two stages in quantitative research: First, for you as a researcher to get familiar with the data; and second, to communicate your findings. R includes two important tools to achieve this: First, the ggplot2 package (included in tidyverse), a powerful tool to explore and plot data. Second, R Markdown which allows you to create integrated quantitative reports. Combining and mastering the two can create very effective results.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#data-visualization",
    "href": "06-Lab6.html#data-visualization",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Visual data exploration with ggplot2 (Artwork by @alison_horst).\n\n\n\n\nVisual data exploration. Source: Horst (n.d.)\n\n\n\n\nDifferent plot types serve different types of data. In the last lab, we introduced some functions to summarise your data and to generate summaries for specific types of variable. This will help you to decide what the most suitable plot is for your data or variable. The table below presents a minimal guide to choose the type of plot as a function of the type of data you intend to visualize. In addition, it splits the type of plot by the number of variables included in the visualization, one for univariate, or two for bivariate. Bivariate plots are useful to explore the relationship between variables.\n\n\n\nUnivariate\nBivariate\n\n\n\nCategorical\nBar plot / Pie chart\nBar plot\n\n\nNumeric\nHistogram / boxplot\nScatter plot\n\n\n\nCategorical + Numeric\n\n-\nBox plot\n\n\n\n\nAs we did in the last session, let’s learn by doing. We will continue working in the same project called NILT in RStudio Cloud. Set up your session as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Quants lab group’;\n\nNext, create a new Rmd document following the next steps.\n\nCreate a new Rmd file titled ‘Lab 6. Data visualization’, and write your name in the ‘Author’ space.\nSave the Rmd file clicking on File&gt;Save as..., using Lab_6 in the ‘File name’ box, and click on the ‘Save’ button.\nDelete all the contents in the default example with the exception of the first bit which contains the YAML and the first code chunk (that is keeping from line 1 to 10 and deleting everything from line 11).\nIn the setup chunk (line 8), change echo from TRUE to FALSE (this will hide the code for all chunks in the output).\n\nOnce your Rmd document is ready, insert a new R code chunk. To do this, click on the ‘Insert’ button on pane 1, and then click on ‘R’. This will insert an R code chunk as the ones we explored in ‘Test1’.\n\n\n\n\nInsert a code chunk.\n\n\n\nIn the body of the first chunk, load the packages that you will need. For now, it will be only tidyverse.\n\nlibrary(tidyverse)\n\nAfter the previous, insert a second chunk. In the body of the code copy and paste the lines below. This is to load the data we downloaded and formatted in the last lab in R format using the readRDS() function (note that we are reading the .rds file and not the original .sav file). Also, we will create a subset called nilt_subset selecting only the variables that we are interested in for now, as we did in the previous lab.\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Create subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\nRun both of the previous chunks individually by clicking on the green arrow located on the top-right of the chunk, as described earlier.\n\nNow that we read the data in, we are ready to start creating our own own plots using the 2012 NILT survey.\nYou do not need to reproduce each of the examples in the following section, but we suggest you to carefully read the description and try to understand the code syntax.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#categorical-variables",
    "href": "06-Lab6.html#categorical-variables",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Let’s start using the same variables we summarised in our last lab. We started by computing the total number of respondents by gender in a One-Way contingency table. We can easily visualize this using a bar plot with ggplot. This package always takes at least three layers, namely data, aesthetics and geometry. Here, we define the data as the first argument of the ggplot()function with the nilt_subset. The second argument, aesthetics, is separated by a comma and is introduced using the function aes(). In this case we define the X axis x = of the plot by the categories included in the variable rsex. The geometry is specified with the function geom_bar() without arguments for now. Note that we added the geometry using the plus sign + at the end of the previous line code. As an extra, I included the main title and the name of the x axis using the labs() function (you don’t need to copy or run the following code chunks in your .Rmd. This is only for demonstration purposes).\n\nggplot(nilt_subset, aes(x = rsex)) +\n  geom_bar() +\n  labs(title = \"Gender\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\nFrom the plot above, we can graphically see what we found out previously: there are more female respondents than males in our sample. The advantage is that we can have a sense of the magnitude of the difference by visualising it.\n\nIn Lab 3, we computed a Two-Way contingency table, which included the count of two categorical variables. This summary can be visualized using a stacked bar plot. This is quite similar to the above, with the addition that the area of the vertical plot is coloured by the size of each group.\nIf we wanted to know how gender is split by religion, we can add the fill argument with a second variable in aesthetics, as shown below.\n\nggplot(nilt_subset, aes(x = rsex, fill = religcat)) +\n  geom_bar() +\n  labs(title = \"Gender by religion\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\nThis plot is not very informative, since the total size of female and male respondents is different. The type of visualization will also depend on your specific research question or the topic you are interested in. For example, if I think it is worthwhile visualizing the religion by respondents’ sex. A plot can show us the magnitudes and composition by respondents’ sex for each religion. To do this, we need to change the aesthetics, specifying the religion by category variable religcat on the x axis and fill with gender rsex.\n\nggplot(nilt_subset, aes(x = religcat, fill = rsex)) +\n  geom_bar() +\n  labs(title = \"Religion by gender\", x = \"Religion\")\n\n\n\n\n\n\n\nAs we can see, catholic and protestant religion are equally popular among the respondents. Also, we can see that these are composed by similar proportions of males and females. One interesting thing is that there are more male respondents with no religion than female participants. Again, we found this out with the descriptive statistics computed in the last lab. However, we have the advantage that we can graphically reprsent and inspect the magnitude of these differences.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#numeric-variables",
    "href": "06-Lab6.html#numeric-variables",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Last time we talked about some measures of centrality and spread for numeric variables. The histogram plot is similar to the bar plot; the difference is that it splits the numeric range into fixed “bins” and computes the frequency/count for each bin instead of counting the number of respondents for each numeric value. The syntax is practically the same as the simple bar plot. At this time, we set the x aesthetic with the numeric variable age rage. Also, the geometry is defined as a histogram using the geom_histogram() function.\n\nggplot(nilt_subset, aes(x = rage)) +\n  geom_histogram() +\n  labs(title = \"Age distribution\")\n\n\n\n\n\n\n\nFrom the histogram, we have age (in bins) on the X axis, and the frequency/count on the y axis. This plot is useful to visualize how respondent’s age is distributed in our sample. For instance, we can quickly see the minimum and maximum value, or the most popular age, or a general trend indicating the largest age group.\nA second option to visualize numeric variables is the box plot. Essentially this draws the quartiles of a numeric vector. For this time, rage is defined in the y axis. This is just a personal preference. The geometry is set by the geom_boxplot() function.\n\nggplot(nilt_subset, aes(y = rage)) +\n  geom_boxplot() +\n  labs(title = \"Age boxplot\")\n\n\n\n\n\n\n\nWhat we see from this plot is the first, second and third quartile. The second quartile (or median) is represented by the black line in the middle of the box. As you can see this is close to 50 years old, as we computed using the quantile() function. The lower edge of the box represents the 2nd quartile, which is somewhere around 35 years old. Similarly the 3rd quartile is represented by the upper edge of the box. We can confirm this by computing the quantiles for this variable.\n\nquantile(nilt_subset$rage, na.rm = T)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\n\nA useful plot to explore the relationship between two numeric variables is the scatter plot. This plot locates a dot for each observation according to their respective numeric values. In the example below, we use age rage on the X axis (horizontal), and personal income persinc2 on the Y axis (vertical). This type of plot is useful to explore a relationship between variables.\n\nTo generate a scatter plot, we need to define x and y in aesthetics aes(). The geometry is a point, expressed by geom_point(). Note that we are specifying some further optional arguments. First, in aes() alpha regulates the opacity of the dots. This goes from 0.0 (completely translucent) to 1.0 (completely solid fill). Second, in geom_point() we defined position as jitter. This arguments slightly moves the point away from their exact location. These two arguments are desired in this plot because the personal income bands are overlapped. Adding some transparency and noise to their position, can allow to visualize possible patterns easily.\n\nggplot(nilt_subset, aes(x = rage, y = persinc2, alpha = 0.8)) +\n  geom_point(position = \"jitter\") +\n  labs(title = \"Personal income vs age\", x = \"Age\", y = \"Personal income (£)\")\n\n\n\n\n\n\n\nThere is not a clear pattern in our previous plot. However, it is interesting to note that most of the people younger than 25 years old earn less than £20K a year. Similarly, most of the people older than 75 earn less than £20K. And only very few earn over £60k a year (looking at the top of the plot).",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#mixed-data",
    "href": "06-Lab6.html#mixed-data",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Very often we want to summarise central or spread measure by categories or groups. For example, let’s go back to the example of age and respondents’ sex. We can visualize these two variables (which include one numeric and one categorical) using a box plot. To create this, we need to specify the x and y value in aes() and include the geom_boxplot() geometry.\n\nggplot(nilt_subset, aes(y = rage, x = rsex)) +\n  geom_boxplot() +\n  ggtitle(\"Age by sex\")\n\n\n\n\n\n\n\nFrom this, we can visualize that female participants are slightly younger than their male counterparts in the sample.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#activity",
    "href": "06-Lab6.html#activity",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Using the nilt_subset object, complete the tasks below in the Rmd file Lab_6, which you created earlier. Insert a new code chunk for each of these activities and include brief comments as text (outside the chunk) in the Rmd document to introduce or describe the plots. Feel free to copy and adapt the code to create the plots in the examples above.\n\nCreate a first-level header to start a section called “Categorical analysis”;\nCreate simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid;\nBased on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables;\nCreate a new first-level header to start a section called “Numeric analysis”;\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime);\nAdd your own (brief) comments to each of the plots as text in your Rmd file;\nKnit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish.\n\n\nThere are a number of features that you can customize in your plots, including the background, text size, colours, adding more variables. But you don’t have to memorise or remember all this, one thing that is very commonly used by R data scientists are R cheat sheets! They are extremely handy when you try to create a visualisation from scratch, check out the Data Visualization with ggplot2 Cheat Sheet. An extra tip is that you can change the overall look of the plot by adding pre-defined themes. You can read more about it here. Another interesting site is the The R Graph Gallery, which includes a comprehensive showcase of plot types and their respective code.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "13-RFAQ.html",
    "href": "13-RFAQ.html",
    "title": "Appendix 2: R Issues FAQ",
    "section": "",
    "text": "This appendix contains information for solving common issues students have reported when working on their interpretive findings report.\nUpdates will continue being added to this appendix with information for any new issues being reported. So, if you experience an issue, this page will be a good first port of call to check for any existing solution.\n\nWhen making a Moodle post or emailing about an issue it is vital to provide sufficient context to enable others to help find a solution. This includes:\n\nThe full text of any error message that you are receiving.\nAny relevant code that is producing the error.\n\nError messages will often flag the relevant code chunk where the error occurred:\n\n\n\n\n\n\n\n\nSo, based on the above error message you would include the code within the code block at lines 49-50.\nAdditionally, when emailing, please also include:\n\nYour lab group number.\nIf already made a Moodle post about the issue, a link to the post.\n\nIf variables go missing from your regression results table or the regression results are different to how they originally appeared when you knitted the template, you may have unintentionally replaced the data included with the template.\nThe preamble code block at the top of the RMarkdown file provide with the template reads in the ‘fullnilt_2012.rds’ dataset and assigns it to the ‘nilt’ data frame object.\n\n\n\n\n\n\n\n\nYou therefore do not need to download and read in anther dataset, this is all setup for you already within the preamble code block. To resolve the issue, remove all code where you download and assign another dataset to the nilt data frame.\nFor example, these would be lines to remove:\n\n\n\n\n\n\n\n\nThe ‘NILT2012GR’ that we used in the labs has a different set of the NILT variables, and does not contain all the variables in the ‘fullnilt_2012’ dataset provided in the template. The above code then would result in variables disappearing from your regression results table.\nSimilarly, this would also be code to remove:\n\n\n\n\n\n\n\n\nThe above code replaces the original nilt data frame with a version that only includes the 5 variables listed within the selection function. Again, resulting in variables disappearing from the regression results table.\nThe select() function keeps variables passed to it and drops the others. The first argument is always the dataframe being selected from, select(dataframe, ...), with variables to keep from it listed after, select(dataframe, variable1, variable2, ...). Assigning, &lt;-, select() to the same dataframe, dataframe &lt;- select(dataframe, ..), is effectively saying, ‘from this dataframe keep these variables and permanently remove the others’. Unless your whole analysis is only going to use that selection, always assign it to a new dataframe instead, dataframe_subset &lt;- select(dataframe, ...).\nIf you need to create a subset of the data, ensure to assign it to a new data frame:\n\n\n\n\n\n\n\n\nNote, always consider whether you actually need to create a subset. Most functions have arguments for defining which variables to use, so you do not necessarily need to provide a subset to them.\n\n‘Object not found’ errors often arise when no data has been assigned to a data frame object. For example, if you ran code using ‘nilt_subset’, such as sumtable(nilt_subset, ...) without first assigning data to it nilt_subset &lt;- ..., you would receive the following error message when running/knitting your code:\n\n\n\n\n\n\n\n\nWhen working with a subset of the data, ensure before the lines identified in the error message that you included code that assigns data to it (dataframe &lt;- ...). For example, if you need to work with a subset of data that used filter() or select():\n\n\n\n\n\n\n\n\nIf you already have code assigning data to the object and do so before the code that is producing the error message, try running the lines of code where you assigning the data again, or ‘Run All’. If you still receive an error message, double-check for typos and capitalisation, nilt_subset, NILT_subset, and ni1t_subset will be treated as different objects.\nIf the code was previously working, it may be the case that you have accidentally deleted code blocks / lines that assigned data to the object, added new code above the lines where you assigned data, or moved code where the lines assigning data to the object and calling it within a function are now in the wrong order.\nNote, always consider whether you actually need to create a subset. Most functions have arguments for defining which variables to use, so you do not necessarily need to provide a subset to them.\n\nIf the word count displayed at the top of your knitted HTML is wrong, check whether the line of code that calculates the word count refers to the correct RMarkdown file you are using.\nWithin the template RMarkdown document, we include an inline code block that uses a word count addin to calculate your word count within the knitted HTML file:\n\n\n\n\n\n\n\n\nWhich when knitted will look as follows in the HTML file:\n\n\n\n\n\n\n\n\nHowever, the code to calculate the word count refers to a specific file “Assignmet2-template.Rmd”. if you created a new RMarkdown file, such as one that included your student number ‘Assignment2-255615q.Rmd’, you also need to update the file name in the code:\n\n\n\n\n\n\n\n\nNote, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to “wordcountaddin::word_count(”Assignmet2-template.Rmd”) - 193”.\n\n\nIf receiving a “package ‘…’ could not be loaded” error message, check the YAML code block at the top of your RMarkdown file. Where this error message has occurred previously, it is due to using the ‘vtable’ package and the YAML code block includes pdf as an output.\nYou should have either of the following for output: ...:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut, will probably have the something like the following if receiving the error message:\n\n\n\n\n\n\n\n\nThis can happen unintentionally as RStudio can automatically add a line for PDF outline, such as if accidentally clicked ‘Knit to PDF’:\n\n\n\n\n\n\n\n\nAnnoyingly, even if you click ‘Knit to HTML’ from the options, if your YAML block includes pdf_document in its outputs, R will still try to create a PDF document and keep running into the error message.\nSo, if you are receiving the error message and have the two output lines, you can easily fix it by removing the pdf_document line:\n\n\n\n\n\n\n\n\nNote, as the YAML block is already specifying to create an HTML output, you can just click the ‘Knit’ button directly each time rather than clicking the wee down arrow to select ‘Knit to HTML’ specifically.\n\nThe most common cause of this error is from renaming the RMarkdown file, but not updating the line of code that calculates the word count. See Wrong Word Count section for general info.\nFor example, if you renamed the RMarkdwon file from ‘Assignmet2-template.Rmd’ to ‘Assignment2-255615q.Rmd’, you will receive an ‘Execution halted’ error when trying to knit:\n\n\n\n\n\n\n\n\nTo fix it, just update the line of code that calculates the word count so it refers to the new name you have given the file:\n\n\n\n\n\n\n\n\n\nTo avoid such potential issues, please ensure to submit an exported version of your knitted HTML file rather than one saved through your browser.\nIf Turnitin is providing an error message that it is unable to open your submitted HTML file, it may be due to having saved the file through your browser (e.g. right-clicking and ‘Save as…’) rather than exporting it from RStudio. Whilst HTML files saved from the browser and exported from RStudio will look absolutely identical when opened, some browsers - and browser extensions - will add additional code within the HTML file. This additional code does not change how the file looks when opened, but creates issues for Turnitin.\nTo export your knitted HTML file - 1. Check the box next to your knitted HTML file in the ‘Files’ panel. By default, the Files panel is on the bottom right of the screen.\n\n\n\n\n\n\n\n\n\nClick ‘More’ from the toolbar at the top of the Files panel.\n\n\n\n\n\n\n\n\n\n\nClick ‘Export’ from the menu options that pop-up.\n\n\n\n\n\n\n\n\n\n\nThis will pop open an ‘Export Files’ dialogue. Here, you can rename the file to include your student number. Just make sure the “.html” at the end remains included.\n\n\n\n\n\n\n\n\n\n\nAfter naming the file, click ‘Download’. This will open a dialogue to select which location to save the file.\n\n\n\n\n\n\n\n\n\nHere’s a short gif running through all the steps together.\n\n\n\n\n\n\n\n\nPlease note: Don’t panic if Turnitin is unable to open your file. We are aware of the issue and you will not be penalised if you submit a file on time, but then have to resubmit after the deadline due to Turnitin being unable to open your original submission.",
    "crumbs": [
      "Appendix 2: R Issues FAQ"
    ]
  },
  {
    "objectID": "13-RFAQ.html#introduction",
    "href": "13-RFAQ.html#introduction",
    "title": "Appendix 2: R Issues FAQ",
    "section": "",
    "text": "This appendix contains information for solving common issues students have reported when working on their interpretive findings report.\nUpdates will continue being added to this appendix with information for any new issues being reported. So, if you experience an issue, this page will be a good first port of call to check for any existing solution.",
    "crumbs": [
      "Appendix 2: R Issues FAQ"
    ]
  },
  {
    "objectID": "13-RFAQ.html#reporting-r-issues",
    "href": "13-RFAQ.html#reporting-r-issues",
    "title": "Appendix 2: R Issues FAQ",
    "section": "",
    "text": "When making a Moodle post or emailing about an issue it is vital to provide sufficient context to enable others to help find a solution. This includes:\n\nThe full text of any error message that you are receiving.\nAny relevant code that is producing the error.\n\nError messages will often flag the relevant code chunk where the error occurred:\n\n\n\n\n\n\n\n\nSo, based on the above error message you would include the code within the code block at lines 49-50.\nAdditionally, when emailing, please also include:\n\nYour lab group number.\nIf already made a Moodle post about the issue, a link to the post.",
    "crumbs": [
      "Appendix 2: R Issues FAQ"
    ]
  },
  {
    "objectID": "13-RFAQ.html#regression-results-table-issues",
    "href": "13-RFAQ.html#regression-results-table-issues",
    "title": "Appendix 2: R Issues FAQ",
    "section": "",
    "text": "If variables go missing from your regression results table or the regression results are different to how they originally appeared when you knitted the template, you may have unintentionally replaced the data included with the template.\nThe preamble code block at the top of the RMarkdown file provide with the template reads in the ‘fullnilt_2012.rds’ dataset and assigns it to the ‘nilt’ data frame object.\n\n\n\n\n\n\n\n\nYou therefore do not need to download and read in anther dataset, this is all setup for you already within the preamble code block. To resolve the issue, remove all code where you download and assign another dataset to the nilt data frame.\nFor example, these would be lines to remove:\n\n\n\n\n\n\n\n\nThe ‘NILT2012GR’ that we used in the labs has a different set of the NILT variables, and does not contain all the variables in the ‘fullnilt_2012’ dataset provided in the template. The above code then would result in variables disappearing from your regression results table.\nSimilarly, this would also be code to remove:\n\n\n\n\n\n\n\n\nThe above code replaces the original nilt data frame with a version that only includes the 5 variables listed within the selection function. Again, resulting in variables disappearing from the regression results table.\nThe select() function keeps variables passed to it and drops the others. The first argument is always the dataframe being selected from, select(dataframe, ...), with variables to keep from it listed after, select(dataframe, variable1, variable2, ...). Assigning, &lt;-, select() to the same dataframe, dataframe &lt;- select(dataframe, ..), is effectively saying, ‘from this dataframe keep these variables and permanently remove the others’. Unless your whole analysis is only going to use that selection, always assign it to a new dataframe instead, dataframe_subset &lt;- select(dataframe, ...).\nIf you need to create a subset of the data, ensure to assign it to a new data frame:\n\n\n\n\n\n\n\n\nNote, always consider whether you actually need to create a subset. Most functions have arguments for defining which variables to use, so you do not necessarily need to provide a subset to them.",
    "crumbs": [
      "Appendix 2: R Issues FAQ"
    ]
  },
  {
    "objectID": "13-RFAQ.html#object-nilt_subset-not-found",
    "href": "13-RFAQ.html#object-nilt_subset-not-found",
    "title": "Appendix 2: R Issues FAQ",
    "section": "",
    "text": "‘Object not found’ errors often arise when no data has been assigned to a data frame object. For example, if you ran code using ‘nilt_subset’, such as sumtable(nilt_subset, ...) without first assigning data to it nilt_subset &lt;- ..., you would receive the following error message when running/knitting your code:\n\n\n\n\n\n\n\n\nWhen working with a subset of the data, ensure before the lines identified in the error message that you included code that assigns data to it (dataframe &lt;- ...). For example, if you need to work with a subset of data that used filter() or select():\n\n\n\n\n\n\n\n\nIf you already have code assigning data to the object and do so before the code that is producing the error message, try running the lines of code where you assigning the data again, or ‘Run All’. If you still receive an error message, double-check for typos and capitalisation, nilt_subset, NILT_subset, and ni1t_subset will be treated as different objects.\nIf the code was previously working, it may be the case that you have accidentally deleted code blocks / lines that assigned data to the object, added new code above the lines where you assigned data, or moved code where the lines assigning data to the object and calling it within a function are now in the wrong order.\nNote, always consider whether you actually need to create a subset. Most functions have arguments for defining which variables to use, so you do not necessarily need to provide a subset to them.",
    "crumbs": [
      "Appendix 2: R Issues FAQ"
    ]
  },
  {
    "objectID": "13-RFAQ.html#wrong-word-count",
    "href": "13-RFAQ.html#wrong-word-count",
    "title": "Appendix 2: R Issues FAQ",
    "section": "",
    "text": "If the word count displayed at the top of your knitted HTML is wrong, check whether the line of code that calculates the word count refers to the correct RMarkdown file you are using.\nWithin the template RMarkdown document, we include an inline code block that uses a word count addin to calculate your word count within the knitted HTML file:\n\n\n\n\n\n\n\n\nWhich when knitted will look as follows in the HTML file:\n\n\n\n\n\n\n\n\nHowever, the code to calculate the word count refers to a specific file “Assignmet2-template.Rmd”. if you created a new RMarkdown file, such as one that included your student number ‘Assignment2-255615q.Rmd’, you also need to update the file name in the code:\n\n\n\n\n\n\n\n\nNote, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to “wordcountaddin::word_count(”Assignmet2-template.Rmd”) - 193”.",
    "crumbs": [
      "Appendix 2: R Issues FAQ"
    ]
  },
  {
    "objectID": "13-RFAQ.html#unable-to-knit",
    "href": "13-RFAQ.html#unable-to-knit",
    "title": "Appendix 2: R Issues FAQ",
    "section": "",
    "text": "If receiving a “package ‘…’ could not be loaded” error message, check the YAML code block at the top of your RMarkdown file. Where this error message has occurred previously, it is due to using the ‘vtable’ package and the YAML code block includes pdf as an output.\nYou should have either of the following for output: ...:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut, will probably have the something like the following if receiving the error message:\n\n\n\n\n\n\n\n\nThis can happen unintentionally as RStudio can automatically add a line for PDF outline, such as if accidentally clicked ‘Knit to PDF’:\n\n\n\n\n\n\n\n\nAnnoyingly, even if you click ‘Knit to HTML’ from the options, if your YAML block includes pdf_document in its outputs, R will still try to create a PDF document and keep running into the error message.\nSo, if you are receiving the error message and have the two output lines, you can easily fix it by removing the pdf_document line:\n\n\n\n\n\n\n\n\nNote, as the YAML block is already specifying to create an HTML output, you can just click the ‘Knit’ button directly each time rather than clicking the wee down arrow to select ‘Knit to HTML’ specifically.\n\nThe most common cause of this error is from renaming the RMarkdown file, but not updating the line of code that calculates the word count. See Wrong Word Count section for general info.\nFor example, if you renamed the RMarkdwon file from ‘Assignmet2-template.Rmd’ to ‘Assignment2-255615q.Rmd’, you will receive an ‘Execution halted’ error when trying to knit:\n\n\n\n\n\n\n\n\nTo fix it, just update the line of code that calculates the word count so it refers to the new name you have given the file:",
    "crumbs": [
      "Appendix 2: R Issues FAQ"
    ]
  },
  {
    "objectID": "13-RFAQ.html#turnitin-unable-to-open-html-file",
    "href": "13-RFAQ.html#turnitin-unable-to-open-html-file",
    "title": "Appendix 2: R Issues FAQ",
    "section": "",
    "text": "To avoid such potential issues, please ensure to submit an exported version of your knitted HTML file rather than one saved through your browser.\nIf Turnitin is providing an error message that it is unable to open your submitted HTML file, it may be due to having saved the file through your browser (e.g. right-clicking and ‘Save as…’) rather than exporting it from RStudio. Whilst HTML files saved from the browser and exported from RStudio will look absolutely identical when opened, some browsers - and browser extensions - will add additional code within the HTML file. This additional code does not change how the file looks when opened, but creates issues for Turnitin.\nTo export your knitted HTML file - 1. Check the box next to your knitted HTML file in the ‘Files’ panel. By default, the Files panel is on the bottom right of the screen.\n\n\n\n\n\n\n\n\n\nClick ‘More’ from the toolbar at the top of the Files panel.\n\n\n\n\n\n\n\n\n\n\nClick ‘Export’ from the menu options that pop-up.\n\n\n\n\n\n\n\n\n\n\nThis will pop open an ‘Export Files’ dialogue. Here, you can rename the file to include your student number. Just make sure the “.html” at the end remains included.\n\n\n\n\n\n\n\n\n\n\nAfter naming the file, click ‘Download’. This will open a dialogue to select which location to save the file.\n\n\n\n\n\n\n\n\n\nHere’s a short gif running through all the steps together.\n\n\n\n\n\n\n\n\nPlease note: Don’t panic if Turnitin is unable to open your file. We are aware of the issue and you will not be penalised if you submit a file on time, but then have to resubmit after the deadline due to Turnitin being unable to open your original submission.",
    "crumbs": [
      "Appendix 2: R Issues FAQ"
    ]
  },
  {
    "objectID": "12-InterpretiveReportStructure.html",
    "href": "12-InterpretiveReportStructure.html",
    "title": "Appendix 1: RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "This appendix provides additional information on using R Markdown and an overview of the structure of the R Markdown document provided with the interpretive report project template.\nPlease ensure to review the relevant lab materials:\n\n\nLab 5 session on R Markdown for an intro to R Markdown.\n\nLab 10 Session for Interpreting Quantitative Findings Report summative assessment for instructions on how to create a project on RStudio Cloud for your interpretive report using the provided template.\n\nSimilarly, if you encounter any errors, please ensure to check the R Issues FAQ first for any existing solutions.\nThe first half of this page provides an overview of R Markdown and the second half covers the Interpretive Report Template in more detail. Importantly, please ensure you at least read the Preamble Code Chunk section for information on best practice for installing and loading any additional packages.\n\nR is a programming language used for data analysis and visualisation. A large community has built up around R through the combination of it being free and open-source software and a full featured programming language. This makes it possible for the community to contribute to and extend upon what it is possible to do with R. The Comprehensive R Archive Network, where packages are downloaded from when you install.packages(), helps illustrate the power that comes from an open-source community. It now lists 21,810 packages created by the R community.\nThese benefits are behind the rapid adoption of R within the social sciences, and academia and data analysis more broadly. However, compared to expensive closed source software where most analysis could be done through menus and mouse-clicks, using R requires learning the programming language. This is not the disadvantage it may at first seem. It does add an additional hurdle at the very start, but it also provides a far more powerful and flexible way to do data analysis. Menus are also not as simple and accessible as may first seem. A graph that can be created with a few lines of code in R may require going through multiple menus with 20+ mouse-clicks and entering text into multiple fields.\n\nRStudio is what is known as an ‘integrated development environment’, which is just a technical way of saying that it easier to write and run R code, manage files, view outputs, and so on within a (somewhat) user-friendly interface. R strictly speaking is merely the programming language. It comes bundled with a ‘command line console’ that runs the code, which is all you would see if you opened plain ‘R’ on your desktop.\nWithin RStudio the R console is placed in the bottom-left panel. The top-left is where ‘Sources’, such as R scripts and RMarkdown files, are opened.When you run a script, RMarkdown file, or individual code chunks, the lines of code they contain are sent by RStudio to the console. RStudio will then, as appropriate, add outputs from the console below code chunks or display them in the bottom-right Viewer panel.\nThe top-right panel includes the main R environment. The environment is the work space where all data and functions are stored during a session. When code that reads in a dataset is run, it stores the read dataset as a dataframe object in the environment. Loading a package similarly adds its functions as objects available in the environment.\n\nObjects are created in R by ‘assigning’ values/data to them. For example, the code below creates objects with the names a and b with numeric values 1 and 2 assigned to them respectively.\n\na &lt;- 1\nb &lt;- 2\n\nAfter running the above, the objects can be referred to by name in subsequent code, such as adding the values they contain together:\n\na + b\n\n[1] 3\n\n\nNote though that whilst R calculates the values of the two objects combined is 3, this output is not stored, as it is not being assigned to an object. If we wanted to store the value we could assign it to a new object:\n\nc &lt;- a + b # 1 + 2 = 3, so 'c' is assigned the value 3\n\nIn contrast, if we were to assign the a + b back to a, it is effectively saying “add a and b together and update a to store the combined value”.\n\na &lt;- a + b # 1 + 2 = 3, so 'a' is now assigned the value 3\na &lt;- a + b # 3 + 2 = 5, so 'a' is now assigned the value 5\na &lt;- a + b # 5 + 2 = 7, so 'a' is now assigned the value 7\na\n\n[1] 7\n\n\nThis is why it is important to ensure you create objects for values/data you want to store and take care that you do not accidentally assign changes to objects that you do not intend to make. If you receive an object not found error it is likely you have either mistyped the name of the object or have not assigned anything to it yet. If variables suddenly disappear or your graphs unexpectedly show different results when re-run, it is likely that you ran code that assigned a changed version of the data back to the object storing it.\n\ndataframe$variable\nrage\n\nreading in data sets\n\n\n\n\nRMarkdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with Code Chunks that can be converted to multiple file formats, such as HTML or PDF. This makes RMarkdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis.\nRMarkdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text (see the Formatting section below). Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. RMarkdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text and passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format.\n\nRMarkdown uses simple, human-readable syntax for basic formatting. Whilst different to how you would write text in a Word document, it is easy to learn. This simple transparent formatting also helps avoid the hidden complexities of Word document, where formatting issues often arise from inconsistent syntax that is hidden from the user.\nBelow is a screenshot from RStudio with text using the key basic syntax for formatting your main text:\n\n\n\n\n\n\n\n\nAnd, this is how it appears when knitted:\n\n\n\n\n\n\n\n\nHeadings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level:\n\n\n\n\n\n\n\n\nAnd, how it appears when knitted:\n\n\n\n\n\n\n\n\n\nThe main thing in RMarkdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between:\n\nEach paragraph\nBefore and after a list\nBefore and after a header\n\nHere’s some example text in an RMarkdown document, the first without line spacing, the second with:\n\n\n\n\n\n\n\n\nAnd, how this looks when knitted:\n\n\n\n\n\n\n\n\nSide-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in RMarkdown.\n\nThere are three main ways to create a code chunk:\n\nClick the ‘Insert Code Chunk’ button and from the drop-down select ‘R’ at the top. (See gif below)\nPress Ctrl + Alt + I (Windows & Linux) or Cmd + Option + I (macOS)\nManually typing three back-ticks with ‘r’ in curly brackets before your code and adding three back-ticks after.\n\n\n\n\n\n\n\n\n\nCurly brackets at the start of a code chunk are used to specify {programming-language optional name, options = values}. Since we are using R, all our code chunks have {r ...}. Within code chunks, the pound sign / hash, #, at the start of a line is used to add any comments:\n\n\n\n\n\n\n\n\nSide-note, this may seem confusing given hash signs are used for headers in the main text. However, inside of a code chunk, all text is treated as code for the language specified in the curly brackets. In R, hash signs are used for comments, so they are treated as comments within the code chunk rather than as Markdown headers.\n\nAdding a label is useful as a quick way to remind yourself the purpose of each code chunk. The quick outline (button at bottom of Source panel) can be used to jump to specific sections and code chunks in your document. If you provide a label for your code chunk it will also appear here:\n\n\n\n\n\n\n\n\n\nOptions are used to specify how code chunks are handled when run/knitted. Two key ones we have used in the labs and in the project template:\n\n\necho - whether the code chunk is displayed in knitted files.\n\ninclude - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files.\n\nBy default, the code chunk and the output are displayed in knitted files. For the interpretive report, you do not want to display the code chunks in your knitted HTML file, so ensure you add echo=FALSE to the options for any new code chunk you create. Alternatively, see the Setup Code Chunk section for information on how to set echo=FALSE as a ‘global option’.\nExamples when working in RMarkdown document:\n\n\n\n\n\n\n\n\nAnd when knitted:\n\n\n\n\n\n\n\n\n\nThere are multiple ways to run your R code.\nWithin the top-right of each code chunk there are two buttons:\n\n\n\n\n\n\n\n\n\nRun Current Chunk - which will run the code within that code chunks only.\nRun All Chunks Above - which will run all code chunks from the top of your document down to this code chunk.\n\nThe second is useful when a code chunk depends upon others being run first. Remember the R environment maintains the results of any previously run code, so you do not need to continuously run all previous chunks. However, this can be useful when debugging issues or if you restart your R environment, losing the results of previously run code, and needing to re-run everything before this chunk.\nFurther options can be found in the ‘Run’ drop-down menu, accessed from the top-right of the Source Panel:\n\n\n\n\n\n\n\n\nUseful additional ones here are:\n\n\nRun Selected Line(s) - running all lines you have manually selected using the mouse / text cursor.\n\nRestart R and Run All Chunks - incredibly useful when you need to reset your R environment, this can be useful when debugging an error to figure out whether the issue stems from your current code or code you previously ran but now removed.\n\nNote as well that many of these options have keyboard shortcuts listed. Learning these pays off long-term as you will be able to write and run your code without needing to move your hands from the keyboard.\n\nKnitting a document runs all your R code from top to bottom, then combines the results from this with the Markdown text to convert these into different file formats. YAML is used to specify which file formats to convert to; see the YAML Block section for information on how this is set up within the project template.\nImportantly, when you knit an RMarkdown document, all code is run sequentially from top to bottom in a clean R environment. This ensures the document is reproducible. Anyone with a copy can knit it and produce the same results. This requires though that code to load any required packages are included within a code chunk in the document. Simply loading a package via the Console adds it to your current R environment and manually run code chunks will be able to access the package. However, when knitting, the clean R environment won’t have access to the package, and you’ll receive an error message when the knit process tries to run a function that requires the package. This is why it is important to include a code chunk at the top of your document that loads all required packages. See the Preamble Code Chunk for how to do this in the RMarkdown file provided in the interpretive report project template.\nAssuming you have YAML specifying which file format(s) to convert to, all you then need to do each time is simply click the ‘Knit’ button:\n\n\n\n\n\n\n\n\n\nIt is possible to access an RMarkdown cheat sheet (as well as ones for ggplot2 and dplyr!) from within RStudio:\n\n\n\n\n\n\n\n\nNote, from the same ‘Help’ menu, just below ‘Cheat Sheets’ is an option for ‘Keyboard Shortcuts Help’. This displays a screen with most of the shortcuts available within RStudio. This also includes a link to ‘See All Shortcuts…’.\n\n\n\n\n\n\n\n\n\n\nThe top of the RMarkdown document includes a YAML block, enclosed with three dashes, ---, at the start and end. The YAML block must be at the very start of the document, so do not add any new lines before it.\nYAML is often used with Markdown as it provides a simple human-readable structure for configuring metadata, such as the document title, author, date, and desired output when knitting:\n\n\n\n\n\n\n\n\nPlease remember to add your student number - and not your name - to line 3.\nTo save having to remember to update the date before knitting and submitting your final version, you can use the following in the YAML block:\n\ndate: \"`r format(Sys.time(), '%d/%m/%y')`\"\n\nWhen knitted that code will automatically add the current date in dd/mm/yy format (e.g. 04/12/24):\n\n\n\n\n\n\n\n\n\nAfter the YAML Block, there is a ‘setup’ code chunk that sets global options for knitr. The include=FALSE in its options means that this chunk and its output are not displayed in your knitted files:\n\n\n\n\n\n\n\n\nGlobal options apply to all code chunks in the document. This means anything set in the global options does not have to be individually added to each code chunk. Note, you can set a global option and still set a different one for a specific code chunk where needed. For example, if you set option=FALSE globally, but needed it to be TRUE for a specific code chunk, all you need to do is add option=TRUE to its options to override the global default.\nThe global options set in the setup code chunk are:\n\n\nmessage=FALSE, which hides all non-warning messages when knitting code chunks. For example, when loading a library it will sometimes display non-warning messages.\n\nwarning=FALSE, which hides all warning messages when knitting code chunks. Again, making sure that no warnings messages displayed when running code is included in your knitted file.\n\nImportantly, you do not want to display your code chunks in your knitted file, only the tables and graphs they generate. To achieve this you could add echo=FALSE to the options for each individual code chunk. Or – you can simply add it as a global option in the setup code chunk!\n\n\n\n\n\n\n\n\n\nIt is best practice to install and load any required packages and read in any data being used at the top of the RMarkdown file. This keeps all package management and data reading in a single location at the start of the document, making your code simpler and clearer. It further ensures that when running/knitting your code that all required packages are loaded and dataframes created before the rest of your code is run.\nThe preamble code block within the template provides a structure following this practice.\n\n\n\n\n\n\n\n\n\n\nInstall packages if missing, outlined in pink. This code may look complex, but all it does is create a list of packages that are going to be used, assigning it to the object list.of.packages. This list is checked against packages already installed, creating a new list new.packages containing only the names of packages not already installed. The final line then basically says “if the length of new.packages is 1 or more, then install all packages in the new.packages list”.\n\nWord count addin if missing, outlined in blue. There is not an R package for calculating word counts, but there is what is known as an ‘addin’ for RStudio. The code here checks if the addin is already installed and if not it downloads a copy of the addin from GitHub. Please see the Word Count Code section for further information on how this addin is used in the document.\n\nLoad packages, outlined in yellow. This is where all packages used in the analysis are loaded.\n\nRead data, outlined in red. The dataset used for the assignment in read in and assigned to the nilt dataframe object.\n\nImportant, as a dataset is provided with the project template and read in already for you, you do not need to download or read in any other dataset. The assignment uses a version of the NILT dataset that includes more variables than the version of the data we used in the lab sessions. Downloading and reading in the dataset used in the labs will result in variables disappearing from the regression results table. Please see the R Issues FAQ for more info.\nIf you require additional packages, the best way to add this would be:\n\nWithin the Install packages if missing section, add the package name at the end of the list that is being assigned to ‘list.of.packages’\nWithin the Load packages section, add a new library() line under the existing ones.\n\nFor example, if needing to install and load ‘vtable’ these are the changes that you would make:\n\n\n\n\n\n\n\n\nAs a gif:\n\n\n\n\n\n\n\n\nNote, moving all code for installing and loading of packages into this preamble code chunk means you do not need, and can safely remove, any other install.package() and library() lines from the rest of your code chunks. This will reduce risk of encountering error messages and make it easier to debug them if any do occur.\n\nTables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. Note, you will need to knit your document each time you want to check the updated word count.\nThis is how the code to calculate the word count looks within the RMarkdown file:\n\n\n\n\n\n\n\n\nSide-note, surrounding code with single back-ticks, (`), creates an “inline code chunk”, enabling you to add short snippets of code. The r at the start specifies that the code is R, similar to adding r in curly brackets for code chunks.\nThis is then how it looks when knitting the project template (assuming you haven’t added any additional text yet):\n\n\n\n\n\n\n\n\nWe use this addin as the built-in RStudio word count, accessed via the ‘Edit’ menu at the top of the screen, includes all code and comments. So, despite the addin calculating ‘0’, RStudio will provide:\n\n\n\n\n\n\n\n\nImportant, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to wordcountaddin::word_count(\"Assignmet2-template.Rmd\") - 193.\n\nThe RMarkdown document provided in the project template includes headed sections you can use alongside suggested word count for each and brief summary reminder of what to include in each section. The Course Handbook, pages 26-30, provide a more detailed breakdown of what to include in each section.\nScreenshot of the outline and commented suggestions:\n\n\n\n\n\n\n\n\nImportant, within the “Results and discussion” section, ensure to add your interpretation after the code chunk that creates the regression results table:\n\n\n\n\n\n\n\n\n\nThe code chunk that runs the multiple linear regression model and creates the table with the regression results can be found in the ‘Results and discussion’ section:\n\n\n\n\n\n\n\n\n\nFirst the model is run, outlined in pink, and assigned to the model object.\nNext a list with labels to use for the independent and control variables in the regression table is created, outlined in blue, and assigned to the cov_labels object.\nFinally, the stargazer package is used to create the regression results table, outlined in red. It is passed the model and cov_label objects, highlighted in yellow, alongside arguments for outputting the table in HTML, a title, and caption & label to use for the dependent variable.\n\nStargazer produces well-formatted regression tables, but with the downside that when running the code chunk in RStudio, you will only see the raw HTML syntax that is created:\n\n\n\n\n\n\n\n\nIn order to view the table, you will need to knit your document and view the outputted HTML file:\n\n\n\n\n\n\n\n\nYour knitted HTML file should then open in a new popup window. However, you might instead receive a dialogue window saying that a pop-up was prevented from opening, if so just click ‘Try again’:\n\n\n\n\n\n\n\n\nAlternatively, you can open the last knitted version of your HTML file from the ‘Files’ panel. Click the HTML file and select ‘View in Web Browser’:\n\n\n\n\n\n\n\n\nBelow is how your regression results table should look in the knitted HTML file:\n\n\n\n\n\n\n\n\nIf it doesn’t, please see the R Issues FAQ.",
    "crumbs": [
      "Appendix 1: RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "12-InterpretiveReportStructure.html#introduction",
    "href": "12-InterpretiveReportStructure.html#introduction",
    "title": "Appendix 1: RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "This appendix provides additional information on using R Markdown and an overview of the structure of the R Markdown document provided with the interpretive report project template.\nPlease ensure to review the relevant lab materials:\n\n\nLab 5 session on R Markdown for an intro to R Markdown.\n\nLab 10 Session for Interpreting Quantitative Findings Report summative assessment for instructions on how to create a project on RStudio Cloud for your interpretive report using the provided template.\n\nSimilarly, if you encounter any errors, please ensure to check the R Issues FAQ first for any existing solutions.\nThe first half of this page provides an overview of R Markdown and the second half covers the Interpretive Report Template in more detail. Importantly, please ensure you at least read the Preamble Code Chunk section for information on best practice for installing and loading any additional packages.",
    "crumbs": [
      "Appendix 1: RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "12-InterpretiveReportStructure.html#r-basics",
    "href": "12-InterpretiveReportStructure.html#r-basics",
    "title": "Appendix 1: RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "R is a programming language used for data analysis and visualisation. A large community has built up around R through the combination of it being free and open-source software and a full featured programming language. This makes it possible for the community to contribute to and extend upon what it is possible to do with R. The Comprehensive R Archive Network, where packages are downloaded from when you install.packages(), helps illustrate the power that comes from an open-source community. It now lists 21,810 packages created by the R community.\nThese benefits are behind the rapid adoption of R within the social sciences, and academia and data analysis more broadly. However, compared to expensive closed source software where most analysis could be done through menus and mouse-clicks, using R requires learning the programming language. This is not the disadvantage it may at first seem. It does add an additional hurdle at the very start, but it also provides a far more powerful and flexible way to do data analysis. Menus are also not as simple and accessible as may first seem. A graph that can be created with a few lines of code in R may require going through multiple menus with 20+ mouse-clicks and entering text into multiple fields.\n\nRStudio is what is known as an ‘integrated development environment’, which is just a technical way of saying that it easier to write and run R code, manage files, view outputs, and so on within a (somewhat) user-friendly interface. R strictly speaking is merely the programming language. It comes bundled with a ‘command line console’ that runs the code, which is all you would see if you opened plain ‘R’ on your desktop.\nWithin RStudio the R console is placed in the bottom-left panel. The top-left is where ‘Sources’, such as R scripts and RMarkdown files, are opened.When you run a script, RMarkdown file, or individual code chunks, the lines of code they contain are sent by RStudio to the console. RStudio will then, as appropriate, add outputs from the console below code chunks or display them in the bottom-right Viewer panel.\nThe top-right panel includes the main R environment. The environment is the work space where all data and functions are stored during a session. When code that reads in a dataset is run, it stores the read dataset as a dataframe object in the environment. Loading a package similarly adds its functions as objects available in the environment.\n\nObjects are created in R by ‘assigning’ values/data to them. For example, the code below creates objects with the names a and b with numeric values 1 and 2 assigned to them respectively.\n\na &lt;- 1\nb &lt;- 2\n\nAfter running the above, the objects can be referred to by name in subsequent code, such as adding the values they contain together:\n\na + b\n\n[1] 3\n\n\nNote though that whilst R calculates the values of the two objects combined is 3, this output is not stored, as it is not being assigned to an object. If we wanted to store the value we could assign it to a new object:\n\nc &lt;- a + b # 1 + 2 = 3, so 'c' is assigned the value 3\n\nIn contrast, if we were to assign the a + b back to a, it is effectively saying “add a and b together and update a to store the combined value”.\n\na &lt;- a + b # 1 + 2 = 3, so 'a' is now assigned the value 3\na &lt;- a + b # 3 + 2 = 5, so 'a' is now assigned the value 5\na &lt;- a + b # 5 + 2 = 7, so 'a' is now assigned the value 7\na\n\n[1] 7\n\n\nThis is why it is important to ensure you create objects for values/data you want to store and take care that you do not accidentally assign changes to objects that you do not intend to make. If you receive an object not found error it is likely you have either mistyped the name of the object or have not assigned anything to it yet. If variables suddenly disappear or your graphs unexpectedly show different results when re-run, it is likely that you ran code that assigned a changed version of the data back to the object storing it.\n\ndataframe$variable\nrage\n\nreading in data sets",
    "crumbs": [
      "Appendix 1: RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "12-InterpretiveReportStructure.html#rmarkdown",
    "href": "12-InterpretiveReportStructure.html#rmarkdown",
    "title": "Appendix 1: RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "RMarkdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with Code Chunks that can be converted to multiple file formats, such as HTML or PDF. This makes RMarkdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis.\nRMarkdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text (see the Formatting section below). Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. RMarkdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text and passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format.\n\nRMarkdown uses simple, human-readable syntax for basic formatting. Whilst different to how you would write text in a Word document, it is easy to learn. This simple transparent formatting also helps avoid the hidden complexities of Word document, where formatting issues often arise from inconsistent syntax that is hidden from the user.\nBelow is a screenshot from RStudio with text using the key basic syntax for formatting your main text:\n\n\n\n\n\n\n\n\nAnd, this is how it appears when knitted:\n\n\n\n\n\n\n\n\nHeadings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level:\n\n\n\n\n\n\n\n\nAnd, how it appears when knitted:\n\n\n\n\n\n\n\n\n\nThe main thing in RMarkdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between:\n\nEach paragraph\nBefore and after a list\nBefore and after a header\n\nHere’s some example text in an RMarkdown document, the first without line spacing, the second with:\n\n\n\n\n\n\n\n\nAnd, how this looks when knitted:\n\n\n\n\n\n\n\n\nSide-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in RMarkdown.\n\nThere are three main ways to create a code chunk:\n\nClick the ‘Insert Code Chunk’ button and from the drop-down select ‘R’ at the top. (See gif below)\nPress Ctrl + Alt + I (Windows & Linux) or Cmd + Option + I (macOS)\nManually typing three back-ticks with ‘r’ in curly brackets before your code and adding three back-ticks after.\n\n\n\n\n\n\n\n\n\nCurly brackets at the start of a code chunk are used to specify {programming-language optional name, options = values}. Since we are using R, all our code chunks have {r ...}. Within code chunks, the pound sign / hash, #, at the start of a line is used to add any comments:\n\n\n\n\n\n\n\n\nSide-note, this may seem confusing given hash signs are used for headers in the main text. However, inside of a code chunk, all text is treated as code for the language specified in the curly brackets. In R, hash signs are used for comments, so they are treated as comments within the code chunk rather than as Markdown headers.\n\nAdding a label is useful as a quick way to remind yourself the purpose of each code chunk. The quick outline (button at bottom of Source panel) can be used to jump to specific sections and code chunks in your document. If you provide a label for your code chunk it will also appear here:\n\n\n\n\n\n\n\n\n\nOptions are used to specify how code chunks are handled when run/knitted. Two key ones we have used in the labs and in the project template:\n\n\necho - whether the code chunk is displayed in knitted files.\n\ninclude - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files.\n\nBy default, the code chunk and the output are displayed in knitted files. For the interpretive report, you do not want to display the code chunks in your knitted HTML file, so ensure you add echo=FALSE to the options for any new code chunk you create. Alternatively, see the Setup Code Chunk section for information on how to set echo=FALSE as a ‘global option’.\nExamples when working in RMarkdown document:\n\n\n\n\n\n\n\n\nAnd when knitted:\n\n\n\n\n\n\n\n\n\nThere are multiple ways to run your R code.\nWithin the top-right of each code chunk there are two buttons:\n\n\n\n\n\n\n\n\n\nRun Current Chunk - which will run the code within that code chunks only.\nRun All Chunks Above - which will run all code chunks from the top of your document down to this code chunk.\n\nThe second is useful when a code chunk depends upon others being run first. Remember the R environment maintains the results of any previously run code, so you do not need to continuously run all previous chunks. However, this can be useful when debugging issues or if you restart your R environment, losing the results of previously run code, and needing to re-run everything before this chunk.\nFurther options can be found in the ‘Run’ drop-down menu, accessed from the top-right of the Source Panel:\n\n\n\n\n\n\n\n\nUseful additional ones here are:\n\n\nRun Selected Line(s) - running all lines you have manually selected using the mouse / text cursor.\n\nRestart R and Run All Chunks - incredibly useful when you need to reset your R environment, this can be useful when debugging an error to figure out whether the issue stems from your current code or code you previously ran but now removed.\n\nNote as well that many of these options have keyboard shortcuts listed. Learning these pays off long-term as you will be able to write and run your code without needing to move your hands from the keyboard.\n\nKnitting a document runs all your R code from top to bottom, then combines the results from this with the Markdown text to convert these into different file formats. YAML is used to specify which file formats to convert to; see the YAML Block section for information on how this is set up within the project template.\nImportantly, when you knit an RMarkdown document, all code is run sequentially from top to bottom in a clean R environment. This ensures the document is reproducible. Anyone with a copy can knit it and produce the same results. This requires though that code to load any required packages are included within a code chunk in the document. Simply loading a package via the Console adds it to your current R environment and manually run code chunks will be able to access the package. However, when knitting, the clean R environment won’t have access to the package, and you’ll receive an error message when the knit process tries to run a function that requires the package. This is why it is important to include a code chunk at the top of your document that loads all required packages. See the Preamble Code Chunk for how to do this in the RMarkdown file provided in the interpretive report project template.\nAssuming you have YAML specifying which file format(s) to convert to, all you then need to do each time is simply click the ‘Knit’ button:\n\n\n\n\n\n\n\n\n\nIt is possible to access an RMarkdown cheat sheet (as well as ones for ggplot2 and dplyr!) from within RStudio:\n\n\n\n\n\n\n\n\nNote, from the same ‘Help’ menu, just below ‘Cheat Sheets’ is an option for ‘Keyboard Shortcuts Help’. This displays a screen with most of the shortcuts available within RStudio. This also includes a link to ‘See All Shortcuts…’.",
    "crumbs": [
      "Appendix 1: RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "12-InterpretiveReportStructure.html#interpretive-report-template",
    "href": "12-InterpretiveReportStructure.html#interpretive-report-template",
    "title": "Appendix 1: RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "The top of the RMarkdown document includes a YAML block, enclosed with three dashes, ---, at the start and end. The YAML block must be at the very start of the document, so do not add any new lines before it.\nYAML is often used with Markdown as it provides a simple human-readable structure for configuring metadata, such as the document title, author, date, and desired output when knitting:\n\n\n\n\n\n\n\n\nPlease remember to add your student number - and not your name - to line 3.\nTo save having to remember to update the date before knitting and submitting your final version, you can use the following in the YAML block:\n\ndate: \"`r format(Sys.time(), '%d/%m/%y')`\"\n\nWhen knitted that code will automatically add the current date in dd/mm/yy format (e.g. 04/12/24):\n\n\n\n\n\n\n\n\n\nAfter the YAML Block, there is a ‘setup’ code chunk that sets global options for knitr. The include=FALSE in its options means that this chunk and its output are not displayed in your knitted files:\n\n\n\n\n\n\n\n\nGlobal options apply to all code chunks in the document. This means anything set in the global options does not have to be individually added to each code chunk. Note, you can set a global option and still set a different one for a specific code chunk where needed. For example, if you set option=FALSE globally, but needed it to be TRUE for a specific code chunk, all you need to do is add option=TRUE to its options to override the global default.\nThe global options set in the setup code chunk are:\n\n\nmessage=FALSE, which hides all non-warning messages when knitting code chunks. For example, when loading a library it will sometimes display non-warning messages.\n\nwarning=FALSE, which hides all warning messages when knitting code chunks. Again, making sure that no warnings messages displayed when running code is included in your knitted file.\n\nImportantly, you do not want to display your code chunks in your knitted file, only the tables and graphs they generate. To achieve this you could add echo=FALSE to the options for each individual code chunk. Or – you can simply add it as a global option in the setup code chunk!\n\n\n\n\n\n\n\n\n\nIt is best practice to install and load any required packages and read in any data being used at the top of the RMarkdown file. This keeps all package management and data reading in a single location at the start of the document, making your code simpler and clearer. It further ensures that when running/knitting your code that all required packages are loaded and dataframes created before the rest of your code is run.\nThe preamble code block within the template provides a structure following this practice.\n\n\n\n\n\n\n\n\n\n\nInstall packages if missing, outlined in pink. This code may look complex, but all it does is create a list of packages that are going to be used, assigning it to the object list.of.packages. This list is checked against packages already installed, creating a new list new.packages containing only the names of packages not already installed. The final line then basically says “if the length of new.packages is 1 or more, then install all packages in the new.packages list”.\n\nWord count addin if missing, outlined in blue. There is not an R package for calculating word counts, but there is what is known as an ‘addin’ for RStudio. The code here checks if the addin is already installed and if not it downloads a copy of the addin from GitHub. Please see the Word Count Code section for further information on how this addin is used in the document.\n\nLoad packages, outlined in yellow. This is where all packages used in the analysis are loaded.\n\nRead data, outlined in red. The dataset used for the assignment in read in and assigned to the nilt dataframe object.\n\nImportant, as a dataset is provided with the project template and read in already for you, you do not need to download or read in any other dataset. The assignment uses a version of the NILT dataset that includes more variables than the version of the data we used in the lab sessions. Downloading and reading in the dataset used in the labs will result in variables disappearing from the regression results table. Please see the R Issues FAQ for more info.\nIf you require additional packages, the best way to add this would be:\n\nWithin the Install packages if missing section, add the package name at the end of the list that is being assigned to ‘list.of.packages’\nWithin the Load packages section, add a new library() line under the existing ones.\n\nFor example, if needing to install and load ‘vtable’ these are the changes that you would make:\n\n\n\n\n\n\n\n\nAs a gif:\n\n\n\n\n\n\n\n\nNote, moving all code for installing and loading of packages into this preamble code chunk means you do not need, and can safely remove, any other install.package() and library() lines from the rest of your code chunks. This will reduce risk of encountering error messages and make it easier to debug them if any do occur.\n\nTables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. Note, you will need to knit your document each time you want to check the updated word count.\nThis is how the code to calculate the word count looks within the RMarkdown file:\n\n\n\n\n\n\n\n\nSide-note, surrounding code with single back-ticks, (`), creates an “inline code chunk”, enabling you to add short snippets of code. The r at the start specifies that the code is R, similar to adding r in curly brackets for code chunks.\nThis is then how it looks when knitting the project template (assuming you haven’t added any additional text yet):\n\n\n\n\n\n\n\n\nWe use this addin as the built-in RStudio word count, accessed via the ‘Edit’ menu at the top of the screen, includes all code and comments. So, despite the addin calculating ‘0’, RStudio will provide:\n\n\n\n\n\n\n\n\nImportant, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to wordcountaddin::word_count(\"Assignmet2-template.Rmd\") - 193.\n\nThe RMarkdown document provided in the project template includes headed sections you can use alongside suggested word count for each and brief summary reminder of what to include in each section. The Course Handbook, pages 26-30, provide a more detailed breakdown of what to include in each section.\nScreenshot of the outline and commented suggestions:\n\n\n\n\n\n\n\n\nImportant, within the “Results and discussion” section, ensure to add your interpretation after the code chunk that creates the regression results table:\n\n\n\n\n\n\n\n\n\nThe code chunk that runs the multiple linear regression model and creates the table with the regression results can be found in the ‘Results and discussion’ section:\n\n\n\n\n\n\n\n\n\nFirst the model is run, outlined in pink, and assigned to the model object.\nNext a list with labels to use for the independent and control variables in the regression table is created, outlined in blue, and assigned to the cov_labels object.\nFinally, the stargazer package is used to create the regression results table, outlined in red. It is passed the model and cov_label objects, highlighted in yellow, alongside arguments for outputting the table in HTML, a title, and caption & label to use for the dependent variable.\n\nStargazer produces well-formatted regression tables, but with the downside that when running the code chunk in RStudio, you will only see the raw HTML syntax that is created:\n\n\n\n\n\n\n\n\nIn order to view the table, you will need to knit your document and view the outputted HTML file:\n\n\n\n\n\n\n\n\nYour knitted HTML file should then open in a new popup window. However, you might instead receive a dialogue window saying that a pop-up was prevented from opening, if so just click ‘Try again’:\n\n\n\n\n\n\n\n\nAlternatively, you can open the last knitted version of your HTML file from the ‘Files’ panel. Click the HTML file and select ‘View in Web Browser’:\n\n\n\n\n\n\n\n\nBelow is how your regression results table should look in the knitted HTML file:\n\n\n\n\n\n\n\n\nIf it doesn’t, please see the R Issues FAQ.",
    "crumbs": [
      "Appendix 1: RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2022. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nFogarty, Brian J. 2019. Quantitative Social Science Data with R: An Introduction. Los Angeles London New Delhi Singapore Washington DC Melbourne: SAGE.\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion to Applied Regression. Third. Thousand Oaks CA: Sage. https://socialsciences.mcmaster.ca/jfox/Books/Companion/.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2021. Car: Companion to Applied Regression. https://CRAN.R-project.org/package=car.\n\n\nHorst, Alison. n.d. “GitHub - Allisonhorst/Stats-Illustrations: R & Stats Illustrations by @Allison_horst.” Accessed July 11, 2022. https://github.com/allisonhorst/stats-illustrations.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2021. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/bookdown.\n\n\n———. 2019. “TinyTeX: A Lightweight, Cross-Platform, and Easy-to-Maintain LaTeX Distribution Based on TeX Live.” TUGboat, no. 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html.\n\n\n———. 2021. Tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents. https://github.com/yihui/tinytex.\n\n\n———. 2022. Bookdown: Authoring Books and Technical Documents with r Markdown. https://CRAN.R-project.org/package=bookdown.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "08-Lab8.html",
    "href": "08-Lab8.html",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "In the previous lab we learned about correlation. We visualised the relationship of different types of variables. Also, we computed one correlation measure for two numeric variables, Pearson correlation. This measure is useful to compute the strength and the direction of the association. However, it presents some limitations, e.g. it can only be used for numeric variables, it allows only one variable at a time, and it is appropriate to describe a linear relationship.\nLinear regression can overcome some of these limitations and it can be extended to achieve further purposes (e.g. use multiple variables or estimate scenarios). This technique is in fact very common and one of the most popular in quantitative research in social sciences. Therefore, getting familiar with it will be important for you not only to perform your own analyses, but also to interpret and critically read literature.\n\nIMPORTANT: Linear regression is appropriate only when the dependent variable is numeric (interval/ratio).\nHowever, the independent variables can be categorical, ordinal, or numeric. Also, you can include more than one independent variable to evaluate how these relate to the dependent variable. In this lab we will start using only one explanatory variable. This is know as simple linear regression.\n\nFor this lab, we will continue using the 2012 NILT survey to introduce linear models. To do so, please set your RStudio environment as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud;\nCreate a copy of the project called ‘NILT2’ which is located in your ‘Quants lab group’ by clicking on the ‘Start’ button;\nOnce in the project, create a new ‘R Script’ file (a simple R script NOT an .Rmd file);\nSave the R file as ‘linear_model_intro’.\n\nReproduce the code below, by copying, pasting and running it from your new script and focus on the intuitive part of this section.\nFirst, load the tidyverse library and read the nilt_r_object.rds file which is stored in the folder called ‘data’ and contains the NILT survey (tidyverse was installed in your session already).\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nThis dataset is exactly the same as the one you processed in previous labs.\nWe will re-revisit the example of the respondent’s spouse/partner characteristics. To keep things simple, let’s create a minimal sample of the dataset including only 40 random observations:\n\n# select a small random sample\nset.seed(3)\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- filter(nilt, !is.na(spage)) %&gt;% sample_n(40)\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- select(nilt_sample, rage, spage)\n\nWe will start by creating a scatter plot using the respondent’s spouse/partner age spage on the Y axis, and the respondent’s rage on the X axis.\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nAs you can see, even with this minimal example using 40 observations, there is a clear trend. From the plot, it seems intuitive to draw a line that describes this general trend. Imagine a friend will draw this line for you. You will have to tell them some basic references on how to do it. You can, for example, specify a start point and an end point in the plot. Alternatively, which is what we will do below, you can specify a start point and a value that describes the slope of the line. For now, our friend is ggplot. You have to pass these two values (start point and slope) in the geom_abline() function to draw a line that describes these points.\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nIn the guess above, it is assumed that the age of the spouse/partner might be exactly the same as the respondent. To draw a line like this, we used 0 as the starting value in the geom_abline() function. In statistics, this is known as the intercept and it is often represented with a Greek letter and a zero sub-index as \\(\\beta_0\\) (beta-naught) or with \\(\\alpha\\) (alpha). The location of the intercept can be found along the vertical axis (in this case it is not visible). The second value we passed to describe the line is the slope, which uses the X axis as the reference. The slope value is multiplied by the value of the X axis. Therefore, a slope of 0 is completely horizontal. In this example, a slope of 1 produces a line at 45º. The slope is often represented with the Greek letter beta and a sequential numeric sub-index like this: \\(\\beta_1\\) (beta-one).\nWe will create some points in the data frame based on these two values, the start point and the steepness factor or, formally speaking, the intercept and the slope respectively. We will store the results in a column called line1 and print the head of this data set.\n\n# create values for the guess line 1\nnilt_sample &lt;- nilt_sample %&gt;%\n  mutate(line1 = 0 + rage * 1)\n# print results\nhead(nilt_sample)\n\n# A tibble: 6 × 3\n   rage spage     line1\n  &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;\n1    40 39           40\n2    54 51           54\n3    66 59           66\n4    60 71           60\n5    35 34           35\n6    67 57           67\n\n\nFrom the above, you can notice that the respondent’s age rage and the column that we just created line1, are identical…So, that is our guess for now. The partner’s age is the same as the respondent’s.\nIs the line we just drew good enough to represent the general trend in the relationship above? To answer this question, we can measure the distance of each point of the plot to the line 1, as shown by the dashed segment in the plot below.\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_segment(aes(xend = rage, yend = line1), linetype = \"dashed\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nThis distance difference is known as the residual or the error and it is often expressed with the Greek letter \\(\\epsilon\\) (epsilon). Numerically, we can calculate this by computing the difference between the known value (the observed) and the guessed number. Formally, the guessed number is called the expected value (also known as predicted/estimated value). Normally, the expected values in statistics are represented by putting a hat like this \\(\\hat{}\\) on the letters. Since the independent variable is usually located on the Y axis, the expected value is differentiated from the observed values by putting the hat on the Y like this: \\(\\hat{y}\\) (y-hat).\nLet’s estimate the residuals for each of the observations by subtracting \\(\\hat{y}_i\\) from \\(y_i\\) (spage - line1) and store it in a column called residuals. Print the head of the data set after.\n\n# estimate residuals\nnilt_sample &lt;- nilt_sample %&gt;%\n  mutate(residuals = spage - line1)\n# print first 6 values\nhead(nilt_sample)\n\n# A tibble: 6 × 4\n   rage spage     line1 residuals\n  &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1    40 39           40        -1\n2    54 51           54        -3\n3    66 59           66        -7\n4    60 71           60        11\n5    35 34           35        -1\n6    67 57           67       -10\n\n\nComing back to the question, is my line good enough? We could sum all these residuals as an overall measure to know how good my line is. From the previous plot, we see that some of the expected ages exceed the actual values and others are below. This produces negative and positive residuals. If we simply sum them, they would compensate each other and this would not tell us much about the overall magnitude of the difference. To overcome this, we can multiply the differences by themselves (to make all numbers positive) and then sum them all together. This is known as the sum of squared residuals (SSR) and we can use this as the criterion to measure how good my line is with respect to the overall trend of the points.\nFor line 1, we can easily calculate the SSR as follows:\n\nsum((nilt_sample$residuals)^2)\n\n[1] 1325\n\n\nThe sum of squared residuals for line 1 is 1,325.\nWe can try different lines to find the combination of intercept and slope that produces the smallest error (SSR). To our luck, we can find the best values using a well established technique called Ordinary Least Squares (OLS). This technique finds the optimal solution by the principle of maxima and minima. We do not need to know the details for now. The important thing is that this procedure guarantees to find a line that produces the smallest possible error (SSR).\nIn R, it is very simple to fit a linear model and we do not need to go through each of the steps above manually nor to memorise all the steps. To do this, simply use the function lm(), save the result in an object called m1 and print it.\n\nm1 &lt;- lm(spage ~ rage, nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage  \n      6.299        0.875  \n\n\nSo, this is the optimal intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) that produces the least sum of the square. Let’s see what the SSR is compared to my arbitrary guess above:\n\nsum(residuals(m1)^2)\n\n[1] 1175.209\n\n\nYes, this is better than before (because the value is smaller)!\nLet’s plot the line we guessed and the optimal line together:\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nThe blue is the optimal solution, whereas the red was just an arbitrary guess. We can see that the optimal is more balanced than the red in relation to the observed data points.\n\nNow you are ready for the formal specification of the linear model. After the introduction above, it is easy to take the pieces apart. In essence, the simple linear model is telling us that the dependent value \\(y\\) is defined by a line that intersects the vertical axis at \\(\\beta_0\\), plus the multiplication of the slope \\(\\beta_1\\) by the value of \\(x_1\\), plus some residual \\(\\epsilon\\). The second part of the equation is just telling us that the residuals are normally distributed (that is when a histogram of the residuals follows a bell-shaped curve).\n\\[\n\\begin{align}\ny = \\beta_0 + \\beta_1 * x_1 + \\epsilon, && \\epsilon ~ N(0, \\sigma)  \n\\end{align}\n\\]\nYou do not need to memorize this equation, but being familiar to this type of notation will help you to expand your skills in quantitative research.\n\n\nWe already fitted our first linear model using the lm() above, which of course stands for linear model, but we did not discuss the specifications needed in this function. The first argument takes the dependent variable, then, we use tilde ~ to express that this is followed by the independent variable. Then, we specify the data set separated by a comma, as shown below (this is just a conceptual example, it does not actually run!):\n\nlm(dependent_variable ~ independent_variable, data)\n\n\nNow, let’s try with the full data set. First, we will fit a linear model using the same variables as the example above but including all the observations (remember that before we were working only with a small random sample). The first argument is the respondent’s spouse/partner age spage, followed by the respondent’s age rage. Let’s assign the model to an object called m2 and print it after.\n\nm2 &lt;- lm(spage ~ rage, nilt)\nm2\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nCoefficients:\n(Intercept)         rage  \n     3.7039       0.9287  \n\n\nThe output first displays the formula employed to estimate the model. Then, it show us the estimated values for the intercept and the slope for the age of the respondent, these estimated values are called coefficients. In this model, the coefficients differ from the ones in the example above (m1). This is because we now have more information to fit this line. Despite the difference, we see that the relationship is still in the same direction (positive). In fact, the value of the slope \\(\\beta_1\\) did not change much if you look at it carefully (0.87 vs 0.92).\n\nBut what are the slope and intercept telling us? The first thing to note is that the slope is positive, which means that the relationship between the respondent age and their partner are in the same direction. When the age of the respondent goes up, the age of their partner is expected to go up as well.\nIn our model, the intercept does not have a meaningful interpretation on its own, it would not be logical to say that when the respondent’s age is 0 their partner would be expected to be 3.70 years old. Interpretations should be made only within the range of the values used to fit the model (the youngest age in this data set is 18). The slope can be interpreted as following: For every year older the respondent is, the partner’s age is expected to change by 0.92. In other words, the respondents are expected to have a partner who is 0.92 times years older than themselves. This means that their partners are expected to be slightly younger.\nIn this case, the units of the dependent variable are the same as the independent (age vs age) which make it easy to interpret. In reality, there might be more factors that potentially affect how people select their partners (e.g. genders, education, race/ethnicity, etc.). For now, we are keeping things simple using only these two variables.\nLet’s practice with another example. What if we are interested in income? We can use personal income persinc2 as the dependent variable and the number of hours worked a week rhourswk as the independent variable. We’ll assign the result to an object called m3.\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\nm3\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nCoefficients:\n(Intercept)     rhourswk  \n     5170.4        463.2  \n\n\nThis time, the coefficients look quite different. The results are telling us that for every additional hour worked a week a respondent is expected to earn £463.2 more a year than other respondent. Note that the input units are not the same in the interpretation. The dependent variable is in pounds earned a year and the independent is in hours worked a week. Does this mean that someone who works 0 hours a week is expected to earn £5170.4 a year (given the fitted model \\(\\hat{persinc2} =  5170.4 + 0*463.2\\)) or that someone who works 110 hours a week (impossible!) is expected to earn £56K a year? We should only make reasonable interpretations of the coefficients within the range analysed.\nFinally, an important thing to know when interpreting linear models using cross sectional data (data collected in a single time point only) is: correlation does not imply causation. Variables are often correlated with other non-observed variables which may cause the effects observed on the independent variable in reality. This is why we cannot always be sure that X is causing the observed changes on Y.\n\nIn the next section, we suggest you to focus on the interpretation of the results of the linear model. You do not need to reproduce the code.\n\nSo far, we have talked about the basics of the linear model. If we print a summary of the model, we can obtain more information to evaluate it.\n\nsummary(m2)\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.211  -2.494   0.075   2.505  17.578 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.70389    0.66566   5.564    4e-08 ***\nrage         0.92869    0.01283  72.386   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.898 on 589 degrees of freedom\n  (613 observations deleted due to missingness)\nMultiple R-squared:  0.8989,    Adjusted R-squared:  0.8988 \nF-statistic:  5240 on 1 and 589 DF,  p-value: &lt; 2.2e-16\n\n\nThere are a lot of numbers from the result. Let’s break down the information into different pieces: first the call, second the residuals, third the coefficients, and fourth the goodness-of-fit measures, as shown below:\n\n\nModel summary\n\nThe first part, Call, is simply printing the variables that were used to produce the models.\nThe second part, the Residuals, provides a small summary of the residuals by quartile, including minimum and maximum residual values. Remember the second part of the formal specification of the model? Well, this is where we can see in practice the distribution of the residuals. If we look at the first and third quartile, we can see that that 50% of the predicted values by the model are \\(\\pm\\) 2.5 years old away form the observed partner’s age…not that bad.\nThe third part is about the coefficients. This time, the summary provides more information about the intercept and the slope than just the estimated coefficients as before. Usually, we focus on the independent variables in this section, rage in our case. The second column, standard error, tell us how large the error of the slope is, which is small for our variable. The third and fourth columns provide measures about the statistical significance of the relationship between the dependent and independent variable. The most commonly reported is the p-value (the fourth column). When these values contains many digits, R reports the result in scientific notation. In our result the p-value is &lt;2e-16 (the real equivalent number is &lt;0.0000000000000002, if you want to know more about scientific notation click here). As a general agreed principle, if the p-value is equal or less than 0.05 (\\(p\\) ≤ 0.05), we can reject the null hypothesis, and say that the relationship between the dependent variable and the independent is significant. In our case, the p-value is far lower than 0.05. So, we can say that the relationship between the respondent’s spouse/partner age and the respondent’s age is significant. In fact, R includes codes for each coefficient to help us to identify the significance. In our result, we got three stars/asterisks. This is because the value is less than 0.001. R assigns two stars if the p-value is less than 0.01, and one star if it is lower than 0.05.\nIn the fourth area, we have various goodness-of-fit measures. Overall, these measures tell us how well the model represents our data. For now, let’s focus on the adjusted r-squared and the sample size. The r-squared is a value that goes from 0 to 1. 0 means that model is not explaining any of the variance, whereas 1 would mean a perfect model. In social sciences we do not see high values often. The adjusted r-squared can be understood as the percentage variance that the model is able to explain. In the example above, the model explains 89.88% percent of the total variance. This is a good fit, but this is because we are modelling a quite “obvious” relationship. The r-squared does not represent how good or bad our work is. You may wonder why we do not report the sum of squared residuals (SSR), as we just learned. This is because the scale of the SSR is in the units used to fit the model, which would make it difficult to compare our model with other models that use different units or different variables. Conversely, the adjusted r-squared is expressed in relative terms which makes it a more suitable measure to compare with other models. An important thing to note is that the output is telling us that this model excluded 613 observations which were missing in our dataset. This is because we do not have the age of the respondent’s partner in 613 of the rows. This missingness may be because the respondent do not have a partner, refused to provide the information, etc.\nIn academic papers in the social sciences, usually the measures reported are the coefficients, p-values, the adjusted r-squared, and the size of the sample used to fit the model (number of observations).\n\nThe linear model, as many other techniques in statistics, relies on assumptions. These refer to characteristics of the data that are taken for granted to generate the estimates. It is the task of the modeller/analyst to make sure the data used follows these assumptions. One simple yet important check is to examine the distribution of the residuals, which ought to follow a normal distribution.\nDoes the residuals follow a normal distribution in m1? We can go further and plot a histogram to graphically evaluate it. Use the function residuals() to extract the residuals from our object m2, and then plot a histogram with the r-base function hist().\n\nhist(residuals(m2), breaks = 20)\n\n\n\n\n\n\n\nOverall, it seems that the residuals follow the normal distribution reasonably well, with the exception of the negative value to the left of the plot. Very often when there is a strange distribution or different to the normal is because one of the assumptions is violated. If that is the case we cannot trust the coefficient estimates. In the next lab we will talk more about the assumptions of the linear model. For now, we will leave it here and get some practice.\n\nIn the linear_model_intro.R script, use the nilt dataset in to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point() jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\nPrint the summary of m3 using the summary() function.\nIs the relationship of hours worked a week significant?\nWhat is the adjusted r-squared? How would you interpret it?\nWhat is the sample size to fit the model?\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#welcome",
    "href": "08-Lab8.html#welcome",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "In the previous lab we learned about correlation. We visualised the relationship of different types of variables. Also, we computed one correlation measure for two numeric variables, Pearson correlation. This measure is useful to compute the strength and the direction of the association. However, it presents some limitations, e.g. it can only be used for numeric variables, it allows only one variable at a time, and it is appropriate to describe a linear relationship.\nLinear regression can overcome some of these limitations and it can be extended to achieve further purposes (e.g. use multiple variables or estimate scenarios). This technique is in fact very common and one of the most popular in quantitative research in social sciences. Therefore, getting familiar with it will be important for you not only to perform your own analyses, but also to interpret and critically read literature.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#introduction-to-simple-linear-regression",
    "href": "08-Lab8.html#introduction-to-simple-linear-regression",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "IMPORTANT: Linear regression is appropriate only when the dependent variable is numeric (interval/ratio).\nHowever, the independent variables can be categorical, ordinal, or numeric. Also, you can include more than one independent variable to evaluate how these relate to the dependent variable. In this lab we will start using only one explanatory variable. This is know as simple linear regression.\n\nFor this lab, we will continue using the 2012 NILT survey to introduce linear models. To do so, please set your RStudio environment as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud;\nCreate a copy of the project called ‘NILT2’ which is located in your ‘Quants lab group’ by clicking on the ‘Start’ button;\nOnce in the project, create a new ‘R Script’ file (a simple R script NOT an .Rmd file);\nSave the R file as ‘linear_model_intro’.\n\nReproduce the code below, by copying, pasting and running it from your new script and focus on the intuitive part of this section.\nFirst, load the tidyverse library and read the nilt_r_object.rds file which is stored in the folder called ‘data’ and contains the NILT survey (tidyverse was installed in your session already).\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nThis dataset is exactly the same as the one you processed in previous labs.\nWe will re-revisit the example of the respondent’s spouse/partner characteristics. To keep things simple, let’s create a minimal sample of the dataset including only 40 random observations:\n\n# select a small random sample\nset.seed(3)\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- filter(nilt, !is.na(spage)) %&gt;% sample_n(40)\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- select(nilt_sample, rage, spage)\n\nWe will start by creating a scatter plot using the respondent’s spouse/partner age spage on the Y axis, and the respondent’s rage on the X axis.\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nAs you can see, even with this minimal example using 40 observations, there is a clear trend. From the plot, it seems intuitive to draw a line that describes this general trend. Imagine a friend will draw this line for you. You will have to tell them some basic references on how to do it. You can, for example, specify a start point and an end point in the plot. Alternatively, which is what we will do below, you can specify a start point and a value that describes the slope of the line. For now, our friend is ggplot. You have to pass these two values (start point and slope) in the geom_abline() function to draw a line that describes these points.\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nIn the guess above, it is assumed that the age of the spouse/partner might be exactly the same as the respondent. To draw a line like this, we used 0 as the starting value in the geom_abline() function. In statistics, this is known as the intercept and it is often represented with a Greek letter and a zero sub-index as \\(\\beta_0\\) (beta-naught) or with \\(\\alpha\\) (alpha). The location of the intercept can be found along the vertical axis (in this case it is not visible). The second value we passed to describe the line is the slope, which uses the X axis as the reference. The slope value is multiplied by the value of the X axis. Therefore, a slope of 0 is completely horizontal. In this example, a slope of 1 produces a line at 45º. The slope is often represented with the Greek letter beta and a sequential numeric sub-index like this: \\(\\beta_1\\) (beta-one).\nWe will create some points in the data frame based on these two values, the start point and the steepness factor or, formally speaking, the intercept and the slope respectively. We will store the results in a column called line1 and print the head of this data set.\n\n# create values for the guess line 1\nnilt_sample &lt;- nilt_sample %&gt;%\n  mutate(line1 = 0 + rage * 1)\n# print results\nhead(nilt_sample)\n\n# A tibble: 6 × 3\n   rage spage     line1\n  &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;\n1    40 39           40\n2    54 51           54\n3    66 59           66\n4    60 71           60\n5    35 34           35\n6    67 57           67\n\n\nFrom the above, you can notice that the respondent’s age rage and the column that we just created line1, are identical…So, that is our guess for now. The partner’s age is the same as the respondent’s.\nIs the line we just drew good enough to represent the general trend in the relationship above? To answer this question, we can measure the distance of each point of the plot to the line 1, as shown by the dashed segment in the plot below.\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_segment(aes(xend = rage, yend = line1), linetype = \"dashed\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nThis distance difference is known as the residual or the error and it is often expressed with the Greek letter \\(\\epsilon\\) (epsilon). Numerically, we can calculate this by computing the difference between the known value (the observed) and the guessed number. Formally, the guessed number is called the expected value (also known as predicted/estimated value). Normally, the expected values in statistics are represented by putting a hat like this \\(\\hat{}\\) on the letters. Since the independent variable is usually located on the Y axis, the expected value is differentiated from the observed values by putting the hat on the Y like this: \\(\\hat{y}\\) (y-hat).\nLet’s estimate the residuals for each of the observations by subtracting \\(\\hat{y}_i\\) from \\(y_i\\) (spage - line1) and store it in a column called residuals. Print the head of the data set after.\n\n# estimate residuals\nnilt_sample &lt;- nilt_sample %&gt;%\n  mutate(residuals = spage - line1)\n# print first 6 values\nhead(nilt_sample)\n\n# A tibble: 6 × 4\n   rage spage     line1 residuals\n  &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1    40 39           40        -1\n2    54 51           54        -3\n3    66 59           66        -7\n4    60 71           60        11\n5    35 34           35        -1\n6    67 57           67       -10\n\n\nComing back to the question, is my line good enough? We could sum all these residuals as an overall measure to know how good my line is. From the previous plot, we see that some of the expected ages exceed the actual values and others are below. This produces negative and positive residuals. If we simply sum them, they would compensate each other and this would not tell us much about the overall magnitude of the difference. To overcome this, we can multiply the differences by themselves (to make all numbers positive) and then sum them all together. This is known as the sum of squared residuals (SSR) and we can use this as the criterion to measure how good my line is with respect to the overall trend of the points.\nFor line 1, we can easily calculate the SSR as follows:\n\nsum((nilt_sample$residuals)^2)\n\n[1] 1325\n\n\nThe sum of squared residuals for line 1 is 1,325.\nWe can try different lines to find the combination of intercept and slope that produces the smallest error (SSR). To our luck, we can find the best values using a well established technique called Ordinary Least Squares (OLS). This technique finds the optimal solution by the principle of maxima and minima. We do not need to know the details for now. The important thing is that this procedure guarantees to find a line that produces the smallest possible error (SSR).\nIn R, it is very simple to fit a linear model and we do not need to go through each of the steps above manually nor to memorise all the steps. To do this, simply use the function lm(), save the result in an object called m1 and print it.\n\nm1 &lt;- lm(spage ~ rage, nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage  \n      6.299        0.875  \n\n\nSo, this is the optimal intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) that produces the least sum of the square. Let’s see what the SSR is compared to my arbitrary guess above:\n\nsum(residuals(m1)^2)\n\n[1] 1175.209\n\n\nYes, this is better than before (because the value is smaller)!\nLet’s plot the line we guessed and the optimal line together:\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nThe blue is the optimal solution, whereas the red was just an arbitrary guess. We can see that the optimal is more balanced than the red in relation to the observed data points.\n\nNow you are ready for the formal specification of the linear model. After the introduction above, it is easy to take the pieces apart. In essence, the simple linear model is telling us that the dependent value \\(y\\) is defined by a line that intersects the vertical axis at \\(\\beta_0\\), plus the multiplication of the slope \\(\\beta_1\\) by the value of \\(x_1\\), plus some residual \\(\\epsilon\\). The second part of the equation is just telling us that the residuals are normally distributed (that is when a histogram of the residuals follows a bell-shaped curve).\n\\[\n\\begin{align}\ny = \\beta_0 + \\beta_1 * x_1 + \\epsilon, && \\epsilon ~ N(0, \\sigma)  \n\\end{align}\n\\]\nYou do not need to memorize this equation, but being familiar to this type of notation will help you to expand your skills in quantitative research.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#fitting-linear-regression-in-r",
    "href": "08-Lab8.html#fitting-linear-regression-in-r",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "We already fitted our first linear model using the lm() above, which of course stands for linear model, but we did not discuss the specifications needed in this function. The first argument takes the dependent variable, then, we use tilde ~ to express that this is followed by the independent variable. Then, we specify the data set separated by a comma, as shown below (this is just a conceptual example, it does not actually run!):\n\nlm(dependent_variable ~ independent_variable, data)\n\n\nNow, let’s try with the full data set. First, we will fit a linear model using the same variables as the example above but including all the observations (remember that before we were working only with a small random sample). The first argument is the respondent’s spouse/partner age spage, followed by the respondent’s age rage. Let’s assign the model to an object called m2 and print it after.\n\nm2 &lt;- lm(spage ~ rage, nilt)\nm2\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nCoefficients:\n(Intercept)         rage  \n     3.7039       0.9287  \n\n\nThe output first displays the formula employed to estimate the model. Then, it show us the estimated values for the intercept and the slope for the age of the respondent, these estimated values are called coefficients. In this model, the coefficients differ from the ones in the example above (m1). This is because we now have more information to fit this line. Despite the difference, we see that the relationship is still in the same direction (positive). In fact, the value of the slope \\(\\beta_1\\) did not change much if you look at it carefully (0.87 vs 0.92).\n\nBut what are the slope and intercept telling us? The first thing to note is that the slope is positive, which means that the relationship between the respondent age and their partner are in the same direction. When the age of the respondent goes up, the age of their partner is expected to go up as well.\nIn our model, the intercept does not have a meaningful interpretation on its own, it would not be logical to say that when the respondent’s age is 0 their partner would be expected to be 3.70 years old. Interpretations should be made only within the range of the values used to fit the model (the youngest age in this data set is 18). The slope can be interpreted as following: For every year older the respondent is, the partner’s age is expected to change by 0.92. In other words, the respondents are expected to have a partner who is 0.92 times years older than themselves. This means that their partners are expected to be slightly younger.\nIn this case, the units of the dependent variable are the same as the independent (age vs age) which make it easy to interpret. In reality, there might be more factors that potentially affect how people select their partners (e.g. genders, education, race/ethnicity, etc.). For now, we are keeping things simple using only these two variables.\nLet’s practice with another example. What if we are interested in income? We can use personal income persinc2 as the dependent variable and the number of hours worked a week rhourswk as the independent variable. We’ll assign the result to an object called m3.\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\nm3\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nCoefficients:\n(Intercept)     rhourswk  \n     5170.4        463.2  \n\n\nThis time, the coefficients look quite different. The results are telling us that for every additional hour worked a week a respondent is expected to earn £463.2 more a year than other respondent. Note that the input units are not the same in the interpretation. The dependent variable is in pounds earned a year and the independent is in hours worked a week. Does this mean that someone who works 0 hours a week is expected to earn £5170.4 a year (given the fitted model \\(\\hat{persinc2} =  5170.4 + 0*463.2\\)) or that someone who works 110 hours a week (impossible!) is expected to earn £56K a year? We should only make reasonable interpretations of the coefficients within the range analysed.\nFinally, an important thing to know when interpreting linear models using cross sectional data (data collected in a single time point only) is: correlation does not imply causation. Variables are often correlated with other non-observed variables which may cause the effects observed on the independent variable in reality. This is why we cannot always be sure that X is causing the observed changes on Y.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#model-evaluation",
    "href": "08-Lab8.html#model-evaluation",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "In the next section, we suggest you to focus on the interpretation of the results of the linear model. You do not need to reproduce the code.\n\nSo far, we have talked about the basics of the linear model. If we print a summary of the model, we can obtain more information to evaluate it.\n\nsummary(m2)\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.211  -2.494   0.075   2.505  17.578 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.70389    0.66566   5.564    4e-08 ***\nrage         0.92869    0.01283  72.386   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.898 on 589 degrees of freedom\n  (613 observations deleted due to missingness)\nMultiple R-squared:  0.8989,    Adjusted R-squared:  0.8988 \nF-statistic:  5240 on 1 and 589 DF,  p-value: &lt; 2.2e-16\n\n\nThere are a lot of numbers from the result. Let’s break down the information into different pieces: first the call, second the residuals, third the coefficients, and fourth the goodness-of-fit measures, as shown below:\n\n\nModel summary\n\nThe first part, Call, is simply printing the variables that were used to produce the models.\nThe second part, the Residuals, provides a small summary of the residuals by quartile, including minimum and maximum residual values. Remember the second part of the formal specification of the model? Well, this is where we can see in practice the distribution of the residuals. If we look at the first and third quartile, we can see that that 50% of the predicted values by the model are \\(\\pm\\) 2.5 years old away form the observed partner’s age…not that bad.\nThe third part is about the coefficients. This time, the summary provides more information about the intercept and the slope than just the estimated coefficients as before. Usually, we focus on the independent variables in this section, rage in our case. The second column, standard error, tell us how large the error of the slope is, which is small for our variable. The third and fourth columns provide measures about the statistical significance of the relationship between the dependent and independent variable. The most commonly reported is the p-value (the fourth column). When these values contains many digits, R reports the result in scientific notation. In our result the p-value is &lt;2e-16 (the real equivalent number is &lt;0.0000000000000002, if you want to know more about scientific notation click here). As a general agreed principle, if the p-value is equal or less than 0.05 (\\(p\\) ≤ 0.05), we can reject the null hypothesis, and say that the relationship between the dependent variable and the independent is significant. In our case, the p-value is far lower than 0.05. So, we can say that the relationship between the respondent’s spouse/partner age and the respondent’s age is significant. In fact, R includes codes for each coefficient to help us to identify the significance. In our result, we got three stars/asterisks. This is because the value is less than 0.001. R assigns two stars if the p-value is less than 0.01, and one star if it is lower than 0.05.\nIn the fourth area, we have various goodness-of-fit measures. Overall, these measures tell us how well the model represents our data. For now, let’s focus on the adjusted r-squared and the sample size. The r-squared is a value that goes from 0 to 1. 0 means that model is not explaining any of the variance, whereas 1 would mean a perfect model. In social sciences we do not see high values often. The adjusted r-squared can be understood as the percentage variance that the model is able to explain. In the example above, the model explains 89.88% percent of the total variance. This is a good fit, but this is because we are modelling a quite “obvious” relationship. The r-squared does not represent how good or bad our work is. You may wonder why we do not report the sum of squared residuals (SSR), as we just learned. This is because the scale of the SSR is in the units used to fit the model, which would make it difficult to compare our model with other models that use different units or different variables. Conversely, the adjusted r-squared is expressed in relative terms which makes it a more suitable measure to compare with other models. An important thing to note is that the output is telling us that this model excluded 613 observations which were missing in our dataset. This is because we do not have the age of the respondent’s partner in 613 of the rows. This missingness may be because the respondent do not have a partner, refused to provide the information, etc.\nIn academic papers in the social sciences, usually the measures reported are the coefficients, p-values, the adjusted r-squared, and the size of the sample used to fit the model (number of observations).\n\nThe linear model, as many other techniques in statistics, relies on assumptions. These refer to characteristics of the data that are taken for granted to generate the estimates. It is the task of the modeller/analyst to make sure the data used follows these assumptions. One simple yet important check is to examine the distribution of the residuals, which ought to follow a normal distribution.\nDoes the residuals follow a normal distribution in m1? We can go further and plot a histogram to graphically evaluate it. Use the function residuals() to extract the residuals from our object m2, and then plot a histogram with the r-base function hist().\n\nhist(residuals(m2), breaks = 20)\n\n\n\n\n\n\n\nOverall, it seems that the residuals follow the normal distribution reasonably well, with the exception of the negative value to the left of the plot. Very often when there is a strange distribution or different to the normal is because one of the assumptions is violated. If that is the case we cannot trust the coefficient estimates. In the next lab we will talk more about the assumptions of the linear model. For now, we will leave it here and get some practice.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#lab-activities",
    "href": "08-Lab8.html#lab-activities",
    "title": "Linear model: Simple linear regression",
    "section": "",
    "text": "In the linear_model_intro.R script, use the nilt dataset in to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point() jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\nPrint the summary of m3 using the summary() function.\nIs the relationship of hours worked a week significant?\nWhat is the adjusted r-squared? How would you interpret it?\nWhat is the sample size to fit the model?\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "11-Answers.html",
    "href": "11-Answers.html",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "This chapter presents the suggested R code to answer the workbook activities and exercises throughout the course labs in Quantitative Research Methods for Social Sciences. This covers from Lab 3 to Lab 9.\nBefore looking at the answers, try asking your tutor for help. Also, we strongly recommend web resources, such as https://stackoverflow.com/ or https://community.rstudio.com/. By solving the issues, you will learn a lot! ;)\n\n\n## Load the packages\nlibrary(tidyverse)\n\n# Read the data from the .rds file\nclean_data &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Glimpse clean_data\nglimpse(clean_data)\n\n# Glimpse the nilt data\nglimpse(nilt)\n\n\nPreamble code\n\n## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nFrom your RStudio Cloud script, do the following activities using the data stored in the nilt_subset object:\n\nCreate a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\n\n\n# Load the vtable package to create summary tables\nlibrary(vtable)\n# Create table\nsumtable(nilt_subset, vars = c(\"uninatid\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nuninatid\n1183\n\n\n\n... Unionist\n348\n29%\n\n\n... Nationalist\n255\n22%\n\n\n... Neither\n580\n49%\n\n\n\n\n\n\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\n\n\nsumtable(nilt_subset, vars = c(\"religcat\"), group = \"uninatid\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nuninatid\n\n\nUnionist\n\n\nNationalist\n\n\nNeither\n\n\n\nVariable\nN\nPercent\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n341\n\n255\n\n558\n\n\n\n... Catholic\n2\n1%\n245\n96%\n238\n43%\n\n\n... Protestant\n305\n89%\n5\n2%\n180\n32%\n\n\n... No religion\n34\n10%\n5\n2%\n140\n25%\n\n\n\n\n\n\nUsing the data in the nilt_subset object, complete the following activities.\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions;\n\n\nhist(nilt_subset$persinc2)\n\n\n\n\n\n\n\n\nCreate a summary of the personal income persinc2 variable, using the sumtable() function.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\npersinc2\n897\n16395\n13466\n260\n6760\n22100\n75000\n\n\n\n\n\nCompute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"), group = \"ruhappy\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nruhappy\n\n\nVery happy\n\n\nFairly happy\n\n\nNot very happy\n\n\nNot at all happy\n\n\nCan't choose\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\n\n\n\npersinc2\n305\n17569\n13429\n500\n16262\n14015\n62\n13445\n9754\n9\n12451\n11501\n15\n11457\n6113\n\n\n\n\n\nPreamble code\n\n## Load the packages\nlibrary(tidyverse)\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Create subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nUsing the nilt_subset object, complete the tasks below in the Rmd file ‘Lab_4’, which you created earlier. Insert a new chunk for each of these activities and include brief comments as text in the Rmd document to introduce the plots and discuss the results (tip —leave an empty line between your text and the next chunk to separate the description and the plots):\n\nCreate a first-level header to start a section called “Categorical analysis”;\n\n\n## Categorical analysis\n\n\nCreate a simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid;\n\n\nggplot(nilt_subset, aes(x = uninatid)) +\n  geom_bar() +\n  labs(title = \"Political afiliation\", x = \"Party\")\n\n\n\n\n\n\n\n\nBased on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables;\n\n\nggplot(nilt_subset, aes(x = uninatid, fill = religcat)) +\n  geom_bar() +\n  labs(\n    title = \"Political affiliation by religion\",\n    x = \"Party\", fill = \"Religion\"\n  )\n\n\n\n\n\n\n\n\nCreate a new first-level header to start a section called “Numeric analysis”;\n\n\n## Numeric analysis\n\n\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\n\n\nggplot(nilt_subset, aes(x = rhourswk, y = persinc2)) +\n  geom_point() +\n  labs(\n    title = \"Income and number of hours worked a week\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime);\n\n\nggplot(nilt_subset, aes(x = ruhappy, y = persinc2)) +\n  geom_boxplot() +\n  labs(\n    title = \"Personal income and happiness\",\n    x = \"Happiness level\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nBriefly comment each of the plots as text in your Rmd file;\nKnit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish.\n\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Load the data from the .rds file we created in lab 3\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n# Migration\nnilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric)\n\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) %&gt;%\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text in the Rmd file (remember to leave an empty line between your text and the chunk).\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use the ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function, set the method argument to 'lm'.\n\n\nggplot(nilt, aes(x = rage, mig_per)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Perception of migration vs age\",\n    x = \"Respondent age\", y = \"Perception of migration (0-30)\"\n  )\n\n\n\n\n\n\n\n\nWhat type of relationship do you observe? Comment the overall result of the plot and whether this is in line with your previous expectation.\n\n\n## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\n\n\nUse the nilt data set object in your linear_model_intro file to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point(), jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nPrint the summary of m3 using the summary() function.\n\n\nsummary(m3)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43694  -8148  -3070   4990  58249 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5170.4     1966.2    2.63  0.00884 ** \nrhourswk       463.2       52.4    8.84  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13860 on 455 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.1466,    Adjusted R-squared:  0.1447 \nF-statistic: 78.15 on 1 and 455 DF,  p-value: &lt; 2.2e-16\n\n\n\nIs the relationship of hours worked a week significant? Re: Yes. The p-value (fourth column of the ‘Coefficients’ table) is lower than 0.05.\nWhat is the adjusted r-squared? How would you interpret it? Re: the adjusted R-squared is 0.14. This can be interpreted in terms of percentage, e.g. 14% of the variance in personal income can be explained by the number of hours worked a week.\nWhat is the sample size to fit the model? Re: The total number of observations in the data set is 1,204 and the model summary says that 747 observations were deleted due to missingness. Therefore, the sample size is 457 (1204-747).\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\n\n\n5170.4 + 463.2 * 30\n\n[1] 19066.4\n\n\n\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\n\n\nhist(residuals(m3))\n\n\n\n\n\n\n\nOverall, the residuals look normally distributed with the exception of the values to the right-hand side of the plot (between 40000 and 60000).\n\n\n\nLoad the packages, and the data that you will need in your file using the code below:\n\n\n## Load the packages\nlibrary(moderndive)\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\n\n\ntable(nilt$highqual)\n\n\nDegree level or higher       Higher education   GCE A level or equiv \n                   230                    102                    243 \n     GCSE A-C or equiv      GCSE D-G or equiv      No qualifications \n                   185                     82                    281 \n  Other, level unknown           Unclassified \n                    27                     54 \n\n\n\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2, color = highqual)) +\n  geom_point(position = \"jitter\") +\n  moderndive::geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Personal income\",\n    subtitle = \"Personal income and number of hours worked a week by education level\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\",\n    color = \"Highest education level\"\n  )\n\n\n\n\n\n\n\n\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary.\n\n\nm4 &lt;- lm(persinc2 ~ rhourswk + rage + highqual, nilt)\nsummary(m4)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk + rage + highqual, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-36228  -6425  -1411   4635  54749 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    3904.64    2714.25   1.439    0.151    \nrhourswk                        444.79      45.32   9.814  &lt; 2e-16 ***\nrage                            236.59      48.47   4.881 1.47e-06 ***\nhighqualHigher education      -8164.91    1939.50  -4.210 3.09e-05 ***\nhighqualGCE A level or equiv -12439.26    1563.42  -7.956 1.47e-14 ***\nhighqualGCSE A-C or equiv    -13037.47    1703.07  -7.655 1.20e-13 ***\nhighqualGCSE D-G or equiv    -11622.07    2665.38  -4.360 1.61e-05 ***\nhighqualNo qualifications    -12968.10    2339.78  -5.542 5.11e-08 ***\nhighqualOther, level unknown  15445.70    3334.75   4.632 4.76e-06 ***\nhighqualUnclassified         -12399.58    2786.16  -4.450 1.08e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11830 on 447 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.3887,    Adjusted R-squared:  0.3764 \nF-statistic: 31.58 on 9 and 447 DF,  p-value: &lt; 2.2e-16\n\n\n\nComment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model. Re: All the independent variables including the number of hours worked a week, age, and all the categories of highest qualification level compared to ‘Degree or higher’ are significant to predict personal income in the model ‘m4’, considering that the p-value is lower than 0.05. We can confirm this from the fourth column of the ‘Coefficients’ table. The adjusted R-squared of the model is 0.37. This means that 37.6% of the variance in personal income can be explained by these variables. The size of the sample used to fit this model is 457, considering that the ‘nilt’ data set contains 1204 observations but 747 were deleted due to missingness (1204 - 747).\nPlot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model?\n\n\nhist(residuals(m4))\n\n\n\n\n\n\n\nThe distribution of the residuals in ‘m4’ look overall normally distributed. However, the distribution is not perfectly symmetric. Therefore, we would advice to conduct further checks to test the linear model assumptions.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "11-Answers.html#introduction",
    "href": "11-Answers.html#introduction",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "This chapter presents the suggested R code to answer the workbook activities and exercises throughout the course labs in Quantitative Research Methods for Social Sciences. This covers from Lab 3 to Lab 9.\nBefore looking at the answers, try asking your tutor for help. Also, we strongly recommend web resources, such as https://stackoverflow.com/ or https://community.rstudio.com/. By solving the issues, you will learn a lot! ;)",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "11-Answers.html#lab-3.-data-wrangling",
    "href": "11-Answers.html#lab-3.-data-wrangling",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "## Load the packages\nlibrary(tidyverse)\n\n# Read the data from the .rds file\nclean_data &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Glimpse clean_data\nglimpse(clean_data)\n\n# Glimpse the nilt data\nglimpse(nilt)",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "11-Answers.html#lab-4.-exploratory-data-analysis",
    "href": "11-Answers.html#lab-4.-exploratory-data-analysis",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "Preamble code\n\n## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nFrom your RStudio Cloud script, do the following activities using the data stored in the nilt_subset object:\n\nCreate a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\n\n\n# Load the vtable package to create summary tables\nlibrary(vtable)\n# Create table\nsumtable(nilt_subset, vars = c(\"uninatid\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nuninatid\n1183\n\n\n\n... Unionist\n348\n29%\n\n\n... Nationalist\n255\n22%\n\n\n... Neither\n580\n49%\n\n\n\n\n\n\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\n\n\nsumtable(nilt_subset, vars = c(\"religcat\"), group = \"uninatid\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nuninatid\n\n\nUnionist\n\n\nNationalist\n\n\nNeither\n\n\n\nVariable\nN\nPercent\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n341\n\n255\n\n558\n\n\n\n... Catholic\n2\n1%\n245\n96%\n238\n43%\n\n\n... Protestant\n305\n89%\n5\n2%\n180\n32%\n\n\n... No religion\n34\n10%\n5\n2%\n140\n25%\n\n\n\n\n\n\nUsing the data in the nilt_subset object, complete the following activities.\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions;\n\n\nhist(nilt_subset$persinc2)\n\n\n\n\n\n\n\n\nCreate a summary of the personal income persinc2 variable, using the sumtable() function.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\npersinc2\n897\n16395\n13466\n260\n6760\n22100\n75000\n\n\n\n\n\nCompute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"), group = \"ruhappy\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nruhappy\n\n\nVery happy\n\n\nFairly happy\n\n\nNot very happy\n\n\nNot at all happy\n\n\nCan't choose\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\n\n\n\npersinc2\n305\n17569\n13429\n500\n16262\n14015\n62\n13445\n9754\n9\n12451\n11501\n15\n11457\n6113",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "11-Answers.html#lab-6.-visual-exploratory-analysis",
    "href": "11-Answers.html#lab-6.-visual-exploratory-analysis",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "Preamble code\n\n## Load the packages\nlibrary(tidyverse)\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Create subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nUsing the nilt_subset object, complete the tasks below in the Rmd file ‘Lab_4’, which you created earlier. Insert a new chunk for each of these activities and include brief comments as text in the Rmd document to introduce the plots and discuss the results (tip —leave an empty line between your text and the next chunk to separate the description and the plots):\n\nCreate a first-level header to start a section called “Categorical analysis”;\n\n\n## Categorical analysis\n\n\nCreate a simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid;\n\n\nggplot(nilt_subset, aes(x = uninatid)) +\n  geom_bar() +\n  labs(title = \"Political afiliation\", x = \"Party\")\n\n\n\n\n\n\n\n\nBased on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables;\n\n\nggplot(nilt_subset, aes(x = uninatid, fill = religcat)) +\n  geom_bar() +\n  labs(\n    title = \"Political affiliation by religion\",\n    x = \"Party\", fill = \"Religion\"\n  )\n\n\n\n\n\n\n\n\nCreate a new first-level header to start a section called “Numeric analysis”;\n\n\n## Numeric analysis\n\n\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\n\n\nggplot(nilt_subset, aes(x = rhourswk, y = persinc2)) +\n  geom_point() +\n  labs(\n    title = \"Income and number of hours worked a week\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime);\n\n\nggplot(nilt_subset, aes(x = ruhappy, y = persinc2)) +\n  geom_boxplot() +\n  labs(\n    title = \"Personal income and happiness\",\n    x = \"Happiness level\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nBriefly comment each of the plots as text in your Rmd file;\nKnit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "11-Answers.html#lab-7.-correlation",
    "href": "11-Answers.html#lab-7.-correlation",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Load the data from the .rds file we created in lab 3\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n# Migration\nnilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric)\n\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) %&gt;%\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text in the Rmd file (remember to leave an empty line between your text and the chunk).\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use the ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function, set the method argument to 'lm'.\n\n\nggplot(nilt, aes(x = rage, mig_per)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Perception of migration vs age\",\n    x = \"Respondent age\", y = \"Perception of migration (0-30)\"\n  )\n\n\n\n\n\n\n\n\nWhat type of relationship do you observe? Comment the overall result of the plot and whether this is in line with your previous expectation.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "11-Answers.html#lab-8.-linear-model.-simple-linear-regression",
    "href": "11-Answers.html#lab-8.-linear-model.-simple-linear-regression",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\n\n\nUse the nilt data set object in your linear_model_intro file to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point(), jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nPrint the summary of m3 using the summary() function.\n\n\nsummary(m3)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43694  -8148  -3070   4990  58249 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5170.4     1966.2    2.63  0.00884 ** \nrhourswk       463.2       52.4    8.84  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13860 on 455 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.1466,    Adjusted R-squared:  0.1447 \nF-statistic: 78.15 on 1 and 455 DF,  p-value: &lt; 2.2e-16\n\n\n\nIs the relationship of hours worked a week significant? Re: Yes. The p-value (fourth column of the ‘Coefficients’ table) is lower than 0.05.\nWhat is the adjusted r-squared? How would you interpret it? Re: the adjusted R-squared is 0.14. This can be interpreted in terms of percentage, e.g. 14% of the variance in personal income can be explained by the number of hours worked a week.\nWhat is the sample size to fit the model? Re: The total number of observations in the data set is 1,204 and the model summary says that 747 observations were deleted due to missingness. Therefore, the sample size is 457 (1204-747).\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\n\n\n5170.4 + 463.2 * 30\n\n[1] 19066.4\n\n\n\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\n\n\nhist(residuals(m3))\n\n\n\n\n\n\n\nOverall, the residuals look normally distributed with the exception of the values to the right-hand side of the plot (between 40000 and 60000).",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "11-Answers.html#lab-9.-multivariate-linear-model",
    "href": "11-Answers.html#lab-9.-multivariate-linear-model",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "Load the packages, and the data that you will need in your file using the code below:\n\n\n## Load the packages\nlibrary(moderndive)\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\n\n\ntable(nilt$highqual)\n\n\nDegree level or higher       Higher education   GCE A level or equiv \n                   230                    102                    243 \n     GCSE A-C or equiv      GCSE D-G or equiv      No qualifications \n                   185                     82                    281 \n  Other, level unknown           Unclassified \n                    27                     54 \n\n\n\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2, color = highqual)) +\n  geom_point(position = \"jitter\") +\n  moderndive::geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Personal income\",\n    subtitle = \"Personal income and number of hours worked a week by education level\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\",\n    color = \"Highest education level\"\n  )\n\n\n\n\n\n\n\n\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary.\n\n\nm4 &lt;- lm(persinc2 ~ rhourswk + rage + highqual, nilt)\nsummary(m4)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk + rage + highqual, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-36228  -6425  -1411   4635  54749 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    3904.64    2714.25   1.439    0.151    \nrhourswk                        444.79      45.32   9.814  &lt; 2e-16 ***\nrage                            236.59      48.47   4.881 1.47e-06 ***\nhighqualHigher education      -8164.91    1939.50  -4.210 3.09e-05 ***\nhighqualGCE A level or equiv -12439.26    1563.42  -7.956 1.47e-14 ***\nhighqualGCSE A-C or equiv    -13037.47    1703.07  -7.655 1.20e-13 ***\nhighqualGCSE D-G or equiv    -11622.07    2665.38  -4.360 1.61e-05 ***\nhighqualNo qualifications    -12968.10    2339.78  -5.542 5.11e-08 ***\nhighqualOther, level unknown  15445.70    3334.75   4.632 4.76e-06 ***\nhighqualUnclassified         -12399.58    2786.16  -4.450 1.08e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11830 on 447 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.3887,    Adjusted R-squared:  0.3764 \nF-statistic: 31.58 on 9 and 447 DF,  p-value: &lt; 2.2e-16\n\n\n\nComment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model. Re: All the independent variables including the number of hours worked a week, age, and all the categories of highest qualification level compared to ‘Degree or higher’ are significant to predict personal income in the model ‘m4’, considering that the p-value is lower than 0.05. We can confirm this from the fourth column of the ‘Coefficients’ table. The adjusted R-squared of the model is 0.37. This means that 37.6% of the variance in personal income can be explained by these variables. The size of the sample used to fit this model is 457, considering that the ‘nilt’ data set contains 1204 observations but 747 were deleted due to missingness (1204 - 747).\nPlot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model?\n\n\nhist(residuals(m4))\n\n\n\n\n\n\n\nThe distribution of the residuals in ‘m4’ look overall normally distributed. However, the distribution is not perfectly symmetric. Therefore, we would advice to conduct further checks to test the linear model assumptions.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "04-Lab4.html",
    "href": "04-Lab4.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "In our previous session we learned about wrangling data in R by implementing useful function such as filter(), select(), and mutate(). In this session we will focus on descriptive statistics. This includes the exploration and description of quantitative data.\n\nWe will continue working on the same project and dataset that you created in the last lab on RStudio Cloud. Please follow the next steps:\n\nGo to your ‘Quants lab group’ in RStudio Cloud.\nOpen the project called ‘NILT’ located in your lab group’.\nContinue working at the bottom of the ‘Exploratory analysis’ script that you created in the last lab.\nLoad nilt dataset that you created in the last session using the following code:\n\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nCreate a subset of the nilt data keeping only few variables using the select() function as shown below:\n\n\n# Subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\n\nAre your summary statistics hiding something interesting?\n\n\n\n\nExploratory analysis.\n\n\n\n\nTo start exploring our data it essential to distinguish the adequate tools and measures available for the type of data in question. As you know now, there are two broad types: (1) categorical and (2) numeric.\nThere are several ways in which we can summarise our data. Today, we will use a useful package called vtable. Install it in your session running the following line from your console:\n\ninstall.packages(\"vtable\")\n\nOnce it is installed, make sure to load it with the next line. This time copy and paste it in your R script, so you can run it every time you restart your session.\n\nlibrary(vtable)\n\n\nA usual way to explore the categorical data is using contingency and proportion tables. The contingency tables include the count for each category while the proportion tables contain the count divided by the total number of observations.\nTry to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown.\nLet’s say we are interested in the data’s break down by respondents’ sex (called rsex in the dataset). We will use function sumtable() of the vtable package to produce a contingency table for a single variable (known as One-Way contingency table).\n\nsumtable(nilt_subset, vars = c(\"rsex\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\n\n\n\nFrom the result, we see that there are more female respondents than males.\nSpecifically, we see that males respondents represent 44.6% of the total sample, whereas females 55.4%.\nWe can do this with any type of categorical variable. Let’s see how the sample is split by religion (religcat). So, we will add it to in the vars argument.\n\nsumtable(nilt_subset, vars = c(\"rsex\", \"religcat\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\nreligcat\n1168\n\n\n\n... Catholic\n491\n42%\n\n\n... Protestant\n497\n43%\n\n\n... No religion\n180\n15%\n\n\n\n\n\nAs you can see, about the same number of people are identified as being catholic or protestant, and a relatively small number with no religion.\nWhat if we want to know the religious affiliation breakdown by males and females. This is where Two-Way contingency tables are useful and very common in quantitative research. To produce it, we have to specify the group argument in the sumtable function as follows:\n\nsumtable(nilt_subset, vars = c(\"religcat\"), group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n520\n\n648\n\n\n\n... Catholic\n209\n40%\n282\n44%\n\n\n... Protestant\n211\n41%\n286\n44%\n\n\n... No religion\n100\n19%\n80\n12%\n\n\n\n\n\nThere are some interesting results from this table. You can see that there are proportionally more female respondents who are either Catholic or Protestant than males, namely 43.5% vs 40.2% and 44.1% vs 40.6%, respectively. We also see that there are almost 20% of male respondents who do not self-identify with a religion which contrast to the 12% of female participants.\n\nFrom your RStudio Cloud script, do the following activities using the data in the nilt_subset object (feel free to copy and adapt the code shown above):\n\nCreate a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\nAre your summary statistics hiding something interesting? Discuss your results with your neighbour or your tutor.\n\nIn the previous section we’ve learnt how to summarise categorical data. But very often we want to work with continuous numeric variables or a combination of both. To summarise and understand numeric data there are two main types: measures of centrality and measures of spread.\nAs before, try to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown.\n\nIn quantitative research, we usually have access to many observations in a sample which contains different attributes for each of them. It would be difficult (and probably not very useful) to talk about each of the NILT respondents one by one. Instead, to describe this sample we need measures that roughly represent all participants.\nThis is actually an important step in quantitative research, since it allows us to characterise the people that we are studying. For example, in the previous section we only talked about the respondents’ sex and political affiliation, but who are the people we are talking about? Probably a place to start digging deeper is to know their age. The first tool that we will use to understand numeric values is a histogram. Let’s see how the age of NILT respondents is distributed.\n\nhist(nilt_subset$rage)\n\n\n\n\n\n\n\nThis plot shows us on the X axis (horizontal) the age and the frequency on the Y axis. We can see that the youngest age in the sample is somewhere close to 20, and the oldest is almost 100. We also observe that the total number of observations (represented by the frequency on the vertical axis) for extreme values (close to 20 on the left-hand side and 100 on the right-hand side) tends to be lower than the values in the centre of the plot (somewhere between 30 and 45). For instance, we can see that there are approximately 120 respondents who are around 40 years old; that seems to be the most popular/frequent age in our sample. Now, we can represent these central values with actual measures, typically mean or median.\nThe median is the mid-point value in a numeric series. If you sort the values and split it by half, the value right in the middle is the median. Luckily there is a function ready to be used called… You guessed it - median().\n\nmedian(nilt_subset$rage, na.rm = TRUE)\n\n[1] 48\n\n\nThe median age is 48, that means that 50% (or half) of the respondents are equal or younger than this, and the other 50% is equal or older.1\nTo compute the mean manually, we need to sum all our values and divide it by the total number of the observations as follows: \\[ mean =\\frac{  x_1 + x_2 + x_3 ...+x_n } {n} \\] The formula above is for you to know that this measure considers the magnitude of all values included in the numeric series. Therefore, the average is sensitive to extreme numbers (e.g. a very, very old person). To compute the mean you need the mean() function.\n\nmean(nilt_subset$rage, na.rm = T)\n\n[1] 49.61532\n\n\nAs you can see, the above measures try to approximate values that fall somewhere in the centre of the histogram plot, and represent all observations in the sample. They tell different things and are sometimes more (or less) suitable in a specific situation.\n\nBy contrast, there are measures that help us to describe how far away a series of numeric values are from the centre. The common measures of spread are quartiles, variance and standard deviation.\nThe quartiles are very useful to quickly see how numeric data is distributed. Imagine that we sort all ages in the sample and split it into four equal parts. The first quartile includes the lowest 25% of values, the second the next 25%, the third another 25%, and the fourth the highest 25%. To compute quartiles, we can use the quantile function.\n\nquantile(nilt_subset$rage, na.rm = T)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\nIn our sample, the youngest quarter of the respondents is between 18 and 35 years old. The second quarter is between 35 and 48 years old. The next quartile is between 48 and 64. The oldest 25% of the respondents is between 64 and 97.\nThe variance is useful to obtain a singe measure of spread (instead of four values, as the above) taking the mean as a reference. This is given by the following formula:\n\\[ var = \\frac{ \\sum(x - \\bar{x})^2 }{n-1 } \\]\nTo decipher the formula above, the \\(\\bar{x}\\) represents the mean, the \\(x\\) represents each of the values in the numeric series. The formula takes each of the \\(x\\) values and subtract it from the mean \\(\\bar{x}\\). Later, it squares the result of the subtraction (that is multiply it by itself). This is done to obtain a positive value, since some numbers in the series will be lower than the mean (resulting in negative values). Then, we sum all of them and divide the sum by the size/length of the numeric sequence \\(n\\) minus 1. To estimate the variance in R we only need the var() function.\n\nvar(nilt_subset$rage, na.rm = T)\n\n[1] 343.3486\n\n\nAs you can see, the result is not very intuitive. That is because we squared the subtraction. Luckily, there is a measure that put it in readable scale. This is the standard deviation. In essence this takes the square root of the variance (the reversed operation of squaring it): \\[sd=\\sqrt{var}\\]\nTo compute it in R, use the sd() function.\n\nsd(nilt_subset$rage, na.rm = T)\n\n[1] 18.52967\n\n\nThis measure is more human readable than the variance. Don’t worry too much about the formula. An important thing to remember is what the measure represents. An informal definition of the standard deviation is the average distance from the mean. In essence, it tell us how far the values in our data are from the mean.\n\nPhew, that was a lot!…\n… Luckily we can use the sumtable function to compute all these measures at the same time!\nIt is very simple. You can compute a quick summary for age as following:\n\nsumtable(nilt_subset, vars = c(\"rage\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\nrage\n1201\n50\n19\n18\n35\n64\n97\n\n\n\n\nThe result displays the number of observations used (N), the mean, the standard deviation, minimum, the 1st (same as ‘Pctl. 25’) and 3rd quartile (same as ‘Pctl. 75’), as well as the maximum (i.e., eldest respondent).\n\nLastly, there will be times in which you will need to compute a summary combining categorical and numeric data, to compare groups for example. The good news is that we can use exactly the same function and syntax to do this. Let’s take the following example to compute the summary of the respondent’s age (rage) by gender:\n\nsumtable(nilt_subset, vars = c(\"rage\"), group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\n\n\n\nrage\n535\n51\n18\n666\n49\n19\n\n\n\n\nIn the code above, we are simple specifying the variable rage and grouping the summary by rsex. This produces a small summary included the number of observations in each category an the main measure of centrality and spread, namely the mean and the std. dev.\n\nCan money buy happiness?\nUsing the data in the nilt_subset dataset, complete the following activities. This will be a good practice run for doing the research report when you run your own analysis on the NILT teaching dataset. Handy!\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions (use the $ symbol after the name of the dataset and then the name of the variable inside the function);\nCreate a summary of the personal income persinc2 variable, using the sumtable() function;\nFinally, compute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy. What do you observe?\nDiscuss the results with your neighbour or your tutor.",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "04-Lab4.html#getting-started",
    "href": "04-Lab4.html#getting-started",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "We will continue working on the same project and dataset that you created in the last lab on RStudio Cloud. Please follow the next steps:\n\nGo to your ‘Quants lab group’ in RStudio Cloud.\nOpen the project called ‘NILT’ located in your lab group’.\nContinue working at the bottom of the ‘Exploratory analysis’ script that you created in the last lab.\nLoad nilt dataset that you created in the last session using the following code:\n\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nCreate a subset of the nilt data keeping only few variables using the select() function as shown below:\n\n\n# Subset\nnilt_subset &lt;- select(nilt, rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "04-Lab4.html#exploratory-analysis",
    "href": "04-Lab4.html#exploratory-analysis",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Are your summary statistics hiding something interesting?\n\n\n\n\nExploratory analysis.\n\n\n\n\nTo start exploring our data it essential to distinguish the adequate tools and measures available for the type of data in question. As you know now, there are two broad types: (1) categorical and (2) numeric.\nThere are several ways in which we can summarise our data. Today, we will use a useful package called vtable. Install it in your session running the following line from your console:\n\ninstall.packages(\"vtable\")\n\nOnce it is installed, make sure to load it with the next line. This time copy and paste it in your R script, so you can run it every time you restart your session.\n\nlibrary(vtable)\n\n\nA usual way to explore the categorical data is using contingency and proportion tables. The contingency tables include the count for each category while the proportion tables contain the count divided by the total number of observations.\nTry to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown.\nLet’s say we are interested in the data’s break down by respondents’ sex (called rsex in the dataset). We will use function sumtable() of the vtable package to produce a contingency table for a single variable (known as One-Way contingency table).\n\nsumtable(nilt_subset, vars = c(\"rsex\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\n\n\n\nFrom the result, we see that there are more female respondents than males.\nSpecifically, we see that males respondents represent 44.6% of the total sample, whereas females 55.4%.\nWe can do this with any type of categorical variable. Let’s see how the sample is split by religion (religcat). So, we will add it to in the vars argument.\n\nsumtable(nilt_subset, vars = c(\"rsex\", \"religcat\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\nreligcat\n1168\n\n\n\n... Catholic\n491\n42%\n\n\n... Protestant\n497\n43%\n\n\n... No religion\n180\n15%\n\n\n\n\n\nAs you can see, about the same number of people are identified as being catholic or protestant, and a relatively small number with no religion.\nWhat if we want to know the religious affiliation breakdown by males and females. This is where Two-Way contingency tables are useful and very common in quantitative research. To produce it, we have to specify the group argument in the sumtable function as follows:\n\nsumtable(nilt_subset, vars = c(\"religcat\"), group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n520\n\n648\n\n\n\n... Catholic\n209\n40%\n282\n44%\n\n\n... Protestant\n211\n41%\n286\n44%\n\n\n... No religion\n100\n19%\n80\n12%\n\n\n\n\n\nThere are some interesting results from this table. You can see that there are proportionally more female respondents who are either Catholic or Protestant than males, namely 43.5% vs 40.2% and 44.1% vs 40.6%, respectively. We also see that there are almost 20% of male respondents who do not self-identify with a religion which contrast to the 12% of female participants.\n\nFrom your RStudio Cloud script, do the following activities using the data in the nilt_subset object (feel free to copy and adapt the code shown above):\n\nCreate a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\nAre your summary statistics hiding something interesting? Discuss your results with your neighbour or your tutor.\n\nIn the previous section we’ve learnt how to summarise categorical data. But very often we want to work with continuous numeric variables or a combination of both. To summarise and understand numeric data there are two main types: measures of centrality and measures of spread.\nAs before, try to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown.\n\nIn quantitative research, we usually have access to many observations in a sample which contains different attributes for each of them. It would be difficult (and probably not very useful) to talk about each of the NILT respondents one by one. Instead, to describe this sample we need measures that roughly represent all participants.\nThis is actually an important step in quantitative research, since it allows us to characterise the people that we are studying. For example, in the previous section we only talked about the respondents’ sex and political affiliation, but who are the people we are talking about? Probably a place to start digging deeper is to know their age. The first tool that we will use to understand numeric values is a histogram. Let’s see how the age of NILT respondents is distributed.\n\nhist(nilt_subset$rage)\n\n\n\n\n\n\n\nThis plot shows us on the X axis (horizontal) the age and the frequency on the Y axis. We can see that the youngest age in the sample is somewhere close to 20, and the oldest is almost 100. We also observe that the total number of observations (represented by the frequency on the vertical axis) for extreme values (close to 20 on the left-hand side and 100 on the right-hand side) tends to be lower than the values in the centre of the plot (somewhere between 30 and 45). For instance, we can see that there are approximately 120 respondents who are around 40 years old; that seems to be the most popular/frequent age in our sample. Now, we can represent these central values with actual measures, typically mean or median.\nThe median is the mid-point value in a numeric series. If you sort the values and split it by half, the value right in the middle is the median. Luckily there is a function ready to be used called… You guessed it - median().\n\nmedian(nilt_subset$rage, na.rm = TRUE)\n\n[1] 48\n\n\nThe median age is 48, that means that 50% (or half) of the respondents are equal or younger than this, and the other 50% is equal or older.1\nTo compute the mean manually, we need to sum all our values and divide it by the total number of the observations as follows: \\[ mean =\\frac{  x_1 + x_2 + x_3 ...+x_n } {n} \\] The formula above is for you to know that this measure considers the magnitude of all values included in the numeric series. Therefore, the average is sensitive to extreme numbers (e.g. a very, very old person). To compute the mean you need the mean() function.\n\nmean(nilt_subset$rage, na.rm = T)\n\n[1] 49.61532\n\n\nAs you can see, the above measures try to approximate values that fall somewhere in the centre of the histogram plot, and represent all observations in the sample. They tell different things and are sometimes more (or less) suitable in a specific situation.\n\nBy contrast, there are measures that help us to describe how far away a series of numeric values are from the centre. The common measures of spread are quartiles, variance and standard deviation.\nThe quartiles are very useful to quickly see how numeric data is distributed. Imagine that we sort all ages in the sample and split it into four equal parts. The first quartile includes the lowest 25% of values, the second the next 25%, the third another 25%, and the fourth the highest 25%. To compute quartiles, we can use the quantile function.\n\nquantile(nilt_subset$rage, na.rm = T)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\nIn our sample, the youngest quarter of the respondents is between 18 and 35 years old. The second quarter is between 35 and 48 years old. The next quartile is between 48 and 64. The oldest 25% of the respondents is between 64 and 97.\nThe variance is useful to obtain a singe measure of spread (instead of four values, as the above) taking the mean as a reference. This is given by the following formula:\n\\[ var = \\frac{ \\sum(x - \\bar{x})^2 }{n-1 } \\]\nTo decipher the formula above, the \\(\\bar{x}\\) represents the mean, the \\(x\\) represents each of the values in the numeric series. The formula takes each of the \\(x\\) values and subtract it from the mean \\(\\bar{x}\\). Later, it squares the result of the subtraction (that is multiply it by itself). This is done to obtain a positive value, since some numbers in the series will be lower than the mean (resulting in negative values). Then, we sum all of them and divide the sum by the size/length of the numeric sequence \\(n\\) minus 1. To estimate the variance in R we only need the var() function.\n\nvar(nilt_subset$rage, na.rm = T)\n\n[1] 343.3486\n\n\nAs you can see, the result is not very intuitive. That is because we squared the subtraction. Luckily, there is a measure that put it in readable scale. This is the standard deviation. In essence this takes the square root of the variance (the reversed operation of squaring it): \\[sd=\\sqrt{var}\\]\nTo compute it in R, use the sd() function.\n\nsd(nilt_subset$rage, na.rm = T)\n\n[1] 18.52967\n\n\nThis measure is more human readable than the variance. Don’t worry too much about the formula. An important thing to remember is what the measure represents. An informal definition of the standard deviation is the average distance from the mean. In essence, it tell us how far the values in our data are from the mean.\n\nPhew, that was a lot!…\n… Luckily we can use the sumtable function to compute all these measures at the same time!\nIt is very simple. You can compute a quick summary for age as following:\n\nsumtable(nilt_subset, vars = c(\"rage\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\nrage\n1201\n50\n19\n18\n35\n64\n97\n\n\n\n\nThe result displays the number of observations used (N), the mean, the standard deviation, minimum, the 1st (same as ‘Pctl. 25’) and 3rd quartile (same as ‘Pctl. 75’), as well as the maximum (i.e., eldest respondent).\n\nLastly, there will be times in which you will need to compute a summary combining categorical and numeric data, to compare groups for example. The good news is that we can use exactly the same function and syntax to do this. Let’s take the following example to compute the summary of the respondent’s age (rage) by gender:\n\nsumtable(nilt_subset, vars = c(\"rage\"), group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\n\n\n\nrage\n535\n51\n18\n666\n49\n19\n\n\n\n\nIn the code above, we are simple specifying the variable rage and grouping the summary by rsex. This produces a small summary included the number of observations in each category an the main measure of centrality and spread, namely the mean and the std. dev.\n\nCan money buy happiness?\nUsing the data in the nilt_subset dataset, complete the following activities. This will be a good practice run for doing the research report when you run your own analysis on the NILT teaching dataset. Handy!\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions (use the $ symbol after the name of the dataset and then the name of the variable inside the function);\nCreate a summary of the personal income persinc2 variable, using the sumtable() function;\nFinally, compute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy. What do you observe?\nDiscuss the results with your neighbour or your tutor.",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "04-Lab4.html#footnotes",
    "href": "04-Lab4.html#footnotes",
    "title": "Exploratory data analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\nNote that the argument na.rm equal TRUE is used in the function. The ‘na’ bit refers to missing values, and the ‘rm’ refers to remove. So we are telling R to remove the missing values when computing the median. This is because we do not know the age of 3 of the respondents in the sample.↩︎",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "10-Lab10.html",
    "href": "10-Lab10.html",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "This is our final practical session. Thank you for tuning in and all your hard work! Today, we will discuss some practical aspects of the summative assessment Interpreting Quantitative Findings Report. You will actually start working on the assignment today by creating a template you can use to write your report in RStudio. Also, you will get familiar with the structure and the contents of the template. Remember, this is the perfect time to clarify as many questions as possible. Your tutor will be more than happy to help!\nBefore proceeding, please take about 10 minutes to read the instructions for the assignment available in Moodle.\n\nWe’ve created a GitHub repository which contains an RStudio project and all the essentials you need to start writing your report. There are two options for you. One is writing in RStudio Cloud, as we have been doing all along this course. Alternatively, you can also use the RStudio Desktop version (this requires having R and RStudio installed locally on your device). The result will be the same and the process is very similar. So, the choice is totally up to you and what your preference is. Here, we explain how to start your own project for both options.\nFor this session, try only one of these. Don’t worry about which one you choose now, you can change your mind later. If you can’t make up your mind, then try the RStudio Cloud route first, as it means you can work on your assignment on any devices, regardless of the computational power of your device, but it does mean you need to work on the assignment online using a web browser. If online connection is an issue for you, then choose the RStudio Desktop route.\n\nIf you wish to write your project using RStudio Cloud, please follow the next steps:\n\nAccess RStudio Cloud as usual.\nMake sure you are in your Quants lab group (not in ‘Your Workspace’).\nClick on the ‘New Project’ button and select ‘New Project from Git Repository’.\n\n\n\n\n\nNew project from GitHub.\n\n\n\n\nPaste the following URL in the box and click ‘OK’: https://github.com/UGQuants/UGQuant-assignment2.\nYou are all set! Just click on the project to access the contents.\n\nIf alternately you decide to write your assessment using the RStudio Desktop version, consider the following steps:\n\nAccess the following GitHub repository copying and pasting the following URL in your browser: https://github.com/UGQuants/UGQuant-assignment2.\nClick on the ‘Code’ button and select the ‘Download ZIP’ option, as shown below.\n\n\n\n\n\nDownlowad ZIP from GitHub.\n\n\n\n\nGo to your local downloads folder, right click on the UGQuant-assignment2-main.zip and choose the ‘Extract all’ option (here, you can also chose the folder where you want to store the RStudio project and write your assignment).\nClick on the ‘Extract’ button.\nAs you can see, this folder contains an RStudio project. Open the UGQuant-assignment2.Rproj which will initialize a new RStudio session.\n\nOnce in your UGQuant-assignment2 project, open the Assignmet2-template.Rmd file in Pane 4 under the ‘Files’ tab, as shown below.\n\n\n\n\nTemplate.\n\n\n\nThis template contains the following:\n\nSuggested structure of the report.\nSuggested word count for each section.\nThe code necessary to run and present the results of a multivariate linear regression.\n\nIn essence, this is the basic structure to start writing your assignment. Of course, you can add, edit, and customize as much as you consider appropriate. Remember, this is just a generic suggestion and you should still address all the points to the best of your abilities. Remember: There are no hard and fast rules to say what’s right or wrong. You are the one who can determine and justify why you did what you did. There are also many ways to write and approach this assignment, so make choices based on your own interest(s) and disciplinary background. It is truly your time to shine! The course handbook also gives important pointers on how your report will be assessed, offering some guiding questions to tackle the report–don’t skip this crucial step.\n\n\nIn the template, fill in the ‘author’ and ‘date’ space in the YAML at the top of the file with your student number and appropriate information using quotation marks.\nKnit the Rmd file as html (RStudio may ask to install some packages; click ‘Yes’).\nIn the output, look at the results of the table under the ‘Results’ section and identify the dependent and the independent variables. You can learn about the meaning of these variables in the NILT documentation (click here to access the documentation).\nIdentify the variables that are significant in this model and the direction of the relationship.\nDiscuss your interpretations with your neighbour or tutor. You can refer back to Lab 8 and Lab 9 to refresh your memory.\n\n\nWrite one introductory paragraph in the ‘Introduction’ section of the template according to the guidance provided and your preliminary insights.\nKnit the document again.\n\nYou are on the right track now!\nWe hope that by getting familiar with this setting, you will easily succeed in writing your research report using all the knowledge and skills acquired during this course. Take this session to ask questions about the assignment or the course in general as much as possible. This is the right time to have specialized one-to-one support from your tutors.\nWe wish you the best of luck! Get in touch with your tutors via email or, even better, post any questions you have about the assignment on your lab group discussion forum on Moodle, if you don’t know where to start or get stuck. Don’t suffer in silence. Remember your Tutors, lab group mates, and the teaching and admin team are here for you. Also don’t forget coding is all about trial and error, so it’s completely normal to write / copy and paste some code, then get an error message, and basically for your code to not work. Keep chipping at it, which sometimes can take hours (if not days), until you can get it to work. That’s a normal process even for professional data scientists and quantitative researchers! So do persevere and don’t panic if you don’t get it to work right away, because troubleshooting your code is part of the work, in addition to making your own choices in the analysis and reporting. For the code, the error message you get often gives you clues about, well, where the error is. So read the error message carefully, it might be you haven’t loaded the package required to run the code (remember to do this every time you open RStudio) or you might have missed a parentheses, or you might have not capitalised a word in the R syntax when you need to. Double check the R cheatsheets or previous sections in the lab workbook to ensure you have specified R syntax/arguments correctly. Again, trial and error is your friend, unlike writing an essay/doing an exam.\nGood luck and have fun! You got this.",
    "crumbs": [
      "**Lab 10** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "10-Lab10.html#introduction",
    "href": "10-Lab10.html#introduction",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "This is our final practical session. Thank you for tuning in and all your hard work! Today, we will discuss some practical aspects of the summative assessment Interpreting Quantitative Findings Report. You will actually start working on the assignment today by creating a template you can use to write your report in RStudio. Also, you will get familiar with the structure and the contents of the template. Remember, this is the perfect time to clarify as many questions as possible. Your tutor will be more than happy to help!\nBefore proceeding, please take about 10 minutes to read the instructions for the assignment available in Moodle.",
    "crumbs": [
      "**Lab 10** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "10-Lab10.html#create-your-research-report-template-from-github",
    "href": "10-Lab10.html#create-your-research-report-template-from-github",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "We’ve created a GitHub repository which contains an RStudio project and all the essentials you need to start writing your report. There are two options for you. One is writing in RStudio Cloud, as we have been doing all along this course. Alternatively, you can also use the RStudio Desktop version (this requires having R and RStudio installed locally on your device). The result will be the same and the process is very similar. So, the choice is totally up to you and what your preference is. Here, we explain how to start your own project for both options.\nFor this session, try only one of these. Don’t worry about which one you choose now, you can change your mind later. If you can’t make up your mind, then try the RStudio Cloud route first, as it means you can work on your assignment on any devices, regardless of the computational power of your device, but it does mean you need to work on the assignment online using a web browser. If online connection is an issue for you, then choose the RStudio Desktop route.\n\nIf you wish to write your project using RStudio Cloud, please follow the next steps:\n\nAccess RStudio Cloud as usual.\nMake sure you are in your Quants lab group (not in ‘Your Workspace’).\nClick on the ‘New Project’ button and select ‘New Project from Git Repository’.\n\n\n\n\n\nNew project from GitHub.\n\n\n\n\nPaste the following URL in the box and click ‘OK’: https://github.com/UGQuants/UGQuant-assignment2.\nYou are all set! Just click on the project to access the contents.\n\nIf alternately you decide to write your assessment using the RStudio Desktop version, consider the following steps:\n\nAccess the following GitHub repository copying and pasting the following URL in your browser: https://github.com/UGQuants/UGQuant-assignment2.\nClick on the ‘Code’ button and select the ‘Download ZIP’ option, as shown below.\n\n\n\n\n\nDownlowad ZIP from GitHub.\n\n\n\n\nGo to your local downloads folder, right click on the UGQuant-assignment2-main.zip and choose the ‘Extract all’ option (here, you can also chose the folder where you want to store the RStudio project and write your assignment).\nClick on the ‘Extract’ button.\nAs you can see, this folder contains an RStudio project. Open the UGQuant-assignment2.Rproj which will initialize a new RStudio session.",
    "crumbs": [
      "**Lab 10** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "10-Lab10.html#about-the-research-report-template",
    "href": "10-Lab10.html#about-the-research-report-template",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "Once in your UGQuant-assignment2 project, open the Assignmet2-template.Rmd file in Pane 4 under the ‘Files’ tab, as shown below.\n\n\n\n\nTemplate.\n\n\n\nThis template contains the following:\n\nSuggested structure of the report.\nSuggested word count for each section.\nThe code necessary to run and present the results of a multivariate linear regression.\n\nIn essence, this is the basic structure to start writing your assignment. Of course, you can add, edit, and customize as much as you consider appropriate. Remember, this is just a generic suggestion and you should still address all the points to the best of your abilities. Remember: There are no hard and fast rules to say what’s right or wrong. You are the one who can determine and justify why you did what you did. There are also many ways to write and approach this assignment, so make choices based on your own interest(s) and disciplinary background. It is truly your time to shine! The course handbook also gives important pointers on how your report will be assessed, offering some guiding questions to tackle the report–don’t skip this crucial step.",
    "crumbs": [
      "**Lab 10** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "10-Lab10.html#activity-1",
    "href": "10-Lab10.html#activity-1",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "In the template, fill in the ‘author’ and ‘date’ space in the YAML at the top of the file with your student number and appropriate information using quotation marks.\nKnit the Rmd file as html (RStudio may ask to install some packages; click ‘Yes’).\nIn the output, look at the results of the table under the ‘Results’ section and identify the dependent and the independent variables. You can learn about the meaning of these variables in the NILT documentation (click here to access the documentation).\nIdentify the variables that are significant in this model and the direction of the relationship.\nDiscuss your interpretations with your neighbour or tutor. You can refer back to Lab 8 and Lab 9 to refresh your memory.",
    "crumbs": [
      "**Lab 10** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "10-Lab10.html#activity-2",
    "href": "10-Lab10.html#activity-2",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "Write one introductory paragraph in the ‘Introduction’ section of the template according to the guidance provided and your preliminary insights.\nKnit the document again.",
    "crumbs": [
      "**Lab 10** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "10-Lab10.html#conclusion",
    "href": "10-Lab10.html#conclusion",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "You are on the right track now!\nWe hope that by getting familiar with this setting, you will easily succeed in writing your research report using all the knowledge and skills acquired during this course. Take this session to ask questions about the assignment or the course in general as much as possible. This is the right time to have specialized one-to-one support from your tutors.\nWe wish you the best of luck! Get in touch with your tutors via email or, even better, post any questions you have about the assignment on your lab group discussion forum on Moodle, if you don’t know where to start or get stuck. Don’t suffer in silence. Remember your Tutors, lab group mates, and the teaching and admin team are here for you. Also don’t forget coding is all about trial and error, so it’s completely normal to write / copy and paste some code, then get an error message, and basically for your code to not work. Keep chipping at it, which sometimes can take hours (if not days), until you can get it to work. That’s a normal process even for professional data scientists and quantitative researchers! So do persevere and don’t panic if you don’t get it to work right away, because troubleshooting your code is part of the work, in addition to making your own choices in the analysis and reporting. For the code, the error message you get often gives you clues about, well, where the error is. So read the error message carefully, it might be you haven’t loaded the package required to run the code (remember to do this every time you open RStudio) or you might have missed a parentheses, or you might have not capitalised a word in the R syntax when you need to. Double check the R cheatsheets or previous sections in the lab workbook to ensure you have specified R syntax/arguments correctly. Again, trial and error is your friend, unlike writing an essay/doing an exam.\nGood luck and have fun! You got this.",
    "crumbs": [
      "**Lab 10** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "09-Lab9.html",
    "href": "09-Lab9.html",
    "title": "Multivariate linear model",
    "section": "",
    "text": "In the last lab we mentioned that one of the advantages of linear regression is that we can include more than one independent (explanatory) variable to evaluate their relationship with the dependent variable. This technique is known as multivariate linear regression. In practice, few studies in quantitative social research rely only on simple linear models. This is because often in reality there is more than one variable associated to a social phenomenon. This is why it is important to familiarise yourself with the multivariate linear model.\n\nThe multivariate linear model is based on the same principles as the simple linear model. It is worth remembering that this model is appropriate only when we have a numeric (interval/ratio) dependent variable. As a rule-of-thumb, a variable can be treated as numeric when you have at least 7 ordered categories (Fogarty, 2019). As mentioned before, we can use any type of independent variables, such as ordinal, categorical and numeric. More than one and a combination of these can be analysed simultaneously.\n\nWe will follow-up with the example we used before introducing the simple linear model to further illustrate the advantages of using more than one variable. To do so, please set your RStudio environment as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud;\nOpen your own ‘NILT2’ project from your ‘Quants lab group’;\nOnce in the project, create a new R Script file (a simple R Script, NOT an .Rmd file).\nSave the script document as ‘multivariate_linear_model’.\n\nReproduce the code below, by copying, pasting and running it from your new script.\nFirst, install the moderndivepackage.\n\ninstall.packages(\"moderndive\")\n\nLoad the tidyverse and moderndive libraries (tidyverse was pre-installed for you before, you do not need to re-install it in this project) and read the nilt_r_object.rds file which is stored in the ‘data’ folder and contains the NILT survey.\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(moderndive)\n\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nOnce again, create a minimal random sample for the following example using the code below:\n\n# select a small random sample\nset.seed(3)\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- filter(nilt, !is.na(spage)) %&gt;% sample_n(40)\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- select(nilt_sample, rage, spage, rsex)\n\nAs a follow-up, we will draw a scatter plot using ggplot specifying the age of the respondent rage on the X axis and the respondent’s partner/spouse age spage on the Y axis. This time, we will add sex rsex as a third variable defining the color argument.\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nWhat do you observe in the plot above? …\nThe first thing to note is that females and males go in the same direction. When the respondent’s age increases the age of the partner increases as well. An interesting thing is that females tend to be located higher with respect to the Y axis compared to the male dots.\nTherefore, we can imagine that not only the age of the respondent is involved in the decision on how people choose their partner, but also the respondents’ sex. We can draw two lines, one for male and the other for female respondents, instead of only a general one as we did in the previous lab. To do this we will use the ggplot function in combination with the geom_parallel_slopes function from the moderndive package:\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nWell, our suspicion that the points representing female respondents were generally above the male ones is turning out to be true. What we have in the plot above are the optimal parallel lines that describe our points the best for each group. The interpretation of our visualizations so far is:\n\nboth males and females partner’s age is positively associated with respondent’s age.\nThis association is different for males and females. Overall, females choose older partners compared to males.\n\nBut what is the magnitude of these relationships? We can easily extend the simple linear model by adding rsex as a second explanatory variable as follows:\n\nm1 &lt;- lm(spage ~ rage + rsex, nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage + rsex, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage   rsexFemale  \n     1.7383       0.9138       4.9190  \n\n\nFrom the output, we see that there is one intercept \\(\\beta_0\\), one slope coefficient \\(\\beta_1\\) for the numeric variable rage, and another coefficient \\(\\beta_2\\) for the categorical variable. If you observe closer, the output appended only one of the two categories for rsex. This is because categorical variables take one of the categories as the reference. The variable that is not shown is the reference, ‘Male’, in this case.\nBeing more precise with our previous interpretation, we can say there is a positive relationship between both males and female participants’ age and their partner’s age by a factor of 0.91 for every additional year in age. Also, female respondent’s partners are expected to be 4.9 years older compared to male respondents.\nBefore we move on, it is worth mentioning that the criterion to fit the coefficient is the same as in the linear model. This procedure guarantees to produce the smallest possible sum of squared residuals (SSR) using the ordinary least square method (OLS). We can check if the SSR was reduced by adding this extra variable by computing the SSR as we did before:\n\nsum(residuals(m1)^2)\n\n[1] 948.263\n\n\nYes, it improved! Before, it was about 1,175, as we saw in the last lab workbook.\n\nAfter the introduction of the simple linear model, the formal definition of the multivariate linear model should not look that scary, right? In essence, this equation tells us how \\(\\hat{y}\\) is described/explained by other variables.\n\\[\n\\begin{aligned}\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}*x_1 + \\hat{\\beta_2}*x_2 + ... +  \\hat{\\epsilon}, && \\epsilon ~ N(0, \\sigma)  \n\\end{aligned}\n\\]\nWe can break down the equation above in smaller pieces as follows:\n\n\n\\(\\hat{y}\\) is the dependent variable, which is explained by\n\n\\(\\hat{\\beta_0}\\) the intercept, plus\n\n\\(\\hat{\\beta_1}\\) the slope coefficient for the first independent variable times the value of \\(x_1\\) variable 1, plus\n\n\\(\\hat{\\beta_2}\\) the slope coefficient for the second independent variable times the value of \\(x_2\\), plus\n\n\\(...\\) any number of independent variables, plus\n\n\\(\\hat{\\epsilon}\\) the error/residual term.\n\n\\(\\epsilon ~ N(0, \\sigma)\\) this bit tell us that the residuals are normally distributed.\n\n(You don’t need to reproduce the code of this section in your script).\nThe general syntax in R is as follows:\n\nlm(formula, data)\n\nWhere the formula is given by a dependent variable which is followed by ~ one or more independent variables joined by a plus sign +:\n\ndependent_variable ~ independent_variable1 + independent_variable2 ...\n\nIn the previous section, we used a small random sample of 40 people only. Now, let’s fit a multivariate linear model using all the observations in the nilt dataset. In addition to the respondent age rage and rsex, we might be interested to know whether the type of place where they live (e.g. big city, village, farm/country, etc.) placeliv plays a role in how people choose their partner.\nFirst, we need to coerce the placeliv variable as factor. And we will have a quick look at a table of this variable to have an idea of how respondents are distributed and see how the factor levels are ordered.\n\n# Coerce to factor\nnilt &lt;- nilt %&gt;%\n  mutate(placeliv = as_factor(placeliv))\n# Create table\ntable(nilt$placeliv)\n\n\n                             Dont know                          ...a big city \n                                     0                                    168 \nthe suburbs or outskirts of a big city                   a small city or town \n                                   233                                    477 \n                     a country village          a farm or home in the country \n                                   176                                    146 \n\n\nWe see that most of the respondents live in ‘a small city or town’. Also, note that the category ‘Dont know’ contains 0 observations. Let’s use the function droplevels() to remove the unused category in our variables. This also means that the first level in this variable will be ‘a big city’. Then, we will fit the model and store it in an object called m2.\n\n# remove unused levels\nnilt &lt;- droplevels(nilt)\n# boxplot\nm2 &lt;- lm(spage ~ rage + rsex + placeliv, nilt)\nm2\n\n\nCall:\nlm(formula = spage ~ rage + rsex + placeliv, data = nilt)\n\nCoefficients:\n                                   (Intercept)  \n                                       1.34251  \n                                          rage  \n                                       0.94147  \n                                    rsexFemale  \n                                       3.79426  \nplacelivthe suburbs or outskirts of a big city  \n                                       0.02099  \n                  placeliva small city or town  \n                                      -0.71196  \n                     placeliva country village  \n                                       0.25645  \n         placeliva farm or home in the country  \n                                      -0.01280  \n\n\nFrom the result:\n\nWe confirm the positive relationship between the respondent’s age and their partner by a factor of 0.94.\nThe difference in years by sex now is smaller. We see that the age of a female respondent’s partner is expected to be 3.8 years older compared to males.\nAs before, we see that the result appended the name of the categories to the placeliv variables. This is because it is comparing each of the categories with the category of reference, in this case, ‘a big city’. The interpretation must be in comparative terms. For instance, the age of the respondent’s partner in ‘the suburbs or outskirts of a big city’ is expected to be 0.02 years older than the partner of someone living in a ‘a big city’. Overall, the coefficients do not differ much as a function of the type of place of where they live. The largest expected difference is for people living in ‘a small city or town’, where partner’s age is expected to be -0.7 years old compared to those living in ‘a big city’.\n\nIn general, our model can be expressed as:\n\\[ \\hat{spage}_i = \\hat{\\beta_0} + \\hat{\\beta_1}*rage_i + \\hat{\\beta_2}*rsex_i + \\hat{\\beta_3} *  placeliv_i \\] So, if we wanted to know the expected age of the partner of a 40 years old female who lives in a small city or town, we can use and apply the formula above:\n\n1.34 + 0.94 * 40 + 3.80 * 1 + -0.7 * 1\n\n[1] 42.04\n\n\nThe expected age of a 40 year old female’s partner is, therefore, 41.7 years old.\n\nWe can obtain more details from our model using the summary() function.\n\nsummary(m2)\n\n\nCall:\nlm(formula = spage ~ rage + rsex + placeliv, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.737  -2.176  -0.139   2.214  19.530 \n\nCoefficients:\n                                               Estimate Std. Error t value\n(Intercept)                                     1.34251    0.86727   1.548\nrage                                            0.94147    0.01202  78.311\nrsexFemale                                      3.79426    0.37761  10.048\nplacelivthe suburbs or outskirts of a big city  0.02099    0.72637   0.029\nplaceliva small city or town                   -0.71196    0.65391  -1.089\nplaceliva country village                       0.25645    0.75903   0.338\nplaceliva farm or home in the country          -0.01280    0.75787  -0.017\n                                               Pr(&gt;|t|)    \n(Intercept)                                       0.122    \nrage                                             &lt;2e-16 ***\nrsexFemale                                       &lt;2e-16 ***\nplacelivthe suburbs or outskirts of a big city    0.977    \nplaceliva small city or town                      0.277    \nplaceliva country village                         0.736    \nplaceliva farm or home in the country             0.987    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.524 on 583 degrees of freedom\n  (614 observations deleted due to missingness)\nMultiple R-squared:  0.9143,    Adjusted R-squared:  0.9134 \nF-statistic:  1037 on 6 and 583 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the summary, we note that the 1st and 3rd quartile of residuals are symmetric (-2.18 to 2.21). This means that 50% of the estimated/predicted ages are approximately \\(\\pm\\) 2.2 years away from the observed partner’s age. Though, the minimum and the maximum residuals are not symmetric, this could be due to some extreme observed values. We can have a look to a histogram of the residuals to have a clearer idea.\n\nhist(m2$residuals)\n\n\n\n\n\n\n\nOverall they seem normally distributed with the exception of the negative value to the left.\nSecondly, from the summary we can see that respondent age rage and respondent sex rsex have a significant association with the dependent variable (indicated by the p-value &lt; 0.001). You can confirm this in the fourth column of the coefficients (‘Pr(&gt;|t|)’). Also, the std. error for the coefficients of this variables is small. However, the std. error for the categories of the place of where they live placeliv is large when compared to the estimated coefficients in the first column. In fact, when we look at the fourth column we see that the p-values are much larger than 0.05, which means that there is not a significant relationship between any of the type of places where they live and the partner’s respondent age. What this means is that the place where people live does not play a role in the age of the respondent’s partner.\nThird, this time we see that the multiple R-squared is slightly different from the adjusted R-squared. This is because the adjusted considers the number of independent variables included in the model. This is why the adjusted r-squared is preferred for multivariate models. This shows our model explains 91.34% of the variance of the respondent’s partner age.\nLastly, it is important to note that even though we use the full nilt data set, 614 of the observations were not considered in the analysis. This is because we do not have information for these respondents. Probably some do not have a partner or preferred not to say their sex, for example.\n\nYou need to know that these estimates are backed by well-established patterns studied in probability and statistics. Therefore, the linear model, as many other statistical techniques, provides reliable estimates if the data follows certain characteristics. These are known as assumptions. We will not go into detail with these. However, it is important that you have them in mind when you evaluate your results and also evaluate others.\nThere are many assumption for the linear regression model, but we will introduce four of the most commonly assessed (Boston University School of Public Health, 2016):\n\nLinearity: The relationship between X and the mean of Y is linear.\nHomoscedasticity: The variance of residual is the same for any value of X.\nIndependence: Observations are independent of each other.\nNormality: For any fixed value of X, Y is normally distributed.\n\nA first step to identify potential violations of the points above is to assess the distribution of the residuals by looking to the quartiles, mean and histogram. There are many other specific tests and techniques that can help us to know whether we met the assumptions and more importantly that can help to correct them.\nFor the moment, we will leave it here. The important thing is to be aware of the existence of these potential problems and to be transparent with the results you produce. For now, I would suggest you to acknowledge the limitations of your assumptions checks.\n\nSet an R Markdown document in your ‘NILT2’ project as following:\n\nCreate a new Rmd file, type ‘Multivariate linear model’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML.\nSave the Rmd document as ‘Multivariate_lab’.\nErase all the contents in the Rmd default example with the exception of the first bit (that contains the YAML) and the first R chunk (which contains the default chunk options), that is all from line 12 and on.\n\nUsing the nilt object do the following by inserting a new chunk for each bullet points below (remember to write the comments and observations for the results as simple text outside the chunks):\n\nLoad the packages, and the data that you will need in your file using the code below:\n\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(moderndive)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary.\nComment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model.\nPlot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model?\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 9** Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "09-Lab9.html#introduction",
    "href": "09-Lab9.html#introduction",
    "title": "Multivariate linear model",
    "section": "",
    "text": "In the last lab we mentioned that one of the advantages of linear regression is that we can include more than one independent (explanatory) variable to evaluate their relationship with the dependent variable. This technique is known as multivariate linear regression. In practice, few studies in quantitative social research rely only on simple linear models. This is because often in reality there is more than one variable associated to a social phenomenon. This is why it is important to familiarise yourself with the multivariate linear model.",
    "crumbs": [
      "**Lab 9** Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "09-Lab9.html#multivariate-model",
    "href": "09-Lab9.html#multivariate-model",
    "title": "Multivariate linear model",
    "section": "",
    "text": "The multivariate linear model is based on the same principles as the simple linear model. It is worth remembering that this model is appropriate only when we have a numeric (interval/ratio) dependent variable. As a rule-of-thumb, a variable can be treated as numeric when you have at least 7 ordered categories (Fogarty, 2019). As mentioned before, we can use any type of independent variables, such as ordinal, categorical and numeric. More than one and a combination of these can be analysed simultaneously.\n\nWe will follow-up with the example we used before introducing the simple linear model to further illustrate the advantages of using more than one variable. To do so, please set your RStudio environment as follows:\n\nGo to your ‘Quants lab group’ in RStudio Cloud;\nOpen your own ‘NILT2’ project from your ‘Quants lab group’;\nOnce in the project, create a new R Script file (a simple R Script, NOT an .Rmd file).\nSave the script document as ‘multivariate_linear_model’.\n\nReproduce the code below, by copying, pasting and running it from your new script.\nFirst, install the moderndivepackage.\n\ninstall.packages(\"moderndive\")\n\nLoad the tidyverse and moderndive libraries (tidyverse was pre-installed for you before, you do not need to re-install it in this project) and read the nilt_r_object.rds file which is stored in the ‘data’ folder and contains the NILT survey.\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(moderndive)\n\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nOnce again, create a minimal random sample for the following example using the code below:\n\n# select a small random sample\nset.seed(3)\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- filter(nilt, !is.na(spage)) %&gt;% sample_n(40)\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- select(nilt_sample, rage, spage, rsex)\n\nAs a follow-up, we will draw a scatter plot using ggplot specifying the age of the respondent rage on the X axis and the respondent’s partner/spouse age spage on the Y axis. This time, we will add sex rsex as a third variable defining the color argument.\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nWhat do you observe in the plot above? …\nThe first thing to note is that females and males go in the same direction. When the respondent’s age increases the age of the partner increases as well. An interesting thing is that females tend to be located higher with respect to the Y axis compared to the male dots.\nTherefore, we can imagine that not only the age of the respondent is involved in the decision on how people choose their partner, but also the respondents’ sex. We can draw two lines, one for male and the other for female respondents, instead of only a general one as we did in the previous lab. To do this we will use the ggplot function in combination with the geom_parallel_slopes function from the moderndive package:\n\n# plot\nggplot(nilt_sample, aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nWell, our suspicion that the points representing female respondents were generally above the male ones is turning out to be true. What we have in the plot above are the optimal parallel lines that describe our points the best for each group. The interpretation of our visualizations so far is:\n\nboth males and females partner’s age is positively associated with respondent’s age.\nThis association is different for males and females. Overall, females choose older partners compared to males.\n\nBut what is the magnitude of these relationships? We can easily extend the simple linear model by adding rsex as a second explanatory variable as follows:\n\nm1 &lt;- lm(spage ~ rage + rsex, nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage + rsex, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage   rsexFemale  \n     1.7383       0.9138       4.9190  \n\n\nFrom the output, we see that there is one intercept \\(\\beta_0\\), one slope coefficient \\(\\beta_1\\) for the numeric variable rage, and another coefficient \\(\\beta_2\\) for the categorical variable. If you observe closer, the output appended only one of the two categories for rsex. This is because categorical variables take one of the categories as the reference. The variable that is not shown is the reference, ‘Male’, in this case.\nBeing more precise with our previous interpretation, we can say there is a positive relationship between both males and female participants’ age and their partner’s age by a factor of 0.91 for every additional year in age. Also, female respondent’s partners are expected to be 4.9 years older compared to male respondents.\nBefore we move on, it is worth mentioning that the criterion to fit the coefficient is the same as in the linear model. This procedure guarantees to produce the smallest possible sum of squared residuals (SSR) using the ordinary least square method (OLS). We can check if the SSR was reduced by adding this extra variable by computing the SSR as we did before:\n\nsum(residuals(m1)^2)\n\n[1] 948.263\n\n\nYes, it improved! Before, it was about 1,175, as we saw in the last lab workbook.\n\nAfter the introduction of the simple linear model, the formal definition of the multivariate linear model should not look that scary, right? In essence, this equation tells us how \\(\\hat{y}\\) is described/explained by other variables.\n\\[\n\\begin{aligned}\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}*x_1 + \\hat{\\beta_2}*x_2 + ... +  \\hat{\\epsilon}, && \\epsilon ~ N(0, \\sigma)  \n\\end{aligned}\n\\]\nWe can break down the equation above in smaller pieces as follows:\n\n\n\\(\\hat{y}\\) is the dependent variable, which is explained by\n\n\\(\\hat{\\beta_0}\\) the intercept, plus\n\n\\(\\hat{\\beta_1}\\) the slope coefficient for the first independent variable times the value of \\(x_1\\) variable 1, plus\n\n\\(\\hat{\\beta_2}\\) the slope coefficient for the second independent variable times the value of \\(x_2\\), plus\n\n\\(...\\) any number of independent variables, plus\n\n\\(\\hat{\\epsilon}\\) the error/residual term.\n\n\\(\\epsilon ~ N(0, \\sigma)\\) this bit tell us that the residuals are normally distributed.",
    "crumbs": [
      "**Lab 9** Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "09-Lab9.html#r-syntax-for-the-multivariate-linear-model",
    "href": "09-Lab9.html#r-syntax-for-the-multivariate-linear-model",
    "title": "Multivariate linear model",
    "section": "",
    "text": "(You don’t need to reproduce the code of this section in your script).\nThe general syntax in R is as follows:\n\nlm(formula, data)\n\nWhere the formula is given by a dependent variable which is followed by ~ one or more independent variables joined by a plus sign +:\n\ndependent_variable ~ independent_variable1 + independent_variable2 ...\n\nIn the previous section, we used a small random sample of 40 people only. Now, let’s fit a multivariate linear model using all the observations in the nilt dataset. In addition to the respondent age rage and rsex, we might be interested to know whether the type of place where they live (e.g. big city, village, farm/country, etc.) placeliv plays a role in how people choose their partner.\nFirst, we need to coerce the placeliv variable as factor. And we will have a quick look at a table of this variable to have an idea of how respondents are distributed and see how the factor levels are ordered.\n\n# Coerce to factor\nnilt &lt;- nilt %&gt;%\n  mutate(placeliv = as_factor(placeliv))\n# Create table\ntable(nilt$placeliv)\n\n\n                             Dont know                          ...a big city \n                                     0                                    168 \nthe suburbs or outskirts of a big city                   a small city or town \n                                   233                                    477 \n                     a country village          a farm or home in the country \n                                   176                                    146 \n\n\nWe see that most of the respondents live in ‘a small city or town’. Also, note that the category ‘Dont know’ contains 0 observations. Let’s use the function droplevels() to remove the unused category in our variables. This also means that the first level in this variable will be ‘a big city’. Then, we will fit the model and store it in an object called m2.\n\n# remove unused levels\nnilt &lt;- droplevels(nilt)\n# boxplot\nm2 &lt;- lm(spage ~ rage + rsex + placeliv, nilt)\nm2\n\n\nCall:\nlm(formula = spage ~ rage + rsex + placeliv, data = nilt)\n\nCoefficients:\n                                   (Intercept)  \n                                       1.34251  \n                                          rage  \n                                       0.94147  \n                                    rsexFemale  \n                                       3.79426  \nplacelivthe suburbs or outskirts of a big city  \n                                       0.02099  \n                  placeliva small city or town  \n                                      -0.71196  \n                     placeliva country village  \n                                       0.25645  \n         placeliva farm or home in the country  \n                                      -0.01280  \n\n\nFrom the result:\n\nWe confirm the positive relationship between the respondent’s age and their partner by a factor of 0.94.\nThe difference in years by sex now is smaller. We see that the age of a female respondent’s partner is expected to be 3.8 years older compared to males.\nAs before, we see that the result appended the name of the categories to the placeliv variables. This is because it is comparing each of the categories with the category of reference, in this case, ‘a big city’. The interpretation must be in comparative terms. For instance, the age of the respondent’s partner in ‘the suburbs or outskirts of a big city’ is expected to be 0.02 years older than the partner of someone living in a ‘a big city’. Overall, the coefficients do not differ much as a function of the type of place of where they live. The largest expected difference is for people living in ‘a small city or town’, where partner’s age is expected to be -0.7 years old compared to those living in ‘a big city’.\n\nIn general, our model can be expressed as:\n\\[ \\hat{spage}_i = \\hat{\\beta_0} + \\hat{\\beta_1}*rage_i + \\hat{\\beta_2}*rsex_i + \\hat{\\beta_3} *  placeliv_i \\] So, if we wanted to know the expected age of the partner of a 40 years old female who lives in a small city or town, we can use and apply the formula above:\n\n1.34 + 0.94 * 40 + 3.80 * 1 + -0.7 * 1\n\n[1] 42.04\n\n\nThe expected age of a 40 year old female’s partner is, therefore, 41.7 years old.\n\nWe can obtain more details from our model using the summary() function.\n\nsummary(m2)\n\n\nCall:\nlm(formula = spage ~ rage + rsex + placeliv, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.737  -2.176  -0.139   2.214  19.530 \n\nCoefficients:\n                                               Estimate Std. Error t value\n(Intercept)                                     1.34251    0.86727   1.548\nrage                                            0.94147    0.01202  78.311\nrsexFemale                                      3.79426    0.37761  10.048\nplacelivthe suburbs or outskirts of a big city  0.02099    0.72637   0.029\nplaceliva small city or town                   -0.71196    0.65391  -1.089\nplaceliva country village                       0.25645    0.75903   0.338\nplaceliva farm or home in the country          -0.01280    0.75787  -0.017\n                                               Pr(&gt;|t|)    \n(Intercept)                                       0.122    \nrage                                             &lt;2e-16 ***\nrsexFemale                                       &lt;2e-16 ***\nplacelivthe suburbs or outskirts of a big city    0.977    \nplaceliva small city or town                      0.277    \nplaceliva country village                         0.736    \nplaceliva farm or home in the country             0.987    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.524 on 583 degrees of freedom\n  (614 observations deleted due to missingness)\nMultiple R-squared:  0.9143,    Adjusted R-squared:  0.9134 \nF-statistic:  1037 on 6 and 583 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the summary, we note that the 1st and 3rd quartile of residuals are symmetric (-2.18 to 2.21). This means that 50% of the estimated/predicted ages are approximately \\(\\pm\\) 2.2 years away from the observed partner’s age. Though, the minimum and the maximum residuals are not symmetric, this could be due to some extreme observed values. We can have a look to a histogram of the residuals to have a clearer idea.\n\nhist(m2$residuals)\n\n\n\n\n\n\n\nOverall they seem normally distributed with the exception of the negative value to the left.\nSecondly, from the summary we can see that respondent age rage and respondent sex rsex have a significant association with the dependent variable (indicated by the p-value &lt; 0.001). You can confirm this in the fourth column of the coefficients (‘Pr(&gt;|t|)’). Also, the std. error for the coefficients of this variables is small. However, the std. error for the categories of the place of where they live placeliv is large when compared to the estimated coefficients in the first column. In fact, when we look at the fourth column we see that the p-values are much larger than 0.05, which means that there is not a significant relationship between any of the type of places where they live and the partner’s respondent age. What this means is that the place where people live does not play a role in the age of the respondent’s partner.\nThird, this time we see that the multiple R-squared is slightly different from the adjusted R-squared. This is because the adjusted considers the number of independent variables included in the model. This is why the adjusted r-squared is preferred for multivariate models. This shows our model explains 91.34% of the variance of the respondent’s partner age.\nLastly, it is important to note that even though we use the full nilt data set, 614 of the observations were not considered in the analysis. This is because we do not have information for these respondents. Probably some do not have a partner or preferred not to say their sex, for example.",
    "crumbs": [
      "**Lab 9** Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "09-Lab9.html#assumptions",
    "href": "09-Lab9.html#assumptions",
    "title": "Multivariate linear model",
    "section": "",
    "text": "You need to know that these estimates are backed by well-established patterns studied in probability and statistics. Therefore, the linear model, as many other statistical techniques, provides reliable estimates if the data follows certain characteristics. These are known as assumptions. We will not go into detail with these. However, it is important that you have them in mind when you evaluate your results and also evaluate others.\nThere are many assumption for the linear regression model, but we will introduce four of the most commonly assessed (Boston University School of Public Health, 2016):\n\nLinearity: The relationship between X and the mean of Y is linear.\nHomoscedasticity: The variance of residual is the same for any value of X.\nIndependence: Observations are independent of each other.\nNormality: For any fixed value of X, Y is normally distributed.\n\nA first step to identify potential violations of the points above is to assess the distribution of the residuals by looking to the quartiles, mean and histogram. There are many other specific tests and techniques that can help us to know whether we met the assumptions and more importantly that can help to correct them.\nFor the moment, we will leave it here. The important thing is to be aware of the existence of these potential problems and to be transparent with the results you produce. For now, I would suggest you to acknowledge the limitations of your assumptions checks.",
    "crumbs": [
      "**Lab 9** Multivariate Linear Regression"
    ]
  },
  {
    "objectID": "09-Lab9.html#lab-activities",
    "href": "09-Lab9.html#lab-activities",
    "title": "Multivariate linear model",
    "section": "",
    "text": "Set an R Markdown document in your ‘NILT2’ project as following:\n\nCreate a new Rmd file, type ‘Multivariate linear model’ in the ‘Title’ section and your name in the ‘Author’ box. Leave the ‘Default Output Format’ as HTML.\nSave the Rmd document as ‘Multivariate_lab’.\nErase all the contents in the Rmd default example with the exception of the first bit (that contains the YAML) and the first R chunk (which contains the default chunk options), that is all from line 12 and on.\n\nUsing the nilt object do the following by inserting a new chunk for each bullet points below (remember to write the comments and observations for the results as simple text outside the chunks):\n\nLoad the packages, and the data that you will need in your file using the code below:\n\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(moderndive)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary.\nComment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model.\nPlot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model?\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 9** Multivariate Linear Regression"
    ]
  }
]