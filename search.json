[
  {
    "objectID": "02-Lab2.html",
    "href": "02-Lab2.html",
    "title": "Data in R",
    "section": "",
    "text": "In the previous lab, we set up an RStudio session in Posit Cloud and we got familiar with the RStudio environment and the purpose and contents of RStudio’s panes. In this Lab we will learn about R packages, how to install them and load them. We will also use different types of data. You will have the chance to practice with additional R operators. Lastly, we will load a real-world data set and put in practice your new skills.\n\n\n\n\n\n\nNoteOverview\n\n\n\nBy the end of this lab you will know how to:\n\ninstall and load packages in R\ndownload a dataset from a URL and assign it to a named object\nuse the pipe (|&gt;) operator to chain steps\nexamine, wrangle, and subset a dataset using functions\n\n\n\n\nAs mentioned in our last lab, R (R Core Team 2021) is a collaborative project. This means that R users are developing, maintaining, and extending the functionalities constantly. When you set up R and RStudio for the first time, as we did last week, it comes only with the ‘basic’ functionalities by default, sometimes referred to as ‘base R’. However, there are literally thousands of extensions that are developed by other users. In R, these non-default extensions are called packages.\nMost of the time, we use packages because they simplify our work in R - such as by replacing what would take dozens of lines of complex code in base R with a simple one-line function - or they allow us to extend beyond the capabilities of base R.\nLet’s install and load some useful packages. We will start with one of R’s most famous packages, and one we will use across the labs, the tidyverse (Wickham 2021) package.1 First, sign-in to Posit Cloud, create/open your Labs-1-2 project, and then open the Lab-2 R Markdown file.\n\nFor those who missed Lab 1:\n\nMake sure you have a free, institutional-subscription Posit Cloud account (in case you have not created one yet, please follow the guidance provided in Lab 1);\nIf you enrolled before teaching started, you will have received an email with a link to join your lab group. If you joined the course after the start of teaching, you can find a link in your Lab Group’s forum on Moodle. You tutor will also be able to provide you with a link.\nFollow the rest of the Lab 1 guidance in the ‘Create a New Project’ section.\n\nFor those who already joined and created a project in Lab 1, log into Posit Cloud and:\n\nFrom the left-hand menu, click “Lab Group ..” where “..” is your lab group number.\nWithin the main screen, click “Labs-1-2” to open the Posit Cloud project you created last week.\n\n\n\nSelect your lab group space and then open ‘Labs-1-2’ project\n\n\nYou will find your project in the same state as you left it last week. To open a tab for the Lab 2 R Markdown file, click to open ‘Lab-02.Rmd’ in the “Files” tab in the bottom-right pane.\n\n\nClick ‘Lab-02.Rmd’ in the “Files” tab in the bottom-right pane\n\nWithin Lab 1, you may have noticed the key: value lines between two ---. This is called YAML. YAML is a simple human-readable markup language for data serialization. It consists of what are referred to as key-value pairs. Within R Markdown, a YAML header can be included at the very top and must be fenced with the three hyphens, ---, at start and end.\nThe R Markdown file for this lab has a couple more lines in the YAML header this time:\n\n\nYAML block at top of Lab-02.Rmd\n\ntitle and author are self-explanatory, but make sure to replace [your name here] with your name, making sure to keep the quotation marks around it. Without the quotation marks, you will receive an error message when trying to knit. (Note: Don’t knit your file just yet anyway as it will result in an error message until you complete the sections below.)\ndate can be any text. However, format(Sys.time(), '%d/%m/%y') is a nice single line of R code that runs when the file is knitted to retrieve the current date and formats it as dd/mm/yyyy, such as 12/09/2025. This then effectively makes the date field “date this file was knitted” and saves you having to manually update it each time.\noutput specifies what file format(s) to output to when knitting the R Markdown file. Across the labs and assessments we are using ‘html_document’ as it is easiest to work with across the packages we are using in the labs and assessments.\nAfter the YAML header, there is a ‘setup’ code chunk setting knitr options:\n\nThis may look intimidating, but it is relatively simple when parsed bit by bit:\n\n\nknitr is the package used to knit R Markdown files\n\nknitr::opts_chunk is an object that stores default settings applied to all code chunks\n\nknitr::opts_chunk$set(...) runs the set() function to change the knitr::opts_chunk default settings\n\nSo, what the code in the setup chunk does is set messages and warnings to “FALSE”. This prevents any messages or warnings that get raised when running code chunks from also appearing in any knitted files. (Why we do this will become clear in a bit.)\n\n\n\n\n\n\nCautionMore R Operators\n\n\n\n\n\n:: is the namespace operator for accessing an object (such as a function or dataset) from a package, in the format package::object. If you load a package using the library() function, as we will do later, you can call objects from it directly as object without needing to include package:: before it.\n\n$ is the component operator for accessing named elements within an object, in the format object$component. In data analysis it is often used to access a column (variable) from a data frame. For example, if a data frame named df has a column named age, you can access it with df$age.\n\n[] (which is introduced later in this lab) is the subsetting operator and is used to extract elements from an object, in the format object[...]. In contrast to $ where we used the name for the component we wanted, [] uses numbers. If we had a data frame (i.e. a table) called df and ran the code df[2, 4], R would return the value found in the fourth column of the second row.\n\n\n\n\n\nOnce you have the Lab 2 R Markdown file open, type the following in the Console (bottom-right pane) and hit ‘Enter’ to run it:\n\n\ninstall.packages(\"tidyverse\")\n\n\nWait until you get the message ‘The downloaded source packages are in …’. The install process can take a couple of minutes to finish.\n\n/\n\nOnce the package is installed, you next need to load it using the library() function. In the preamble code chunk already setup in the R Markdown file, add the following in the line under the # Load packages comment:\n\n\nlibrary(tidyverse)\n\n\nNow run the code chunk. Either by pressing Ctrl+Shift+Enter with your text cursor within the code chunk, or by clicking the green triangle in the top-right of the code chunk.\n\n\nAnd that’s it, tidyverse is ready to use in your current session!\nNote, we run install.packages(\"package_name\") in the Console as we only need to install the package once for each Posit Cloud project. We load packages with library(package_name) in a code chunk at the top of the R Markdown file as packages need to be loaded for every R session. If we ran library(...) in the Console instead of adding it to a code chunk, our packages would not be available in the fresh R session that is created when knitting and would result in an error message.\nAnother thing to note is that when you install a package, you need to use quotation marks, install.packages(\"package_name\"), whereas when loading a package you just use the package name without quotation marks, library(package_name).\nWhat you may also notice is the message raised in the Console when loading the tidyverse. (If you already ran the setup code chunk that set messages and warnings to FALSE you won’t see any text). This is one of the reasons why we have the setup chunk to exclude messages and warnings in knitted files. If we knitted the Lab-02.Rmd file without the setup chunk, the text on loading the tidyverse would also appear in the knitted HTML file after the preamble chunk. We will cover other options you can set to customise what appears in your knitted files in later weeks.\n\nThis text often causes users new to R to think an error has occurred. However, it is merely a ‘message’ about conflicts that can be ignored.\nThe tidyverse is a meta package that bundles a collection of packages together, such as dplyr and ggplot2. These packages share a common design philosophy and are often used together in data analysis. Loading the tidyverse meta package, library(tidyverse), saves us from having to load each of these packages individually with library(dplyr), library(ggplot2), and so on.\nWhat the message is showing then is that each of the core tidyverse packages were loaded OK, and there are conflicts for functions provided by the dplyr package. Two of its functions, filter() and lag(), share the same name as functions in the base R stats package. After loading the tidyverse, any code calling filter() will use the dplyr function with that name rather than the stats ones. (This applies only to filter() directly, the stats function can still be accessed when the tidyverse is loaded by using the :: namespace operator, stats::filter().)\n\n\n\n\n\n\nTipNew Terms\n\n\n\n\n\nYAML: a human-readable data serialisation language, often used for configuration/settings; in R Markdown the YAML front matter between --- sets document metadata and other options.\n\ntidyverse: a collection of R packages that share a consistent design philosophy and work together for importing, wrangling, and visualising data.\n\ndata frame: an object in R that stores data in a two-dimensional table with rows (observations) and columns (variables). We load a file containing a data set into a data frame within R.\n\ntibble: a modern version of a data frame used in tidyverse packages that has more consistent data handling than base R’s data frames. For simplicity, we will still refer to these in the labs as data frames.\n\n\n\n\n\n\n\n\n\nCautioninstall.packages() function\n\n\n\nInstall an R package that is available from the Comprehensive R Archive Network.\nUsage: install.packages(\"package_name\")\nArguments: \"package_name\" with the name of the package you want to install that must be in quotes, such as install.packages(\"tidyverse\").\nNotes: Only needs to be run once per computer, or once per project in Posit Cloud.\n\n\n\n\n\n\n\n\nCautionlibrary() function\n\n\n\nLoad an installed package so its functions are available in the R session.\nUsage: library(package_name)\nArguments: package_name with the name of the installed package you want to load.\nNotes:\n\nA package needs to be installed first using install.packages(\"package_name\") in the Console.\nIt is best practice to load packages in a code chunk at the top of R Markdown files.\n\n\n\n\nIn quantitative analysis, we often distinguish two main types of variables:\n\n\nNumeric variables have values that describe measurable or countable quantities.\n\n\nContinuous variables have values that can fall anywhere within a range, such as time and speed.\n\nDiscrete variables have values that are whole numbers that count something, such as number of children in a household.\n\n\n\nCategorical variables have values that describe distinct, mutually exclusive categories.\n\n\nNominal variables have categories without any order, such as country, name, political party, and gender.\n\nOrdinal variables have categories with a meaningful order, such as education level or level of satisfaction.\n\n\n\nIn R, the most basic data building blocks are atomic vectors. There are six atomic types: logical, integer, double, character, complex, and raw.\nIn relation to our two main types of variables, we use the following vectors:\n\n\nNumeric vectors (integer or double) to store continuous and discrete values.\n\nFactor vectors (a class built on integer codes + character labels) to store nominal and ordinal categorical values.\n\nA numeric vector is usually stored as a double internally by default. The difference between integer and double is based on how they are computationally stored. Base R and most R packages handle the difference behind the scenes, so the difference in how they are stored does not matter to us.\nA factor is a class combining integer codes and a levels attribute containing the human-readable character labels for those codes. Each stored integer points to a character label in levels, so 1 might correspond to \"Conservative\", 2 might correspond to \"Labour\", and so on. For ordered factors, used for ordinal variables, R also records an explicit order of the levels to enable comparisons and sorting by that order.\nTwo other vectors we commonly use are:\n\n\nCharacter vectors (character) to store text, such as unique respondent IDs (“A102”, “C006”) and free-text answers.\n\nLogical vectors (logical) to store boolean TRUE / FALSE values, useful for filtering and conditions, such as age &gt; 30.\n\nIn R, there are couple of functions that will help us to identify the type of data. First, we have glimpse(). This prints some of the main characteristics of a data set, namely its overall dimension, name of each variable (column), the first values for each variable, and the type of the variable. Second we have the function class(), that will help us to determine the overall class(type) of on R object.\n\nWe are now going to use some datasets that are available to us in the R session. R comes with some example datasets out of the box, such as iris with 150 observations of iris flowers. Some R packages also include additional example datasets, such as starwars - included in the dplyr tidyverse package - with info about 87 Star Wars characters. Unlike other datasets that we have to manually load, and will cover in later section, we can access these in any R session out the box or after loading the relevant package that provides them.\nPlease go to the ‘Types of variables’ section in your Lab-02.Rmd file. You will see a few code chunks already setup for you:\n\nWe will start with a classic dataset example in R called iris. (For more info about the dataset, you can type ?iris or help(iris) in the Console). Please go to the “iris-glimpse” code chunk in your R Markdown file and run it.\n\n\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nWhat do you observe from the output?\nFirst, it tells you the number of rows and the columns on the top. After, it lists the name of each variable. Additionally, it tells you the type of the variable between these symbols &lt; &gt;. The first five variables in this dataset are of type &lt;dbl&gt; (double) which as covered above is a type of numeric variable. The last, Species, is a factor &lt;fct&gt;. So, for each of the 150 iris flowers observed, there is information on its species and four types of continuous measures. Though, as glimpse only provides a preview, we only see the values for the first few observations.\n\nNow you know that each iris flower belongs to a species, but what are the specific categories in this data set? To find out, add the following in the ‘iris-levels’ code chunk and then run it.\n\n\nlevels(iris$Species)\n\n(If you receive ‘NULL’ when running the chunk, it because you have species and not Species.)\nAs you can see, there are three categories, or levels as they are called in R factor variables, which are three types of iris flower species. Notice here we used the $ component operator mentioned earlier. Here it basically means, from the iris data frame select the Species column/variable. levels() is then a nice little utility function that shows all the levels for a column given to it as an argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s get serious and explore Star Wars.\nThe starwars data set from the dplyr package contains information about the characters, including height, hair colour, and sex. (Again, to get more information run ?starwars or help(starwars) in the Console). For the purpose of keeping things relatively simple to focus on variable types, we will use a reduced version of the full data set.\n\nFirst, run the ‘starwars-glimpse’ code chunk to create a reduced version of the data set named starwars2 and then glimpse the Star Wars characters in that reduced data set:\n\n\nstarwars2 &lt;- starwars[, 1:11]\nglimpse(starwars2)\n\nCreating a reduced version of the data frame uses another selection operator, [...]. The basic format is data_frame[rows, columns]. Here starwars[, 1:11] leaves the rows argument blank, which effectively means select all rows, and the columns argument has 1:11, which effectively means select columns 1 to 11. So, the code returns a copy of the data frame with all rows but only the first 11 columns.\nWhat do you observe this time?\nIt seems that the data type is not consistent with their content. For example, the variables species, gender, and hair_color are of type &lt;chr&gt; (that is character), when according to what we covered above these should be factors. To transform them, we will use the function ´factor()´. This process is known as coercing a variable, that is when you change a variable from one type to another. factor() is another nice little helper function\n\n\n\n\n\n\n\n\n\nLet’s coerce the species variable from character to factor and assign the result to the same column in the dataset. Add the following to the ‘starwars-factor’ code chunk and run it.\n\n\nstarwars2$species &lt;- factor(starwars2$species)\n\nNote, most functions in R for data analysis do not directly change the object -such as a data frame - passed to it. Instead, the functions return a new object with the changes. An explicit assignment back to the original object is required if you want to change it. For example, if you run factor(starwars2$species on its own in the Console, it will return all the values for species and the levels created. However, for the species column in starwars2 to be updated with the results returned by factor(starwars2$species) we need the starwars2$species &lt;- ... assignment as well. What the code is effectively saying is “replace the species column in starwars2 with the version returned by the factor function”.\n\nLet’s check if the type of variable really changed by glimpsing the data and checking the levels of species. Add the following to the ‘starwars-glimpse2’ chunk and run it.\n\n\nglimpse(starwars2)\nlevels(starwars2$species)\n\nThe glimpse result now is telling us that species is a &lt;fct&gt;, as expected. Furthermore, the levels() function reveals that there are 37 types of species, including Human, Ewok, Droid, and more.\nHopefully, these examples will help you to identify the main vector types and, more importantly, an initial understanding of how to coerce them into an appropriate type. Be aware that many data sets represent categories with numeric values without the labels, for example, using ‘0’ for male and ‘1’ for female - and as part of the initial process turning these variables into factors you will also need to provide a list mapping these numbers to the labels to use. Usually, large data sets are accompanied by extra information in a code book or documentation file, which specifies the values for the numeric code and their respective meaning. It’s important to read the code book/documentation of every data set rather than assuming what the numbers stand for as the conventions and meanings can vary. For example, some surveys now put gender in alphabetical order, with 0 for female and 1 for male.\n\n\n\n\n\n\nCautionglimpse() function\n\n\n\nProvides a ‘glimpse’ of an object. From the dplyr package, and loaded with the tidyverse package.\nUsage: glimpse(x)\nArguments:\n\n\nx - The object (usually a data frame) to glimpse.\n\nReturns: For data frames, it returns counts for rows and columns and a compact, transposed preview showing column names, types, and a few example values. Types are shown as &lt;int&gt; (integer), &lt;dbl&gt; (double), &lt;chr&gt; (character), &lt;lgl&gt; (logical), &lt;fct&gt; (factor).\n\n\n\n\n\n\n\n\nCautionfactor() function\n\n\n\nCreate a categorical variable (factor) with fixed, named levels.\nUsage: factor(x, levels, ordered = is.ordered(x))\nArguments:\n\n\nx - A vector, such as a data frame column, to turn into a factor.\n\nlevels - The allowed values and their order. If omitted, all unique values in x are used to create the levels.\n\nordered - Set to TRUE where order matters when creating an ordered factor.\n\nNotes: Whenever you see text like ordered = is.ordered(x) in a function documentation, this shows the name of the argument and its default value. In this example, the default value is for ordered is calculated using another function. The is.ordered(x) checks whether x is already an ordered factor and returns TRUE or FALSE. If you are using factor() to adjust the levels/labels of an existing ordered factor then, you can leave the ordered = TRUE out.\n\n\n\n\n\n\n\n\nCautionlevels() function\n\n\n\nGet the category labels (“levels”) of a factor.\nUsage: levels(x)\nArguments: x - A factor variable, such as data_frame$country.\nReturns: A character vector of level labels for the factor variable.\n\n\n\n\n\nA useful operator is the pipe |&gt;. This is the base R version of the pipe. This operator is what is known in programming as syntactic sugar. It is syntax which helps make code easier to read and write without changing any functionality. The pipe operator passes the result of one function as the first argument to the next (left to right).\nRun the ‘pipe-1’ code chunk and check its results.\n\n1 |&gt; sum(1)\n1 |&gt; sum(1) |&gt; sum(5)\n\nThe sum() function adds the value of two numbers in format sum(x, y). Using a pipe for 1 |&gt; sum(1) is the equivalent to sum(1, 1). If we were to write sum(x, y) with a pipe, it would be x |&gt; sum(y). So, in our example code chunk, the pipe let’s us ‘unpack’ the first argument in each function, whereby line 1 reads equivalent to saying ‘take 1, then add it with 1’ and line 2 ‘take 1, then add it with 1, then add it with 5’.\nThis may seem as if it is overcomplicating things, but as the code becomes more complex the value of the pipe operator becomes clearer. The second line, 1 |&gt; sum(1) |&gt; sum(5) would be written as sum(sum(1,1), 5) without pipes. Going left to right, 1 |&gt; sum(1) would be sum(1,1) and then sum(1,1) |&gt; sum(5) would be sum(sum(1,1), 5).\nPipes then help us avoid foo(foo(foo(...), ...) ...) monstrosities in our code. In coding examples, you will often see words like foo, bar and baz used as placeholder names when explaining how coding syntax works rather than using real functions. Similarly, x, y, and z often get used as placeholder names for variables / arguments.\nAs we will cover in examples in the next section below, plenty of functions take more than two arguments. Let’s imagine we had a data frame named df and we were updating it by using - in order - foo, bar, and baz functions that each take arguments x, y, and z. Without pipes our code would be:\n\ndf &lt;- baz(bar(foo(df, y, z), y, z), y, z)\n\nThis is a tangled mess, we want the result from foo passed to bar and its result in turn passed to baz. This ordering though gets visually reversed when nesting each function within the other - we have to read it inside out for it to make sense.\nWith pipes, it instead becomes:\n\ndf &lt; - df |&gt;\n  foo(y, z) |&gt;\n  bar(y, z) |&gt;\n  baz(y, z)\n\nNow it is easy to read and the ordering of the functions matches how we could verbally describe what the code does, “we are updating a data frame called df by running the functions foo, bar and baz on it in turn”. In the code chunk above, we make use of another wonderful thing about pipes - it let’s us break complex code over multiple lines. To do so you add the pipe at the end of the line before each function, then start each new line with an indent followed by the function. If you type |&gt; at the end of a line and hit enter, RStudio will add the spaces at the start of the new line for you.\nNote, in the readings and other materials, you may come across another %&gt;% pipe. This pipe comes from the tidyverse. The tidyverse pipe came before the base R one, where due to its popularity an equivalent was added to base R. Whilst you will still see it in use, and it remains supported in the tidyverse, the tidyverse itself recommends people start using the base R |&gt; pipe. To see that they work the same, run the ‘pipe-2’ chunk.\n\nIn this section we will work with data originally collected by The Guardian in 2015, for more information click here. The data set we will use today is an extended version which was openly shared in GitHub by the American news website FiveThirtyEight. This data set contains information about the people that were killed by police or other law enforcement bodies in the US, such as age, gender, race/ethnicity, etc. Additionally, it includes information about the city or region where the event happened. For more information click here.\n\nAs with installing packages, we only want to download the data once. For this lab, we will download the data running code in the Console. Next week though, we will go through how to set up an R script for downloading data. This is a common strategy for sharing the code used to download data whilst still keeping it separate to your main R Markdown file.\nFirst, we will create a new folder in our project directory to store the data. To do it from the Console, run this line. (Don’t worry if you get a warning. This appears because you already have a folder with this name):\n\ndir.create(\"data\")\n\nNote that in the ‘Files’ tab of Pane 4, the bottom-right pane, there is a new folder called data.\n\nNow, download the data from the GitHub repository using the function download.file(). This function takes two arguments separated by a comma: (1) the URL and (2) the destination (including the directory, file name, and file extension), as shown below.\nCopy and paste the following lines into the Console:\n\ndownload.file(\"https://github.com/fivethirtyeight/data/raw/master/police-killings/police_killings.csv\", \"data/police_killings.csv\")\n\n(Note, to avoid any issues typing out the URL, you can click the clipboard icon in the top-right of the code above to copy the full contents to your clipboard.)\nAfter that, we are ready to read the data. As the data comes as a .csv file, we can use the read_csv() function included in the tidyverse package (make sure the package is loaded in your session as explained in a previous section).\nThe read_csv() reads the data in the file and returns it as a data frame in R. We need then to assign this data frame to a named object so we can continue working with it. In our example below we will do this by assigning it to a named object police. So, in effect, our new police named object will be a data frame containing the data from the .csv file we downloaded.\nImportantly, as with loading packages, when loading data it is good practice to do so at the top of the file. So, go all the way back up to the preamable code chunk and add the following line after the ‘# Read data’ comment.\n\npolice &lt;- read_csv(\"data/police_killings.csv\")\n\nYou should now have:\n\nRun the code chunk and you’ll know it has run OK if can see ‘police’ in your Global Environment in the top-right pane. It will have 467 observations and 34 variables (columns).\n\n\nOK, before we look at this police data, a reminder about creating code chunks. Scroll back in your R Markdown file to “## Black lives matter!”. For this section, you will need to create your own code chunks.\nTo create a new code chunk, you have four options:\n\nPut your text cursor on a line in your R Markdown file and press Ctrl+Alt+I on the keyboard.\nFrom the menu bar at the top of RStudio, you can select Code &gt; Insert Chunk.\nManually type three backticks, on UK layout keyboards the key to the left of the 1 key, and then {r} (remember to add three backticks on a line below to close the code chunk as well).\nClick on the icon that is a c in green square with a ‘+’ in a circle in top-left corner, and then click on R to create an R code chunk.\n\n\nBy default, a new code chunk will be plain {r}, but you can edit this to give it an optional name:\n\n\nBack to the data. Let’s first glimpse the police data. Create a code chunk then add and run the following:\n\nglimpse(police)\n\nAs you can see, there are several variables included in the dataset, such as age, gender, law enforcement agency (lawenforcementagency), or whether the victim was armed (armed). You will see some of these variables are not in the appropriate type. For instance, some are categorical and should be a factor (&lt;fct&gt;) instead of character (&lt;chr&gt;).\n\nBefore coercing these variables, we will create a smaller subset of the data frame by selecting only the variables that we are interested in. To do so, we can use the select() function. The select() function takes the name of the data frame as the first argument and then the names of the variables we want to keep separated by commas (no quotation marks needed).\nRemember with pipes we can replace select(data_frame, variable1, variable2, ...) with data_frame |&gt; select(variable1, variable2, ... making it easier to read as “from our data frame select the following variables”. Let’s create a code chunk to select a few variables and assign the result to a new object called police_2.\n\npolice_2 &lt;- police |&gt; select(age, gender, raceethnicity, lawenforcementagency, armed)\n\nThe select() function is from the dplyr tidyverse package. Many tidyverse functions that take a data frame as the first argument, foo(date_frame, ...) or when using a pipe data_frame |&gt; foo(...), will assume all variables listed are from that data frame. Within the function, this then let’s us type age and gender rather than police$age and police$gender.\nIf you look again to the ‘Environment’ tab, there is now a second data frame with the same number of observations but only 5 variables. Create a new code chunk, then add and run the following code to glimpse its contents.\n\nglimpse(police_2)\n\nHaving a closer look at the reduced version, we can see that in fact all the variables are of type &lt;chr&gt; (character), including age.\nLet’s coerce the variables into their correct type. Start by creating a code chunk and naming it ‘coerce-police_2’ or similar.\nThen within the chunk, let’s start with age, coercing it from character to numeric by adding the following lines:\n\n# Coerce numeric\npolice_2 &lt;- police_2 |&gt; mutate(age = as.numeric(age))\n\nThe # Coerce numeric line let’s us add a comment to our code. Any text within an R code chunk after a # is treated as a comment and is not run as part of the code. This is useful for recording the ‘why’ behind the code, so future-you (and others) can quickly read and understand it.\n(You may be wondering though why outside the code chunks there are # but these become headers when knitting the document. The reason is within Markdown syntax # at the start of a line is used for Headings, with the number of # denoting the Heading level - # for Heading 1, ## for Heading 2, and so on. As R and Markdown developed separately before the creation of R Markdown brought the two together, they both had by that point decided to use # for different things. Whilst confusing at first, it all boils down to “# inside code chunk = comments” and “# outside code chunks = headers”.)\nAge is not known for some cases. Thus, it is recorded as ‘Unknown’ in the dataset. Since this is not recognized as a numeric value in the coercion process, R automatically sets it as a missing value, NA. This is why it will give you a warning message. (Though you will not see a warning message if you ran the knitr setup chunk that disabled warning messages.)\nWe can continue coercing raceethnicity and gender from character to a factor by adding the following lines to our chunk:\n\n# Coerce factors\npolice_2 &lt;- police_2 |&gt; mutate(raceethnicity = factor(raceethnicity))\npolice_2 &lt;- police_2 |&gt; mutate(gender = factor(gender))\n\nRun the chunk and then create and run a new chunk with the following to glimpse the data frame again:\n\nglimpse(police_2)\n\nYou should hopefully see the three coerced variables now have the correct types:\n\n\nage as numeric &lt;dbl&gt;\n\n\ngender and raceethnicity as factor &lt;fct&gt;\n\n\nNow, let’s run a summary of the data using the summary() function. This shows the number of observations in each category or a summary of a numeric variable. Create a code chunk, then add and run the following:\n\nsummary(police_2)\n\nThere are some interesting figures coming out from the summary. For instance, in age you can see that the youngest (Min.) is… 16 years old(?!), and the oldest (Max.) 87 years old. Also, the vast majority are male (445 vs 22). In relation to race/ethnicity, roughly half are ‘White’, whereas ‘Black’ individuals represent an important share. One may question the proportion of people killed in terms of race/ethnicity compared to the composition of the total population (considering Black is a minority group in the US).\nLet’s suppose that we only want observations in which race/ethnicity is not unknown. To ‘remove’ undesired observation we can use the filter() function. Remember, to make changes to an object we need to assign the result of a function back to it. We will then need to assign the result of filter back to the police_2 object.\nCreate a chunk, then add and run the following:\n\npolice_2 &lt;- police_2 |&gt; filter(raceethnicity != \"Unknown\")\n\nSo, what just happened in the code above? First, the pipe operator, |&gt;: What we are doing verbally is take the object police_2, THEN filter raceethnicity based on a condition.\nThen, what is happening inside the filter() function? Let’s have a look at what R does in the background for us (Artwork by @alison_horst):\n\n\n\n\nFilter. Source: Artwork by Horst (n.d.).\n\n\n\nNote, df for us is police_2. Since we are using the pipe operator, police_2 |&gt; filter(raceethnicity != \"Unkown\") is equivalent to writing filter(police_2, raceethnicity != \"Unkown\") without the pipe operator.\nImportantly, the filter() function returns a data frame with only the rows from the data frame that was passed to it that meet the conditions specified. In our example then, filter() returns a data frame that only has the rows from the police_2 data frame where the row’s value in the raceethnicity column is NOT EQUAL (!=) to ‘Unknown’. When we then assign the result to an object named the same as our existing object, police_2 &lt;- ..., we replace the old police_2 data frame with the new filtered version. In effect, our police_2 data frame is updated to a version with all rows where the value for raceethnicity was “Unknown” removed.\nFinally then, create another code chunk to get a glimpse and summary of the data frame again:\n\nglimpse(police_2)\n\nRows: 452\nColumns: 5\n$ age                  &lt;dbl&gt; 16, 27, 26, 25, 29, 29, 22, 35, 44, 31, 76, 40, N…\n$ gender               &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, M…\n$ raceethnicity        &lt;fct&gt; Black, White, White, Hispanic/Latino, White, Whit…\n$ lawenforcementagency &lt;chr&gt; \"Millbrook Police Department\", \"Rapides Parish Sh…\n$ armed                &lt;chr&gt; \"No\", \"No\", \"No\", \"Firearm\", \"No\", \"No\", \"Firearm…\n\nsummary(police_2)\n\n      age           gender                   raceethnicity lawenforcementagency\n Min.   :16.00   Female: 20   Asian/Pacific Islander: 10   Length:452          \n 1st Qu.:28.00   Male  :432   Black                 :135   Class :character    \n Median :35.00                Hispanic/Latino       : 67   Mode  :character    \n Mean   :37.15                Native American       :  4                       \n 3rd Qu.:45.00                Unknown               :  0                       \n Max.   :87.00                White                 :236                       \n NA's   :2                                                                     \n    armed          \n Length:452        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nTo make it easier to compare, the results shown when running the code chunks are included here as well. Within your R Markdown file, scroll up to your previous glimpse and summary chunks for the police_2 data frame. Now compare that to the results we now receive after filtering/ You should see in the filtered version that under raceethnicity, ‘Unkown’ is now 0, when it was 15 before we filtered the dataset. Correspondingly, the total number of rows has dropped by 15, going from 467 initially, to 452 after filtering.\n\n\n\n\n\n\n\n\n\nAs we introduced a lot of new functions in this section, here is a final sub-section with the mini help documentation for them all together -\n\n\n\n\n\n\nCautionsummary() function\n\n\n\nGet a quick summary for an object. Supports many different object types.\nUsage: summary(object)\nArguments: object - The named object to summarise. This object can be a data frame, a specific column from a data frame, etc.\nReturns: Depends on object.\nFor data frame columns it summarises:\n\n\nnumeric columns: Min, 1st Qu., Median, Mean, 3rd Qu., Max, NA count\n\nfactor columns: frequency counts of each category, and NA count\n\nIf provided a data frame it will provide this summary for each column.\n\n\n\n\n\n\n\n\nCautionas.numeric() function\n\n\n\nCoerce a vector to numeric (double) if possible.\nUsage: as.numeric(x)\nArguments: x - The object, such as a data frame column, to coerce.\nReturns: A numeric vector. Any text such as “10” will become a numeric value, but text such as “house”, “missing”, “unknown”, will all become NA values with a warning.\n\n\n\n\n\n\n\n\nCautionmutate() function\n\n\n\nModify existing column(s) or add new column(s). From the dplyr package, which is loaded with the tidyverse package.\nUsage: mutate(data_frame, column = expression, ...) or with pipe data_frame |&gt; mutate(column1 = expression, column2 = expression, ...)\nArguments:\n\n\ndata_frame - The data frame to modify columns from.\n\ncolumn = expression - On the left, the column name want to create or overwrite. On the right, an expression using existing columns and functions. For example age = as.numeric(age) will overwrite the existing age column, but new_age = as.numeric(age) would create a new new_age column.\n\n\n\n\n\n\n\n\n\nCautionselect() function\n\n\n\nSelect specific columns (i.e. variables) from a data frame. From the dplyr package, which is loaded with the tidyverse package.\nUsage: select(data_frame, column1, column2, ...) or with pipe data_frame |&gt; select(column1, column2, ...)\nArguments:\n\n\ndata_frame - The data frame to select columns from.\n\ncolumn1, column2, ... - The columns in the data frame to keep, separated by commas.\n\nReturns: A data frame containing only the selected columns, in the order they were listed in the function.\n\n\n\n\n\n\n\n\nCautionfilter() function\n\n\n\nFilter rows in a data frame to keep only those meeting given conditions. From the dplyr package, which is loaded with the tidyverse package.\nUsage: filter(data_frame, condition1, condition2, ...) or with pipe data_frame |&gt; filter(condition1, condition2, ...)\nArguments:\n\n\ndata_frame - The data frame to filter.\n\ncondition1, condition2, ... - Conditions stated as logical tests, such as age &gt; 18, country == \"Scotland\".\n\nReturns: A data frame containing only the rows where the conditions are met.\n\n\n\nAs a final side note - and a wee preview of code you will see in later labs and in R textbooks - all the steps we did above for creating and modifying police_2 can also be written in a single code chunk. The code chunk below does all the same steps. It creates a police_2 data frame by first selecting variables from the police data frame, then coerces the types of some variables, and finally filters the data.\n\npolice_2 &lt;- police |&gt;\n  select(age, gender, raceethnicity, lawenforcementagency, armed) |&gt;\n  mutate(\n    age = as.numeric(age),\n    raceethnicity = factor(raceethnicity),\n    gender = factor(gender)\n  ) |&gt;\n  filter(raceethnicity != \"Unknown\")\n\nNotice here that we can also pass multiple variables to mutate() and able to create a line for each variable being coerced. We can do this as R also lets us split anything after a bracket or comma onto a new line. With long functions, it is common to start a new line after the (, then add each element that is separated by a comma on a line of its own, then add a final line with the closing ). Notice as well that the elements are indented another two spaces (making the indent for them 4 spaces in total) and the closing ) is indented the same as mutate(.\nIt may take a while to get the hang of reading code with ease, but this formatting helps a lot. The initial 2 space indent after the first line let’s us know that “all the following lines are still part of the same series of steps”. The additional 2 space indent for the lines between the opening ( and closing ) for the mutate() function then also visually lets us know, “all the following lines are arguments within the mutate() function”. This code would be incredibly difficult to read at a glance if it was written instead as a single line and without any pipes.\n\nDiscuss the following questions with your neighbour or tutor:\n\nWhat is the main purpose of the functions select() and filter()?\nWhat does coerce mean in the context of R? and Why do we need to coerce some variables?\nWhat is the mutate() function useful for?\n\nUsing the police_2 dataset:\n\nFilter how many observations are ‘White’ in raceethnicity? How may rows/observations are left?\nHow many ‘Hispanic/Latino’ are there in the dataset?\nUsing the example of Figure 2.3, could you filter how many people were killed that were (a) ‘Black’ and (b) possessed a firearm on them at the time (‘Firearm’)?\nWhat about ‘White’ and ‘Firearm’?\n\nFinally, ‘Knit’ your R Markdown file.\nExtra activities:\n\nWhy did you have to use quotes in the following: police |&gt; filter(raceethnicity==\"White\" & armed==\"Firearm\")?\nWhat do you have to use == rather than =?\n\nNote, police |&gt; filter(raceethnicity==\"White\" & armed==\"Firearm\") could also be written as police |&gt; filter(raceethnicity==\"White\", armed==\"Firearm\"). However, using & makes it clearer that we want to keep rows that meet both conditions. If we wanted to filter for rows where either condition is true, we would use | (which is often used as an operator for ‘or’ in programing languages) instead of &.\nThis is the end of Lab 2. Again, the changes in your R Markdown file should be saved automatically in RStudio. However, make sure this is the case as covered in Lab 1. After this, you can close the tab in your web browser. Hope you had fun!",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#welcome-back",
    "href": "02-Lab2.html#welcome-back",
    "title": "Data in R",
    "section": "",
    "text": "In the previous lab, we set up an RStudio session in Posit Cloud and we got familiar with the RStudio environment and the purpose and contents of RStudio’s panes. In this Lab we will learn about R packages, how to install them and load them. We will also use different types of data. You will have the chance to practice with additional R operators. Lastly, we will load a real-world data set and put in practice your new skills.\n\n\n\n\n\n\nNoteOverview\n\n\n\nBy the end of this lab you will know how to:\n\ninstall and load packages in R\ndownload a dataset from a URL and assign it to a named object\nuse the pipe (|&gt;) operator to chain steps\nexamine, wrangle, and subset a dataset using functions",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#r-packages",
    "href": "02-Lab2.html#r-packages",
    "title": "Data in R",
    "section": "",
    "text": "As mentioned in our last lab, R (R Core Team 2021) is a collaborative project. This means that R users are developing, maintaining, and extending the functionalities constantly. When you set up R and RStudio for the first time, as we did last week, it comes only with the ‘basic’ functionalities by default, sometimes referred to as ‘base R’. However, there are literally thousands of extensions that are developed by other users. In R, these non-default extensions are called packages.\nMost of the time, we use packages because they simplify our work in R - such as by replacing what would take dozens of lines of complex code in base R with a simple one-line function - or they allow us to extend beyond the capabilities of base R.\nLet’s install and load some useful packages. We will start with one of R’s most famous packages, and one we will use across the labs, the tidyverse (Wickham 2021) package.1 First, sign-in to Posit Cloud, create/open your Labs-1-2 project, and then open the Lab-2 R Markdown file.\n\nFor those who missed Lab 1:\n\nMake sure you have a free, institutional-subscription Posit Cloud account (in case you have not created one yet, please follow the guidance provided in Lab 1);\nIf you enrolled before teaching started, you will have received an email with a link to join your lab group. If you joined the course after the start of teaching, you can find a link in your Lab Group’s forum on Moodle. You tutor will also be able to provide you with a link.\nFollow the rest of the Lab 1 guidance in the ‘Create a New Project’ section.\n\nFor those who already joined and created a project in Lab 1, log into Posit Cloud and:\n\nFrom the left-hand menu, click “Lab Group ..” where “..” is your lab group number.\nWithin the main screen, click “Labs-1-2” to open the Posit Cloud project you created last week.\n\n\n\nSelect your lab group space and then open ‘Labs-1-2’ project\n\n\nYou will find your project in the same state as you left it last week. To open a tab for the Lab 2 R Markdown file, click to open ‘Lab-02.Rmd’ in the “Files” tab in the bottom-right pane.\n\n\nClick ‘Lab-02.Rmd’ in the “Files” tab in the bottom-right pane\n\nWithin Lab 1, you may have noticed the key: value lines between two ---. This is called YAML. YAML is a simple human-readable markup language for data serialization. It consists of what are referred to as key-value pairs. Within R Markdown, a YAML header can be included at the very top and must be fenced with the three hyphens, ---, at start and end.\nThe R Markdown file for this lab has a couple more lines in the YAML header this time:\n\n\nYAML block at top of Lab-02.Rmd\n\ntitle and author are self-explanatory, but make sure to replace [your name here] with your name, making sure to keep the quotation marks around it. Without the quotation marks, you will receive an error message when trying to knit. (Note: Don’t knit your file just yet anyway as it will result in an error message until you complete the sections below.)\ndate can be any text. However, format(Sys.time(), '%d/%m/%y') is a nice single line of R code that runs when the file is knitted to retrieve the current date and formats it as dd/mm/yyyy, such as 12/09/2025. This then effectively makes the date field “date this file was knitted” and saves you having to manually update it each time.\noutput specifies what file format(s) to output to when knitting the R Markdown file. Across the labs and assessments we are using ‘html_document’ as it is easiest to work with across the packages we are using in the labs and assessments.\nAfter the YAML header, there is a ‘setup’ code chunk setting knitr options:\n\nThis may look intimidating, but it is relatively simple when parsed bit by bit:\n\n\nknitr is the package used to knit R Markdown files\n\nknitr::opts_chunk is an object that stores default settings applied to all code chunks\n\nknitr::opts_chunk$set(...) runs the set() function to change the knitr::opts_chunk default settings\n\nSo, what the code in the setup chunk does is set messages and warnings to “FALSE”. This prevents any messages or warnings that get raised when running code chunks from also appearing in any knitted files. (Why we do this will become clear in a bit.)\n\n\n\n\n\n\nCautionMore R Operators\n\n\n\n\n\n:: is the namespace operator for accessing an object (such as a function or dataset) from a package, in the format package::object. If you load a package using the library() function, as we will do later, you can call objects from it directly as object without needing to include package:: before it.\n\n$ is the component operator for accessing named elements within an object, in the format object$component. In data analysis it is often used to access a column (variable) from a data frame. For example, if a data frame named df has a column named age, you can access it with df$age.\n\n[] (which is introduced later in this lab) is the subsetting operator and is used to extract elements from an object, in the format object[...]. In contrast to $ where we used the name for the component we wanted, [] uses numbers. If we had a data frame (i.e. a table) called df and ran the code df[2, 4], R would return the value found in the fourth column of the second row.\n\n\n\n\n\nOnce you have the Lab 2 R Markdown file open, type the following in the Console (bottom-right pane) and hit ‘Enter’ to run it:\n\n\ninstall.packages(\"tidyverse\")\n\n\nWait until you get the message ‘The downloaded source packages are in …’. The install process can take a couple of minutes to finish.\n\n/\n\nOnce the package is installed, you next need to load it using the library() function. In the preamble code chunk already setup in the R Markdown file, add the following in the line under the # Load packages comment:\n\n\nlibrary(tidyverse)\n\n\nNow run the code chunk. Either by pressing Ctrl+Shift+Enter with your text cursor within the code chunk, or by clicking the green triangle in the top-right of the code chunk.\n\n\nAnd that’s it, tidyverse is ready to use in your current session!\nNote, we run install.packages(\"package_name\") in the Console as we only need to install the package once for each Posit Cloud project. We load packages with library(package_name) in a code chunk at the top of the R Markdown file as packages need to be loaded for every R session. If we ran library(...) in the Console instead of adding it to a code chunk, our packages would not be available in the fresh R session that is created when knitting and would result in an error message.\nAnother thing to note is that when you install a package, you need to use quotation marks, install.packages(\"package_name\"), whereas when loading a package you just use the package name without quotation marks, library(package_name).\nWhat you may also notice is the message raised in the Console when loading the tidyverse. (If you already ran the setup code chunk that set messages and warnings to FALSE you won’t see any text). This is one of the reasons why we have the setup chunk to exclude messages and warnings in knitted files. If we knitted the Lab-02.Rmd file without the setup chunk, the text on loading the tidyverse would also appear in the knitted HTML file after the preamble chunk. We will cover other options you can set to customise what appears in your knitted files in later weeks.\n\nThis text often causes users new to R to think an error has occurred. However, it is merely a ‘message’ about conflicts that can be ignored.\nThe tidyverse is a meta package that bundles a collection of packages together, such as dplyr and ggplot2. These packages share a common design philosophy and are often used together in data analysis. Loading the tidyverse meta package, library(tidyverse), saves us from having to load each of these packages individually with library(dplyr), library(ggplot2), and so on.\nWhat the message is showing then is that each of the core tidyverse packages were loaded OK, and there are conflicts for functions provided by the dplyr package. Two of its functions, filter() and lag(), share the same name as functions in the base R stats package. After loading the tidyverse, any code calling filter() will use the dplyr function with that name rather than the stats ones. (This applies only to filter() directly, the stats function can still be accessed when the tidyverse is loaded by using the :: namespace operator, stats::filter().)\n\n\n\n\n\n\nTipNew Terms\n\n\n\n\n\nYAML: a human-readable data serialisation language, often used for configuration/settings; in R Markdown the YAML front matter between --- sets document metadata and other options.\n\ntidyverse: a collection of R packages that share a consistent design philosophy and work together for importing, wrangling, and visualising data.\n\ndata frame: an object in R that stores data in a two-dimensional table with rows (observations) and columns (variables). We load a file containing a data set into a data frame within R.\n\ntibble: a modern version of a data frame used in tidyverse packages that has more consistent data handling than base R’s data frames. For simplicity, we will still refer to these in the labs as data frames.\n\n\n\n\n\n\n\n\n\nCautioninstall.packages() function\n\n\n\nInstall an R package that is available from the Comprehensive R Archive Network.\nUsage: install.packages(\"package_name\")\nArguments: \"package_name\" with the name of the package you want to install that must be in quotes, such as install.packages(\"tidyverse\").\nNotes: Only needs to be run once per computer, or once per project in Posit Cloud.\n\n\n\n\n\n\n\n\nCautionlibrary() function\n\n\n\nLoad an installed package so its functions are available in the R session.\nUsage: library(package_name)\nArguments: package_name with the name of the installed package you want to load.\nNotes:\n\nA package needs to be installed first using install.packages(\"package_name\") in the Console.\nIt is best practice to load packages in a code chunk at the top of R Markdown files.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#types-of-variables",
    "href": "02-Lab2.html#types-of-variables",
    "title": "Data in R",
    "section": "",
    "text": "In quantitative analysis, we often distinguish two main types of variables:\n\n\nNumeric variables have values that describe measurable or countable quantities.\n\n\nContinuous variables have values that can fall anywhere within a range, such as time and speed.\n\nDiscrete variables have values that are whole numbers that count something, such as number of children in a household.\n\n\n\nCategorical variables have values that describe distinct, mutually exclusive categories.\n\n\nNominal variables have categories without any order, such as country, name, political party, and gender.\n\nOrdinal variables have categories with a meaningful order, such as education level or level of satisfaction.\n\n\n\nIn R, the most basic data building blocks are atomic vectors. There are six atomic types: logical, integer, double, character, complex, and raw.\nIn relation to our two main types of variables, we use the following vectors:\n\n\nNumeric vectors (integer or double) to store continuous and discrete values.\n\nFactor vectors (a class built on integer codes + character labels) to store nominal and ordinal categorical values.\n\nA numeric vector is usually stored as a double internally by default. The difference between integer and double is based on how they are computationally stored. Base R and most R packages handle the difference behind the scenes, so the difference in how they are stored does not matter to us.\nA factor is a class combining integer codes and a levels attribute containing the human-readable character labels for those codes. Each stored integer points to a character label in levels, so 1 might correspond to \"Conservative\", 2 might correspond to \"Labour\", and so on. For ordered factors, used for ordinal variables, R also records an explicit order of the levels to enable comparisons and sorting by that order.\nTwo other vectors we commonly use are:\n\n\nCharacter vectors (character) to store text, such as unique respondent IDs (“A102”, “C006”) and free-text answers.\n\nLogical vectors (logical) to store boolean TRUE / FALSE values, useful for filtering and conditions, such as age &gt; 30.\n\nIn R, there are couple of functions that will help us to identify the type of data. First, we have glimpse(). This prints some of the main characteristics of a data set, namely its overall dimension, name of each variable (column), the first values for each variable, and the type of the variable. Second we have the function class(), that will help us to determine the overall class(type) of on R object.\n\nWe are now going to use some datasets that are available to us in the R session. R comes with some example datasets out of the box, such as iris with 150 observations of iris flowers. Some R packages also include additional example datasets, such as starwars - included in the dplyr tidyverse package - with info about 87 Star Wars characters. Unlike other datasets that we have to manually load, and will cover in later section, we can access these in any R session out the box or after loading the relevant package that provides them.\nPlease go to the ‘Types of variables’ section in your Lab-02.Rmd file. You will see a few code chunks already setup for you:\n\nWe will start with a classic dataset example in R called iris. (For more info about the dataset, you can type ?iris or help(iris) in the Console). Please go to the “iris-glimpse” code chunk in your R Markdown file and run it.\n\n\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nWhat do you observe from the output?\nFirst, it tells you the number of rows and the columns on the top. After, it lists the name of each variable. Additionally, it tells you the type of the variable between these symbols &lt; &gt;. The first five variables in this dataset are of type &lt;dbl&gt; (double) which as covered above is a type of numeric variable. The last, Species, is a factor &lt;fct&gt;. So, for each of the 150 iris flowers observed, there is information on its species and four types of continuous measures. Though, as glimpse only provides a preview, we only see the values for the first few observations.\n\nNow you know that each iris flower belongs to a species, but what are the specific categories in this data set? To find out, add the following in the ‘iris-levels’ code chunk and then run it.\n\n\nlevels(iris$Species)\n\n(If you receive ‘NULL’ when running the chunk, it because you have species and not Species.)\nAs you can see, there are three categories, or levels as they are called in R factor variables, which are three types of iris flower species. Notice here we used the $ component operator mentioned earlier. Here it basically means, from the iris data frame select the Species column/variable. levels() is then a nice little utility function that shows all the levels for a column given to it as an argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s get serious and explore Star Wars.\nThe starwars data set from the dplyr package contains information about the characters, including height, hair colour, and sex. (Again, to get more information run ?starwars or help(starwars) in the Console). For the purpose of keeping things relatively simple to focus on variable types, we will use a reduced version of the full data set.\n\nFirst, run the ‘starwars-glimpse’ code chunk to create a reduced version of the data set named starwars2 and then glimpse the Star Wars characters in that reduced data set:\n\n\nstarwars2 &lt;- starwars[, 1:11]\nglimpse(starwars2)\n\nCreating a reduced version of the data frame uses another selection operator, [...]. The basic format is data_frame[rows, columns]. Here starwars[, 1:11] leaves the rows argument blank, which effectively means select all rows, and the columns argument has 1:11, which effectively means select columns 1 to 11. So, the code returns a copy of the data frame with all rows but only the first 11 columns.\nWhat do you observe this time?\nIt seems that the data type is not consistent with their content. For example, the variables species, gender, and hair_color are of type &lt;chr&gt; (that is character), when according to what we covered above these should be factors. To transform them, we will use the function ´factor()´. This process is known as coercing a variable, that is when you change a variable from one type to another. factor() is another nice little helper function\n\n\n\n\n\n\n\n\n\nLet’s coerce the species variable from character to factor and assign the result to the same column in the dataset. Add the following to the ‘starwars-factor’ code chunk and run it.\n\n\nstarwars2$species &lt;- factor(starwars2$species)\n\nNote, most functions in R for data analysis do not directly change the object -such as a data frame - passed to it. Instead, the functions return a new object with the changes. An explicit assignment back to the original object is required if you want to change it. For example, if you run factor(starwars2$species on its own in the Console, it will return all the values for species and the levels created. However, for the species column in starwars2 to be updated with the results returned by factor(starwars2$species) we need the starwars2$species &lt;- ... assignment as well. What the code is effectively saying is “replace the species column in starwars2 with the version returned by the factor function”.\n\nLet’s check if the type of variable really changed by glimpsing the data and checking the levels of species. Add the following to the ‘starwars-glimpse2’ chunk and run it.\n\n\nglimpse(starwars2)\nlevels(starwars2$species)\n\nThe glimpse result now is telling us that species is a &lt;fct&gt;, as expected. Furthermore, the levels() function reveals that there are 37 types of species, including Human, Ewok, Droid, and more.\nHopefully, these examples will help you to identify the main vector types and, more importantly, an initial understanding of how to coerce them into an appropriate type. Be aware that many data sets represent categories with numeric values without the labels, for example, using ‘0’ for male and ‘1’ for female - and as part of the initial process turning these variables into factors you will also need to provide a list mapping these numbers to the labels to use. Usually, large data sets are accompanied by extra information in a code book or documentation file, which specifies the values for the numeric code and their respective meaning. It’s important to read the code book/documentation of every data set rather than assuming what the numbers stand for as the conventions and meanings can vary. For example, some surveys now put gender in alphabetical order, with 0 for female and 1 for male.\n\n\n\n\n\n\nCautionglimpse() function\n\n\n\nProvides a ‘glimpse’ of an object. From the dplyr package, and loaded with the tidyverse package.\nUsage: glimpse(x)\nArguments:\n\n\nx - The object (usually a data frame) to glimpse.\n\nReturns: For data frames, it returns counts for rows and columns and a compact, transposed preview showing column names, types, and a few example values. Types are shown as &lt;int&gt; (integer), &lt;dbl&gt; (double), &lt;chr&gt; (character), &lt;lgl&gt; (logical), &lt;fct&gt; (factor).\n\n\n\n\n\n\n\n\nCautionfactor() function\n\n\n\nCreate a categorical variable (factor) with fixed, named levels.\nUsage: factor(x, levels, ordered = is.ordered(x))\nArguments:\n\n\nx - A vector, such as a data frame column, to turn into a factor.\n\nlevels - The allowed values and their order. If omitted, all unique values in x are used to create the levels.\n\nordered - Set to TRUE where order matters when creating an ordered factor.\n\nNotes: Whenever you see text like ordered = is.ordered(x) in a function documentation, this shows the name of the argument and its default value. In this example, the default value is for ordered is calculated using another function. The is.ordered(x) checks whether x is already an ordered factor and returns TRUE or FALSE. If you are using factor() to adjust the levels/labels of an existing ordered factor then, you can leave the ordered = TRUE out.\n\n\n\n\n\n\n\n\nCautionlevels() function\n\n\n\nGet the category labels (“levels”) of a factor.\nUsage: levels(x)\nArguments: x - A factor variable, such as data_frame$country.\nReturns: A character vector of level labels for the factor variable.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#pipes",
    "href": "02-Lab2.html#pipes",
    "title": "Data in R",
    "section": "",
    "text": "A useful operator is the pipe |&gt;. This is the base R version of the pipe. This operator is what is known in programming as syntactic sugar. It is syntax which helps make code easier to read and write without changing any functionality. The pipe operator passes the result of one function as the first argument to the next (left to right).\nRun the ‘pipe-1’ code chunk and check its results.\n\n1 |&gt; sum(1)\n1 |&gt; sum(1) |&gt; sum(5)\n\nThe sum() function adds the value of two numbers in format sum(x, y). Using a pipe for 1 |&gt; sum(1) is the equivalent to sum(1, 1). If we were to write sum(x, y) with a pipe, it would be x |&gt; sum(y). So, in our example code chunk, the pipe let’s us ‘unpack’ the first argument in each function, whereby line 1 reads equivalent to saying ‘take 1, then add it with 1’ and line 2 ‘take 1, then add it with 1, then add it with 5’.\nThis may seem as if it is overcomplicating things, but as the code becomes more complex the value of the pipe operator becomes clearer. The second line, 1 |&gt; sum(1) |&gt; sum(5) would be written as sum(sum(1,1), 5) without pipes. Going left to right, 1 |&gt; sum(1) would be sum(1,1) and then sum(1,1) |&gt; sum(5) would be sum(sum(1,1), 5).\nPipes then help us avoid foo(foo(foo(...), ...) ...) monstrosities in our code. In coding examples, you will often see words like foo, bar and baz used as placeholder names when explaining how coding syntax works rather than using real functions. Similarly, x, y, and z often get used as placeholder names for variables / arguments.\nAs we will cover in examples in the next section below, plenty of functions take more than two arguments. Let’s imagine we had a data frame named df and we were updating it by using - in order - foo, bar, and baz functions that each take arguments x, y, and z. Without pipes our code would be:\n\ndf &lt;- baz(bar(foo(df, y, z), y, z), y, z)\n\nThis is a tangled mess, we want the result from foo passed to bar and its result in turn passed to baz. This ordering though gets visually reversed when nesting each function within the other - we have to read it inside out for it to make sense.\nWith pipes, it instead becomes:\n\ndf &lt; - df |&gt;\n  foo(y, z) |&gt;\n  bar(y, z) |&gt;\n  baz(y, z)\n\nNow it is easy to read and the ordering of the functions matches how we could verbally describe what the code does, “we are updating a data frame called df by running the functions foo, bar and baz on it in turn”. In the code chunk above, we make use of another wonderful thing about pipes - it let’s us break complex code over multiple lines. To do so you add the pipe at the end of the line before each function, then start each new line with an indent followed by the function. If you type |&gt; at the end of a line and hit enter, RStudio will add the spaces at the start of the new line for you.\nNote, in the readings and other materials, you may come across another %&gt;% pipe. This pipe comes from the tidyverse. The tidyverse pipe came before the base R one, where due to its popularity an equivalent was added to base R. Whilst you will still see it in use, and it remains supported in the tidyverse, the tidyverse itself recommends people start using the base R |&gt; pipe. To see that they work the same, run the ‘pipe-2’ chunk.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#black-lives-matter",
    "href": "02-Lab2.html#black-lives-matter",
    "title": "Data in R",
    "section": "",
    "text": "In this section we will work with data originally collected by The Guardian in 2015, for more information click here. The data set we will use today is an extended version which was openly shared in GitHub by the American news website FiveThirtyEight. This data set contains information about the people that were killed by police or other law enforcement bodies in the US, such as age, gender, race/ethnicity, etc. Additionally, it includes information about the city or region where the event happened. For more information click here.\n\nAs with installing packages, we only want to download the data once. For this lab, we will download the data running code in the Console. Next week though, we will go through how to set up an R script for downloading data. This is a common strategy for sharing the code used to download data whilst still keeping it separate to your main R Markdown file.\nFirst, we will create a new folder in our project directory to store the data. To do it from the Console, run this line. (Don’t worry if you get a warning. This appears because you already have a folder with this name):\n\ndir.create(\"data\")\n\nNote that in the ‘Files’ tab of Pane 4, the bottom-right pane, there is a new folder called data.\n\nNow, download the data from the GitHub repository using the function download.file(). This function takes two arguments separated by a comma: (1) the URL and (2) the destination (including the directory, file name, and file extension), as shown below.\nCopy and paste the following lines into the Console:\n\ndownload.file(\"https://github.com/fivethirtyeight/data/raw/master/police-killings/police_killings.csv\", \"data/police_killings.csv\")\n\n(Note, to avoid any issues typing out the URL, you can click the clipboard icon in the top-right of the code above to copy the full contents to your clipboard.)\nAfter that, we are ready to read the data. As the data comes as a .csv file, we can use the read_csv() function included in the tidyverse package (make sure the package is loaded in your session as explained in a previous section).\nThe read_csv() reads the data in the file and returns it as a data frame in R. We need then to assign this data frame to a named object so we can continue working with it. In our example below we will do this by assigning it to a named object police. So, in effect, our new police named object will be a data frame containing the data from the .csv file we downloaded.\nImportantly, as with loading packages, when loading data it is good practice to do so at the top of the file. So, go all the way back up to the preamable code chunk and add the following line after the ‘# Read data’ comment.\n\npolice &lt;- read_csv(\"data/police_killings.csv\")\n\nYou should now have:\n\nRun the code chunk and you’ll know it has run OK if can see ‘police’ in your Global Environment in the top-right pane. It will have 467 observations and 34 variables (columns).\n\n\nOK, before we look at this police data, a reminder about creating code chunks. Scroll back in your R Markdown file to “## Black lives matter!”. For this section, you will need to create your own code chunks.\nTo create a new code chunk, you have four options:\n\nPut your text cursor on a line in your R Markdown file and press Ctrl+Alt+I on the keyboard.\nFrom the menu bar at the top of RStudio, you can select Code &gt; Insert Chunk.\nManually type three backticks, on UK layout keyboards the key to the left of the 1 key, and then {r} (remember to add three backticks on a line below to close the code chunk as well).\nClick on the icon that is a c in green square with a ‘+’ in a circle in top-left corner, and then click on R to create an R code chunk.\n\n\nBy default, a new code chunk will be plain {r}, but you can edit this to give it an optional name:\n\n\nBack to the data. Let’s first glimpse the police data. Create a code chunk then add and run the following:\n\nglimpse(police)\n\nAs you can see, there are several variables included in the dataset, such as age, gender, law enforcement agency (lawenforcementagency), or whether the victim was armed (armed). You will see some of these variables are not in the appropriate type. For instance, some are categorical and should be a factor (&lt;fct&gt;) instead of character (&lt;chr&gt;).\n\nBefore coercing these variables, we will create a smaller subset of the data frame by selecting only the variables that we are interested in. To do so, we can use the select() function. The select() function takes the name of the data frame as the first argument and then the names of the variables we want to keep separated by commas (no quotation marks needed).\nRemember with pipes we can replace select(data_frame, variable1, variable2, ...) with data_frame |&gt; select(variable1, variable2, ... making it easier to read as “from our data frame select the following variables”. Let’s create a code chunk to select a few variables and assign the result to a new object called police_2.\n\npolice_2 &lt;- police |&gt; select(age, gender, raceethnicity, lawenforcementagency, armed)\n\nThe select() function is from the dplyr tidyverse package. Many tidyverse functions that take a data frame as the first argument, foo(date_frame, ...) or when using a pipe data_frame |&gt; foo(...), will assume all variables listed are from that data frame. Within the function, this then let’s us type age and gender rather than police$age and police$gender.\nIf you look again to the ‘Environment’ tab, there is now a second data frame with the same number of observations but only 5 variables. Create a new code chunk, then add and run the following code to glimpse its contents.\n\nglimpse(police_2)\n\nHaving a closer look at the reduced version, we can see that in fact all the variables are of type &lt;chr&gt; (character), including age.\nLet’s coerce the variables into their correct type. Start by creating a code chunk and naming it ‘coerce-police_2’ or similar.\nThen within the chunk, let’s start with age, coercing it from character to numeric by adding the following lines:\n\n# Coerce numeric\npolice_2 &lt;- police_2 |&gt; mutate(age = as.numeric(age))\n\nThe # Coerce numeric line let’s us add a comment to our code. Any text within an R code chunk after a # is treated as a comment and is not run as part of the code. This is useful for recording the ‘why’ behind the code, so future-you (and others) can quickly read and understand it.\n(You may be wondering though why outside the code chunks there are # but these become headers when knitting the document. The reason is within Markdown syntax # at the start of a line is used for Headings, with the number of # denoting the Heading level - # for Heading 1, ## for Heading 2, and so on. As R and Markdown developed separately before the creation of R Markdown brought the two together, they both had by that point decided to use # for different things. Whilst confusing at first, it all boils down to “# inside code chunk = comments” and “# outside code chunks = headers”.)\nAge is not known for some cases. Thus, it is recorded as ‘Unknown’ in the dataset. Since this is not recognized as a numeric value in the coercion process, R automatically sets it as a missing value, NA. This is why it will give you a warning message. (Though you will not see a warning message if you ran the knitr setup chunk that disabled warning messages.)\nWe can continue coercing raceethnicity and gender from character to a factor by adding the following lines to our chunk:\n\n# Coerce factors\npolice_2 &lt;- police_2 |&gt; mutate(raceethnicity = factor(raceethnicity))\npolice_2 &lt;- police_2 |&gt; mutate(gender = factor(gender))\n\nRun the chunk and then create and run a new chunk with the following to glimpse the data frame again:\n\nglimpse(police_2)\n\nYou should hopefully see the three coerced variables now have the correct types:\n\n\nage as numeric &lt;dbl&gt;\n\n\ngender and raceethnicity as factor &lt;fct&gt;\n\n\nNow, let’s run a summary of the data using the summary() function. This shows the number of observations in each category or a summary of a numeric variable. Create a code chunk, then add and run the following:\n\nsummary(police_2)\n\nThere are some interesting figures coming out from the summary. For instance, in age you can see that the youngest (Min.) is… 16 years old(?!), and the oldest (Max.) 87 years old. Also, the vast majority are male (445 vs 22). In relation to race/ethnicity, roughly half are ‘White’, whereas ‘Black’ individuals represent an important share. One may question the proportion of people killed in terms of race/ethnicity compared to the composition of the total population (considering Black is a minority group in the US).\nLet’s suppose that we only want observations in which race/ethnicity is not unknown. To ‘remove’ undesired observation we can use the filter() function. Remember, to make changes to an object we need to assign the result of a function back to it. We will then need to assign the result of filter back to the police_2 object.\nCreate a chunk, then add and run the following:\n\npolice_2 &lt;- police_2 |&gt; filter(raceethnicity != \"Unknown\")\n\nSo, what just happened in the code above? First, the pipe operator, |&gt;: What we are doing verbally is take the object police_2, THEN filter raceethnicity based on a condition.\nThen, what is happening inside the filter() function? Let’s have a look at what R does in the background for us (Artwork by @alison_horst):\n\n\n\n\nFilter. Source: Artwork by Horst (n.d.).\n\n\n\nNote, df for us is police_2. Since we are using the pipe operator, police_2 |&gt; filter(raceethnicity != \"Unkown\") is equivalent to writing filter(police_2, raceethnicity != \"Unkown\") without the pipe operator.\nImportantly, the filter() function returns a data frame with only the rows from the data frame that was passed to it that meet the conditions specified. In our example then, filter() returns a data frame that only has the rows from the police_2 data frame where the row’s value in the raceethnicity column is NOT EQUAL (!=) to ‘Unknown’. When we then assign the result to an object named the same as our existing object, police_2 &lt;- ..., we replace the old police_2 data frame with the new filtered version. In effect, our police_2 data frame is updated to a version with all rows where the value for raceethnicity was “Unknown” removed.\nFinally then, create another code chunk to get a glimpse and summary of the data frame again:\n\nglimpse(police_2)\n\nRows: 452\nColumns: 5\n$ age                  &lt;dbl&gt; 16, 27, 26, 25, 29, 29, 22, 35, 44, 31, 76, 40, N…\n$ gender               &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, M…\n$ raceethnicity        &lt;fct&gt; Black, White, White, Hispanic/Latino, White, Whit…\n$ lawenforcementagency &lt;chr&gt; \"Millbrook Police Department\", \"Rapides Parish Sh…\n$ armed                &lt;chr&gt; \"No\", \"No\", \"No\", \"Firearm\", \"No\", \"No\", \"Firearm…\n\nsummary(police_2)\n\n      age           gender                   raceethnicity lawenforcementagency\n Min.   :16.00   Female: 20   Asian/Pacific Islander: 10   Length:452          \n 1st Qu.:28.00   Male  :432   Black                 :135   Class :character    \n Median :35.00                Hispanic/Latino       : 67   Mode  :character    \n Mean   :37.15                Native American       :  4                       \n 3rd Qu.:45.00                Unknown               :  0                       \n Max.   :87.00                White                 :236                       \n NA's   :2                                                                     \n    armed          \n Length:452        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nTo make it easier to compare, the results shown when running the code chunks are included here as well. Within your R Markdown file, scroll up to your previous glimpse and summary chunks for the police_2 data frame. Now compare that to the results we now receive after filtering/ You should see in the filtered version that under raceethnicity, ‘Unkown’ is now 0, when it was 15 before we filtered the dataset. Correspondingly, the total number of rows has dropped by 15, going from 467 initially, to 452 after filtering.\n\n\n\n\n\n\n\n\n\nAs we introduced a lot of new functions in this section, here is a final sub-section with the mini help documentation for them all together -\n\n\n\n\n\n\nCautionsummary() function\n\n\n\nGet a quick summary for an object. Supports many different object types.\nUsage: summary(object)\nArguments: object - The named object to summarise. This object can be a data frame, a specific column from a data frame, etc.\nReturns: Depends on object.\nFor data frame columns it summarises:\n\n\nnumeric columns: Min, 1st Qu., Median, Mean, 3rd Qu., Max, NA count\n\nfactor columns: frequency counts of each category, and NA count\n\nIf provided a data frame it will provide this summary for each column.\n\n\n\n\n\n\n\n\nCautionas.numeric() function\n\n\n\nCoerce a vector to numeric (double) if possible.\nUsage: as.numeric(x)\nArguments: x - The object, such as a data frame column, to coerce.\nReturns: A numeric vector. Any text such as “10” will become a numeric value, but text such as “house”, “missing”, “unknown”, will all become NA values with a warning.\n\n\n\n\n\n\n\n\nCautionmutate() function\n\n\n\nModify existing column(s) or add new column(s). From the dplyr package, which is loaded with the tidyverse package.\nUsage: mutate(data_frame, column = expression, ...) or with pipe data_frame |&gt; mutate(column1 = expression, column2 = expression, ...)\nArguments:\n\n\ndata_frame - The data frame to modify columns from.\n\ncolumn = expression - On the left, the column name want to create or overwrite. On the right, an expression using existing columns and functions. For example age = as.numeric(age) will overwrite the existing age column, but new_age = as.numeric(age) would create a new new_age column.\n\n\n\n\n\n\n\n\n\nCautionselect() function\n\n\n\nSelect specific columns (i.e. variables) from a data frame. From the dplyr package, which is loaded with the tidyverse package.\nUsage: select(data_frame, column1, column2, ...) or with pipe data_frame |&gt; select(column1, column2, ...)\nArguments:\n\n\ndata_frame - The data frame to select columns from.\n\ncolumn1, column2, ... - The columns in the data frame to keep, separated by commas.\n\nReturns: A data frame containing only the selected columns, in the order they were listed in the function.\n\n\n\n\n\n\n\n\nCautionfilter() function\n\n\n\nFilter rows in a data frame to keep only those meeting given conditions. From the dplyr package, which is loaded with the tidyverse package.\nUsage: filter(data_frame, condition1, condition2, ...) or with pipe data_frame |&gt; filter(condition1, condition2, ...)\nArguments:\n\n\ndata_frame - The data frame to filter.\n\ncondition1, condition2, ... - Conditions stated as logical tests, such as age &gt; 18, country == \"Scotland\".\n\nReturns: A data frame containing only the rows where the conditions are met.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#code-formatting",
    "href": "02-Lab2.html#code-formatting",
    "title": "Data in R",
    "section": "",
    "text": "As a final side note - and a wee preview of code you will see in later labs and in R textbooks - all the steps we did above for creating and modifying police_2 can also be written in a single code chunk. The code chunk below does all the same steps. It creates a police_2 data frame by first selecting variables from the police data frame, then coerces the types of some variables, and finally filters the data.\n\npolice_2 &lt;- police |&gt;\n  select(age, gender, raceethnicity, lawenforcementagency, armed) |&gt;\n  mutate(\n    age = as.numeric(age),\n    raceethnicity = factor(raceethnicity),\n    gender = factor(gender)\n  ) |&gt;\n  filter(raceethnicity != \"Unknown\")\n\nNotice here that we can also pass multiple variables to mutate() and able to create a line for each variable being coerced. We can do this as R also lets us split anything after a bracket or comma onto a new line. With long functions, it is common to start a new line after the (, then add each element that is separated by a comma on a line of its own, then add a final line with the closing ). Notice as well that the elements are indented another two spaces (making the indent for them 4 spaces in total) and the closing ) is indented the same as mutate(.\nIt may take a while to get the hang of reading code with ease, but this formatting helps a lot. The initial 2 space indent after the first line let’s us know that “all the following lines are still part of the same series of steps”. The additional 2 space indent for the lines between the opening ( and closing ) for the mutate() function then also visually lets us know, “all the following lines are arguments within the mutate() function”. This code would be incredibly difficult to read at a glance if it was written instead as a single line and without any pipes.",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#activities",
    "href": "02-Lab2.html#activities",
    "title": "Data in R",
    "section": "",
    "text": "Discuss the following questions with your neighbour or tutor:\n\nWhat is the main purpose of the functions select() and filter()?\nWhat does coerce mean in the context of R? and Why do we need to coerce some variables?\nWhat is the mutate() function useful for?\n\nUsing the police_2 dataset:\n\nFilter how many observations are ‘White’ in raceethnicity? How may rows/observations are left?\nHow many ‘Hispanic/Latino’ are there in the dataset?\nUsing the example of Figure 2.3, could you filter how many people were killed that were (a) ‘Black’ and (b) possessed a firearm on them at the time (‘Firearm’)?\nWhat about ‘White’ and ‘Firearm’?\n\nFinally, ‘Knit’ your R Markdown file.\nExtra activities:\n\nWhy did you have to use quotes in the following: police |&gt; filter(raceethnicity==\"White\" & armed==\"Firearm\")?\nWhat do you have to use == rather than =?\n\nNote, police |&gt; filter(raceethnicity==\"White\" & armed==\"Firearm\") could also be written as police |&gt; filter(raceethnicity==\"White\", armed==\"Firearm\"). However, using & makes it clearer that we want to keep rows that meet both conditions. If we wanted to filter for rows where either condition is true, we would use | (which is often used as an operator for ‘or’ in programing languages) instead of &.\nThis is the end of Lab 2. Again, the changes in your R Markdown file should be saved automatically in RStudio. However, make sure this is the case as covered in Lab 1. After this, you can close the tab in your web browser. Hope you had fun!",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "02-Lab2.html#footnotes",
    "href": "02-Lab2.html#footnotes",
    "title": "Data in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.tidyverse.org/↩︎",
    "crumbs": [
      "**Lab 2** Data in R"
    ]
  },
  {
    "objectID": "assessments/summative.html",
    "href": "assessments/summative.html",
    "title": "Summative Assessments",
    "section": "",
    "text": "ImportantSummative Submission Deadline\n\n\n\n12 noon, Monday 15th December 2025\n\n\nThere is one summative assessment, split into two parts:\n\nReflective Course Summary. 500 words, worth 30% of your total grade\nInterpreting Quantitative Findings Report. 2500 words, worth the remaining 70% of your total grade.\n\nThese are submitted together as one document: with the report first; and summary second.\nThere is no end-of-course examination for this course. All assessment is based on the summative assessment components that must be completed.",
    "crumbs": [
      "Assessments",
      "Summative Assessments"
    ]
  },
  {
    "objectID": "assessments/summative.html#overview",
    "href": "assessments/summative.html#overview",
    "title": "Summative Assessments",
    "section": "",
    "text": "ImportantSummative Submission Deadline\n\n\n\n12 noon, Monday 15th December 2025\n\n\nThere is one summative assessment, split into two parts:\n\nReflective Course Summary. 500 words, worth 30% of your total grade\nInterpreting Quantitative Findings Report. 2500 words, worth the remaining 70% of your total grade.\n\nThese are submitted together as one document: with the report first; and summary second.\nThere is no end-of-course examination for this course. All assessment is based on the summative assessment components that must be completed.",
    "crumbs": [
      "Assessments",
      "Summative Assessments"
    ]
  },
  {
    "objectID": "assessments/summative.html#interpreting-quantitative-findings-report",
    "href": "assessments/summative.html#interpreting-quantitative-findings-report",
    "title": "Summative Assessments",
    "section": "Interpreting Quantitative Findings Report",
    "text": "Interpreting Quantitative Findings Report\nThis component is worth 70% of your total grade.\nYou must prepare the report in R Markdown (which you will learn to do through the lab workbook) and submit your file as a knitted HTML. You will get used to working in R Markdown in the labs each week, but remember you can always refer to the handy R Markdown cheatsheet if you need to.\nYou are required to write a 2,500-word report in R Markdown interpreting a regression model. The model has been prepared for you; you will find it in lab workbook 10’s instructions. You are not being asked to conduct your own regression therefore, but to show you can interpret the results from the model prepared for you and use the skills you have learned through the labs.\nThe aim is to give you some practical experience of designing and carrying out quantitative research in social science, including designing research questions and hypothesis based on theory and literature, preparing and wrangling data, running descriptive statistics and other statistical methods, reporting and interpreting findings, and showing reflexivity on your experience of working with quantitative data. You will have to decide what research questions should be asked and define and test your own hypothesis.\nThe report should be all your own work, but guidance will be provided in the lab sessions to discuss the statistical tests, visualisations, and other techniques that can be further conducted on the data for your report. It will give you experience in what it is like to analyse quantitative data for research.\nSatisfactory completion of this assignment demonstrates achievement of the following ILOs:\n\ndemonstrate an understanding of the basic principles of quantitative research design and strategy.\nconstruct research hypotheses and demonstrate basic skills in question formulation and questionnaire design.\ndemonstrate practical skills in the computer analysis and presentation of quantitative data (descriptive statistical analysis, tabulation, graphical presentation of numerical data).\ncritically assess social research from a methodological standpoint.\n\nIn what follows you will find a guide to writing the report. This is in the form of a list of questions. If you answer each of these questions you will have all of the content needed for a good report. Remember to take care with referencing and bibliographies, it can be easy to forget the basics when you are learning advanced statistical methods, but bibliographies are very important. There is reading material assigned for this class and you are expected to cite it, thereby demonstrating that you kept up with the reading, and using them to inform your understanding of relevant concepts and techniques.\n\nHow to write my research report?\nFirst, make sure you follow the instructions in lab workbook chapter 10 to download the report template we prepared for you in RStudio Cloud, which includes the regression model and R Markdown set up.\n\nPart 1: Introduction (approx. 350 words)\nLook at the regression model included in the template. Based on this choose and state your own research question and hypothesis. Remember a hypothesis needs at least two variables.\n\n\nPart 2: Data and Method (approx. 600 words)\nIn this section your goal is to demonstrate that you can describe data and that you understand research design and data collection. Therefore, you should make sure to cite the mandatory textbooks in support of your answers. The general rules of good scholarship apply to quantitative research as much as any other subject, so do take care with your references and bibliography. As in any other class you need to demonstrate you’ve kept up with reading and can use the reading to inform your understanding of the relevant concepts and data analytics.\n\nWhat is the data set? Who collected the data and for what purpose? What does the data describe?\nWhat is the sample size? How was the data collected? Why is this reliable and are there any potential shortcomings that could limit the interpretation of the data? Note that the sample size for the model is reduced compared to the data set overall. Account for this in preparing visualisations.\nPresent an appropriate visualisation of the dependent variable. Remember to number and label figures and tables. Describe the distribution. What does this plot tell us as a descriptive finding, and does it have any implications for the model?\nPresent a table of descriptive statistics for the variables included in the model. Discuss the descriptive findings, does the distribution of any variables have any implications for the model?\n\n\n\nPart 3: Results and Discussion (approx. 1200 words)\nIn this section your goal is to demonstrate that you can interpret quantitative results. You are likely to get a higher grade if you are able to relate these findings to social science theories and literature or if you can put the findings in context.\n\nWhat kind of model is this? (clue: is the dependent variable quantitative or dichotomous?) What is it a model of? Provide a very brief summary of your understanding of the whole model. \nWhich variables are significant? Which are insignificant?\nOf the significant results, discuss the coefficients. Which are negative, which are positive? You can focus on variables of your own choice from here, selected based on your hypothesis, and explain the numbers to your reader.\nDo any of the uncertainty estimates give you any cause for concern? Which and why?\nHow does the model fit? How do you account for the model fit?\nPresent and discuss appropriate visualizations of particularly interesting relationships in the model. You may choose your own variable to focus on, based on your hypothesis.\nAre any of these findings surprising? What do they mean for your hypothesis?\n\n\n\nPart 4: Conclusion (Approx. 350 words)\nClearly state your findings. Do the findings raise any questions for future research? The tone you are trying to achieve here is one of a confident researcher. Be proud of your findings and your interpretation. Throughout the paper, if you have addressed each of the questions above then you have already acknowledged shortcomings with the research, you still need to persuade your readers of your findings despite these limitations. That’s what a conclusion is for. A good start to a conclusion is with the phrase “in conclusion…” Be explicit in your conclusion, a reader should be able to read only the conclusion and know the research question, the findings and the importance of those findings.\nIn writing your report, bear in mind you will be assessed based on your ability to do the following:\n\nIntroduce the context of your research topic (albeit briefly).\nClearly articulate your research questions, hypothesis, and identification of variable(s) and how the literature guided this process.\nJustify choices made in the research design and analysis, and how these are informed by your understanding of a social science research methodology.\nExplain what technique and tests are run in R to produce your analysis, and what purposes they served to extract insights and/or ensure your analysis is robust.\nReport the results of your analysis accurately. This means interpreting coefficients, significance, and uncertainty estimates, as well as model fit statistics (also, e.g., use of tables, graphs, and visualisations) and provide a meaningful discussion of the results, based on a critical engagement with the literature, theories, and concepts, e.g., how have the results confirmed/contested the existing theories and concepts in the literature.\nIdentify potential limitations and weaknesses in your analysis, e.g., what statistical tests for significance are used? What extra questions/variables you would have liked to have included in the dataset and how could the dataset be improved?\nDrawing convincing conclusions based on an accurate analysis of the data and demonstrate a command of the relevant academic literature, theories, or concepts to explain the results and any emerging trends.\nArticulate any lessons and assumptions you learned as a researcher in this process, and/or identify suggestions of future avenues for research, policy, or theoretical debates, if appropriate.\nYour work should be consistently and fully referenced, with a complete bibliography.",
    "crumbs": [
      "Assessments",
      "Summative Assessments"
    ]
  },
  {
    "objectID": "assessments/summative.html#reflective-course-summary",
    "href": "assessments/summative.html#reflective-course-summary",
    "title": "Summative Assessments",
    "section": "Reflective Course Summary",
    "text": "Reflective Course Summary\nThis component is worth 30% of your total grade.\nYou will be required to submit a 500-word reflective summary, reflecting on your use of feedback from across the course. Learning quantitative methods can be new and challenging and require new ways of approaching how you learn. To recognise this, the Reflective Course summary asks you to capture this learning process, demonstrating how you used feedback on the course: proactively sought it out, and utilised these various sources to improve your learning. It therefore demonstrates your participation and learning journey.\nThere will be various sources of feedback you get on a course like this, both formal and informal. For example:\n\nThe Formative Assessment\nTutor feedback in labs\nExchanges with teaching staff: via email, during/after class, office hours, and on Moodle\nQuestions asked to or discussed with other students, informal chats with students in labs and outside classrooms\nDataCamp, if you choose to use this\nLearning process feedback:\n\nFeedback from R Studio: error messages, code not working or not working as you thought it might.\nRealising when you did not quite understand something like you thought.\n\n\nThese are suggestions and you do not need to include them all.\n\nWhat do I learn by doing a reflective summary?\nTo know what you have learned is an important a valuable metacognitive skill—to understand how you have learned. Feedback, and using feedback effectively, is also an important part of the learning process.\n\n\nWhat do you mean by ‘reflective’?\nHere it means therefore how you reflect on the use of feedback in the learning process.\nHow you sought it out, used it, and any differences it made. To show how you used feedback, you should use concrete, analytical examples rather than general description. You can – and should – then draw more general conclusions from these concrete examples.\nA reflective summary therefore means analytical reflection (thinking about what/why/how) rather than mere description (I had Problem X where I found issues with A, B, C but I used feedback to help me solve it). It’s fine to describe the issue but the reflective element comes from analysing (a) specifically you identified an issue (b) where/why/how you sought feedback (c) how you used the feedback (perhaps from several sources and through several attempts to use it) and (d) the concrete difference it made to the result or outcome. You should minimize description and focus on reflective analysis.\nSince you have 500 words, you should focus on those core issues which made the most difference to your learning journey. Specifically, you will want to demonstrate how feedback shaped your learning journey on the course:\n\nhighlight the main or core issues you encountered\nexplain how you used feedback to diagnose these\nexplain how you used feedback to improve and develop through the course: reflecting on what your main improvements were and how the use of feedback helped these take place\n\nPrioritise discussing issues about your reflections on:\n\nHow you utilize feedback in the core learning issues for you: this might involve learning how to code in the Labs, or learn new statistical concepts for example.\nHow you adapt the ways you learn based on feedback from other students and/or tutors, and other sources highlighted above. Your traditional approaches to learning might not be as effective when learning to code, say,\nThe ways and extent you used feedback: this the ability to reflect on your own strengths and weaknesses in quantitative methods, and how you adapted your learning based on feedback and your active use of feedback,\n\n\n\nHow do I reflect on my learning? \nThe aim of reflective learning is to think about and demonstrate what and how you have learned from feedback on the course.\nYou can do this by asking yourself a series of guiding questions as soon as possible\n\nafter each lab session.\nafter each lecture\nafter each study session\n\nIt might be helpful to jot down some brief notes on a regular basis (e.g., on paper, your phone, or in a computer). You can just write one or two bullet points or even do voice notes on your phone/device if you want.\nYou can then use these notes to complete your reflective summary. It is important that these notes are honest and authentic, otherwise the point of the exercise is lost.\nYou can use the following guiding questions as prompts to structure your reflective log entry:\n\nWhat did I do in the lab and what did I learn from what I did today? (e.g., what are the key “take home” points or “notes to self”?)\nHow did the course content (e.g., readings, lectures) and other resources help me to learn and complete the activities in the lab workbook?\nHow did I respond to feedback given by other students and/or tutors?\nLooking back, how could I have done better or what could I have done more efficiently?\nWhat else might I have done (before, in, or after the lab)?\nHow will this shape my actions for learning next week and in the future?\n\nIf you do this short exercise regularly it will provide you with a strong evidence base for summarising the progress you have made throughout this course and how you used feedback to inform this.\n\n\nHow will my Reflective Summary be assessed?\nYour completed Reflective Summary will be graded as a summative assessment at the end of the course. It will submitted with the Interpreting Quantitative Findings report.\nIt will be written in the same R Markdown document as one final Summative submission of 3,000 words:\nOverall Summative Submission (3,000 words)\nYou will be assessed based on the extent, depth, and quality of the reflection demonstrated in your learning log.\nBecause the word count is small, focus on the issues that are most relevant and pertinent.",
    "crumbs": [
      "Assessments",
      "Summative Assessments"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html",
    "href": "assessments/assessment-faq.html",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "Please find below answers to questions about the summitive interpreting quantitative findings report assessment. For FAQ about R issues in relation to the summative assessment, please see the separate R Issues FAQ page.\nPlease get in touch if you have any other questions or where any of the answers below are unclear.\n\n\nPlease see the Setup Assessment Projects page in the Assessments section of the Lab Workbook. It provides details for how to create a Posit Cloud project for the interpretive report from a template we provide. The template project includes an R Markdown file with an advised outline, suggested word counts for each section, and code-blocks for installing/loading packages, reading in the dataset, and producing the regression results table. You will need to create additional code chunks for any tables and visualisations you include as part of your interpretive report.\nTime will be made available during Lab 11 to create a Posit Cloud project for the summative assessment, if you have not done so already. This lab will also provide opportunity to discuss the assessment with your lab tutors.\n\n\n\nWhilst the variables included in the model are the same, there are a number of things that will make your report different to others. Critically, what is key within quantitative methods is providing context and narrative to the data and analysis:\nThe introduction will differ based on your research topic, literature used to provide context, research question, and hypothesis/hypotheses. Your hypothesis/hypotheses will determine which variable/variables in the model are being treated as independent, making the others control variables. Based on this, there may be particular aspects you may want to focus on in more detail within the ‘Data and Method’ section. Then, central to the report is ‘interpreting’ the regression results across the ‘Results and Discussion’ and ‘Conclusion’ sections. Importantly, this requires not just reporting the numbers, but being able to contextualise the statistical results, drawing out what their implications are for your hypothesis and research question, the broader literature, and future research. Similarly, the guidance on How to write my research report? advises to provide and discuss visualisations of any interesting relationships in the ‘Results and Discussion’ section, where again your research question and hypothesis/hypotheses will influence what these are and how they are discussed.\n\n\n\nAs covered in online lectures, some contemporary research design textbooks argue that we should speak more of “research contribution” and not just a “research gap”. What is it that your research is adding to the existing literature? In practice, research contribution/gap is more a different way to frame essentially the same thing, but initially thinking of ‘contribution’ can be a helpful way to identify a ‘gap’.\nFor example, existing research might show a relation between marital status and reported well-being. However, this research may be 20+ years old where changes in societal views of marriage, ability of households to live on a single income, policies that benefit married couples, etc may raise questions about whether that relationship still exists, and even if it does are differences in well-being still the same. Alternatively, the research showing the relation may predominantly be in countries with strong cultural views towards marriage, policies that strongly benefit married couples, etc, which raises questions about whether the relationship still holds in countries without these. Similarly, and importantly, even where your research is in a country with similar characteristics to the countries predominantly used, you are still making a contribution by researching whether the relationship still holds elsewhere. If it does, this further helps solidify the evidence and theory, if it doesn’t it raises questions about what current understandings of the relationship may be leaving out.\nWith each of these you would be ‘contributing’ new research that addresses a ‘gap’. The first, contributing new research to address the gap in more contemporary research, using any key changes over time to further justify its importance. The second, contributing research from another country to address the gap created by most research on the topic predominantly being within specific countries, using any key differences between the countries to further justify its importance. The third, is more subtle, contributing research from another similar country to similarly address the gap from research predominantly being within specific countries, using the similar but under-researched country to justify seeing whether the relation still holds.\nNote, those are just three examples, there are a plethora of other potential contributions you could be making to address a gap in the literature.\n\n\n\nA few areas that influence the grade that are relatively easy fixes:\n\nNot citing any literature. Ensure to cite research literature to provide context to your research question, when interpreting findings, and in the conclusion. Similarly, cite relevant methods literature to support your discussion of data and method and interpretation of findings.\nNot providing a clear research question and/or hypothesis. As covered above, key to interpreting findings is going beyond just the numbers. To effectively do that you need to have a clear research question and hypothesis/hypotheses. Fogarty chapter 6 “Developing Hypotheses” provides a good range of examples for how to clearly phrase hypotheses.\nNot including any additional tables or graphs. The template includes the code for creating the regression results table, but you should also provide an appropriate visualisation of the dependent variable, table of descriptive statistics for variables included in the model, and appropriate visualisations of any particularly interesting relationships in the model.\nNot interpreting all or part of the regression results table. Interpreting the regression results table is a critical part of the assignment. The guidance for the Results and Discussion provides information on what you should be covering in doing so. Importantly, this includes the model fit statistics, which is the part that most often gets missed. The in-person lectures in Week 9 (evaluating coefficient information) and Week 10 (model evaluation) will cover these in more detail.\n\n\n\n\nWe are not looking for a huge amount. For the literature on your research topic, at least 3-4 texts would be acceptable. For quantitative methods literature, at least citing Fogarty where relevant would be acceptable, though it would be good to cite 1-2+ other methods text as relevant as well. Also, ensure to cite the NILT documentation where relevant, such as in the ‘Data and Methods’ section.\n\n\n\nExcellent question. It is always key to consider what it is that variables actually measure and represent. As covered in the online lectures, it is good research practice to provide documentation alongside datasets, which the NILT project does.\nWithin the main questionnaire, Q12 on page 41 is the question used for the “rsuper” variable:\n\n\nSo, the phrasing of the question would be inclusive of what would also be called ‘line management’ rather than limited to people with ‘supervisor’ in their job title or description.\nImportant to note as well is the “ASK IF EMPLOYEE” routing information. This means the “rsuper” question Q12 was only asked if the respondent answered “Employee” to Q11. As a result, anyone who answered “Self-employed” to Q11 were not asked Q12, the question for the rsuper variable, resulting in ‘NA’ (i.e. missing) values:\n\n\n\n\n\nYour interpretive report should remain an ‘independent piece of work’. It is perfectly OK to help each other solve R errors and general coding issues, but your research question, hypothesis/hypotheses, and interpretation should all remain your own independent work. Decisions on what tables and figures to include should also remain your own. It is OK to help someone who is stuck figuring out what code they need to add a title to their graph, but you should not be writing the code for them nor producing the exact same tables and figures throughout, which would constitute plagiarism. Note, emphasis here is on ‘exact’ and ‘throughout’ that would suggest the report is not your own independent work, we recognise that there will be general similarities with some tables and graphs.\n\n\n\nTables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. In other words, you will need to knit your file and view the knitted HTML file to see the word count.\nThis is how it looks within the R Markdown file:\n\n\nAnd, this is what it then looks like within the knitted HTML file:\n\n\nAs can see, despite the project template’s R Markdown file having over 300 words and the knitted HTML having a regression results table that contains 50+ words, none of the code nor table/figure text is included within the actual word count.\nThe ‘- 14’ in the code is so the words in “Word count:” and each of the headers - “Introduction”, “Data and method”, etc - are substracted from the calculated word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 184 words then change the code to “wordcountaddin::word_count(”Summative-template.Rmd”) - 198”.\n\n\n\nWhen modelling a categorical variable as an independent / control variable, one category is selected as the ‘reference category’, and the model tests whether each of the other values are significant in relation to it. This means the statistical significance of each category is evaluated independently of the others. For example, if we had a categorical variable with categories A, B, C, D and we used A as the reference category, the regression results might show that C is statistically significant, while B and D are not. We could not claim from this that the categorical variable as a whole (B, C, and D) is statistically significant, only that C is significant in relation to the reference category (A).\n\n\n\nWithin the University of Glasgow’s Code of Assessment, a C is a good grade. A = Excellent, B = Very Good, C = Good, D = Satisfactory.\nDue to data protection regulations, in order to share previous student work on Moodle the university requires us to have students sign a form providing permission. As this is an honours level course and we need to wait until after the Exam Board takes place before approaching students, we tend to have a very low response rate to such requests.",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html#what-is-the-process-for-starting-the-report-in-r-can-we-have-an-outline",
    "href": "assessments/assessment-faq.html#what-is-the-process-for-starting-the-report-in-r-can-we-have-an-outline",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "Please see the Setup Assessment Projects page in the Assessments section of the Lab Workbook. It provides details for how to create a Posit Cloud project for the interpretive report from a template we provide. The template project includes an R Markdown file with an advised outline, suggested word counts for each section, and code-blocks for installing/loading packages, reading in the dataset, and producing the regression results table. You will need to create additional code chunks for any tables and visualisations you include as part of your interpretive report.\nTime will be made available during Lab 11 to create a Posit Cloud project for the summative assessment, if you have not done so already. This lab will also provide opportunity to discuss the assessment with your lab tutors.",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html#how-will-everyones-research-differ-if-the-variables-in-the-assignment-are-the-same",
    "href": "assessments/assessment-faq.html#how-will-everyones-research-differ-if-the-variables-in-the-assignment-are-the-same",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "Whilst the variables included in the model are the same, there are a number of things that will make your report different to others. Critically, what is key within quantitative methods is providing context and narrative to the data and analysis:\nThe introduction will differ based on your research topic, literature used to provide context, research question, and hypothesis/hypotheses. Your hypothesis/hypotheses will determine which variable/variables in the model are being treated as independent, making the others control variables. Based on this, there may be particular aspects you may want to focus on in more detail within the ‘Data and Method’ section. Then, central to the report is ‘interpreting’ the regression results across the ‘Results and Discussion’ and ‘Conclusion’ sections. Importantly, this requires not just reporting the numbers, but being able to contextualise the statistical results, drawing out what their implications are for your hypothesis and research question, the broader literature, and future research. Similarly, the guidance on How to write my research report? advises to provide and discuss visualisations of any interesting relationships in the ‘Results and Discussion’ section, where again your research question and hypothesis/hypotheses will influence what these are and how they are discussed.",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html#how-would-you-form-a-rationale-when-you-cannot-find-a-research-gap",
    "href": "assessments/assessment-faq.html#how-would-you-form-a-rationale-when-you-cannot-find-a-research-gap",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "As covered in online lectures, some contemporary research design textbooks argue that we should speak more of “research contribution” and not just a “research gap”. What is it that your research is adding to the existing literature? In practice, research contribution/gap is more a different way to frame essentially the same thing, but initially thinking of ‘contribution’ can be a helpful way to identify a ‘gap’.\nFor example, existing research might show a relation between marital status and reported well-being. However, this research may be 20+ years old where changes in societal views of marriage, ability of households to live on a single income, policies that benefit married couples, etc may raise questions about whether that relationship still exists, and even if it does are differences in well-being still the same. Alternatively, the research showing the relation may predominantly be in countries with strong cultural views towards marriage, policies that strongly benefit married couples, etc, which raises questions about whether the relationship still holds in countries without these. Similarly, and importantly, even where your research is in a country with similar characteristics to the countries predominantly used, you are still making a contribution by researching whether the relationship still holds elsewhere. If it does, this further helps solidify the evidence and theory, if it doesn’t it raises questions about what current understandings of the relationship may be leaving out.\nWith each of these you would be ‘contributing’ new research that addresses a ‘gap’. The first, contributing new research to address the gap in more contemporary research, using any key changes over time to further justify its importance. The second, contributing research from another country to address the gap created by most research on the topic predominantly being within specific countries, using any key differences between the countries to further justify its importance. The third, is more subtle, contributing research from another similar country to similarly address the gap from research predominantly being within specific countries, using the similar but under-researched country to justify seeing whether the relation still holds.\nNote, those are just three examples, there are a plethora of other potential contributions you could be making to address a gap in the literature.",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html#are-there-any-parts-where-people-often-drop-marks-but-is-an-easy-fix",
    "href": "assessments/assessment-faq.html#are-there-any-parts-where-people-often-drop-marks-but-is-an-easy-fix",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "A few areas that influence the grade that are relatively easy fixes:\n\nNot citing any literature. Ensure to cite research literature to provide context to your research question, when interpreting findings, and in the conclusion. Similarly, cite relevant methods literature to support your discussion of data and method and interpretation of findings.\nNot providing a clear research question and/or hypothesis. As covered above, key to interpreting findings is going beyond just the numbers. To effectively do that you need to have a clear research question and hypothesis/hypotheses. Fogarty chapter 6 “Developing Hypotheses” provides a good range of examples for how to clearly phrase hypotheses.\nNot including any additional tables or graphs. The template includes the code for creating the regression results table, but you should also provide an appropriate visualisation of the dependent variable, table of descriptive statistics for variables included in the model, and appropriate visualisations of any particularly interesting relationships in the model.\nNot interpreting all or part of the regression results table. Interpreting the regression results table is a critical part of the assignment. The guidance for the Results and Discussion provides information on what you should be covering in doing so. Importantly, this includes the model fit statistics, which is the part that most often gets missed. The in-person lectures in Week 9 (evaluating coefficient information) and Week 10 (model evaluation) will cover these in more detail.",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html#how-much-secondary-literature-should-we-include-i.e.-referencing-the-key-reading",
    "href": "assessments/assessment-faq.html#how-much-secondary-literature-should-we-include-i.e.-referencing-the-key-reading",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "We are not looking for a huge amount. For the literature on your research topic, at least 3-4 texts would be acceptable. For quantitative methods literature, at least citing Fogarty where relevant would be acceptable, though it would be good to cite 1-2+ other methods text as relevant as well. Also, ensure to cite the NILT documentation where relevant, such as in the ‘Data and Methods’ section.",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html#what-does-the-variable-rsuper-signify-what-does-it-mean-by-supervisor",
    "href": "assessments/assessment-faq.html#what-does-the-variable-rsuper-signify-what-does-it-mean-by-supervisor",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "Excellent question. It is always key to consider what it is that variables actually measure and represent. As covered in the online lectures, it is good research practice to provide documentation alongside datasets, which the NILT project does.\nWithin the main questionnaire, Q12 on page 41 is the question used for the “rsuper” variable:\n\n\nSo, the phrasing of the question would be inclusive of what would also be called ‘line management’ rather than limited to people with ‘supervisor’ in their job title or description.\nImportant to note as well is the “ASK IF EMPLOYEE” routing information. This means the “rsuper” question Q12 was only asked if the respondent answered “Employee” to Q11. As a result, anyone who answered “Self-employed” to Q11 were not asked Q12, the question for the rsuper variable, resulting in ‘NA’ (i.e. missing) values:",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html#to-what-extent-is-collaborative-work-allowed-for-the-project",
    "href": "assessments/assessment-faq.html#to-what-extent-is-collaborative-work-allowed-for-the-project",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "Your interpretive report should remain an ‘independent piece of work’. It is perfectly OK to help each other solve R errors and general coding issues, but your research question, hypothesis/hypotheses, and interpretation should all remain your own independent work. Decisions on what tables and figures to include should also remain your own. It is OK to help someone who is stuck figuring out what code they need to add a title to their graph, but you should not be writing the code for them nor producing the exact same tables and figures throughout, which would constitute plagiarism. Note, emphasis here is on ‘exact’ and ‘throughout’ that would suggest the report is not your own independent work, we recognise that there will be general similarities with some tables and graphs.",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html#are-tablesfigurescode-included-in-the-word-count",
    "href": "assessments/assessment-faq.html#are-tablesfigurescode-included-in-the-word-count",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "Tables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. In other words, you will need to knit your file and view the knitted HTML file to see the word count.\nThis is how it looks within the R Markdown file:\n\n\nAnd, this is what it then looks like within the knitted HTML file:\n\n\nAs can see, despite the project template’s R Markdown file having over 300 words and the knitted HTML having a regression results table that contains 50+ words, none of the code nor table/figure text is included within the actual word count.\nThe ‘- 14’ in the code is so the words in “Word count:” and each of the headers - “Introduction”, “Data and method”, etc - are substracted from the calculated word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 184 words then change the code to “wordcountaddin::word_count(”Summative-template.Rmd”) - 198”.",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html#for-a-categorical-variable-if-one-category-is-statistically-significant-but-another-isnt-can-we-say-the-variable-is-significant",
    "href": "assessments/assessment-faq.html#for-a-categorical-variable-if-one-category-is-statistically-significant-but-another-isnt-can-we-say-the-variable-is-significant",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "When modelling a categorical variable as an independent / control variable, one category is selected as the ‘reference category’, and the model tests whether each of the other values are significant in relation to it. This means the statistical significance of each category is evaluated independently of the others. For example, if we had a categorical variable with categories A, B, C, D and we used A as the reference category, the regression results might show that C is statistically significant, while B and D are not. We could not claim from this that the categorical variable as a whole (B, C, and D) is statistically significant, only that C is significant in relation to the reference category (A).",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/assessment-faq.html#can-we-see-an-interpretive-report-that-got-a-good-grade-the-moodle-example-got-a-c.",
    "href": "assessments/assessment-faq.html#can-we-see-an-interpretive-report-that-got-a-good-grade-the-moodle-example-got-a-c.",
    "title": "Interpretive Report Q&A",
    "section": "",
    "text": "Within the University of Glasgow’s Code of Assessment, a C is a good grade. A = Excellent, B = Very Good, C = Good, D = Satisfactory.\nDue to data protection regulations, in order to share previous student work on Moodle the university requires us to have students sign a form providing permission. As this is an honours level course and we need to wait until after the Exam Board takes place before approaching students, we tend to have a very low response rate to such requests.",
    "crumbs": [
      "Assessments",
      "Interpretive Report Q&A"
    ]
  },
  {
    "objectID": "assessments/formative.html",
    "href": "assessments/formative.html",
    "title": "Formative Assessment",
    "section": "",
    "text": "ImportantFormative Submission Deadline\n\n\n\n12 noon, Friday 31st October 2025\n\n\nFor the Formative Assessment, you will be asked to consolidate the main elements of the course across Weeks 1-5 in the form of a short report (500 words maximum, requiring the use of R Markdown). This should be submitted as an R Markdown report. You will learn how to do this in the Labs.\nBy Week 6, you will have become familiar with the NILT Northern Ireland Life and Times 2012 dataset. You will be using this dataset, and for the final summative assessment report we are going to be using the following variables:\n\nAnnual Personal Income\nSex\nReligion\nSexual Orientation\nNI Constitutional View\nTrade Union Membership\nSupervisor \nAge \n\nThis formative assessment is designed to help you get some initial practice on and feedback on the summative assessment. Although it is not graded therefore, it will help directly inform your final summative assessment grade.  \nYou will be asked to write a report with the following headings: \n\n\nIn this part you need to set up your Research Question and explain its rationale. And then break the Research Question down into a hypothesis or hypotheses that can be tested in a quantitative research design.\n\n\nState your research question clearly here. Remember from the lectures, that the research question tends to have two elements:\n\na relationship\na specific context\n\nNext, break your Research Question down into a hypothesis or, if relevant, hypotheses. Again from the lectures, these need to be in a specific form. Please revisit the guidance here to help form your hypothesis/hypotheses correctly.\n\n\n\nWhy is your Research Question necessary to ask in the first place? Give some context and background to help the reader understand the rationale for your study and use some literature to help you establish this: 2-3 references are fine as a minimum for the formative assessment.\nDon’t worry overly about being original. You are using a dataset from 2012 with restricted number of variables already chosen for you. The main thing is to demonstrate you can contextualize your Research Question by explaining its background and by using some relevant literature to support this.\n\n\n\nYour Research Question will contain a dependent variable and an independent variable (or possibly two maximum). These need to be identified:\n\nDependent variable: Be clear on what your dependent variable is: state it for the reader.\nIndependent variable: You also should have one key independent variable (two maximum), which you want to focus on in your study. Again, state what this variable is, clearly, for the reader.\n\n\n\n\n\nYou have now set up your Research Question and explained its background. You have identified the key variables (dependent and independent variables) contained within it. And you have then used these to break your Research Question down into a hypothesis i.e. into a relationship between variables that you can now go onto test.\nTo do so though, you now need a dataset which collects data on these variables of interest. For our course, this dataset is NILT 2012.\n\n\nDescribe the dataset you are using:\n\nWhat is the data set? Who collected the data and for what purpose? What does the data describe?\nWhat is the sample size? How was the data collected? Why is this reliable and are there any potential shortcomings that could limit the interpretation of the data?\n\nAs per the lectures, think of the general details, technical details and any limitations we need to know.\n\n\n\n\nPresent a table of descriptive statistics for the variables included in the model.\nDiscuss the descriptive findings for your dependent variable and independent variable(s).\nDoes the distribution of the dependent variable have any implications for the model?\n\nYou should submit this formative assessment report as an R Markdown report.\nFinally, this formative is designed to help you practice the summative assessment. Specifically, note both:\n\nPart 1: Introduction and Part 2: Data Collection and Descriptive Statistics of this formative assessment\n\nRelate directly to:\n\nPart 1: Introduction and Part 2: Data and Method of the summative report (Interpreting Quantitative Findings).\n\nMake sure you utilise the feedback given to you on your Formative Assessment report therefore in your final Summative Assessment report:\n\nby using it to inform your Interpreting Quantitative Findings report\nand explaining in your Reflective Course Summary how you used this important source of formative feedback to help your learning on the course",
    "crumbs": [
      "Assessments",
      "Formative Assessment"
    ]
  },
  {
    "objectID": "assessments/formative.html#setting-up-a-quantitative-project",
    "href": "assessments/formative.html#setting-up-a-quantitative-project",
    "title": "Formative Assessment",
    "section": "",
    "text": "ImportantFormative Submission Deadline\n\n\n\n12 noon, Friday 31st October 2025\n\n\nFor the Formative Assessment, you will be asked to consolidate the main elements of the course across Weeks 1-5 in the form of a short report (500 words maximum, requiring the use of R Markdown). This should be submitted as an R Markdown report. You will learn how to do this in the Labs.\nBy Week 6, you will have become familiar with the NILT Northern Ireland Life and Times 2012 dataset. You will be using this dataset, and for the final summative assessment report we are going to be using the following variables:\n\nAnnual Personal Income\nSex\nReligion\nSexual Orientation\nNI Constitutional View\nTrade Union Membership\nSupervisor \nAge \n\nThis formative assessment is designed to help you get some initial practice on and feedback on the summative assessment. Although it is not graded therefore, it will help directly inform your final summative assessment grade.  \nYou will be asked to write a report with the following headings: \n\n\nIn this part you need to set up your Research Question and explain its rationale. And then break the Research Question down into a hypothesis or hypotheses that can be tested in a quantitative research design.\n\n\nState your research question clearly here. Remember from the lectures, that the research question tends to have two elements:\n\na relationship\na specific context\n\nNext, break your Research Question down into a hypothesis or, if relevant, hypotheses. Again from the lectures, these need to be in a specific form. Please revisit the guidance here to help form your hypothesis/hypotheses correctly.\n\n\n\nWhy is your Research Question necessary to ask in the first place? Give some context and background to help the reader understand the rationale for your study and use some literature to help you establish this: 2-3 references are fine as a minimum for the formative assessment.\nDon’t worry overly about being original. You are using a dataset from 2012 with restricted number of variables already chosen for you. The main thing is to demonstrate you can contextualize your Research Question by explaining its background and by using some relevant literature to support this.\n\n\n\nYour Research Question will contain a dependent variable and an independent variable (or possibly two maximum). These need to be identified:\n\nDependent variable: Be clear on what your dependent variable is: state it for the reader.\nIndependent variable: You also should have one key independent variable (two maximum), which you want to focus on in your study. Again, state what this variable is, clearly, for the reader.\n\n\n\n\n\nYou have now set up your Research Question and explained its background. You have identified the key variables (dependent and independent variables) contained within it. And you have then used these to break your Research Question down into a hypothesis i.e. into a relationship between variables that you can now go onto test.\nTo do so though, you now need a dataset which collects data on these variables of interest. For our course, this dataset is NILT 2012.\n\n\nDescribe the dataset you are using:\n\nWhat is the data set? Who collected the data and for what purpose? What does the data describe?\nWhat is the sample size? How was the data collected? Why is this reliable and are there any potential shortcomings that could limit the interpretation of the data?\n\nAs per the lectures, think of the general details, technical details and any limitations we need to know.\n\n\n\n\nPresent a table of descriptive statistics for the variables included in the model.\nDiscuss the descriptive findings for your dependent variable and independent variable(s).\nDoes the distribution of the dependent variable have any implications for the model?\n\nYou should submit this formative assessment report as an R Markdown report.\nFinally, this formative is designed to help you practice the summative assessment. Specifically, note both:\n\nPart 1: Introduction and Part 2: Data Collection and Descriptive Statistics of this formative assessment\n\nRelate directly to:\n\nPart 1: Introduction and Part 2: Data and Method of the summative report (Interpreting Quantitative Findings).\n\nMake sure you utilise the feedback given to you on your Formative Assessment report therefore in your final Summative Assessment report:\n\nby using it to inform your Interpreting Quantitative Findings report\nand explaining in your Reflective Course Summary how you used this important source of formative feedback to help your learning on the course",
    "crumbs": [
      "Assessments",
      "Formative Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html",
    "href": "11-Lab11.html",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "ImportantUpdate In Progress\n\n\n\nWe are refreshing the contents of the lab workbook this year. The newest version of this page for 2025/2026 will be uploaded nearer the time for the lab.\n\n\n\nThis is our final practical session. Thank you for tuning in and all your hard work!\nToday, we will discuss some practical aspects of the summative assessment Interpreting Quantitative Findings Report. You will actually start working on the assignment today by creating a template you can use to write your report in RStudio. Also, you will get familiar with the structure and the contents of the template. Remember, this is the perfect time to clarify as many questions as possible. Your tutor will be more than happy to help!\n\n\n\n\n\n\nNoteSetup Assessment Project\n\n\n\nYou can find the step-by-step guide for how to setup a Posit Cloud project for the summative assessment using the provided template on the Setup Assessment Projects page.\nAfter following the steps, you will have a project with every package you need - except one - installed and loaded. The NILT data set is already downloaded, cleaned, and loaded. There is a nilt_subset data frame object already created that contains just the variables used in the regression model. The regression model and regression table results are also already setup for you.\nIn terms of coding, all you need to do is create code chunks for any visualisations and tables or other additional stats. You should not remove any of the code from the template. In particular, you should not replace the code that sets up the regression model and creates the regression results table.\nDo not worry about the code in the template that creates the regression results table. The code for it is more complex than usual solely because we have customised the table to more closely match the output from the summary() function that we cover in the labs and align more clearly with what we ask you to do for the assessment. For the assessment, you only need to interpret the regression results table. A page will be made available that breaks down the code, but that will be entirely optional to read and understanding the code that produces the regression results table is not required for the assessment.\n\n\nBefore proceeding, please take about 10 minutes to read the guidance for the Summative Assessments.\nOther lab workbook pages that will be useful to check are:\n\n\nExporting HTML Files for Submission - which covers how to export your HTML file for submitting to Turnitin. Importantly, please follow and read the instructions all the way to the end. On some browsers after hitting ‘Download’ it will auto-open your file in the browser. If you ‘Save as…’ the file, you will experience the same issue when uploading to Turnitin as would if didn’t export your file.\n\nInterpretive Report Q&A - that includes additional information based on questions from previous years.\n\nR Issues FAQ - covering solutions to common issues you might encounter when wroking on your interpretive findings report.\n\n\n\n\n\n\n\nPlease note - the rest of the content below has not yet been updated for this year. The above though provides you with all you will need for setting up your summative and making a start if you want to do so before the lab itself. As always, please get in touch if you experience any issues or are unsure about anything.\n\n\n\n\nOnce in your UGQuant-assignment2 project, open the Assignmet2-template.Rmd file in Pane 4 under the ‘Files’ tab, as shown below.\n\n\n\n\nTemplate.\n\n\n\nThis template contains the following:\n\nSuggested structure of the report.\nSuggested word count for each section.\nThe code necessary to run and present the results of a multivariate linear regression.\n\nIn essence, this is the basic structure to start writing your assignment. Of course, you can add, edit, and customize as much as you consider appropriate. Remember, this is just a generic suggestion and you should still address all the points to the best of your abilities. Remember: There are no hard and fast rules to say what’s right or wrong. You are the one who can determine and justify why you did what you did. There are also many ways to write and approach this assignment, so make choices based on your own interest(s) and disciplinary background. It is truly your time to shine! The course handbook also gives important pointers on how your report will be assessed, offering some guiding questions to tackle the report–don’t skip this crucial step.\n\n\nIn the template, fill in the ‘author’ and ‘date’ space in the YAML at the top of the file with your student number and appropriate information using quotation marks.\nKnit the Rmd file as html (RStudio may ask to install some packages; click ‘Yes’).\nIn the output, look at the results of the table under the ‘Results’ section and identify the dependent and the independent variables. You can learn about the meaning of these variables in the NILT documentation (click here to access the documentation).\nIdentify the variables that are significant in this model and the direction of the relationship.\nDiscuss your interpretations with your neighbour or tutor. You can refer back to Lab 8 and Lab 9 to refresh your memory.\n\n\nWrite one introductory paragraph in the ‘Introduction’ section of the template according to the guidance provided and your preliminary insights.\nKnit the document again.\n\nYou are on the right track now!\nWe hope that by getting familiar with this setting, you will easily succeed in writing your research report using all the knowledge and skills acquired during this course. Take this session to ask questions about the assignment or the course in general as much as possible. This is the right time to have specialized one-to-one support from your tutors.\nWe wish you the best of luck! Get in touch with your tutors via email or, even better, post any questions you have about the assignment on your lab group discussion forum on Moodle, if you don’t know where to start or get stuck. Don’t suffer in silence. Remember your Tutors, lab group mates, and the teaching and admin team are here for you. Also don’t forget coding is all about trial and error, so it’s completely normal to write / copy and paste some code, then get an error message, and basically for your code to not work. Keep chipping at it, which sometimes can take hours (if not days), until you can get it to work. That’s a normal process even for professional data scientists and quantitative researchers! So do persevere and don’t panic if you don’t get it to work right away, because troubleshooting your code is part of the work, in addition to making your own choices in the analysis and reporting. For the code, the error message you get often gives you clues about, well, where the error is. So read the error message carefully, it might be you haven’t loaded the package required to run the code (remember to do this every time you open RStudio) or you might have missed a parentheses, or you might have not capitalised a word in the R syntax when you need to. Double check the R cheatsheets or previous sections in the lab workbook to ensure you have specified R syntax/arguments correctly. Again, trial and error is your friend, unlike writing an essay/doing an exam.\nGood luck and have fun! You got this.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#introduction",
    "href": "11-Lab11.html#introduction",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "This is our final practical session. Thank you for tuning in and all your hard work!\nToday, we will discuss some practical aspects of the summative assessment Interpreting Quantitative Findings Report. You will actually start working on the assignment today by creating a template you can use to write your report in RStudio. Also, you will get familiar with the structure and the contents of the template. Remember, this is the perfect time to clarify as many questions as possible. Your tutor will be more than happy to help!\n\n\n\n\n\n\nNoteSetup Assessment Project\n\n\n\nYou can find the step-by-step guide for how to setup a Posit Cloud project for the summative assessment using the provided template on the Setup Assessment Projects page.\nAfter following the steps, you will have a project with every package you need - except one - installed and loaded. The NILT data set is already downloaded, cleaned, and loaded. There is a nilt_subset data frame object already created that contains just the variables used in the regression model. The regression model and regression table results are also already setup for you.\nIn terms of coding, all you need to do is create code chunks for any visualisations and tables or other additional stats. You should not remove any of the code from the template. In particular, you should not replace the code that sets up the regression model and creates the regression results table.\nDo not worry about the code in the template that creates the regression results table. The code for it is more complex than usual solely because we have customised the table to more closely match the output from the summary() function that we cover in the labs and align more clearly with what we ask you to do for the assessment. For the assessment, you only need to interpret the regression results table. A page will be made available that breaks down the code, but that will be entirely optional to read and understanding the code that produces the regression results table is not required for the assessment.\n\n\nBefore proceeding, please take about 10 minutes to read the guidance for the Summative Assessments.\nOther lab workbook pages that will be useful to check are:\n\n\nExporting HTML Files for Submission - which covers how to export your HTML file for submitting to Turnitin. Importantly, please follow and read the instructions all the way to the end. On some browsers after hitting ‘Download’ it will auto-open your file in the browser. If you ‘Save as…’ the file, you will experience the same issue when uploading to Turnitin as would if didn’t export your file.\n\nInterpretive Report Q&A - that includes additional information based on questions from previous years.\n\nR Issues FAQ - covering solutions to common issues you might encounter when wroking on your interpretive findings report.\n\n\n\n\n\n\n\nPlease note - the rest of the content below has not yet been updated for this year. The above though provides you with all you will need for setting up your summative and making a start if you want to do so before the lab itself. As always, please get in touch if you experience any issues or are unsure about anything.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#about-the-research-report-template",
    "href": "11-Lab11.html#about-the-research-report-template",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "Once in your UGQuant-assignment2 project, open the Assignmet2-template.Rmd file in Pane 4 under the ‘Files’ tab, as shown below.\n\n\n\n\nTemplate.\n\n\n\nThis template contains the following:\n\nSuggested structure of the report.\nSuggested word count for each section.\nThe code necessary to run and present the results of a multivariate linear regression.\n\nIn essence, this is the basic structure to start writing your assignment. Of course, you can add, edit, and customize as much as you consider appropriate. Remember, this is just a generic suggestion and you should still address all the points to the best of your abilities. Remember: There are no hard and fast rules to say what’s right or wrong. You are the one who can determine and justify why you did what you did. There are also many ways to write and approach this assignment, so make choices based on your own interest(s) and disciplinary background. It is truly your time to shine! The course handbook also gives important pointers on how your report will be assessed, offering some guiding questions to tackle the report–don’t skip this crucial step.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#activity-1",
    "href": "11-Lab11.html#activity-1",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "In the template, fill in the ‘author’ and ‘date’ space in the YAML at the top of the file with your student number and appropriate information using quotation marks.\nKnit the Rmd file as html (RStudio may ask to install some packages; click ‘Yes’).\nIn the output, look at the results of the table under the ‘Results’ section and identify the dependent and the independent variables. You can learn about the meaning of these variables in the NILT documentation (click here to access the documentation).\nIdentify the variables that are significant in this model and the direction of the relationship.\nDiscuss your interpretations with your neighbour or tutor. You can refer back to Lab 8 and Lab 9 to refresh your memory.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#activity-2",
    "href": "11-Lab11.html#activity-2",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "Write one introductory paragraph in the ‘Introduction’ section of the template according to the guidance provided and your preliminary insights.\nKnit the document again.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "11-Lab11.html#conclusion",
    "href": "11-Lab11.html#conclusion",
    "title": "UG Quants summative assessment: Interpreting Quantitative Findings Report",
    "section": "",
    "text": "You are on the right track now!\nWe hope that by getting familiar with this setting, you will easily succeed in writing your research report using all the knowledge and skills acquired during this course. Take this session to ask questions about the assignment or the course in general as much as possible. This is the right time to have specialized one-to-one support from your tutors.\nWe wish you the best of luck! Get in touch with your tutors via email or, even better, post any questions you have about the assignment on your lab group discussion forum on Moodle, if you don’t know where to start or get stuck. Don’t suffer in silence. Remember your Tutors, lab group mates, and the teaching and admin team are here for you. Also don’t forget coding is all about trial and error, so it’s completely normal to write / copy and paste some code, then get an error message, and basically for your code to not work. Keep chipping at it, which sometimes can take hours (if not days), until you can get it to work. That’s a normal process even for professional data scientists and quantitative researchers! So do persevere and don’t panic if you don’t get it to work right away, because troubleshooting your code is part of the work, in addition to making your own choices in the analysis and reporting. For the code, the error message you get often gives you clues about, well, where the error is. So read the error message carefully, it might be you haven’t loaded the package required to run the code (remember to do this every time you open RStudio) or you might have missed a parentheses, or you might have not capitalised a word in the R syntax when you need to. Double check the R cheatsheets or previous sections in the lab workbook to ensure you have specified R syntax/arguments correctly. Again, trial and error is your friend, unlike writing an essay/doing an exam.\nGood luck and have fun! You got this.",
    "crumbs": [
      "**Lab 11** Interpretive Report Assessment"
    ]
  },
  {
    "objectID": "05-Lab5.html",
    "href": "05-Lab5.html",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "In the previous labs you were exploring the 2012 Northern Ireland Life and Times Survey (NILT). You’ve learnt how to download, read and format the data. Also, you’ve learnt how to explore categorical and numeric data and a mix of them.\nAcross the labs we have also been using R Markdown, incrementally introducing aspects of it each week. In this lab, we will consolidate what we have covered and learn about how to efficiently report quantitative results. R Markdown is used by many academics and professionals in a workplace setting to communicate quantitative findings to a wider audience. R Markdown is also what you will use to write your research report assignment for this course - where we are keeping this week’s lab relatively short so you have time to setup and apply what we have covered to your formative assessment. Let’s dive in and learn more!\n\nTo briefly summarise what covered so far - R Markdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with code chunks that can be converted to multiple file formats, such as HTML or PDF. This makes R Markdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis.\nR Markdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text. Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. R Markdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text is passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format.\nR Markdown then demonstrates another wonderful aspect of free and open source software - bringing together a combination of existing software into more powerful tools. It may look ugly at first, especially as most software follows the “What You See is What You Get” (aka WYSIWYG) principle, such as Microsoft Word. Indeed, it can seem odd needing to use #s in Markdown at the start of lines to create headers, and the text not looking the same as when you knit it. The use of # and other syntax in Markdown is what is known as ‘markup’, using code to specify the structure and formatting. Markdown is a play on the term ‘markup’ as compared to many alternatives, it is lightweight and human-readable.\nIt is easier to explain what is meant by lightweight and human-readable if we compare it to Microsoft Word documents. Whilst using Word you may have WYSIWYG, behind the scenes Word is recording all text in a markup language. More specifically, it uses XML, which has a system of tags, e.g. &lt;tag&gt; to start section formatted in specific way and &lt;/tag&gt; to mark where that formatting ends. In Markdown we can simply write ## My Section Title to add a 2nd level header with the text “My Section Title”. Within the XML for Word documents this would instead be:\n\n&lt;w:p&gt;\n  &lt;w:pPr&gt;\n    &lt;w:pStyle w:val=\"Heading2\"/&gt;\n  &lt;/w:pPr&gt;\n  \n  &lt;w:r&gt;\n    &lt;w:t&gt;My Section Title&lt;/w:t&gt;\n  &lt;/w:r&gt;\n&lt;/w:p&gt;\n\nWhilst it can be beneficial then that Word handles all this for you, it can also result in a series of problems. Different versions of Word can display the same document differently. I have collaborated on lengthy documents with colleagues where on one person’s screen a new section begins on page 32 and on someone else’s page 33. When you copy and paste text from elsewhere you need to ensure you “paste without source formatting” to avoid the formatting from other documents being used in your current document. You have likely also experienced the horror when working in large documents of adding an image and all the formatting going awry. With complex formatting, the document can also develop unfixable issues, as issues within the underlying XML accummulate that Word itself is unable to fix and the user is unable to directly edit.\n\nFor this lab, we will continue working in the same project called NILT in Posit Cloud.\n\nPlease go to your ‘Quants lab group’ in Posit Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Lab Group ##’;\n\nNote, if you received an error last week when trying to run the code to load the NILT dataset, the most likely cause was that you created your R Markdown file in a subfolder, rather than the top-level project folder. Please see the final section below for how to check if this was the problem and how to fix it.\n\nCreate a new Rmd file, from the ‘File’ tab on the top-left: `File&gt;New File&gt;R Markdown…’\nRStudio may display a yellow banner at the top of your file, asking to install some packages, click ‘Yes’. (Don’t worry if this doesn’t show, if followed the instructions last week you will likely not see a yellow banner when creating a new file this week.)\nType ‘R Markdown Test’ in the ‘Title’ section, your name in the ‘Author’ box, and tick the checkbox for ‘Use current date when rendering document’. Leave the ‘Default Output Format’ as HTML. Then, click ‘OK’.\nSave the Rmd file clicking on File&gt;Save as..., type RMarkdown in the ‘File name’ box, and click on the Save button.\n\nAfter completing the previous steps, your screen should look like this:\n\n\n\n\nRmd file.\n\n\n\nNote that you can have multiple files open in the Sources pane (top-left) at the same time. These will appear as a series of tabs along the top of the Sources pane. If what you were working on suddenly changes, make sure you haven’t accidentally switched tabs to another file.\n\n\nCompared to how we created RMarkdown files in previous weeks, when creating a new R Markdown file via the File menu, it creates one that includes some example content by default.\nRmd document RMarkdown contains an example by default. The first bit on the top enclosed by the dashes ---, you should be used to by now and contains the general metadata to format the output. This bit is called YAML. In the default example, it contains the title, name of the author, date, and the type of output (html).\nWe can do a lot with this YAML header. Let’s specify some more options. Under the line for title add a new line and add:\n\nsubtitle: \"This is a test\"\n\nNext modify the “output” line by firstly on the output: line placing your cursor at the start of html_document and hit enter. This will place html_document on a new line with a tab / 2-space indent at the start. Next add : at the end of html_document, so you have html_document:, and hit enter again. This will start a new line with a double tab / 4-space indent at the start. Add the following lines:\n\n    toc: true\n    toc_float: true\n    theme: cosmo\n\nYour YAML header should now look at follows:\n\nsubtitle is self-explanatory. What we have done by placing html_document on a new line with : at the end and nested list of further key-value pairs underneath, is we have specified some options for how we want our knitted HTML document to look. toc stands for “table of content”, toc_float places the table of contents on left-hand side of the document rather than at the top, and theme changes the default HTML template used to the “cosmo” theme instead.\nTo render the document from Rmd to HTML, we need to Knit it by clicking on the icon shown below. Try it!\n\n\nKnit button\n\nIf this is your first time knitting in your NILT project, RStudio may ask you if you want to update some packages, click ‘Yes’.\nAfter you knit the document, a window with the output will pop up automatically. As you can see, this document contains the main title and subtitle, followed by your name and the current date, as specified in the YAML. A table of contents appears on the left, showing the headers used in the document, with the header for the section currently in view on the screen highlighted in blue (notice how it will change if you scroll up/down the HTML file).\n\nYou may notice that compared to previous R Markdown files we have knitted, the style of this one may look familiar…\nThis is because the “cosmo” theme is the same one that this lab workbook itself uses as its base theme. This is another powerful aspect of using R Markdown. Your document itself remains fairly simple, but with a few simple options you can quickly customise its look and add features like a table of content. There is a lot more options you can use with HTML documents and range of themes can choose from. As you build your coding skills, you can also write additional custom scripts to further style your documents and add even more features. Importantly, despite the increasing customisation and complexity, the R Markdown files with your narrative and code remain simple and will knit to consistently and neatly formatted HTML files.\n\nBack to the R Markdown file. After the YAML header, there is a second-level header which includes the first section of this example document. Also, the word ‘Knit’ is shown in bold, as it was wrapped by double asterisk **. You can also wrap text by a single asterisk * to make it italics.\nThese are further examples of R Markdown’s simple, human-readable syntax for formatting text. Whilst different to what you will be used to using WYSIWYG apps, it is easy to learn. This simple transparent syntax makes it simple and quick to format our text, and avoids all the potential issues arising from hiding the syntax from users, as happens with Word document.\nBelow is a screenshot from RStudio with text using the key basic syntax for formatting your main text:\n\n\n\n\n\n\n\n\nAnd, this is how it appears when knitted:\n\n\n\n\n\n\n\n\nAs seen already, headings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level:\n\n\n\n\n\n\n\n\nAnd, how it appears when knitted:\n\n\n\n\n\n\n\n\n\nThe main thing in R Markdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between:\n\nEach paragraph\nBefore and after a list\nBefore and after a header\n\nHere’s some example text in an R Markdown document, the first without line spacing, the second with:\n\n\n\n\n\n\n\n\nAnd, how this looks when knitted:\n\n\n\n\n\n\n\n\nSide-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in R Markdown.\n\nIncluded at the top of the example content in our R Markdown file is a code chunk named “setup”. Similar to how we have setup our R Markdown files in previous labs, this contains knitr::opts_chunk$set() code to set global default options for our code chunks. By global, what we mean is that the options set here will apply to all of our code chunks unless we set individual options for any of them.\nFor example, if we scroll down to the code chunk named “pressure”, you will see it has “echo=FALSE”.\n\nThis overrides the global default that was set - echo=TRUE - which tells R when knitting to include the code chunk in the knitted output. So, by setting echo=FALSE for this code chunk, it will not display in our knitted file. If you look at your knitted file, you will see that whilst the summary(cars) code is included, plot(pressure) is not.\n\nSome key options that you can set for code chunks - either globally in the “setup” code chunk or on a per code chunk basis - are:\n\n\necho - whether the code chunk is shown in knitted files.\n\ninclude - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files.\n\nmessage and warning - whether messages and warnings raised when running code are displayed in knitted files.\n\neval - whether the code is run or not.\n\nAs noted at the second in-person lecture, setting eval=FALSE is useful if wanting to include example code or code that would result in an error message. This makes it incredibly useful when writing your own notes as - for example - you can keep a record of code that resulted in an error message followed by another code chunk that has the working code.\nImportantly, whilst across the labs we have setup our R Markdown files to show the code from our code chunks in our knitted files, we do this so the files and their knitted outputs are useful reference material. When writing academic articles and research reports for public audience, we usually do not want to include our code. As you will see, because of that, the R Markdown files in the project templates for the formative and summative assessments already includes echo=FALSE as a global option in the setup code chunk. That saves you from then needing to manually add echo=FALSE to any additional code chunks you create. If you want to create code chunks to run code in your R Markdown file, but don’t want either the code or its output included in your knitted file, you can add include=FALSE in the code chunk options.\n\nWhen you knitted the document, RStudio actually created a new .html file with the same name as your R Markdown document. You can confirm this by looking in the ‘Files’ tab in bottom-right.\n\nFor the formative and summative assessments, you will need to export your HTML file rather than saving it via your browser. Whilst HTML files saved from the browser and exported from RStudio will look absolutely identical when opened, some browsers - and browser extensions - will add additional code within the HTML file. This additional code does not change how the file looks when opened, but the code in the file creates issues for Turnitin.\nTo export your knitted HTML file then -\n\nCheck the box next to the knitted HTML file you want to export in the ‘Files’ panel. By default, the Files panel is on the bottom right of the screen.\n\n\n\n\n\n\n\n\n\n\nClick ‘More’ from the toolbar at the top of the Files panel.\n\n\n\n\n\n\n\n\n\n\nClick ‘Export’ from the menu options that pop-up.\n\n\n\n\n\n\n\n\n\n\nThis will pop open an ‘Export Files’ dialogue. Here, for the assessments, you can rename the file to include your student number. Just make sure the “.html” at the end remains included.\n\n\n\n\n\n\n\n\n\n\nAfter naming the file, click ‘Download’. This should open a dialogue to select which location to save the file. (If not see the ‘Important’ note below.)\n\n\n\n\n\n\n\n\n\nHere’s a short gif running through all the steps together.\n\n\n\n\n\n\n\n\nImportant: Some browsers by default will download to a ‘Downloads’ folder and instantly open the file within the browser after its finished downloading. This will open your knitted file in a tab within your browser. Do not then ‘Save as…’ the file from within the browser. Instead, close the tab and use your file explorer to navigate to your ‘Downloads’ folder to find the exported copy of your file to submit.\n\nIn the RMarkdown.Rmd file that you just created, do the following:\n\nChange the title of the document in the YAML to ‘Lab 5 - R Markdown Test’.\nIn the code chunk called ‘pressure’, change echo=FALSE to echo=TRUE.\nAdd/modify the html_document: options and theme. (Themes you can use: bootstrap, cerulean, cosmo, darkly, flatly, journal, lumen, paper, readable, sandstone, simplex, spacelab, united, and yeti.)\nAt the very bottom of the script, create a new paragraph and write one or two lines briefly describing how you think quantitative methods are improving your discipline (e.g. politics, sociology, social and public policy, or central and eastern European studies).\nKnit the document in html format.\nExport the newly edited version of the RMarkdown.html document to your machine.\nDiscuss how each of the edits suggested above modify the output with your neighbour or your tutor.\n\nMake sure you’ve got the basics of R Markdown, including exporting your knitted HTML files, since this what you will use to write your formative and summative assessments.\nAs a final activity for this lab, use the time remaining to create an RStudio project for the formative assessment and make a start.\nIf there is something not very clear, or you are curious about, feel free to ask your tutor. They will be happy to answer your questions.\n\n\nFor those who received an error message last week when trying to load the NILT dataset.\nYou can check whether you created a file last week in a sub-folder in the bottom-right ‘Files’ tab. You should see a folder path hierarchy such as “Cloud &gt; project”. If this contains additional text, such as “&gt; data” or “&gt; R”, you are currently in a subfolder and any new file you create will be placed in it rather than the top-level project folder.\n\nTo move your R Markdown file created last week -\n\nTick the checkbox next to it in the file list.\nClick ‘More’ in the toolbar at the top of the ‘Files’ tab.\nFrom the drop-down options select ‘Move…’\n\n\nThis will open a ‘Choose Folder’ dialogue -\n\nClick ‘project’ within the “cloud &gt; project &gt; data” folder hierachy.\nClick ‘Choose’ to confirm.\n\n\nIf you have the file open in a tab in the Source view, you will be asked if you wish to close the tab now that it is moved, click ‘Yes’.\nYou can then navigate back to the top-level folder by clicking ‘project’. Any new files you create will then be created in the top-level folder.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#introduction",
    "href": "05-Lab5.html#introduction",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "In the previous labs you were exploring the 2012 Northern Ireland Life and Times Survey (NILT). You’ve learnt how to download, read and format the data. Also, you’ve learnt how to explore categorical and numeric data and a mix of them.\nAcross the labs we have also been using R Markdown, incrementally introducing aspects of it each week. In this lab, we will consolidate what we have covered and learn about how to efficiently report quantitative results. R Markdown is used by many academics and professionals in a workplace setting to communicate quantitative findings to a wider audience. R Markdown is also what you will use to write your research report assignment for this course - where we are keeping this week’s lab relatively short so you have time to setup and apply what we have covered to your formative assessment. Let’s dive in and learn more!",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#r-markdown",
    "href": "05-Lab5.html#r-markdown",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "To briefly summarise what covered so far - R Markdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with code chunks that can be converted to multiple file formats, such as HTML or PDF. This makes R Markdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis.\nR Markdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text. Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. R Markdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text is passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format.\nR Markdown then demonstrates another wonderful aspect of free and open source software - bringing together a combination of existing software into more powerful tools. It may look ugly at first, especially as most software follows the “What You See is What You Get” (aka WYSIWYG) principle, such as Microsoft Word. Indeed, it can seem odd needing to use #s in Markdown at the start of lines to create headers, and the text not looking the same as when you knit it. The use of # and other syntax in Markdown is what is known as ‘markup’, using code to specify the structure and formatting. Markdown is a play on the term ‘markup’ as compared to many alternatives, it is lightweight and human-readable.\nIt is easier to explain what is meant by lightweight and human-readable if we compare it to Microsoft Word documents. Whilst using Word you may have WYSIWYG, behind the scenes Word is recording all text in a markup language. More specifically, it uses XML, which has a system of tags, e.g. &lt;tag&gt; to start section formatted in specific way and &lt;/tag&gt; to mark where that formatting ends. In Markdown we can simply write ## My Section Title to add a 2nd level header with the text “My Section Title”. Within the XML for Word documents this would instead be:\n\n&lt;w:p&gt;\n  &lt;w:pPr&gt;\n    &lt;w:pStyle w:val=\"Heading2\"/&gt;\n  &lt;/w:pPr&gt;\n  \n  &lt;w:r&gt;\n    &lt;w:t&gt;My Section Title&lt;/w:t&gt;\n  &lt;/w:r&gt;\n&lt;/w:p&gt;\n\nWhilst it can be beneficial then that Word handles all this for you, it can also result in a series of problems. Different versions of Word can display the same document differently. I have collaborated on lengthy documents with colleagues where on one person’s screen a new section begins on page 32 and on someone else’s page 33. When you copy and paste text from elsewhere you need to ensure you “paste without source formatting” to avoid the formatting from other documents being used in your current document. You have likely also experienced the horror when working in large documents of adding an image and all the formatting going awry. With complex formatting, the document can also develop unfixable issues, as issues within the underlying XML accummulate that Word itself is unable to fix and the user is unable to directly edit.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#new-r-markdown-document",
    "href": "05-Lab5.html#new-r-markdown-document",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "For this lab, we will continue working in the same project called NILT in Posit Cloud.\n\nPlease go to your ‘Quants lab group’ in Posit Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Lab Group ##’;\n\nNote, if you received an error last week when trying to run the code to load the NILT dataset, the most likely cause was that you created your R Markdown file in a subfolder, rather than the top-level project folder. Please see the final section below for how to check if this was the problem and how to fix it.\n\nCreate a new Rmd file, from the ‘File’ tab on the top-left: `File&gt;New File&gt;R Markdown…’\nRStudio may display a yellow banner at the top of your file, asking to install some packages, click ‘Yes’. (Don’t worry if this doesn’t show, if followed the instructions last week you will likely not see a yellow banner when creating a new file this week.)\nType ‘R Markdown Test’ in the ‘Title’ section, your name in the ‘Author’ box, and tick the checkbox for ‘Use current date when rendering document’. Leave the ‘Default Output Format’ as HTML. Then, click ‘OK’.\nSave the Rmd file clicking on File&gt;Save as..., type RMarkdown in the ‘File name’ box, and click on the Save button.\n\nAfter completing the previous steps, your screen should look like this:\n\n\n\n\nRmd file.\n\n\n\nNote that you can have multiple files open in the Sources pane (top-left) at the same time. These will appear as a series of tabs along the top of the Sources pane. If what you were working on suddenly changes, make sure you haven’t accidentally switched tabs to another file.\n\n\nCompared to how we created RMarkdown files in previous weeks, when creating a new R Markdown file via the File menu, it creates one that includes some example content by default.\nRmd document RMarkdown contains an example by default. The first bit on the top enclosed by the dashes ---, you should be used to by now and contains the general metadata to format the output. This bit is called YAML. In the default example, it contains the title, name of the author, date, and the type of output (html).\nWe can do a lot with this YAML header. Let’s specify some more options. Under the line for title add a new line and add:\n\nsubtitle: \"This is a test\"\n\nNext modify the “output” line by firstly on the output: line placing your cursor at the start of html_document and hit enter. This will place html_document on a new line with a tab / 2-space indent at the start. Next add : at the end of html_document, so you have html_document:, and hit enter again. This will start a new line with a double tab / 4-space indent at the start. Add the following lines:\n\n    toc: true\n    toc_float: true\n    theme: cosmo\n\nYour YAML header should now look at follows:\n\nsubtitle is self-explanatory. What we have done by placing html_document on a new line with : at the end and nested list of further key-value pairs underneath, is we have specified some options for how we want our knitted HTML document to look. toc stands for “table of content”, toc_float places the table of contents on left-hand side of the document rather than at the top, and theme changes the default HTML template used to the “cosmo” theme instead.\nTo render the document from Rmd to HTML, we need to Knit it by clicking on the icon shown below. Try it!\n\n\nKnit button\n\nIf this is your first time knitting in your NILT project, RStudio may ask you if you want to update some packages, click ‘Yes’.\nAfter you knit the document, a window with the output will pop up automatically. As you can see, this document contains the main title and subtitle, followed by your name and the current date, as specified in the YAML. A table of contents appears on the left, showing the headers used in the document, with the header for the section currently in view on the screen highlighted in blue (notice how it will change if you scroll up/down the HTML file).\n\nYou may notice that compared to previous R Markdown files we have knitted, the style of this one may look familiar…\nThis is because the “cosmo” theme is the same one that this lab workbook itself uses as its base theme. This is another powerful aspect of using R Markdown. Your document itself remains fairly simple, but with a few simple options you can quickly customise its look and add features like a table of content. There is a lot more options you can use with HTML documents and range of themes can choose from. As you build your coding skills, you can also write additional custom scripts to further style your documents and add even more features. Importantly, despite the increasing customisation and complexity, the R Markdown files with your narrative and code remain simple and will knit to consistently and neatly formatted HTML files.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#r-markdown-syntax",
    "href": "05-Lab5.html#r-markdown-syntax",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "Back to the R Markdown file. After the YAML header, there is a second-level header which includes the first section of this example document. Also, the word ‘Knit’ is shown in bold, as it was wrapped by double asterisk **. You can also wrap text by a single asterisk * to make it italics.\nThese are further examples of R Markdown’s simple, human-readable syntax for formatting text. Whilst different to what you will be used to using WYSIWYG apps, it is easy to learn. This simple transparent syntax makes it simple and quick to format our text, and avoids all the potential issues arising from hiding the syntax from users, as happens with Word document.\nBelow is a screenshot from RStudio with text using the key basic syntax for formatting your main text:\n\n\n\n\n\n\n\n\nAnd, this is how it appears when knitted:\n\n\n\n\n\n\n\n\nAs seen already, headings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level:\n\n\n\n\n\n\n\n\nAnd, how it appears when knitted:\n\n\n\n\n\n\n\n\n\nThe main thing in R Markdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between:\n\nEach paragraph\nBefore and after a list\nBefore and after a header\n\nHere’s some example text in an R Markdown document, the first without line spacing, the second with:\n\n\n\n\n\n\n\n\nAnd, how this looks when knitted:\n\n\n\n\n\n\n\n\nSide-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in R Markdown.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#code-chunks-options",
    "href": "05-Lab5.html#code-chunks-options",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "Included at the top of the example content in our R Markdown file is a code chunk named “setup”. Similar to how we have setup our R Markdown files in previous labs, this contains knitr::opts_chunk$set() code to set global default options for our code chunks. By global, what we mean is that the options set here will apply to all of our code chunks unless we set individual options for any of them.\nFor example, if we scroll down to the code chunk named “pressure”, you will see it has “echo=FALSE”.\n\nThis overrides the global default that was set - echo=TRUE - which tells R when knitting to include the code chunk in the knitted output. So, by setting echo=FALSE for this code chunk, it will not display in our knitted file. If you look at your knitted file, you will see that whilst the summary(cars) code is included, plot(pressure) is not.\n\nSome key options that you can set for code chunks - either globally in the “setup” code chunk or on a per code chunk basis - are:\n\n\necho - whether the code chunk is shown in knitted files.\n\ninclude - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files.\n\nmessage and warning - whether messages and warnings raised when running code are displayed in knitted files.\n\neval - whether the code is run or not.\n\nAs noted at the second in-person lecture, setting eval=FALSE is useful if wanting to include example code or code that would result in an error message. This makes it incredibly useful when writing your own notes as - for example - you can keep a record of code that resulted in an error message followed by another code chunk that has the working code.\nImportantly, whilst across the labs we have setup our R Markdown files to show the code from our code chunks in our knitted files, we do this so the files and their knitted outputs are useful reference material. When writing academic articles and research reports for public audience, we usually do not want to include our code. As you will see, because of that, the R Markdown files in the project templates for the formative and summative assessments already includes echo=FALSE as a global option in the setup code chunk. That saves you from then needing to manually add echo=FALSE to any additional code chunks you create. If you want to create code chunks to run code in your R Markdown file, but don’t want either the code or its output included in your knitted file, you can add include=FALSE in the code chunk options.\n\nWhen you knitted the document, RStudio actually created a new .html file with the same name as your R Markdown document. You can confirm this by looking in the ‘Files’ tab in bottom-right.\n\nFor the formative and summative assessments, you will need to export your HTML file rather than saving it via your browser. Whilst HTML files saved from the browser and exported from RStudio will look absolutely identical when opened, some browsers - and browser extensions - will add additional code within the HTML file. This additional code does not change how the file looks when opened, but the code in the file creates issues for Turnitin.\nTo export your knitted HTML file then -\n\nCheck the box next to the knitted HTML file you want to export in the ‘Files’ panel. By default, the Files panel is on the bottom right of the screen.\n\n\n\n\n\n\n\n\n\n\nClick ‘More’ from the toolbar at the top of the Files panel.\n\n\n\n\n\n\n\n\n\n\nClick ‘Export’ from the menu options that pop-up.\n\n\n\n\n\n\n\n\n\n\nThis will pop open an ‘Export Files’ dialogue. Here, for the assessments, you can rename the file to include your student number. Just make sure the “.html” at the end remains included.\n\n\n\n\n\n\n\n\n\n\nAfter naming the file, click ‘Download’. This should open a dialogue to select which location to save the file. (If not see the ‘Important’ note below.)\n\n\n\n\n\n\n\n\n\nHere’s a short gif running through all the steps together.\n\n\n\n\n\n\n\n\nImportant: Some browsers by default will download to a ‘Downloads’ folder and instantly open the file within the browser after its finished downloading. This will open your knitted file in a tab within your browser. Do not then ‘Save as…’ the file from within the browser. Instead, close the tab and use your file explorer to navigate to your ‘Downloads’ folder to find the exported copy of your file to submit.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "05-Lab5.html#activity",
    "href": "05-Lab5.html#activity",
    "title": "Reporting in R Markdown",
    "section": "",
    "text": "In the RMarkdown.Rmd file that you just created, do the following:\n\nChange the title of the document in the YAML to ‘Lab 5 - R Markdown Test’.\nIn the code chunk called ‘pressure’, change echo=FALSE to echo=TRUE.\nAdd/modify the html_document: options and theme. (Themes you can use: bootstrap, cerulean, cosmo, darkly, flatly, journal, lumen, paper, readable, sandstone, simplex, spacelab, united, and yeti.)\nAt the very bottom of the script, create a new paragraph and write one or two lines briefly describing how you think quantitative methods are improving your discipline (e.g. politics, sociology, social and public policy, or central and eastern European studies).\nKnit the document in html format.\nExport the newly edited version of the RMarkdown.html document to your machine.\nDiscuss how each of the edits suggested above modify the output with your neighbour or your tutor.\n\nMake sure you’ve got the basics of R Markdown, including exporting your knitted HTML files, since this what you will use to write your formative and summative assessments.\nAs a final activity for this lab, use the time remaining to create an RStudio project for the formative assessment and make a start.\nIf there is something not very clear, or you are curious about, feel free to ask your tutor. They will be happy to answer your questions.\n\n\nFor those who received an error message last week when trying to load the NILT dataset.\nYou can check whether you created a file last week in a sub-folder in the bottom-right ‘Files’ tab. You should see a folder path hierarchy such as “Cloud &gt; project”. If this contains additional text, such as “&gt; data” or “&gt; R”, you are currently in a subfolder and any new file you create will be placed in it rather than the top-level project folder.\n\nTo move your R Markdown file created last week -\n\nTick the checkbox next to it in the file list.\nClick ‘More’ in the toolbar at the top of the ‘Files’ tab.\nFrom the drop-down options select ‘Move…’\n\n\nThis will open a ‘Choose Folder’ dialogue -\n\nClick ‘project’ within the “cloud &gt; project &gt; data” folder hierachy.\nClick ‘Choose’ to confirm.\n\n\nIf you have the file open in a tab in the Source view, you will be asked if you wish to close the tab now that it is moved, click ‘Yes’.\nYou can then navigate back to the top-level folder by clicking ‘project’. Any new files you create will then be created in the top-level folder.",
    "crumbs": [
      "**Lab 5** Reporting in R Markdown"
    ]
  },
  {
    "objectID": "07-Lab7.html",
    "href": "07-Lab7.html",
    "title": "Correlation",
    "section": "",
    "text": "When conducting empirical research, we are often interested in associations between two variables, for example, personal income and attitudes towards migrants. In this lab we will focus on visualizing relationship between variables and how to measure it. In quantitative research, the main variable of interest in an analysis is called the dependent or response variable, and the second is known as the independent or explanatory variable. In our example, we can think of personal income as the independent variable and attitudes as the dependent.\nThe relationship between variables can be positive, negative, or non-existent. The figure below shows these types of relationships to different extents. The association is positive when one of the variables increases and the second variable tends to go in the same direction (that is increasing as well). The first plot on the left-hand side shows a strong positive relationship. As you can see, the points are closely clustered around the straight line. The next plot also shows a positive relationship. This time the relationship is moderate. Therefore, the points are more dispersed in relation to the line compared to the previous one.\n\n\n\n\n Types of correlation.\n\n\n\nThe plot in the middle, shows two variables that are not correlated. The location of the points is not following any pattern and the line is flat. By contrast, the last two plots on the right hand-side show a negative relationship. When the values on the X axis increase, the values on the Y axis tend to decrease.\n\nWe will continue working on the same ‘NILT’ Posit Cloud project with the the 2012 Northern Ireland Life and Times Survey (NILT) data.\nAs usual, we will need to setup an R Markdown file for today’s lab. I have kept the steps brief this week. However, if you are unsure about anything, take a look back at Lab 6 which has more detailed explanation of the steps.\nTo set up a new R Markdown file for this lab, please use the following steps:\n\nPlease go to your ‘Lab Group ##’ in Posit Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Lab Group ##’;\nWithin the Files tab (bottom-right pane) click ‘New File’, then ‘R Markdown’ from the drop-down list of options;\nWithin the ‘Create a New File in Current Directory’ dialogue, name it ‘Lab-7-Correlation.Rmd’ and click OK.\nFeel free to make any adjustments to the YAML header.\nCreate a new code chunk, with ```{r setup, include=FALSE} and in the chunk:\n\n\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\n\nCreate another code chunk, named premable and again with include=FALSE. Then add the following in the chunk:\n\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n\n# Read NILT\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nPlease note that we are loading the haven package again this week as we will be coercing more variables. As a reminder, we need to load haven as the initial dataset we started with was in the SPSS file format. Whilst we coerced variables and saved it in R’s file format, the file still maintains info from SPSS. We will be using this info in coercing more variables in this session, so need to have the haven package loaded.\n\nRun the preamble code chunk and if no errors, then everything is setup for this session.\n\nWe opened this lab with example of attitude towards migrants. Let’s continue with this example by using new variables from the NILT survey. As this is the first time we are using them, we will need to coerce them into their appropriate variable type first.\nCreate a new code chunk, add the code below, and then run the chunk:\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n\n# Perception of migrants\nnilt &lt;- nilt |&gt; mutate_at(vars(mil10yrs, miecono, micultur), as.numeric)\n\nWe used as.numeric() previously in Lab 2 and Lab 3 for coercing numeric variables. Since this is a base R function, we pass our variable of interest to the function in the form dataframe$column. Here, we are coercing nilt$spage. If you look at page 7 of the NILT Teaching Resources documentation, you will see this variable is for the age of the respondent’s spouse or partner.\nNext, we use the mutate_at() function. As it comes from the Tidyverse, we can use dataframe |&gt; function() and name our variables directly - mil10yrs rather than nilt$mil10yrs. You can read more about mil10yrs, miecono, and micultur on page 14 of the NILT Teaching Resources documentation.\nThe mutate_at() function is similar to the mutate() function we used in Labs 2 and 3. However, this function has the nice feature of letting us apply the same transformation to multiple columns. It’s basic usage is data_frame |&gt; mutate_at(variables, function), where variables is the variables to transform and function the function to apply to them. The vars() function is just a nice Tidyverse way to do similar as c(\"variable1\", \"variable2\", ...) that we used with sumtable(). Then as.numeric() is the function that will be applied to our variables.\nBonus activity - In our code chunk above, we used base R style code to coerce our spage variable and then Tidyverse for the rest. However, as we are applying the same transformation - coercing the variables as numeric - we could actually do coerce all four with a single line of code. Can you see how?\nAgain as wee reminder - whilst these slight differences between base R, Tidyverse, and other packages - such as vtable - can be confusing at first, it is just different design philosophies behind the packages and functions. The general rule of thumb is that base R functions tend to require dataframe$column as the functions work with multiple object types, so do not have separate arguments for the dataframe and the variables of interest. In contrast, the Tidyverse as it is built around using dataframes has functions with separate arguments for the dataframe and variables of interest. Most Tidyverse functions then use dataframe |&gt; function(... format to specify the dataframe and then let you specify the variables directly - column rather than dataframe$column. Functions from packages outwith the Tidyverse can vary, with some following the Tidyverse approach or others - like vtable - have functions that have similar arguments but with slightly different syntax. Again - to re-emphasise - you do not need to memorise all this. Instead, this is why it is important to learn how to find and read package/function documentation. Similarly, this is another reason why it can be invaluable to curate your own notes containing working code examples with comments for future reference.\nOK back to our variables. We will also create a new variable called mig_per by summing the respondent’s opinion in relation to migration using the three variables we just coerced: mil10yrs, miecono and micultur. This then let’s us have an overall perception value by combining the individual values for each of the variables.\nTo create the variable - insert a new chunk, add the code below, and then run the chunk:\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) |&gt;\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\nThis code again mixes new functions with ones we have previously used:\n\n\nrowwise() is a Tidyverse function that is the equivalent of saying ‘for each row’. We are using it here as we are wanting to calculate a value for each row for the new column we are creating. Without rowwise() the sum() function below, adding the values for the three existing variables, would add the values for all rows for the three columns together rather than calculating the value for each row individually.\n\nmutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = TRUE)) uses the - hopefully - now familar mutate() function, specifying we are creating a new mig_per variable that is equal to the value of our three existing variables summed together - using the sum() function. The na.rm = TRUE tells sum() to ignore NA values.\n\nungroup() is a Tidyverse function that signals that we are ending our rowwise(). So, it is basically the equivalent to saying ‘OK stop doing this for each row individually’.\n\nmutate(mig_per = na_if(mig_per, 0)) then goes through our new mig_per variable and sets any row where the value is 0 to NA instead.\n\nVisualizing two or more variables can help to uncover or understand the relationship between these variables. As briefly introduced in the previous session, different types of plots are appropriate for different types of variables. Therefore, we split the following sections according to the type of data to be analysed.\nWithin your R Markdown file, remember to create a new code chunk for each example and you can use ## Heading 2, ### Heading 3, and so on to help structure it into sections.\n\nTo illustrate this type of correlation, let’s start with a relatively obvious but useful example. Suppose we are interested in how people choose their spouse or partner. The first characteristic that we might look at is age. We might suspect that there is a correlation between the nilt respondents’ own age and their partner’s age.\nSince both ages are numeric variables, a scatter plot is appropriate to visualise the correlation. To do this, let’s construct a ggplot.\n\nnilt |&gt; ggplot(aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nAs covered last week, we start with the function ggplot(), and use + at the end of each line to compose our functions for the plot together. In aesthetics aes(), define the respondent’s age rage on the X axis and the respondent’s spouse/partner age spage on the Y axis. As a general convention in quantitative research, the response/dependent variable is visualised on the Y axis and the independent on the X axis. Here we are using the geom_point() function to specify the plot type. In contrast to the usual 3 line minimum pattern for constructing a plot, we have four here, with geom_smooth() added after geom_point(). This adds a straight line which describes the best fit between all the points in the graph.\nFrom the plot above, we see that there is a strong positive correlation between the respondent’s age and their partner’s age. We see that for some individuals their partner’s age is older, whereas others is younger. Also, there are some dots that are far away from the straight line. For example, in one case the respondent is around 60 years old and the age of their partner is around 30 years old (can you find that dot on the plot?). These extreme values are known as outliers.\nWe may also suspect that the respondents’ gender is playing a role in this relationship. We can include this as a third variable in the plot by colouring the dots by the respondents’ sex. To do this, let’s specify the colour argument in aesthetics aes() with a categorical variable rsex.\n\nnilt |&gt; ggplot(aes(x = rage, y = spage, colour = rsex)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"gray20\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nThis time rather than geom_smooth(), we have used geom_abline() to add a line showing where we would expect to see dots if the respondent and their spouse/partner were exactly the same age - the slope and intercept part of this code will become clearer when we cover simple linear regression next week. We observe a clear pattern in which most female participants are on one side of the line and most males on the other. As we can see, most female respondents tend to choose/have partners who are older, whereas males choose/have younger partners.\n\nIn your R Markdown file, use the nilt data object to visualise the relationship of the following variables by creating a new chunk.\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function set the method argument to 'lm'.\nRun the code chunk. What type of relationship do you observe? Comment as regular text (i.e. as text not inside the code chunk) in your R Markdown file the overall result of the plot and whether this is in line with your previous expectation.\n\n\nAs briefly introduced in the last lab, correlations often occur between categorical and numeric data. A good way to observe the relationship between these types of variables is using a box plot. Which essentially shows the distribution of the numeric values by category/group.\nLet’s say we are interested in the relationship between education level and perception of migration. The variable highqual contains the respondent’s highest education qualification. Using ggplot(), we can situate mig_per on the X axis and highqual on the Y axis, and plot it with the geom_boxplot() function. Note that before passing the dataset to ggplot, we can filter out two categories of the variable highqual where education level is unknown (i.e. “Other, level unknown” or “Unclassified”). Also notice how we can do both filter highqual and use the result of that filtering in a ggplot using pipes - |&gt;.\n\nnilt |&gt;\n  filter(highqual != \"Other, level unknown\" & highqual != \"Unclassified\") |&gt;\n  ggplot(aes(x = mig_per, y = highqual)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFrom the plot above, we see that respondents with higher education level (on the bottom) appear to have more positive opinion on migration when compared to respondents with lower education level or no qualifications (on the top). Overall, the data shows a pattern that the lower one’s education level is, the worse their opinion towards migration is likely to be. Since education level is an ordinal variable, we can say this is a positive relationship.\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk.\n\nCreate a boxplot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the political party which the respondent identify with uninatid. Use ggplot() in combination with geom_boxplot(). Make sure to specify mig_per on the Y axis and uninatid on the X axis in aes().\nDo you think the opinion towards migration differs among the groups in the plot? Comment on the overall results in regular text of your R Markodwn document.\n\nSo far we have examined correlation by visualizing variables only. A useful practice in quantitative research is to actually measure the magnitude of the relationship between these variables. One common measure is the Pearson correlation coefficient. This measure results in a number that goes from -1 to 1. A coefficient below 0 implies a negative correlation whereas a coefficient over 0 a positive one. When the coefficient is close to positive one (1) or negative one (-1), it implies that the relationship is strong. By contrast, coefficients close to 0 indicate a weak relationship. This technique is appropriate to measure linear numeric relationships, which is when we have numeric variables with a normal distribution, e.g. age in our dataset.\nLet’s start measuring the relationship between the respondent’s age and their partner’s age. To do this in R, we should use the cor() function. In the R syntax, first we specify the variables separated by a comma. As cor() is a base R function, we need to be explicit when specifying our variable names, using dataframe$column, as shown below. Also, I set the use argument as 'pairwise.complete.obs'. This is because one or both of the variables contain more than one missing value. Therefore, we are telling R to use complete observations only.\n\ncor(nilt$rage, nilt$spage, use = \"pairwise.complete.obs\")\n\n[1] 0.9481297\n\n\nThe correlation coefficient between this variables is 0.95. This is close to positive 1. Therefore, it is a strong positive correlation. The result is completely in line with the plot above, since we saw how the dots were close to the straight line.\nWhat about the relationship between age and mig_per that you plotted earlier?\n\ncor(nilt$rage, nilt$mig_per, use = \"pairwise.complete.obs\")\n\n[1] -0.05680918\n\n\nThe coefficient is very close to 0, which means that the correlation is practically non-existent. The absence of correlation is also interesting in research. For instance, one might expect that younger people would be more open to migration. However, it seems that age does not play a role on people’s opinion about migration in Northern Ireland according to this data.\nLet’s say that we are interested in the correlation between mig_per and all other numeric variables in the dataset. Instead of continuing computing the correlation one by one, we can run a correlation matrix. The code below combines Tidyverse’s select() function, the cor() function we just covered, and a new function round(). It can be read as follows: from the nilt data select these variables, then compute the correlation coefficient using only complete cases, and then round the results to 3 decimals. Notice as we specify our variables using the Tidyverse select() function, we specify our dataframe with dataframe |&gt; function(... and can then name our variables directly without dataframe$....\n\nnilt |&gt;\n  select(mig_per, rage, spage, rhourswk, persinc2) |&gt;\n  cor(use = \"pairwise.complete.obs\") |&gt;\n  round(3)\n\n         mig_per   rage  spage rhourswk persinc2\nmig_per    1.000 -0.057 -0.132    0.082    0.228\nrage      -0.057  1.000  0.948   -0.013   -0.036\nspage     -0.132  0.948  1.000   -0.182   -0.090\nrhourswk   0.082 -0.013 -0.182    1.000    0.383\npersinc2   0.228 -0.036 -0.090    0.383    1.000\n\n\nFrom the result above, we have a correlation matrix that computes the Person correlation coefficient for the selected variables. In the first row we have migration perception. You will notice that the first value is 1.00, this is because it is measuring the correlation against the same variable (i.e. itself). The next value in the first row is age, which is nearly 0. The next variables also result in low coefficients, with the exception of the personal income, where we see a moderate/low positive correlation. This can be interpreted that respondents with high income are associated with more positive opinion towards migration compared to low-income respondents.\n\n\nInsert a new chunk in your R Markdown file;\nUsing the nilt data object, compute a correlation matrix using the following variables: rage, persinc2, mil10yrs, miecono and micultur, setting the use argument to 'pairwise.complete.obs' and rounding the result to 3 decimals;\nRun the chunk individually and comment whether personal income or age is correlated with the perception of migrants in relation to the specific aspects asked in the variables measured (consult page 14 of the NILT Teaching Resources documentation to get a description of these variables);\nKnit your R Markdown document to .html. The output document will automatically be saved in your project.\nDiscuss your previous results with your neighbour or tutor.",
    "crumbs": [
      "**Lab 7** Correlation"
    ]
  },
  {
    "objectID": "07-Lab7.html#what-is-correlation",
    "href": "07-Lab7.html#what-is-correlation",
    "title": "Correlation",
    "section": "",
    "text": "When conducting empirical research, we are often interested in associations between two variables, for example, personal income and attitudes towards migrants. In this lab we will focus on visualizing relationship between variables and how to measure it. In quantitative research, the main variable of interest in an analysis is called the dependent or response variable, and the second is known as the independent or explanatory variable. In our example, we can think of personal income as the independent variable and attitudes as the dependent.\nThe relationship between variables can be positive, negative, or non-existent. The figure below shows these types of relationships to different extents. The association is positive when one of the variables increases and the second variable tends to go in the same direction (that is increasing as well). The first plot on the left-hand side shows a strong positive relationship. As you can see, the points are closely clustered around the straight line. The next plot also shows a positive relationship. This time the relationship is moderate. Therefore, the points are more dispersed in relation to the line compared to the previous one.\n\n\n\n\n Types of correlation.\n\n\n\nThe plot in the middle, shows two variables that are not correlated. The location of the points is not following any pattern and the line is flat. By contrast, the last two plots on the right hand-side show a negative relationship. When the values on the X axis increase, the values on the Y axis tend to decrease.\n\nWe will continue working on the same ‘NILT’ Posit Cloud project with the the 2012 Northern Ireland Life and Times Survey (NILT) data.\nAs usual, we will need to setup an R Markdown file for today’s lab. I have kept the steps brief this week. However, if you are unsure about anything, take a look back at Lab 6 which has more detailed explanation of the steps.\nTo set up a new R Markdown file for this lab, please use the following steps:\n\nPlease go to your ‘Lab Group ##’ in Posit Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Lab Group ##’;\nWithin the Files tab (bottom-right pane) click ‘New File’, then ‘R Markdown’ from the drop-down list of options;\nWithin the ‘Create a New File in Current Directory’ dialogue, name it ‘Lab-7-Correlation.Rmd’ and click OK.\nFeel free to make any adjustments to the YAML header.\nCreate a new code chunk, with ```{r setup, include=FALSE} and in the chunk:\n\n\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\n\nCreate another code chunk, named premable and again with include=FALSE. Then add the following in the chunk:\n\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n\n# Read NILT\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nPlease note that we are loading the haven package again this week as we will be coercing more variables. As a reminder, we need to load haven as the initial dataset we started with was in the SPSS file format. Whilst we coerced variables and saved it in R’s file format, the file still maintains info from SPSS. We will be using this info in coercing more variables in this session, so need to have the haven package loaded.\n\nRun the preamble code chunk and if no errors, then everything is setup for this session.\n\nWe opened this lab with example of attitude towards migrants. Let’s continue with this example by using new variables from the NILT survey. As this is the first time we are using them, we will need to coerce them into their appropriate variable type first.\nCreate a new code chunk, add the code below, and then run the chunk:\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n\n# Perception of migrants\nnilt &lt;- nilt |&gt; mutate_at(vars(mil10yrs, miecono, micultur), as.numeric)\n\nWe used as.numeric() previously in Lab 2 and Lab 3 for coercing numeric variables. Since this is a base R function, we pass our variable of interest to the function in the form dataframe$column. Here, we are coercing nilt$spage. If you look at page 7 of the NILT Teaching Resources documentation, you will see this variable is for the age of the respondent’s spouse or partner.\nNext, we use the mutate_at() function. As it comes from the Tidyverse, we can use dataframe |&gt; function() and name our variables directly - mil10yrs rather than nilt$mil10yrs. You can read more about mil10yrs, miecono, and micultur on page 14 of the NILT Teaching Resources documentation.\nThe mutate_at() function is similar to the mutate() function we used in Labs 2 and 3. However, this function has the nice feature of letting us apply the same transformation to multiple columns. It’s basic usage is data_frame |&gt; mutate_at(variables, function), where variables is the variables to transform and function the function to apply to them. The vars() function is just a nice Tidyverse way to do similar as c(\"variable1\", \"variable2\", ...) that we used with sumtable(). Then as.numeric() is the function that will be applied to our variables.\nBonus activity - In our code chunk above, we used base R style code to coerce our spage variable and then Tidyverse for the rest. However, as we are applying the same transformation - coercing the variables as numeric - we could actually do coerce all four with a single line of code. Can you see how?\nAgain as wee reminder - whilst these slight differences between base R, Tidyverse, and other packages - such as vtable - can be confusing at first, it is just different design philosophies behind the packages and functions. The general rule of thumb is that base R functions tend to require dataframe$column as the functions work with multiple object types, so do not have separate arguments for the dataframe and the variables of interest. In contrast, the Tidyverse as it is built around using dataframes has functions with separate arguments for the dataframe and variables of interest. Most Tidyverse functions then use dataframe |&gt; function(... format to specify the dataframe and then let you specify the variables directly - column rather than dataframe$column. Functions from packages outwith the Tidyverse can vary, with some following the Tidyverse approach or others - like vtable - have functions that have similar arguments but with slightly different syntax. Again - to re-emphasise - you do not need to memorise all this. Instead, this is why it is important to learn how to find and read package/function documentation. Similarly, this is another reason why it can be invaluable to curate your own notes containing working code examples with comments for future reference.\nOK back to our variables. We will also create a new variable called mig_per by summing the respondent’s opinion in relation to migration using the three variables we just coerced: mil10yrs, miecono and micultur. This then let’s us have an overall perception value by combining the individual values for each of the variables.\nTo create the variable - insert a new chunk, add the code below, and then run the chunk:\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) |&gt;\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\nThis code again mixes new functions with ones we have previously used:\n\n\nrowwise() is a Tidyverse function that is the equivalent of saying ‘for each row’. We are using it here as we are wanting to calculate a value for each row for the new column we are creating. Without rowwise() the sum() function below, adding the values for the three existing variables, would add the values for all rows for the three columns together rather than calculating the value for each row individually.\n\nmutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = TRUE)) uses the - hopefully - now familar mutate() function, specifying we are creating a new mig_per variable that is equal to the value of our three existing variables summed together - using the sum() function. The na.rm = TRUE tells sum() to ignore NA values.\n\nungroup() is a Tidyverse function that signals that we are ending our rowwise(). So, it is basically the equivalent to saying ‘OK stop doing this for each row individually’.\n\nmutate(mig_per = na_if(mig_per, 0)) then goes through our new mig_per variable and sets any row where the value is 0 to NA instead.\n\nVisualizing two or more variables can help to uncover or understand the relationship between these variables. As briefly introduced in the previous session, different types of plots are appropriate for different types of variables. Therefore, we split the following sections according to the type of data to be analysed.\nWithin your R Markdown file, remember to create a new code chunk for each example and you can use ## Heading 2, ### Heading 3, and so on to help structure it into sections.\n\nTo illustrate this type of correlation, let’s start with a relatively obvious but useful example. Suppose we are interested in how people choose their spouse or partner. The first characteristic that we might look at is age. We might suspect that there is a correlation between the nilt respondents’ own age and their partner’s age.\nSince both ages are numeric variables, a scatter plot is appropriate to visualise the correlation. To do this, let’s construct a ggplot.\n\nnilt |&gt; ggplot(aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nAs covered last week, we start with the function ggplot(), and use + at the end of each line to compose our functions for the plot together. In aesthetics aes(), define the respondent’s age rage on the X axis and the respondent’s spouse/partner age spage on the Y axis. As a general convention in quantitative research, the response/dependent variable is visualised on the Y axis and the independent on the X axis. Here we are using the geom_point() function to specify the plot type. In contrast to the usual 3 line minimum pattern for constructing a plot, we have four here, with geom_smooth() added after geom_point(). This adds a straight line which describes the best fit between all the points in the graph.\nFrom the plot above, we see that there is a strong positive correlation between the respondent’s age and their partner’s age. We see that for some individuals their partner’s age is older, whereas others is younger. Also, there are some dots that are far away from the straight line. For example, in one case the respondent is around 60 years old and the age of their partner is around 30 years old (can you find that dot on the plot?). These extreme values are known as outliers.\nWe may also suspect that the respondents’ gender is playing a role in this relationship. We can include this as a third variable in the plot by colouring the dots by the respondents’ sex. To do this, let’s specify the colour argument in aesthetics aes() with a categorical variable rsex.\n\nnilt |&gt; ggplot(aes(x = rage, y = spage, colour = rsex)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"gray20\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nThis time rather than geom_smooth(), we have used geom_abline() to add a line showing where we would expect to see dots if the respondent and their spouse/partner were exactly the same age - the slope and intercept part of this code will become clearer when we cover simple linear regression next week. We observe a clear pattern in which most female participants are on one side of the line and most males on the other. As we can see, most female respondents tend to choose/have partners who are older, whereas males choose/have younger partners.\n\nIn your R Markdown file, use the nilt data object to visualise the relationship of the following variables by creating a new chunk.\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function set the method argument to 'lm'.\nRun the code chunk. What type of relationship do you observe? Comment as regular text (i.e. as text not inside the code chunk) in your R Markdown file the overall result of the plot and whether this is in line with your previous expectation.\n\n\nAs briefly introduced in the last lab, correlations often occur between categorical and numeric data. A good way to observe the relationship between these types of variables is using a box plot. Which essentially shows the distribution of the numeric values by category/group.\nLet’s say we are interested in the relationship between education level and perception of migration. The variable highqual contains the respondent’s highest education qualification. Using ggplot(), we can situate mig_per on the X axis and highqual on the Y axis, and plot it with the geom_boxplot() function. Note that before passing the dataset to ggplot, we can filter out two categories of the variable highqual where education level is unknown (i.e. “Other, level unknown” or “Unclassified”). Also notice how we can do both filter highqual and use the result of that filtering in a ggplot using pipes - |&gt;.\n\nnilt |&gt;\n  filter(highqual != \"Other, level unknown\" & highqual != \"Unclassified\") |&gt;\n  ggplot(aes(x = mig_per, y = highqual)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFrom the plot above, we see that respondents with higher education level (on the bottom) appear to have more positive opinion on migration when compared to respondents with lower education level or no qualifications (on the top). Overall, the data shows a pattern that the lower one’s education level is, the worse their opinion towards migration is likely to be. Since education level is an ordinal variable, we can say this is a positive relationship.\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk.\n\nCreate a boxplot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the political party which the respondent identify with uninatid. Use ggplot() in combination with geom_boxplot(). Make sure to specify mig_per on the Y axis and uninatid on the X axis in aes().\nDo you think the opinion towards migration differs among the groups in the plot? Comment on the overall results in regular text of your R Markodwn document.",
    "crumbs": [
      "**Lab 7** Correlation"
    ]
  },
  {
    "objectID": "07-Lab7.html#measuring-correlation",
    "href": "07-Lab7.html#measuring-correlation",
    "title": "Correlation",
    "section": "",
    "text": "So far we have examined correlation by visualizing variables only. A useful practice in quantitative research is to actually measure the magnitude of the relationship between these variables. One common measure is the Pearson correlation coefficient. This measure results in a number that goes from -1 to 1. A coefficient below 0 implies a negative correlation whereas a coefficient over 0 a positive one. When the coefficient is close to positive one (1) or negative one (-1), it implies that the relationship is strong. By contrast, coefficients close to 0 indicate a weak relationship. This technique is appropriate to measure linear numeric relationships, which is when we have numeric variables with a normal distribution, e.g. age in our dataset.\nLet’s start measuring the relationship between the respondent’s age and their partner’s age. To do this in R, we should use the cor() function. In the R syntax, first we specify the variables separated by a comma. As cor() is a base R function, we need to be explicit when specifying our variable names, using dataframe$column, as shown below. Also, I set the use argument as 'pairwise.complete.obs'. This is because one or both of the variables contain more than one missing value. Therefore, we are telling R to use complete observations only.\n\ncor(nilt$rage, nilt$spage, use = \"pairwise.complete.obs\")\n\n[1] 0.9481297\n\n\nThe correlation coefficient between this variables is 0.95. This is close to positive 1. Therefore, it is a strong positive correlation. The result is completely in line with the plot above, since we saw how the dots were close to the straight line.\nWhat about the relationship between age and mig_per that you plotted earlier?\n\ncor(nilt$rage, nilt$mig_per, use = \"pairwise.complete.obs\")\n\n[1] -0.05680918\n\n\nThe coefficient is very close to 0, which means that the correlation is practically non-existent. The absence of correlation is also interesting in research. For instance, one might expect that younger people would be more open to migration. However, it seems that age does not play a role on people’s opinion about migration in Northern Ireland according to this data.\nLet’s say that we are interested in the correlation between mig_per and all other numeric variables in the dataset. Instead of continuing computing the correlation one by one, we can run a correlation matrix. The code below combines Tidyverse’s select() function, the cor() function we just covered, and a new function round(). It can be read as follows: from the nilt data select these variables, then compute the correlation coefficient using only complete cases, and then round the results to 3 decimals. Notice as we specify our variables using the Tidyverse select() function, we specify our dataframe with dataframe |&gt; function(... and can then name our variables directly without dataframe$....\n\nnilt |&gt;\n  select(mig_per, rage, spage, rhourswk, persinc2) |&gt;\n  cor(use = \"pairwise.complete.obs\") |&gt;\n  round(3)\n\n         mig_per   rage  spage rhourswk persinc2\nmig_per    1.000 -0.057 -0.132    0.082    0.228\nrage      -0.057  1.000  0.948   -0.013   -0.036\nspage     -0.132  0.948  1.000   -0.182   -0.090\nrhourswk   0.082 -0.013 -0.182    1.000    0.383\npersinc2   0.228 -0.036 -0.090    0.383    1.000\n\n\nFrom the result above, we have a correlation matrix that computes the Person correlation coefficient for the selected variables. In the first row we have migration perception. You will notice that the first value is 1.00, this is because it is measuring the correlation against the same variable (i.e. itself). The next value in the first row is age, which is nearly 0. The next variables also result in low coefficients, with the exception of the personal income, where we see a moderate/low positive correlation. This can be interpreted that respondents with high income are associated with more positive opinion towards migration compared to low-income respondents.\n\n\nInsert a new chunk in your R Markdown file;\nUsing the nilt data object, compute a correlation matrix using the following variables: rage, persinc2, mil10yrs, miecono and micultur, setting the use argument to 'pairwise.complete.obs' and rounding the result to 3 decimals;\nRun the chunk individually and comment whether personal income or age is correlated with the perception of migrants in relation to the specific aspects asked in the variables measured (consult page 14 of the NILT Teaching Resources documentation to get a description of these variables);\nKnit your R Markdown document to .html. The output document will automatically be saved in your project.\nDiscuss your previous results with your neighbour or tutor.",
    "crumbs": [
      "**Lab 7** Correlation"
    ]
  },
  {
    "objectID": "Answers.html",
    "href": "Answers.html",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "This chapter presents the suggested R code to answer the workbook activities and exercises throughout the course labs in Quantitative Research Methods for Social Sciences. This covers from Lab 3 to Lab 9.\nBefore looking at the answers, try asking your tutor for help. Also, we strongly recommend web resources, such as https://stackoverflow.com/ or https://community.rstudio.com/. By solving the issues, you will learn a lot! ;)\n\n\n## Load the packages\nlibrary(tidyverse)\n\n# Read the data from the .rds file\nclean_data &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Glimpse clean_data\nglimpse(clean_data)\n\n# Glimpse the nilt data\nglimpse(nilt)\n\n\nPreamble code\n\n## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Subset\nnilt_subset &lt;- nilt |&gt; \n  select(rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nFrom your RStudio Cloud script, do the following activities using the data stored in the nilt_subset object:\n\nCreate a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\n\n\n# Load the vtable package to create summary tables\nlibrary(vtable)\n# Create table\nsumtable(nilt_subset, vars = c(\"uninatid\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nuninatid\n1183\n\n\n\n... Unionist\n348\n29%\n\n\n... Nationalist\n255\n22%\n\n\n... Neither\n580\n49%\n\n\n\n\n\n\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\n\n\nsumtable(nilt_subset, vars = c(\"religcat\"), group = \"uninatid\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nuninatid\n\n\nUnionist\n\n\nNationalist\n\n\nNeither\n\n\n\nVariable\nN\nPercent\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n341\n\n255\n\n558\n\n\n\n... Catholic\n2\n1%\n245\n96%\n238\n43%\n\n\n... Protestant\n305\n89%\n5\n2%\n180\n32%\n\n\n... No religion\n34\n10%\n5\n2%\n140\n25%\n\n\n\n\n\n\nUsing the data in the nilt_subset object, complete the following activities.\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions;\n\n\nhist(nilt_subset$persinc2)\n\n\n\n\n\n\n\n\nCreate a summary of the personal income persinc2 variable, using the sumtable() function.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\npersinc2\n897\n16395\n13466\n260\n6760\n22100\n75000\n\n\n\n\n\nCompute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"), group = \"ruhappy\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nruhappy\n\n\nVery happy\n\n\nFairly happy\n\n\nNot very happy\n\n\nNot at all happy\n\n\nCan't choose\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\n\n\n\npersinc2\n305\n17569\n13429\n500\n16262\n14015\n62\n13445\n9754\n9\n12451\n11501\n15\n11457\n6113\n\n\n\n\n\nPreamble code\n\n## Load the packages\nlibrary(tidyverse)\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Create subset\nnilt_subset &lt;- nilt |&gt;\n  select(rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nUsing the nilt_subset object, complete the tasks below in the Rmd file ‘Lab_4’, which you created earlier. Insert a new chunk for each of these activities and include brief comments as text in the Rmd document to introduce the plots and discuss the results (tip —leave an empty line between your text and the next chunk to separate the description and the plots):\n\nCreate a first-level header to start a section called “Categorical analysis”;\n\n\n## Categorical analysis\n\n\nCreate a simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid;\n\n\nggplot(nilt_subset, aes(x = uninatid)) +\n  geom_bar() +\n  labs(title = \"Political afiliation\", x = \"Party\")\n\n\n\n\n\n\n\n\nBased on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables;\n\n\nggplot(nilt_subset, aes(x = uninatid, fill = religcat)) +\n  geom_bar() +\n  labs(\n    title = \"Political affiliation by religion\",\n    x = \"Party\", fill = \"Religion\"\n  )\n\n\n\n\n\n\n\n\nCreate a new first-level header to start a section called “Numeric analysis”;\n\n\n## Numeric analysis\n\n\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\n\n\nggplot(nilt_subset, aes(x = rhourswk, y = persinc2)) +\n  geom_point() +\n  labs(\n    title = \"Income and number of hours worked a week\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime);\n\n\nggplot(nilt_subset, aes(x = ruhappy, y = persinc2)) +\n  geom_boxplot() +\n  labs(\n    title = \"Personal income and happiness\",\n    x = \"Happiness level\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nBriefly comment each of the plots as text in your Rmd file;\nKnit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish.\n\n\n## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Load the data from the .rds file we created in lab 3\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n# Migration\nnilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric)\n\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) %&gt;%\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text in the Rmd file (remember to leave an empty line between your text and the chunk).\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use the ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function, set the method argument to 'lm'.\n\n\nnilt |&gt; ggplot(aes(x = rage, mig_per)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Perception of migration vs age\",\n    x = \"Respondent age\", y = \"Perception of migration (0-30)\"\n  )\n\n\n\n\n\n\n\n\nWhat type of relationship do you observe? Comment the overall result of the plot and whether this is in line with your previous expectation.\n\n\n## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\n\n\nUse the nilt data set object in your linear_model_intro file to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point(), jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nPrint the summary of m3 using the summary() function.\n\n\nsummary(m3)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43694  -8148  -3070   4990  58249 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5170.4     1966.2    2.63  0.00884 ** \nrhourswk       463.2       52.4    8.84  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13860 on 455 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.1466,    Adjusted R-squared:  0.1447 \nF-statistic: 78.15 on 1 and 455 DF,  p-value: &lt; 2.2e-16\n\n\n\nIs the relationship of hours worked a week significant? Re: Yes. The p-value (fourth column of the ‘Coefficients’ table) is lower than 0.05.\nWhat is the adjusted r-squared? How would you interpret it? Re: the adjusted R-squared is 0.14. This can be interpreted in terms of percentage, e.g. 14% of the variance in personal income can be explained by the number of hours worked a week.\nWhat is the sample size to fit the model? Re: The total number of observations in the data set is 1,204 and the model summary says that 747 observations were deleted due to missingness. Therefore, the sample size is 457 (1204-747).\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\n\n\n5170.4 + 463.2 * 30\n\n[1] 19066.4\n\n\n\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\n\n\nhist(residuals(m3))\n\n\n\n\n\n\n\nOverall, the residuals look normally distributed with the exception of the values to the right-hand side of the plot (between 40000 and 60000).\n\n\n\nLoad the packages, and the data that you will need in your file using the code below:\n\n\n## Load the packages\nlibrary(moderndive)\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\n\n\ntable(nilt$highqual)\n\n\nDegree level or higher       Higher education   GCE A level or equiv \n                   230                    102                    243 \n     GCSE A-C or equiv      GCSE D-G or equiv      No qualifications \n                   185                     82                    281 \n  Other, level unknown           Unclassified \n                    27                     54 \n\n\n\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2, color = highqual)) +\n  geom_point(position = \"jitter\") +\n  moderndive::geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Personal income\",\n    subtitle = \"Personal income and number of hours worked a week by education level\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\",\n    color = \"Highest education level\"\n  )\n\n\n\n\n\n\n\n\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary.\n\n\nm4 &lt;- lm(persinc2 ~ rhourswk + rage + highqual, nilt)\nsummary(m4)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk + rage + highqual, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-36228  -6425  -1411   4635  54749 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    3904.64    2714.25   1.439    0.151    \nrhourswk                        444.79      45.32   9.814  &lt; 2e-16 ***\nrage                            236.59      48.47   4.881 1.47e-06 ***\nhighqualHigher education      -8164.91    1939.50  -4.210 3.09e-05 ***\nhighqualGCE A level or equiv -12439.26    1563.42  -7.956 1.47e-14 ***\nhighqualGCSE A-C or equiv    -13037.47    1703.07  -7.655 1.20e-13 ***\nhighqualGCSE D-G or equiv    -11622.07    2665.38  -4.360 1.61e-05 ***\nhighqualNo qualifications    -12968.10    2339.78  -5.542 5.11e-08 ***\nhighqualOther, level unknown  15445.70    3334.75   4.632 4.76e-06 ***\nhighqualUnclassified         -12399.58    2786.16  -4.450 1.08e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11830 on 447 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.3887,    Adjusted R-squared:  0.3764 \nF-statistic: 31.58 on 9 and 447 DF,  p-value: &lt; 2.2e-16\n\n\n\nComment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model. Re: All the independent variables including the number of hours worked a week, age, and all the categories of highest qualification level compared to ‘Degree or higher’ are significant to predict personal income in the model ‘m4’, considering that the p-value is lower than 0.05. We can confirm this from the fourth column of the ‘Coefficients’ table. The adjusted R-squared of the model is 0.37. This means that 37.6% of the variance in personal income can be explained by these variables. The size of the sample used to fit this model is 457, considering that the ‘nilt’ data set contains 1204 observations but 747 were deleted due to missingness (1204 - 747).\nPlot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model?\n\n\nhist(residuals(m4))\n\n\n\n\n\n\n\nThe distribution of the residuals in ‘m4’ look overall normally distributed. However, the distribution is not perfectly symmetric. Therefore, we would advice to conduct further checks to test the linear model assumptions.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#introduction",
    "href": "Answers.html#introduction",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "This chapter presents the suggested R code to answer the workbook activities and exercises throughout the course labs in Quantitative Research Methods for Social Sciences. This covers from Lab 3 to Lab 9.\nBefore looking at the answers, try asking your tutor for help. Also, we strongly recommend web resources, such as https://stackoverflow.com/ or https://community.rstudio.com/. By solving the issues, you will learn a lot! ;)",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-3.-data-wrangling",
    "href": "Answers.html#lab-3.-data-wrangling",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "## Load the packages\nlibrary(tidyverse)\n\n# Read the data from the .rds file\nclean_data &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Glimpse clean_data\nglimpse(clean_data)\n\n# Glimpse the nilt data\nglimpse(nilt)",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-4.-exploratory-data-analysis",
    "href": "Answers.html#lab-4.-exploratory-data-analysis",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "Preamble code\n\n## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Subset\nnilt_subset &lt;- nilt |&gt; \n  select(rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nFrom your RStudio Cloud script, do the following activities using the data stored in the nilt_subset object:\n\nCreate a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\n\n\n# Load the vtable package to create summary tables\nlibrary(vtable)\n# Create table\nsumtable(nilt_subset, vars = c(\"uninatid\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nuninatid\n1183\n\n\n\n... Unionist\n348\n29%\n\n\n... Nationalist\n255\n22%\n\n\n... Neither\n580\n49%\n\n\n\n\n\n\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\n\n\nsumtable(nilt_subset, vars = c(\"religcat\"), group = \"uninatid\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nuninatid\n\n\nUnionist\n\n\nNationalist\n\n\nNeither\n\n\n\nVariable\nN\nPercent\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n341\n\n255\n\n558\n\n\n\n... Catholic\n2\n1%\n245\n96%\n238\n43%\n\n\n... Protestant\n305\n89%\n5\n2%\n180\n32%\n\n\n... No religion\n34\n10%\n5\n2%\n140\n25%\n\n\n\n\n\n\nUsing the data in the nilt_subset object, complete the following activities.\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions;\n\n\nhist(nilt_subset$persinc2)\n\n\n\n\n\n\n\n\nCreate a summary of the personal income persinc2 variable, using the sumtable() function.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\npersinc2\n897\n16395\n13466\n260\n6760\n22100\n75000\n\n\n\n\n\nCompute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy.\n\n\nsumtable(nilt_subset, vars = c(\"persinc2\"), group = \"ruhappy\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nruhappy\n\n\nVery happy\n\n\nFairly happy\n\n\nNot very happy\n\n\nNot at all happy\n\n\nCan't choose\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\nN\nMean\nSD\n\n\n\npersinc2\n305\n17569\n13429\n500\n16262\n14015\n62\n13445\n9754\n9\n12451\n11501\n15\n11457\n6113",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-6.-visual-exploratory-analysis",
    "href": "Answers.html#lab-6.-visual-exploratory-analysis",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "Preamble code\n\n## Load the packages\nlibrary(tidyverse)\n\n# Load the data from the .rds file we created in the last lab\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n# Create subset\nnilt_subset &lt;- nilt |&gt;\n  select(rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\n\nUsing the nilt_subset object, complete the tasks below in the Rmd file ‘Lab_4’, which you created earlier. Insert a new chunk for each of these activities and include brief comments as text in the Rmd document to introduce the plots and discuss the results (tip —leave an empty line between your text and the next chunk to separate the description and the plots):\n\nCreate a first-level header to start a section called “Categorical analysis”;\n\n\n## Categorical analysis\n\n\nCreate a simple bar plot using the geom_bar() geometry to visualize the political affiliation reported by the respondents using the variable uninatid;\n\n\nggplot(nilt_subset, aes(x = uninatid)) +\n  geom_bar() +\n  labs(title = \"Political afiliation\", x = \"Party\")\n\n\n\n\n\n\n\n\nBased on the plot above, create a ‘stacked bar plot’ to visualize the political affiliation by religion, using the uninatid and religcat variables;\n\n\nggplot(nilt_subset, aes(x = uninatid, fill = religcat)) +\n  geom_bar() +\n  labs(\n    title = \"Political affiliation by religion\",\n    x = \"Party\", fill = \"Religion\"\n  )\n\n\n\n\n\n\n\n\nCreate a new first-level header to start a section called “Numeric analysis”;\n\n\n## Numeric analysis\n\n\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\n\n\nggplot(nilt_subset, aes(x = rhourswk, y = persinc2)) +\n  geom_point() +\n  labs(\n    title = \"Income and number of hours worked a week\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis… Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results on Zoom (live) or your Lab Group on Teams (online anytime);\n\n\nggplot(nilt_subset, aes(x = ruhappy, y = persinc2)) +\n  geom_boxplot() +\n  labs(\n    title = \"Personal income and happiness\",\n    x = \"Happiness level\", y = \"Personal income (£ a year)\"\n  )\n\n\n\n\n\n\n\n\nBriefly comment each of the plots as text in your Rmd file;\nKnit the .Rmd document as HTML or PDF. The knitted file will be saved automatically in your project. You can come back to the Rmd file to make changes if needed and knit it again as many times as you wish.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-7.-correlation",
    "href": "Answers.html#lab-7.-correlation",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "## Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n# Load the data from the .rds file we created in lab 3\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\n# Age of respondent’s spouse/partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n# Migration\nnilt &lt;- mutate_at(nilt, vars(mil10yrs, miecono, micultur), as.numeric)\n\n\n# overall perception towards migrants\nnilt &lt;- rowwise(nilt) %&gt;%\n  # sum values\n  mutate(mig_per = sum(mil10yrs, miecono, micultur, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  # assign NA to values that sum 0\n  mutate(mig_per = na_if(mig_per, 0))\n\n\nUsing the nilt data object, visualize the relationship of the following variables by creating a new chunk. Run the chunk individually and comment on what you observe from the result as text in the Rmd file (remember to leave an empty line between your text and the chunk).\n\nCreate a scatter plot to visualize the correlation between the respondent’s overall opinion in relation to migration mig_per and the respondent’s age rage. Remember that we just created the mig_per variable by summing three variables which were in a 0-10 scale (the higher the value, the better the person’s perception is). In aes(), specify rage on the X axis and mig_per on the Y axis. Use the ggplot() function and geom_point(). Also, include a straight line describing the points using the geom_smooth() function. Within this function, set the method argument to 'lm'.\n\n\nnilt |&gt; ggplot(aes(x = rage, mig_per)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Perception of migration vs age\",\n    x = \"Respondent age\", y = \"Perception of migration (0-30)\"\n  )\n\n\n\n\n\n\n\n\nWhat type of relationship do you observe? Comment the overall result of the plot and whether this is in line with your previous expectation.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-8.-linear-model.-simple-linear-regression",
    "href": "Answers.html#lab-8.-linear-model.-simple-linear-regression",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "## Load the packages\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\n\n\nUse the nilt data set object in your linear_model_intro file to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point(), jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nPrint the summary of m3 using the summary() function.\n\n\nsummary(m3)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43694  -8148  -3070   4990  58249 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5170.4     1966.2    2.63  0.00884 ** \nrhourswk       463.2       52.4    8.84  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13860 on 455 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.1466,    Adjusted R-squared:  0.1447 \nF-statistic: 78.15 on 1 and 455 DF,  p-value: &lt; 2.2e-16\n\n\n\nIs the relationship of hours worked a week significant? Re: Yes. The p-value (fourth column of the ‘Coefficients’ table) is lower than 0.05.\nWhat is the adjusted r-squared? How would you interpret it? Re: the adjusted R-squared is 0.14. This can be interpreted in terms of percentage, e.g. 14% of the variance in personal income can be explained by the number of hours worked a week.\nWhat is the sample size to fit the model? Re: The total number of observations in the data set is 1,204 and the model summary says that 747 observations were deleted due to missingness. Therefore, the sample size is 457 (1204-747).\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\n\n\n5170.4 + 463.2 * 30\n\n[1] 19066.4\n\n\n\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\n\n\nhist(residuals(m3))\n\n\n\n\n\n\n\nOverall, the residuals look normally distributed with the exception of the values to the right-hand side of the plot (between 40000 and 60000).",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "Answers.html#lab-9.-multivariate-linear-model",
    "href": "Answers.html#lab-9.-multivariate-linear-model",
    "title": "Workbook suggested answers",
    "section": "",
    "text": "Load the packages, and the data that you will need in your file using the code below:\n\n\n## Load the packages\nlibrary(moderndive)\nlibrary(tidyverse)\n# Read the data from the .rds file\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\n\n\ntable(nilt$highqual)\n\n\nDegree level or higher       Higher education   GCE A level or equiv \n                   230                    102                    243 \n     GCSE A-C or equiv      GCSE D-G or equiv      No qualifications \n                   185                     82                    281 \n  Other, level unknown           Unclassified \n                    27                     54 \n\n\n\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\n\n\nggplot(nilt, aes(x = rhourswk, y = persinc2, color = highqual)) +\n  geom_point(position = \"jitter\") +\n  moderndive::geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Personal income\",\n    subtitle = \"Personal income and number of hours worked a week by education level\",\n    x = \"Number of hours worked a week\", y = \"Personal income (£ a year)\",\n    color = \"Highest education level\"\n  )\n\n\n\n\n\n\n\n\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m4 and print the summary.\n\n\nm4 &lt;- lm(persinc2 ~ rhourswk + rage + highqual, nilt)\nsummary(m4)\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk + rage + highqual, data = nilt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-36228  -6425  -1411   4635  54749 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    3904.64    2714.25   1.439    0.151    \nrhourswk                        444.79      45.32   9.814  &lt; 2e-16 ***\nrage                            236.59      48.47   4.881 1.47e-06 ***\nhighqualHigher education      -8164.91    1939.50  -4.210 3.09e-05 ***\nhighqualGCE A level or equiv -12439.26    1563.42  -7.956 1.47e-14 ***\nhighqualGCSE A-C or equiv    -13037.47    1703.07  -7.655 1.20e-13 ***\nhighqualGCSE D-G or equiv    -11622.07    2665.38  -4.360 1.61e-05 ***\nhighqualNo qualifications    -12968.10    2339.78  -5.542 5.11e-08 ***\nhighqualOther, level unknown  15445.70    3334.75   4.632 4.76e-06 ***\nhighqualUnclassified         -12399.58    2786.16  -4.450 1.08e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11830 on 447 degrees of freedom\n  (747 observations deleted due to missingness)\nMultiple R-squared:  0.3887,    Adjusted R-squared:  0.3764 \nF-statistic: 31.58 on 9 and 447 DF,  p-value: &lt; 2.2e-16\n\n\n\nComment on the results of the model by mentioning which of the variables is significant and their respective p-value, the adjusted r-squared of the model, and the number of observations used to fit the model. Re: All the independent variables including the number of hours worked a week, age, and all the categories of highest qualification level compared to ‘Degree or higher’ are significant to predict personal income in the model ‘m4’, considering that the p-value is lower than 0.05. We can confirm this from the fourth column of the ‘Coefficients’ table. The adjusted R-squared of the model is 0.37. This means that 37.6% of the variance in personal income can be explained by these variables. The size of the sample used to fit this model is 457, considering that the ‘nilt’ data set contains 1204 observations but 747 were deleted due to missingness (1204 - 747).\nPlot a histogram of the residuals for model m4. Do they look normally distributed? Can we trust our estimates or would you advise to carry out further actions to verify the adequate interpretation of this model?\n\n\nhist(residuals(m4))\n\n\n\n\n\n\n\nThe distribution of the residuals in ‘m4’ look overall normally distributed. However, the distribution is not perfectly symmetric. Therefore, we would advice to conduct further checks to test the linear model assumptions.",
    "crumbs": [
      "Workbook suggested answers"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "03-Lab3.html",
    "href": "03-Lab3.html",
    "title": "Data wrangling",
    "section": "",
    "text": "Welcome to Lab 3!\nIn our previous session we learned about R packages, including how to install and load them. We talked about the main types of data used in social science research and how to represent them in R. We also played around with some datasets using some key functions, such as: filter(), select(), and mutate(). In this session, we will build on these principles and learn how to import data in R, as well as clean and format the data using a real-world dataset. This is a common and important phase in quantitative research.\n\n\n\n\n\n\nNoteOverview\n\n\n\nBy the end of this lab you will know how to:\n\nset up an RStudio project file from scratch\ncreate an R script to download, wrangle, and save a dataset\nload a saved dataset within an R Markdown file\nwork with the dataset used in the formative and summative assessments\n\n\n\n\nToday, we will be working with data generated by the Access Research Knowledge (ARK) hub. ARK conducts a series of surveys about society and life in Northern Ireland. For this lab, we will be working with the results of the Northern Ireland Life and Times Survey (NILT) in the year 2012. In particular, we will be using a teaching dataset that focuses on community relations and political attitudes. This includes background information of the participants and their household.\n\nWe will continue using Posit Cloud, as we did in the previous labs. This time though we are going to make a new RStudio project from scratch.\nWithin your “Lab Group …” workspace in Posit Cloud (if you have not joined a shared space, follow the instructions in Lab 2) and:\n\nClick the blue ‘New Project’ button in the top-right.\nFrom the list of options select ‘New RStudio Project’.\n\n\nOnce the project has loaded, click on ‘Untitled Project’ at the top of the screen.\n\nYou can now give your project a name. This is how it will appear in your list of projects in your lab group workspace. Let’s name it “NILT” as we will use this project to work with the NILT data. Type the name and hit Enter to confirm.\n\n\nAlthough this project is just for the labs, we are going to set it up in a ‘reproducible’ way. This means anyone with a copy of our project would be able to run the code and receive the same results as us. Last week, we downloaded a data set using the Console. This week we will instead cover one way to include the code used to download and wrangle our data in a file separate to the R Markdown files we use for the analysis.1\n(As you will see with the RStudio templates used for the assessments, it is possible to share an RStudio Project with the data. However, that is not always possible or desirable. For instance, the data whilst available for anyone to download from an organisation’s website or data archive may have license restrictions prohibiting making a version of it available elsewhere.)\nTo do this we will use an R script file. In our first two labs we used R Markdown, which lets us create code chunks for adding our R code. An R script file is simply a file that contains only R code, like a giant code chunk.\nBy convention, R scripts for doing set up, data prep, and similar are placed in an ‘R’ subfolder. To create one:\n\nIn the “Files” pane (bottom-right) click the folder icon that also has a green circle with white plus in it.\n\n\nThen, within the ‘New Folder’ dialog that will pop up:\n\nType a single letter capital ‘R’ as the name.\nClick the ‘OK’ button to confirm creation of the folder.\n\n\nYou should now see the new folder in the bottom-right panel, so -\n\nClick the ‘R’ folder to navigate into it (Note, you need to click the ‘R’ and not the folder icon.)\n\n\nYou should now see an empty folder, and can double-check you are in the right folder by looking at the navigation bar which should be “Cloud &gt; project &gt; R”.\n\nOK, now:\n\nClick the white document icon that is to the right of the new folder icon you clicked before. (See screenshot below!)\nFrom the list of options click ‘R Script’\n\n\nThat will bring up a ‘Create a New File in Current Directory’ dialog:\n\nName your file 01_prep_nilt_2012.R\n\nClick the ‘OK’ button to confirm creation of the script\n\n\nThe file should then auto open in the top-left pane.\n\nLastly, before we move to writing our R script, let’s navigate back to the top-level folder of our project in the ‘Files’ pane (bottom-right). To navigate back up to the top-level folder, click either the ‘..’ at the top of the folder content list or in the navigation bar click the ‘project’ text.\n\n\nBack to our top-left pane with our R script. It is good practice to include some key information in this file, such as what it does, when it was written, and who wrote it. Given R script is all R code, we can add this info using comments. As a reminder, any line in R that begins with a ‘#’ is treated as a comment and will not be run as part of the code.\nBelow is a template you can use.\n\n## Download and process NILT 2012 survey\n\n# Date: [add today's date here]\n# Author: [add your name here]\n\nThe ‘##’, with an extra ‘#’, at the top doesn’t do anything extra or different, we just include it to make visually clear this is the title / short description of what the script does.\nNext, it is good practice to include details for the data set being used, especially important if the data is not our own to ensure proper attribution. In addition to being good practice, it is often a condition for data sets where it is permitted to re-share them elsewhere that you can do so only as long as you include an attribution (i.e. a reference).\nLet’s then add a reference section. (Feel free to use the clipboard button in top-right of the box below to copy and paste the text.)\n\n# References  -----------------------------------------------------------------\n\n# https://www.ark.ac.uk/teaching/\n# 2012 Good relations: ARK. Northern Ireland Life and Times Survey Teaching Dataset 2012, Good relations [computer file]. ARK www.ark.ac.uk/nilt [distributor], March 2014.\n\nThe ----- after # References again doesn’t do anything aside from help make the section visually distinct. This may seem odd at the moment, but as the file grows in length, the importance of making it easy to find sections quickly from visual glance starts to make sense.\nOK, let’s now install packages that are needed for our project. Remember we need to do this every time we create a new project on Posit Cloud. If using RStudio Desktop, we would only need to install packages once per device we are using rather than per project.\nHowever, compared to last week, we want to set up our project so that anyone can run the code and achieve the same results. (If we run into irrepairable errors we may also want to re-run our R scipt to ‘reset’ our project). We will not know though what packages someone will already have installed or not.\nThankfully, we can write R code in our script that will check whether specific packages are already installed, and then install any packages needed that are missing.\nFirst, add a comment to make clear there is a new section:\n\n# Packages -------------------------------------------------------------------\n\nNext, add the following comment and code:\n\n# Install tidyverse if needed\nif (!\"tidyverse\" %in% rownames(installed.packages())) install.packages(\"tidyverse\")\n\nThis code may look complex, but it is largely the number of brackets making it looks more complex than it actually is:\n\n\nif checks whether a given object is TRUE or FALSE, and is used in format if x y, where if x returns TRUE then y code is run.\n\n(...) we use to enclose a chunk of code to make clear this is our x.\n\n!\"tidyverse\" %in% uses the NOT ! logical operator and the %in% operator, which is equivalent then of saying “If the word ‘tidyverse’ is not in…”\n\nrownames() gets the label names for rows in a data frame or matrix (a matrix is like a simpler data frame).\n\ninstalled.packages() (note installed) is a function that returns a matrix with all packages currently installed on the system, so within Posit Cloud it returns all the installed packages within the current project. Each row’s label name matches the installed package name.\n\ninstall.packages(\"tidyverse\") as we know already from last week, this code installs the tidyverse package.\n\nPutting it all today, we have if the tidyverse is not in our currently installed packages then install the tidyverse. Logically then, if the tidyverse is installed our if check will return FALSE and the code to install the tidyverse will not be run. That way we - and anyone we share the project with - avoid installing and re-installing the tidyverse needlessly when the script is run. (Also, whilst anyone we would share our project with should only need to run this file once, they can also run it again to fix any issues resulting from things like accidentally deleting or making irreversible changes to the data.)\nWe can test it by putting the text cursor on the line with the code and pressing Ctrl+Enter, or click the Run button at the top of the file.\n\nYou should see the tidyverse installing in the Console. Once it has finished installing, try running the same line of code again.\n\nAs can see, the code will be sent to the Console but the tidyverse is not installed again. As the if now returns FALSE as the tidyverse is in the names of our installed packages, then the code to install it is not run this time.\nNow, we need to load the packages we will be using. As you can probably guess the tidyverse is one, and we will be adding a new one as well haven. Add the following lines to your script.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(haven)\n\nUse your mouse to select both the library(...) lines and press Ctrl+Enter or click the run button to load the tidyverse and haven.\nWhy did we not need to install haven first? It is one of the additional tidyverse packages for importing data. On the main tidyverse packages page, there is an explanation of all the packages that you have access to when installing the tidyverse. The ‘core tidyverse’ set of packages are the ones that get loaded with library(tidyverse). These are the ones that most data analysis will likely use. The tidyverse though also comes with additional packages that you might not need to use as often and these we library() load on an as needed basis.\nThe haven package is used for loading data saved in file formats from three of the proprietary software tools that used to dominate quantitative analysis before R - SAS, SPSS, and Stata. The tidyverse core package for loading data, readr, works with comma-separated value (CSV) and tab-separated value files (TSV). All this means is either a comma or tab is used to separate values, such as “value1, value2, …” with new lines used for each row of values. As the NILT 2012 data we are interested in is made available as either an SPSS or Stata file, and not CSV, we will need to use the haven package.\n\nOK, let’s actually download the NILT data we will be using across the labs.\nStart by using a comment to make clear in your R script that we are starting a new section:\n\n### Download and read data ----------------------------------------------------\n\nNext, we will create a folder to store the data. Last week, we used dir.create() in the Console to create a folder for our data (‘dir’ is merely short for directory, another name used for folders). As we are setting up our project in a reproducible way we will want to create a folder this way in our script. This way R automatically creates the folder for anyone running the code, rather than them having to do it manually. It also then ensures the data is downloaded to and read in from the location the rest of our R code will assume.\nIn your script, add the code to create a new folder called ‘data’:\n\n# create folder\ndir.create(\"data\")\n\nAgain, with your text cursor on the dir.create(...) line of code, press Ctrl+Enter or click the run button. You should see the data folder appear in the files pane (bottom-right). (Though, if not, check whether you are still in the R sub-folder, and if so navigate back up to the parent directory.)\nWe now need the code to download the file with the NILT data set. We can do this by using another function we used last week, the download.file() function. The arguments for this are download.file(\"URL\", \"folder/filename.type\"). So, we need a URL for where the file is available online and we need to then specify where and with what name we want to download it.\nAdd the following then next within your R script. As this involves a URL, it is OK to copy and paste this code rather than typing it out manually.\n\n# Download NILT 2012\ndownload.file(\n  \"https://www.ark.ac.uk/teaching/NILT2012GR.sav\",\n  \"data/nilt2012.sav\"\n)\n\nAgain select the code and run it.\nTake a look in the ‘Files’ tab in bottom-right, you should now have a folder called ‘data’, click on it, and you will see the nilt2012.sav file.\n\nRemember within the Files tab to navigate back to the main project folder to also click project in the nav-bar or the .. at the top of the list of files - so, given we only have one file, right-above the nilt2012.sav.\nThe .sav is the SPSS file format. We will need to use a function made available to us by haven to read it into R, read_sav().\nLet’s use it to read in the .sav file and assign it to an object called nilt.\n\n# Read NILT\nnilt &lt;- read_sav(\"data/nilt2012.sav\")\n\nOnce more, put your text cursor on the line that reads in the data and run it.\nAnd that’s it! You should see a new data object in your ‘Environment’ tab (Pane 2 - top-right) ready to be used. If you recall last week, we read in the police_killings.csv file using read_csv(). Despite the NILT data being stored in the SPSS file-type, once read in, we can see it and interact with it the same as our data last week.\n\nYou can also see that this contains 1204 observations (rows) and 133 variables (columns). If you click on it, you can open it up in a new tab like an Excel sheet.\n\n(You can get back to your R script by either (1) clicking the small ‘x’ to close the tab showing the nilt data frame or (2) clicking on the tab for the R script.)\nWhilst that gives us a visual interactive view of our data frame. You will notice scrolling along the columns that variables like rsex has values of 1 and 2 rather than showing the category labels. To explore this in more detail we need to switch back to code. Also, as you build up your R skills you will find it much quicker - and more intuitive - to start exploring your data using code rather than opening it up in visual views like this.\nLet’s glimpse() our data frame to see the types of variables included and understand why we do not see our category labels.\nAs we are now moving from code we are writing so anyone with our script can setup their own R project the same as our own to taking a quick look at our data, do not add glimpse() to your R script. Instead, type the following in the Console.\n\nglimpse(nilt)\n\n(Being able to move from the source pane (top-left) to the Console (bottom-left) to switch between ‘writing code we want to save in our file’ and ‘writing code to quickly check our data’ is a benefit of RStudio. Whilst it may have looked intimidating at first with, all of its panes and tabs, you can hopefully now see how this supports quickly moving between different tasks.)\n\nAs you can see from the result of glimpse(), the class for practically all the variables is &lt;dbl+lbl&gt; (which if recall from last week, stands for double - i.e. numeric - and label).\nWhat does this mean? This happened because usually datasets use numbers to represent each of the categories/levels in categorical variables. These numbers are labelled with their respective meaning. This is why we have a combination of value types (&lt;dbl+lbl&gt;).\nLook at rsex in the glimpse() output, as you can see from the values displayed this includes numbers only, e.g. 1,1,2,2.... This is because ‘1’ represents ‘Male’ respondents and ‘2’ represents ‘Female’ respondents in the NILT dataset (n.b. the authors of this lab workbook recognise that sex and gender are different concepts, and we acknowledge this tension and that it will be problematic to imply or define gender identities as binary, as with any dataset. More recent surveys normally approach this in a more inclusive way by offering self-describe options). You can check the pre-defined parameters of the variable in NILT in the documentation or running print_labels(nilt$rsex) in your Console, which returns the numeric value (i.e. the code) and its respective label. As with rsex, this is the case for many other variables in this data set.\n\nYou should be aware that this type of ‘mix’ variable is a special case since we imported a file from a SPSS file that saves metadata for each variable (containing the names of the categories). The read_sav() function let us read this in as a data frame so we can use it in R, preserving the metadata as well. However, as with any data we load in, we still need to clean and ‘wrangle’ it to make it useable.\nAs you learned in the last lab, in R we treat categorical variables as factor. Therefore, we will coerce some variables as factor. This time we will use the function as_factor() instead of the simple factor() that we used before. This is because as_factor() allows us to keep the existing category labels in the variables - in other words, it will use the codes (&lt;dbl&gt;) and category labels (&lt;lbl&gt;) already in the data to turn it into a factor variable. The syntax is exactly the same as before.\nBack to our R script then. First, use a comment to make clear we are starting a new section:\n\n# Coerce variables   ---------------------------------------------------------\n\nNext add and run the following in your script:\n\n# Gender of the respondent\nnilt &lt;- nilt |&gt; mutate(rsex = as_factor(rsex))\n# Highest Educational qualification\nnilt &lt;- nilt |&gt; mutate(highqual = as_factor(highqual))\n# Religion\nnilt &lt;- nilt |&gt; mutate(religcat = as_factor(religcat))\n# Political identification\nnilt &lt;- nilt |&gt; mutate(uninatid = as_factor(uninatid))\n# Happiness\nnilt &lt;- nilt |&gt; mutate(ruhappy = as_factor(ruhappy))\n\nNotice from the code above that we assign the result of mutate() back to our nilt data frame, the nilt &lt;- ... part of the code. This replaces our ‘old’ data frame with a version with the mutated variables that are of type factor.\nBack in the Console again, run glimpse(nilt) once more and scroll up to see the line for rsex in the output.\n\nR has taken the codes (the &lt;dbl&gt; values) and mapped them to the labels (the &lt;lbl&gt; values), whereby we can now more easily see and work with our categories.\nLast week, with the police killings data we could simply use factor() on our variables as the values were saved in the file as category labels without any codes. The data for it was stored in a CSV file, that does not store metadata mapping codes and category labels. As a way around that limitation, some data made available via CSV files will store categorical variable values as labels only, with no codes. Running factor() in such cases creates codes for each unique label. The only danger with that, and something to keep eye out for, is any typos in the data, such as Scotland and Scoltand, as they are ‘unique’ labels would result in these being treated as two different categories after running factor().\nIf we read in our data from a file type that stored the values as codes, but did not include the metadata for category labels, we would instead need to manually add this mapping, along the lines of:\n\nsurvey &lt;- survey |&gt;\n  mutate(\n    country = factor(\n      country,\n      levels = c(1, 2),\n      labels = c(\"Scotland\", \"England\")\n    )\n  )\n\nBack to our NILT data again - What about the numeric variables? In the NILT documentation (covered in later section) there is a table in which you will see a type of measure ‘scale’. This usually refers to continuous numeric variables (e.g. age or income).2 Let’s coerce some variables to the appropriate numeric type.\nIn the previous operation we coerced the variables as factor one by one, but as covered at the end of last week’s lab we can also transform several variables at once within a single mutate function.\nAdd and run the following code in your script:\n\n# Coerce several variables as numeric\nnilt &lt;- nilt |&gt;\n  mutate(\n    rage = as.numeric(rage),\n    rhourswk = as.numeric(rhourswk),\n    persinc2 = as.numeric(persinc2),\n  )\n\nAnother thing we need to do is drop any unused levels (or categories) in our dataset using the function droplevels(), by adding and running the following in our script:\n\n# drop unused levels\nnilt &lt;- droplevels(nilt)\n\nThis function is useful to remove some categories that are not being used in the dataset (e.g. categories that have 0 observations).\nFinally, as we now have our data and done some wrangling, we do not want to have to repeat it each time we load the data in our R Markdown files. So, let’s save it as an .rds file - R’s own file-type.\nThis will save us time in future labs, as we will be able to read in an already formatted version of the NILT in future labs.\nAdd a comment then to make clear we have a new section:\n\n# Save data ------------------------------------------------------------------\n\nThen add and run the code to save our nilt data frame as an RDS file.\n\n# save as rds\nsaveRDS(nilt, \"data/nilt_r_object.rds\")\n\nLastly, as this script is merely to download, prep, and save a cleaned version of our data, it is good practice to end by clearing the global environment. This removes all loaded objects and puts it can into a clear state. We do this for a few reasons, including (1) if we did more complex setup operations the global environment could be filled with a lot of objects we no longer need, and (2) whilst we will continue naming the object containing our data frame as ‘nilt’ in the labs, we would not want to enforce this on anyone we share the project with.\nAdd and run then the following at the end of your R script:\n\n# clean global environment\nrm(list=ls())\n\nYou should now see your Global Environment is empty:\n\nNote, whilst this removes objects, like our nilt data frame, it does not remove any loaded libraries, so the functions provided by the tidyverse and haven remain available to us after running this line of code.\n\nPlease take 5-10 minutes to read the documentation for the dataset. Such documentation is always important to read as it will usually cover the research design, sampling, and information on the variables. For instance, page 7 onwards has the codebook with columns listing variable name, variable label, values, and measure. Page 6 of the documentation provides a “Navigating the codebook (example section)” overview for how to understand the table.\nIt is also worthwhile taking a look through the questionnaires for the survey as well. You will have to regularly consult the technical document and questionnaires to understand and use the data in the NILT data set. So, I recommend you to bookmark the links / save copies of the PDF files.\nThis NILT teaching dataset is also what you will be using for the research report assignment in this course (smart, isn’t it?) - so it’s worth investing the time to learn how to work with this data through the next few labs, as part of the preparation and practice for your assignment.\n(Aside: Best practice would be to read through key info in the documentation first before downloading and coercing variables - we have done the inverse today solely to ensure you have adequate time within the lab itself to download and prepare the data.)\n\nLet’s first go through the codebook in more detail. This will help you understand how to read the information in it, and some common ‘gotchas’ to look out for.\nThe “Variable Name” is how the variables are named in the data set - except in all caps, whereas once we start working with the data they will be lowercase. If we read the data set into a data frame named nilt, the rage variable name would be accessed as nilt$rage. For consistency with how they will appear when working with them in R, we will stick to using all lower case in the lab workbook.\nThe “Label” column mostly shows the corresponding text used in the survey for the variable. For rage it is “Q1. Household grid: Age of respondent”. If you look at page 2 of the main questionnaire, you will see that the ‘household grid’ was a repeat set of questions used to gather key information about each member of the household. The r in rage stands for “respondent”, i.e. the person who answered the survey questions.\nSome variable names, such as work and tunionsa on page 9, note under the name “(compute)”. This means the variable rather than representing a direct answer to a survey question is instead ‘computed’ / ‘derived’ from answers to other questions. The variable label for these then detail which questions were used to compute these variables. Work status may seem an odd variable to compute rather than ask directly.\n\nHowever, if we look at work, we can see from page 39 in the main questionnaire, a series of questions were asked to ascertain employment status and economic activity. Given the diversity of situations people can be in, it is more practical to ask a series of yes/no and other questions and then use these to calculate categories for other variables.\n\nWhilst it may look more complex having all these questions on paper, it is simpler for respondents. Let’s imagine being a 32 year old respondent who answers ‘No’ to Q8, whether taken part in paid work in last seven days, and then ‘Yes’ to Q9a, whether taking part in a government scheme for employment training.\n\nFrom these two answers we can compute the respondent is ‘Government training scheme’ for empst, ‘Not in paid employment’ for empst2, and ‘Economically active’ for econact. Note as well that Q9a was only asked based on whether the participant would be eligible for the government training scheme, calculated using age and gender. (Highlighted in blue above.)\n\nImportantly, we gathered this information by asking simple yes/no questions, structured the questions in way to only ask the minimum number of questions relevant for the respondent, and avoided needing to explain jargon such as ‘economically active’. Economically active means someone is either in employment (including waiting to start a job they have been offered and accepted) or unemployed but looking for work and if offered could start a job within the next two weeks. Imagine now asking a respondent whether they are economically active and then having to explain what that means and figuring out whether they are or not. The way the survey questions are structured captures this complexity - and more - and all without ever having to provide a definition of economically active.\nThe tunionsa variable is also a good example of we why cannot assume what a variable measures by its name alone. As can see bottom of page 9 in the documentation, tunionsa is derived from Q22 and Q22a.\n\nHowever, if we look at those two questions on page 43 of the questionniare - screenshot below to save having to switch tabs - we can see Q22 asks the respondent whether they are currently a member of a trade union or staff association and Q22a is whether they have ever been a member.\n\nIf a respondent answered ‘Yes, trade union’ or ‘Yes, staff association’ for either Q22 or Q22a the value computed for tunionsa is ‘Yes’. Whilst tunionsa then is broadly a measure of trade union membership it would be more accurate to say it is a measure of whether the respondent currently is or ever was a member of a trade union or staff association.\nDo not worry if you could not tell that from just the codebook. Normally, dataset documentation should also include a separate section with information on how each computed variable was derived. I was only able to confirm this was how tunionsa was computed by checking the values with those for Q22 and Q22a in the main data set. Expectations of what documentation for data sets should include are continuing to improve over time. However, whilst you should encounter such situations less often with newer data sets, it can still remain unclear - where you may need to do your own additional checks, or contact the original researchers to clarify.\nThis though does show the importance of not assuming what a variable measures by just its name and label. If we mistook tunionsa as measuring current membership only, we may then later be surprised seeing the high number of people currently unemployed with ‘Yes’ for tunionsa. It is not surprising to see that though when we understand that ‘Yes’ includes those who have ever been a member.\n\nThe ‘Measure’ column in the codebook tells us whether the variable is ‘scale’, ‘nominal’, or ‘ordinal’. This is based on how SPSS, the proprietary statistical analysis program that was used by the NILT project, stores variables. Within R, these correspond respectively with numeric, (unordered) factor, and ordered factor variables that we covered last week. As we covered earlier in the lab, the tidyverse also provides us with tools to work with data created in SPSS.\nThe ‘Values’ column then tells us what the numbers represent. For numeric variables this is the unit being measured, such as “years” and “number of people”. This may seem ‘obvious’ for some variables, but good documentation should still always provide this information. The importance of this becomes clearer if we look at livearea. Without it clarifying “Numeric (years)” someone could mistake it as representing number of months instead. Similarly, the “0 Less than 1 year” is an important clarifier, helping us understand that anything less than 1 year was recorded as 0. In other words, if someone had lived in the area for 7 months, it was still recorded as 0 rather than rounded up to 1.\n\nFor the categorical variables it then provides the code and label for the categories. For example, with rsex the categories are 1 (code) Male (label) and 2 (code) Female (label). As mentioned last week, the ‘raw’ data is often stored in numeric codes as - among other reasons - it is more efficient to store it this way. Using the codebook, someone looking at the raw data would then know that a value of 1 represents Male. And as we covered above, knowing the code and label means we can set up the data in R to show the labels instead of the raw codes. (And if we were working with data in a file with only the codes and not the labels, we can use the codebook to write our own code to map codes and labels.)\n\nYou may have spotted though that the Values column for some variables also includes additional grey text, “-9 Non Applicable”, “-99 Don’t Know”, and “-999 Not answered/refused”. These are part of what is known as ‘missing values’, where we do not have an answer to a question for a participant, where these values record the reason why an answer is missing. Recording such reasons is important for multiple reasons. Let’s take ‘-9 Non Applicable’, which following convention is used in the NILT to denote the reason there is no answer is the participant was not asked the question. For example, as shown in image below if a person answered ‘Yes’ to question 8 to say they were in paid work, then they were not asked question 9a.\n\nFor clarity, we record it as ‘non applicable’ instead of leaving it blank or ‘No’. If we left it blank we would not know whether it was blank because the question was skipped deliberately or accidentally. If we recorded it as ‘No’ rather than ‘non applicable’, we lose distinction between our participants. For instance, say we had 100 respondents and 10 answered yes, 20 answered no, and 70 were non applicable. We know from those values that of the 30 who could potentially be on a government scheme 20 were not.\n“-99 Don’t Know” and “-999 Not answered/refused” are then useful to know whilst the question was applicable to the respondent, it was not answered for another reason. During a pilot of the survey, a higher than expected number of “Don’t know” and “Not answered” can also help indicate where a question is potentially unclear or phrased in a way that respondents do not feel comfortable answering.\nImportantly though as ‘missing values’ they are treated the same in analysis. Within R, values designated as ‘missing values’ are grouped together and labelled “NA”. Take note, this stands for “Not available” - covering all reasons a value is not available (i.e. missing). A common mistake people make is to assume NA stands for ‘non applicable’, resulting in inaccurate interpretations.\nAs with all conventions though, there are situations where it can be useful to not treat certain values as ‘missing’. Whilst the NILT treats “Don’t know” as a missing value, some surveys will have questions where “Don’t know” is a meaningful answer for the analysis. For example, with a question like “Who is the current UK Prime Minister?” as part of research on public understanding of politics, a “Don’t know” is a meaningful answer rather than a missing value. In such cases, the “Don’t know” would not have a “-99” or equivalent code.\nIn some cases it can also be worthwhile exploring whether there is any pattern behind missing values. It may turn out that certain groups were more likely to refuse to answer or respond “Don’t know” to specific questions. This can then open discussion as to why and what changes to the survey design may help address it. Again that we record the reason the value is ‘missing’ rather than leaving the value blank makes it possible to still do that.\n\nPhew! Good job. You have completed the basics for wrangling the data and producing a workable dataset and gone through the documentation to understand it better.\nAs a final step, just double check that things went as expected. For this purpose, we will re-read the clean dataset in the Activities below.\nFirst though, let’s set up a new R Markdown file you can use for the Activities. Within the ‘Files’ pane (bottom-right):\n\nClick ‘New file’\nClick ‘R Markdown’\n\n\nThis will open a dialogue for creating a new file:\n\nAdd nilt_test.Rmd as the new file name.\nClick the OK button to confirm creation of an R Markdown file.\n\n\nThis will create the file and open it in a new tab:\n\nYou are now ready for the activities for the end of this week’s lab.\n\n\nWithin your R Markdown file create a code chunk to load the tidyverse and haven packages.\n\n(Aside: Whilst we saved our file as .rds as the original file was .sav we still need to load haven to read all the information stored in it. These packages also remain in our environment, but this ensures our R Markdown file will still run without issue if we restart our R session.)\n\n\nUsing the readRDS() function, create a code chunk and:\n\nread the .rds file that you just created in the last step and assign it to an object called cleaned_nilt. (Hint: You need to read it in by providing “folder/file-name.rds” as an argument.)\nread in the original .sav file that we downloaded and assign it to an object called unclean_nilt.\n\n\nRun the glimpse function on the cleaned_nilt object.\nRun the glimpse function on the unclean_nilt object.\nDo they look the same? (If yes, it means that you successfully saved a version of the nilt data with our coerced variables.)\nFinally, write code in the Console to clean your global environment, so we have a clear environment to start with next week.\n\n\n\n(This is a lengthy aside providing more info on file-types for those interested, it is optional to read.)\nFile-types like CSV and TSV are what is known as ‘interoperable’, meaning they are not limited to a specific app. Ideally then this is the file-type data should be shared in to support openness, transparency, and reproducibility.\nHowever, as covered in the online lecture this week, CSV does not store ‘metadata’. If using 1, 2, 3, … codes for a categorical variable, this information is stored separately, and if loading in data requires mapping codes and labels (e.g. 1 = UK, 2 = France, …). This can quickly become tedious and time-consuming. Whilst various attempts have been made to create ‘data packaging formats’ that build on top of CSV and/or TSV, none have gained wide-spread popularity. As a result, it is still common to see data being shared in data archives as an SPSS and/or Stata file - sometimes with a CSV file as well. (Some archives also auto-convert data into SPSS and Stata file type versions to download.)\nDecades ago, data being in the SPSS file type would have forced you to use - and pay an expensive license for - SPSS. To help end this situation many researchers found themselves within, The Free Software Foundation (FSF) created ‘PSPP’, a free alternative to SPSS, that can read and save to the same SPSS sav file-type. And, it is code from ‘PSPP’ that haven uses to read SPSS files. This involved pain-staking work ‘reverse engineering’ the SPSS file-type to understand how the data is stored and writing code so that other software - and not just SPSS - could use it. It was then not SPSS’s benevolence that made it possible to read SPSS files in R - indeed, proprietary software companies often do all they can to prevent other software being able to use their proprietary file-types. It was instead the work of an open community striving to ensure people have freedom in how they use their computers and are not forced into using specific software. (Sadly, for all the good they do, the FSF likes giving their projects terrible names, with PSPP not actually standing for anything, it is instead the ‘inverse’ of SPSS, playing on it being a free open alternative to proprietary closed software.)\nIndeed, PSPP and haven are the victims of their own success here. With the haven package, it does not matter that the data is in a proprietary SAS, SPSS, or Stata file type, it can be read into R. Importantly, it can be read into R in a way that can then immediately start being used with all the core tidyverse functions.\nDespite SPSS files being a closed proprietary format then, because it is easy to read the data into R and it maintains metadata, a lot of researchers and data archives use it as if it were an open file type, using it still to archive data. This has likely contributed to the lack of widespread adoption of the open data packages aiming to replace the proprietary file formats, as having an alternative way to share data is not a pressing need. However, I would still encourage you if looking to archive your own data in future to consider ways in which you can make it available using a proper open standard.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#the-nilt-2012-dataset",
    "href": "03-Lab3.html#the-nilt-2012-dataset",
    "title": "Data wrangling",
    "section": "",
    "text": "Today, we will be working with data generated by the Access Research Knowledge (ARK) hub. ARK conducts a series of surveys about society and life in Northern Ireland. For this lab, we will be working with the results of the Northern Ireland Life and Times Survey (NILT) in the year 2012. In particular, we will be using a teaching dataset that focuses on community relations and political attitudes. This includes background information of the participants and their household.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#nilt-rstudio-project",
    "href": "03-Lab3.html#nilt-rstudio-project",
    "title": "Data wrangling",
    "section": "",
    "text": "We will continue using Posit Cloud, as we did in the previous labs. This time though we are going to make a new RStudio project from scratch.\nWithin your “Lab Group …” workspace in Posit Cloud (if you have not joined a shared space, follow the instructions in Lab 2) and:\n\nClick the blue ‘New Project’ button in the top-right.\nFrom the list of options select ‘New RStudio Project’.\n\n\nOnce the project has loaded, click on ‘Untitled Project’ at the top of the screen.\n\nYou can now give your project a name. This is how it will appear in your list of projects in your lab group workspace. Let’s name it “NILT” as we will use this project to work with the NILT data. Type the name and hit Enter to confirm.\n\n\nAlthough this project is just for the labs, we are going to set it up in a ‘reproducible’ way. This means anyone with a copy of our project would be able to run the code and receive the same results as us. Last week, we downloaded a data set using the Console. This week we will instead cover one way to include the code used to download and wrangle our data in a file separate to the R Markdown files we use for the analysis.1\n(As you will see with the RStudio templates used for the assessments, it is possible to share an RStudio Project with the data. However, that is not always possible or desirable. For instance, the data whilst available for anyone to download from an organisation’s website or data archive may have license restrictions prohibiting making a version of it available elsewhere.)\nTo do this we will use an R script file. In our first two labs we used R Markdown, which lets us create code chunks for adding our R code. An R script file is simply a file that contains only R code, like a giant code chunk.\nBy convention, R scripts for doing set up, data prep, and similar are placed in an ‘R’ subfolder. To create one:\n\nIn the “Files” pane (bottom-right) click the folder icon that also has a green circle with white plus in it.\n\n\nThen, within the ‘New Folder’ dialog that will pop up:\n\nType a single letter capital ‘R’ as the name.\nClick the ‘OK’ button to confirm creation of the folder.\n\n\nYou should now see the new folder in the bottom-right panel, so -\n\nClick the ‘R’ folder to navigate into it (Note, you need to click the ‘R’ and not the folder icon.)\n\n\nYou should now see an empty folder, and can double-check you are in the right folder by looking at the navigation bar which should be “Cloud &gt; project &gt; R”.\n\nOK, now:\n\nClick the white document icon that is to the right of the new folder icon you clicked before. (See screenshot below!)\nFrom the list of options click ‘R Script’\n\n\nThat will bring up a ‘Create a New File in Current Directory’ dialog:\n\nName your file 01_prep_nilt_2012.R\n\nClick the ‘OK’ button to confirm creation of the script\n\n\nThe file should then auto open in the top-left pane.\n\nLastly, before we move to writing our R script, let’s navigate back to the top-level folder of our project in the ‘Files’ pane (bottom-right). To navigate back up to the top-level folder, click either the ‘..’ at the top of the folder content list or in the navigation bar click the ‘project’ text.\n\n\nBack to our top-left pane with our R script. It is good practice to include some key information in this file, such as what it does, when it was written, and who wrote it. Given R script is all R code, we can add this info using comments. As a reminder, any line in R that begins with a ‘#’ is treated as a comment and will not be run as part of the code.\nBelow is a template you can use.\n\n## Download and process NILT 2012 survey\n\n# Date: [add today's date here]\n# Author: [add your name here]\n\nThe ‘##’, with an extra ‘#’, at the top doesn’t do anything extra or different, we just include it to make visually clear this is the title / short description of what the script does.\nNext, it is good practice to include details for the data set being used, especially important if the data is not our own to ensure proper attribution. In addition to being good practice, it is often a condition for data sets where it is permitted to re-share them elsewhere that you can do so only as long as you include an attribution (i.e. a reference).\nLet’s then add a reference section. (Feel free to use the clipboard button in top-right of the box below to copy and paste the text.)\n\n# References  -----------------------------------------------------------------\n\n# https://www.ark.ac.uk/teaching/\n# 2012 Good relations: ARK. Northern Ireland Life and Times Survey Teaching Dataset 2012, Good relations [computer file]. ARK www.ark.ac.uk/nilt [distributor], March 2014.\n\nThe ----- after # References again doesn’t do anything aside from help make the section visually distinct. This may seem odd at the moment, but as the file grows in length, the importance of making it easy to find sections quickly from visual glance starts to make sense.\nOK, let’s now install packages that are needed for our project. Remember we need to do this every time we create a new project on Posit Cloud. If using RStudio Desktop, we would only need to install packages once per device we are using rather than per project.\nHowever, compared to last week, we want to set up our project so that anyone can run the code and achieve the same results. (If we run into irrepairable errors we may also want to re-run our R scipt to ‘reset’ our project). We will not know though what packages someone will already have installed or not.\nThankfully, we can write R code in our script that will check whether specific packages are already installed, and then install any packages needed that are missing.\nFirst, add a comment to make clear there is a new section:\n\n# Packages -------------------------------------------------------------------\n\nNext, add the following comment and code:\n\n# Install tidyverse if needed\nif (!\"tidyverse\" %in% rownames(installed.packages())) install.packages(\"tidyverse\")\n\nThis code may look complex, but it is largely the number of brackets making it looks more complex than it actually is:\n\n\nif checks whether a given object is TRUE or FALSE, and is used in format if x y, where if x returns TRUE then y code is run.\n\n(...) we use to enclose a chunk of code to make clear this is our x.\n\n!\"tidyverse\" %in% uses the NOT ! logical operator and the %in% operator, which is equivalent then of saying “If the word ‘tidyverse’ is not in…”\n\nrownames() gets the label names for rows in a data frame or matrix (a matrix is like a simpler data frame).\n\ninstalled.packages() (note installed) is a function that returns a matrix with all packages currently installed on the system, so within Posit Cloud it returns all the installed packages within the current project. Each row’s label name matches the installed package name.\n\ninstall.packages(\"tidyverse\") as we know already from last week, this code installs the tidyverse package.\n\nPutting it all today, we have if the tidyverse is not in our currently installed packages then install the tidyverse. Logically then, if the tidyverse is installed our if check will return FALSE and the code to install the tidyverse will not be run. That way we - and anyone we share the project with - avoid installing and re-installing the tidyverse needlessly when the script is run. (Also, whilst anyone we would share our project with should only need to run this file once, they can also run it again to fix any issues resulting from things like accidentally deleting or making irreversible changes to the data.)\nWe can test it by putting the text cursor on the line with the code and pressing Ctrl+Enter, or click the Run button at the top of the file.\n\nYou should see the tidyverse installing in the Console. Once it has finished installing, try running the same line of code again.\n\nAs can see, the code will be sent to the Console but the tidyverse is not installed again. As the if now returns FALSE as the tidyverse is in the names of our installed packages, then the code to install it is not run this time.\nNow, we need to load the packages we will be using. As you can probably guess the tidyverse is one, and we will be adding a new one as well haven. Add the following lines to your script.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(haven)\n\nUse your mouse to select both the library(...) lines and press Ctrl+Enter or click the run button to load the tidyverse and haven.\nWhy did we not need to install haven first? It is one of the additional tidyverse packages for importing data. On the main tidyverse packages page, there is an explanation of all the packages that you have access to when installing the tidyverse. The ‘core tidyverse’ set of packages are the ones that get loaded with library(tidyverse). These are the ones that most data analysis will likely use. The tidyverse though also comes with additional packages that you might not need to use as often and these we library() load on an as needed basis.\nThe haven package is used for loading data saved in file formats from three of the proprietary software tools that used to dominate quantitative analysis before R - SAS, SPSS, and Stata. The tidyverse core package for loading data, readr, works with comma-separated value (CSV) and tab-separated value files (TSV). All this means is either a comma or tab is used to separate values, such as “value1, value2, …” with new lines used for each row of values. As the NILT 2012 data we are interested in is made available as either an SPSS or Stata file, and not CSV, we will need to use the haven package.\n\nOK, let’s actually download the NILT data we will be using across the labs.\nStart by using a comment to make clear in your R script that we are starting a new section:\n\n### Download and read data ----------------------------------------------------\n\nNext, we will create a folder to store the data. Last week, we used dir.create() in the Console to create a folder for our data (‘dir’ is merely short for directory, another name used for folders). As we are setting up our project in a reproducible way we will want to create a folder this way in our script. This way R automatically creates the folder for anyone running the code, rather than them having to do it manually. It also then ensures the data is downloaded to and read in from the location the rest of our R code will assume.\nIn your script, add the code to create a new folder called ‘data’:\n\n# create folder\ndir.create(\"data\")\n\nAgain, with your text cursor on the dir.create(...) line of code, press Ctrl+Enter or click the run button. You should see the data folder appear in the files pane (bottom-right). (Though, if not, check whether you are still in the R sub-folder, and if so navigate back up to the parent directory.)\nWe now need the code to download the file with the NILT data set. We can do this by using another function we used last week, the download.file() function. The arguments for this are download.file(\"URL\", \"folder/filename.type\"). So, we need a URL for where the file is available online and we need to then specify where and with what name we want to download it.\nAdd the following then next within your R script. As this involves a URL, it is OK to copy and paste this code rather than typing it out manually.\n\n# Download NILT 2012\ndownload.file(\n  \"https://www.ark.ac.uk/teaching/NILT2012GR.sav\",\n  \"data/nilt2012.sav\"\n)\n\nAgain select the code and run it.\nTake a look in the ‘Files’ tab in bottom-right, you should now have a folder called ‘data’, click on it, and you will see the nilt2012.sav file.\n\nRemember within the Files tab to navigate back to the main project folder to also click project in the nav-bar or the .. at the top of the list of files - so, given we only have one file, right-above the nilt2012.sav.\nThe .sav is the SPSS file format. We will need to use a function made available to us by haven to read it into R, read_sav().\nLet’s use it to read in the .sav file and assign it to an object called nilt.\n\n# Read NILT\nnilt &lt;- read_sav(\"data/nilt2012.sav\")\n\nOnce more, put your text cursor on the line that reads in the data and run it.\nAnd that’s it! You should see a new data object in your ‘Environment’ tab (Pane 2 - top-right) ready to be used. If you recall last week, we read in the police_killings.csv file using read_csv(). Despite the NILT data being stored in the SPSS file-type, once read in, we can see it and interact with it the same as our data last week.\n\nYou can also see that this contains 1204 observations (rows) and 133 variables (columns). If you click on it, you can open it up in a new tab like an Excel sheet.\n\n(You can get back to your R script by either (1) clicking the small ‘x’ to close the tab showing the nilt data frame or (2) clicking on the tab for the R script.)\nWhilst that gives us a visual interactive view of our data frame. You will notice scrolling along the columns that variables like rsex has values of 1 and 2 rather than showing the category labels. To explore this in more detail we need to switch back to code. Also, as you build up your R skills you will find it much quicker - and more intuitive - to start exploring your data using code rather than opening it up in visual views like this.\nLet’s glimpse() our data frame to see the types of variables included and understand why we do not see our category labels.\nAs we are now moving from code we are writing so anyone with our script can setup their own R project the same as our own to taking a quick look at our data, do not add glimpse() to your R script. Instead, type the following in the Console.\n\nglimpse(nilt)\n\n(Being able to move from the source pane (top-left) to the Console (bottom-left) to switch between ‘writing code we want to save in our file’ and ‘writing code to quickly check our data’ is a benefit of RStudio. Whilst it may have looked intimidating at first with, all of its panes and tabs, you can hopefully now see how this supports quickly moving between different tasks.)\n\nAs you can see from the result of glimpse(), the class for practically all the variables is &lt;dbl+lbl&gt; (which if recall from last week, stands for double - i.e. numeric - and label).\nWhat does this mean? This happened because usually datasets use numbers to represent each of the categories/levels in categorical variables. These numbers are labelled with their respective meaning. This is why we have a combination of value types (&lt;dbl+lbl&gt;).\nLook at rsex in the glimpse() output, as you can see from the values displayed this includes numbers only, e.g. 1,1,2,2.... This is because ‘1’ represents ‘Male’ respondents and ‘2’ represents ‘Female’ respondents in the NILT dataset (n.b. the authors of this lab workbook recognise that sex and gender are different concepts, and we acknowledge this tension and that it will be problematic to imply or define gender identities as binary, as with any dataset. More recent surveys normally approach this in a more inclusive way by offering self-describe options). You can check the pre-defined parameters of the variable in NILT in the documentation or running print_labels(nilt$rsex) in your Console, which returns the numeric value (i.e. the code) and its respective label. As with rsex, this is the case for many other variables in this data set.\n\nYou should be aware that this type of ‘mix’ variable is a special case since we imported a file from a SPSS file that saves metadata for each variable (containing the names of the categories). The read_sav() function let us read this in as a data frame so we can use it in R, preserving the metadata as well. However, as with any data we load in, we still need to clean and ‘wrangle’ it to make it useable.\nAs you learned in the last lab, in R we treat categorical variables as factor. Therefore, we will coerce some variables as factor. This time we will use the function as_factor() instead of the simple factor() that we used before. This is because as_factor() allows us to keep the existing category labels in the variables - in other words, it will use the codes (&lt;dbl&gt;) and category labels (&lt;lbl&gt;) already in the data to turn it into a factor variable. The syntax is exactly the same as before.\nBack to our R script then. First, use a comment to make clear we are starting a new section:\n\n# Coerce variables   ---------------------------------------------------------\n\nNext add and run the following in your script:\n\n# Gender of the respondent\nnilt &lt;- nilt |&gt; mutate(rsex = as_factor(rsex))\n# Highest Educational qualification\nnilt &lt;- nilt |&gt; mutate(highqual = as_factor(highqual))\n# Religion\nnilt &lt;- nilt |&gt; mutate(religcat = as_factor(religcat))\n# Political identification\nnilt &lt;- nilt |&gt; mutate(uninatid = as_factor(uninatid))\n# Happiness\nnilt &lt;- nilt |&gt; mutate(ruhappy = as_factor(ruhappy))\n\nNotice from the code above that we assign the result of mutate() back to our nilt data frame, the nilt &lt;- ... part of the code. This replaces our ‘old’ data frame with a version with the mutated variables that are of type factor.\nBack in the Console again, run glimpse(nilt) once more and scroll up to see the line for rsex in the output.\n\nR has taken the codes (the &lt;dbl&gt; values) and mapped them to the labels (the &lt;lbl&gt; values), whereby we can now more easily see and work with our categories.\nLast week, with the police killings data we could simply use factor() on our variables as the values were saved in the file as category labels without any codes. The data for it was stored in a CSV file, that does not store metadata mapping codes and category labels. As a way around that limitation, some data made available via CSV files will store categorical variable values as labels only, with no codes. Running factor() in such cases creates codes for each unique label. The only danger with that, and something to keep eye out for, is any typos in the data, such as Scotland and Scoltand, as they are ‘unique’ labels would result in these being treated as two different categories after running factor().\nIf we read in our data from a file type that stored the values as codes, but did not include the metadata for category labels, we would instead need to manually add this mapping, along the lines of:\n\nsurvey &lt;- survey |&gt;\n  mutate(\n    country = factor(\n      country,\n      levels = c(1, 2),\n      labels = c(\"Scotland\", \"England\")\n    )\n  )\n\nBack to our NILT data again - What about the numeric variables? In the NILT documentation (covered in later section) there is a table in which you will see a type of measure ‘scale’. This usually refers to continuous numeric variables (e.g. age or income).2 Let’s coerce some variables to the appropriate numeric type.\nIn the previous operation we coerced the variables as factor one by one, but as covered at the end of last week’s lab we can also transform several variables at once within a single mutate function.\nAdd and run the following code in your script:\n\n# Coerce several variables as numeric\nnilt &lt;- nilt |&gt;\n  mutate(\n    rage = as.numeric(rage),\n    rhourswk = as.numeric(rhourswk),\n    persinc2 = as.numeric(persinc2),\n  )\n\nAnother thing we need to do is drop any unused levels (or categories) in our dataset using the function droplevels(), by adding and running the following in our script:\n\n# drop unused levels\nnilt &lt;- droplevels(nilt)\n\nThis function is useful to remove some categories that are not being used in the dataset (e.g. categories that have 0 observations).\nFinally, as we now have our data and done some wrangling, we do not want to have to repeat it each time we load the data in our R Markdown files. So, let’s save it as an .rds file - R’s own file-type.\nThis will save us time in future labs, as we will be able to read in an already formatted version of the NILT in future labs.\nAdd a comment then to make clear we have a new section:\n\n# Save data ------------------------------------------------------------------\n\nThen add and run the code to save our nilt data frame as an RDS file.\n\n# save as rds\nsaveRDS(nilt, \"data/nilt_r_object.rds\")\n\nLastly, as this script is merely to download, prep, and save a cleaned version of our data, it is good practice to end by clearing the global environment. This removes all loaded objects and puts it can into a clear state. We do this for a few reasons, including (1) if we did more complex setup operations the global environment could be filled with a lot of objects we no longer need, and (2) whilst we will continue naming the object containing our data frame as ‘nilt’ in the labs, we would not want to enforce this on anyone we share the project with.\nAdd and run then the following at the end of your R script:\n\n# clean global environment\nrm(list=ls())\n\nYou should now see your Global Environment is empty:\n\nNote, whilst this removes objects, like our nilt data frame, it does not remove any loaded libraries, so the functions provided by the tidyverse and haven remain available to us after running this line of code.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#the-nilt-documentation",
    "href": "03-Lab3.html#the-nilt-documentation",
    "title": "Data wrangling",
    "section": "",
    "text": "Please take 5-10 minutes to read the documentation for the dataset. Such documentation is always important to read as it will usually cover the research design, sampling, and information on the variables. For instance, page 7 onwards has the codebook with columns listing variable name, variable label, values, and measure. Page 6 of the documentation provides a “Navigating the codebook (example section)” overview for how to understand the table.\nIt is also worthwhile taking a look through the questionnaires for the survey as well. You will have to regularly consult the technical document and questionnaires to understand and use the data in the NILT data set. So, I recommend you to bookmark the links / save copies of the PDF files.\nThis NILT teaching dataset is also what you will be using for the research report assignment in this course (smart, isn’t it?) - so it’s worth investing the time to learn how to work with this data through the next few labs, as part of the preparation and practice for your assignment.\n(Aside: Best practice would be to read through key info in the documentation first before downloading and coercing variables - we have done the inverse today solely to ensure you have adequate time within the lab itself to download and prepare the data.)\n\nLet’s first go through the codebook in more detail. This will help you understand how to read the information in it, and some common ‘gotchas’ to look out for.\nThe “Variable Name” is how the variables are named in the data set - except in all caps, whereas once we start working with the data they will be lowercase. If we read the data set into a data frame named nilt, the rage variable name would be accessed as nilt$rage. For consistency with how they will appear when working with them in R, we will stick to using all lower case in the lab workbook.\nThe “Label” column mostly shows the corresponding text used in the survey for the variable. For rage it is “Q1. Household grid: Age of respondent”. If you look at page 2 of the main questionnaire, you will see that the ‘household grid’ was a repeat set of questions used to gather key information about each member of the household. The r in rage stands for “respondent”, i.e. the person who answered the survey questions.\nSome variable names, such as work and tunionsa on page 9, note under the name “(compute)”. This means the variable rather than representing a direct answer to a survey question is instead ‘computed’ / ‘derived’ from answers to other questions. The variable label for these then detail which questions were used to compute these variables. Work status may seem an odd variable to compute rather than ask directly.\n\nHowever, if we look at work, we can see from page 39 in the main questionnaire, a series of questions were asked to ascertain employment status and economic activity. Given the diversity of situations people can be in, it is more practical to ask a series of yes/no and other questions and then use these to calculate categories for other variables.\n\nWhilst it may look more complex having all these questions on paper, it is simpler for respondents. Let’s imagine being a 32 year old respondent who answers ‘No’ to Q8, whether taken part in paid work in last seven days, and then ‘Yes’ to Q9a, whether taking part in a government scheme for employment training.\n\nFrom these two answers we can compute the respondent is ‘Government training scheme’ for empst, ‘Not in paid employment’ for empst2, and ‘Economically active’ for econact. Note as well that Q9a was only asked based on whether the participant would be eligible for the government training scheme, calculated using age and gender. (Highlighted in blue above.)\n\nImportantly, we gathered this information by asking simple yes/no questions, structured the questions in way to only ask the minimum number of questions relevant for the respondent, and avoided needing to explain jargon such as ‘economically active’. Economically active means someone is either in employment (including waiting to start a job they have been offered and accepted) or unemployed but looking for work and if offered could start a job within the next two weeks. Imagine now asking a respondent whether they are economically active and then having to explain what that means and figuring out whether they are or not. The way the survey questions are structured captures this complexity - and more - and all without ever having to provide a definition of economically active.\nThe tunionsa variable is also a good example of we why cannot assume what a variable measures by its name alone. As can see bottom of page 9 in the documentation, tunionsa is derived from Q22 and Q22a.\n\nHowever, if we look at those two questions on page 43 of the questionniare - screenshot below to save having to switch tabs - we can see Q22 asks the respondent whether they are currently a member of a trade union or staff association and Q22a is whether they have ever been a member.\n\nIf a respondent answered ‘Yes, trade union’ or ‘Yes, staff association’ for either Q22 or Q22a the value computed for tunionsa is ‘Yes’. Whilst tunionsa then is broadly a measure of trade union membership it would be more accurate to say it is a measure of whether the respondent currently is or ever was a member of a trade union or staff association.\nDo not worry if you could not tell that from just the codebook. Normally, dataset documentation should also include a separate section with information on how each computed variable was derived. I was only able to confirm this was how tunionsa was computed by checking the values with those for Q22 and Q22a in the main data set. Expectations of what documentation for data sets should include are continuing to improve over time. However, whilst you should encounter such situations less often with newer data sets, it can still remain unclear - where you may need to do your own additional checks, or contact the original researchers to clarify.\nThis though does show the importance of not assuming what a variable measures by just its name and label. If we mistook tunionsa as measuring current membership only, we may then later be surprised seeing the high number of people currently unemployed with ‘Yes’ for tunionsa. It is not surprising to see that though when we understand that ‘Yes’ includes those who have ever been a member.\n\nThe ‘Measure’ column in the codebook tells us whether the variable is ‘scale’, ‘nominal’, or ‘ordinal’. This is based on how SPSS, the proprietary statistical analysis program that was used by the NILT project, stores variables. Within R, these correspond respectively with numeric, (unordered) factor, and ordered factor variables that we covered last week. As we covered earlier in the lab, the tidyverse also provides us with tools to work with data created in SPSS.\nThe ‘Values’ column then tells us what the numbers represent. For numeric variables this is the unit being measured, such as “years” and “number of people”. This may seem ‘obvious’ for some variables, but good documentation should still always provide this information. The importance of this becomes clearer if we look at livearea. Without it clarifying “Numeric (years)” someone could mistake it as representing number of months instead. Similarly, the “0 Less than 1 year” is an important clarifier, helping us understand that anything less than 1 year was recorded as 0. In other words, if someone had lived in the area for 7 months, it was still recorded as 0 rather than rounded up to 1.\n\nFor the categorical variables it then provides the code and label for the categories. For example, with rsex the categories are 1 (code) Male (label) and 2 (code) Female (label). As mentioned last week, the ‘raw’ data is often stored in numeric codes as - among other reasons - it is more efficient to store it this way. Using the codebook, someone looking at the raw data would then know that a value of 1 represents Male. And as we covered above, knowing the code and label means we can set up the data in R to show the labels instead of the raw codes. (And if we were working with data in a file with only the codes and not the labels, we can use the codebook to write our own code to map codes and labels.)\n\nYou may have spotted though that the Values column for some variables also includes additional grey text, “-9 Non Applicable”, “-99 Don’t Know”, and “-999 Not answered/refused”. These are part of what is known as ‘missing values’, where we do not have an answer to a question for a participant, where these values record the reason why an answer is missing. Recording such reasons is important for multiple reasons. Let’s take ‘-9 Non Applicable’, which following convention is used in the NILT to denote the reason there is no answer is the participant was not asked the question. For example, as shown in image below if a person answered ‘Yes’ to question 8 to say they were in paid work, then they were not asked question 9a.\n\nFor clarity, we record it as ‘non applicable’ instead of leaving it blank or ‘No’. If we left it blank we would not know whether it was blank because the question was skipped deliberately or accidentally. If we recorded it as ‘No’ rather than ‘non applicable’, we lose distinction between our participants. For instance, say we had 100 respondents and 10 answered yes, 20 answered no, and 70 were non applicable. We know from those values that of the 30 who could potentially be on a government scheme 20 were not.\n“-99 Don’t Know” and “-999 Not answered/refused” are then useful to know whilst the question was applicable to the respondent, it was not answered for another reason. During a pilot of the survey, a higher than expected number of “Don’t know” and “Not answered” can also help indicate where a question is potentially unclear or phrased in a way that respondents do not feel comfortable answering.\nImportantly though as ‘missing values’ they are treated the same in analysis. Within R, values designated as ‘missing values’ are grouped together and labelled “NA”. Take note, this stands for “Not available” - covering all reasons a value is not available (i.e. missing). A common mistake people make is to assume NA stands for ‘non applicable’, resulting in inaccurate interpretations.\nAs with all conventions though, there are situations where it can be useful to not treat certain values as ‘missing’. Whilst the NILT treats “Don’t know” as a missing value, some surveys will have questions where “Don’t know” is a meaningful answer for the analysis. For example, with a question like “Who is the current UK Prime Minister?” as part of research on public understanding of politics, a “Don’t know” is a meaningful answer rather than a missing value. In such cases, the “Don’t know” would not have a “-99” or equivalent code.\nIn some cases it can also be worthwhile exploring whether there is any pattern behind missing values. It may turn out that certain groups were more likely to refuse to answer or respond “Don’t know” to specific questions. This can then open discussion as to why and what changes to the survey design may help address it. Again that we record the reason the value is ‘missing’ rather than leaving the value blank makes it possible to still do that.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#read-the-clean-dataset",
    "href": "03-Lab3.html#read-the-clean-dataset",
    "title": "Data wrangling",
    "section": "",
    "text": "Phew! Good job. You have completed the basics for wrangling the data and producing a workable dataset and gone through the documentation to understand it better.\nAs a final step, just double check that things went as expected. For this purpose, we will re-read the clean dataset in the Activities below.\nFirst though, let’s set up a new R Markdown file you can use for the Activities. Within the ‘Files’ pane (bottom-right):\n\nClick ‘New file’\nClick ‘R Markdown’\n\n\nThis will open a dialogue for creating a new file:\n\nAdd nilt_test.Rmd as the new file name.\nClick the OK button to confirm creation of an R Markdown file.\n\n\nThis will create the file and open it in a new tab:\n\nYou are now ready for the activities for the end of this week’s lab.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#activities",
    "href": "03-Lab3.html#activities",
    "title": "Data wrangling",
    "section": "",
    "text": "Within your R Markdown file create a code chunk to load the tidyverse and haven packages.\n\n(Aside: Whilst we saved our file as .rds as the original file was .sav we still need to load haven to read all the information stored in it. These packages also remain in our environment, but this ensures our R Markdown file will still run without issue if we restart our R session.)\n\n\nUsing the readRDS() function, create a code chunk and:\n\nread the .rds file that you just created in the last step and assign it to an object called cleaned_nilt. (Hint: You need to read it in by providing “folder/file-name.rds” as an argument.)\nread in the original .sav file that we downloaded and assign it to an object called unclean_nilt.\n\n\nRun the glimpse function on the cleaned_nilt object.\nRun the glimpse function on the unclean_nilt object.\nDo they look the same? (If yes, it means that you successfully saved a version of the nilt data with our coerced variables.)\nFinally, write code in the Console to clean your global environment, so we have a clear environment to start with next week.\n\n\n\n(This is a lengthy aside providing more info on file-types for those interested, it is optional to read.)\nFile-types like CSV and TSV are what is known as ‘interoperable’, meaning they are not limited to a specific app. Ideally then this is the file-type data should be shared in to support openness, transparency, and reproducibility.\nHowever, as covered in the online lecture this week, CSV does not store ‘metadata’. If using 1, 2, 3, … codes for a categorical variable, this information is stored separately, and if loading in data requires mapping codes and labels (e.g. 1 = UK, 2 = France, …). This can quickly become tedious and time-consuming. Whilst various attempts have been made to create ‘data packaging formats’ that build on top of CSV and/or TSV, none have gained wide-spread popularity. As a result, it is still common to see data being shared in data archives as an SPSS and/or Stata file - sometimes with a CSV file as well. (Some archives also auto-convert data into SPSS and Stata file type versions to download.)\nDecades ago, data being in the SPSS file type would have forced you to use - and pay an expensive license for - SPSS. To help end this situation many researchers found themselves within, The Free Software Foundation (FSF) created ‘PSPP’, a free alternative to SPSS, that can read and save to the same SPSS sav file-type. And, it is code from ‘PSPP’ that haven uses to read SPSS files. This involved pain-staking work ‘reverse engineering’ the SPSS file-type to understand how the data is stored and writing code so that other software - and not just SPSS - could use it. It was then not SPSS’s benevolence that made it possible to read SPSS files in R - indeed, proprietary software companies often do all they can to prevent other software being able to use their proprietary file-types. It was instead the work of an open community striving to ensure people have freedom in how they use their computers and are not forced into using specific software. (Sadly, for all the good they do, the FSF likes giving their projects terrible names, with PSPP not actually standing for anything, it is instead the ‘inverse’ of SPSS, playing on it being a free open alternative to proprietary closed software.)\nIndeed, PSPP and haven are the victims of their own success here. With the haven package, it does not matter that the data is in a proprietary SAS, SPSS, or Stata file type, it can be read into R. Importantly, it can be read into R in a way that can then immediately start being used with all the core tidyverse functions.\nDespite SPSS files being a closed proprietary format then, because it is easy to read the data into R and it maintains metadata, a lot of researchers and data archives use it as if it were an open file type, using it still to archive data. This has likely contributed to the lack of widespread adoption of the open data packages aiming to replace the proprietary file formats, as having an alternative way to share data is not a pressing need. However, I would still encourage you if looking to archive your own data in future to consider ways in which you can make it available using a proper open standard.",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "03-Lab3.html#footnotes",
    "href": "03-Lab3.html#footnotes",
    "title": "Data wrangling",
    "section": "Footnotes",
    "text": "Footnotes\n\nAnother way to do this is using the renv package. This package creates a unique separate environment, including for installed packages, for each project you are working on. If you were using RStudio Desktop and were working on analysis projects that required different specific versions of the same packages, renv makes that possible as it creates a folder with the installed packages for each project. Even when working on a single project as renv maintains a record of which version of a package was used this is more ‘future-proof’. If a package used made major changes, renaming functions or changing how arguments are passed to them, the code may no longer work. renv though would retrieve and install the exact same version as originally used, helping ensure it will run now and well into the future.↩︎\nBe careful, in some cases these actually correspond to discrete numeric values in this dataset (things that can be counted, e.g. number of…).↩︎",
    "crumbs": [
      "**Lab 3** Data Wrangling"
    ]
  },
  {
    "objectID": "06-Lab6.html",
    "href": "06-Lab6.html",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "In this lab, we will extend your skills to explore data by visualizing it… and R is great for this! It is actually a highly-demand skill in the job market.\nVisualizing data is an important process in at least two stages in quantitative research: First, for you as a researcher to get familiar with the data; and second, to communicate your findings. R includes two important tools to achieve this: First, the wonderful ggplot2 package (included in tidyverse), a powerful tool to explore and plot data. Second, R Markdown which allows you to create integrated quantitative reports. Combining and mastering the two can create very effective results.\n\nVisual data exploration with ggplot2 (Artwork by @alison_horst).\n\n\n\n\nVisual data exploration. Source: Horst (n.d.)\n\n\n\nVisualizations are important for any quantitative analysis. These are helpful to identify overall trends, problems, or extreme values in your data at an initial stage. Additionally, visualizations are key to communicate your results at the final stage of the research process. These two stages are known as exploratory and explanatory visualizations, respectively. Base R includes some functionalities to create basic plots. These are often used to generate quick exploratory visualizations. In addition, ggplot2, which is one of the most popular data visualization tools for R, allows you to extend the base R capabilities and create publishable high-quality plots. In this lab we will focus on ggplot2.\nDifferent plot types serve different types of data. In the last lab, we introduced some functions to summarise your data and to generate summaries for specific types of variable. This will help you to decide what the most suitable plot is for your data or variable. The table below presents a minimal guide to choose the type of plot as a function of the type of data you intend to visualize. In addition, it splits the type of plot by the number of variables included in the visualization, one for univariate, or two for bivariate. Bivariate plots are useful to explore the relationship between variables.\n\n\n\nUnivariate\nBivariate\n\n\n\nCategorical\nBar plot / Pie chart\nBar plot\n\n\nNumeric\nHistogram / boxplot\nScatter plot\n\n\n\nCategorical + Numeric\n\n-\nBox plot\n\n\n\nNote that it is possible to include more than two variables in one plot. However, as more variables are added, careful considerations are needed on whether they are actually adding more useful information or instead making the graph difficult - or impossible - to interpret.\n\nWe will continue working in the same project called NILT in Posit Cloud.\nSet up your session as follows:\n\nGo to your ‘Lab Group ##’ in Posit Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Lab Group’;\n\nWithin your ‘NILT’ project, ensure you are in the top-level project directory. You can tell by checking in the ‘Files’ tab in the bottom-right pane. Near the top of the tab you’ll see “Cloud &gt; project”. If that’s all you see, you are already in the top-level folder. If instead you see “Cloud &gt; project &gt; R” or “Cloud &gt; project &gt; data” then click on the text for “project” to navigate back to the top-level folder.\n\nNext, create a new R Markdown document. Within the ‘Files’ tab in bottom-right pane -\n\nClick ‘New File’ in the tool-bar.\nSelect ‘R Markdown’ from the list of options.\n\n\nWithin the ‘Create a New File in Current Directory’ dialogue that pops up -\n\nType Lab-6-Visual.Rmd as the name.\nClick the ‘OK’ button to confirm.\n\n\nFeel free to then adjust the YAML header, such as adding a more full descriptive title and your name as the author.\n\nOnce you have modified the YAML, we next need our setup code chunk with the knitr options. So -\n\nCreate a new code chunk\nModify the fence options to ```{r setup, include=FALSE}\n\nThen in the main body of code chunk add -\n\n\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\nThen, create another code chunk and name it preamble and again include=FALSE.\nWithin it, we want to load the tidyverse, read in our NILT file, and setup a subset with the variables we’ll be using.\n\n# Load Packages\nlibrary(tidyverse)\n\n# Read NILT\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n# Create subset\nnilt_subset &lt;- nilt |&gt;\n  select(rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\nRun both of the previous chunks individually by clicking on the green arrow located on the top-right of the chunk.\nAs wee reminder, despite in Lab 4 loading the tidyverse and setting up the nilt and nilt_subset data frame objects, we still need to do this again in each new R Markdown file. The reason for this is whilst they are available in our Global Environment, the top-right pane, each time you ‘knit’ your document it starts with a clean Global Environment. It does this as for reproducibility anyone with a copy of the R Markdown file and data being used should be able to run the code and receive the exact same results.\nNow that we have read the data in, we are ready to start creating our own plots using the 2012 NILT survey.\n\nFortunately, ggplot is part of the Tidyverse set of packages, which places strong emphasis on simplicity, readability, and consistency. As covered in the online lecture for this week, the gg in ggplot stands for “grammar of graphics”. This breaks down graphs into different components that - similar to using grammar to compose words together in a sentence - you compose together to create graphs. This combination of Tidyverse’s overall design philosophy and Wilkinson’s Grammar of Graphics approach, makes ggplot incredibly powerful. We can create complex plots with only a few lines of (relatively) simple code.\nAt its most basic, ggplot always takes at least three layers, namely data, aesthetics and geometry. It is good practice though to also include text labels - such as a title and labels for x and y axes - to make clear what is being visualised in your plot. In general then, with ggplot we use the general format -\n\ndata_frame |&gt; ggplot(aes(x = column_name1, ...)) +\n  geom_...() +\n  labs(title = \"Title text\", x = \"Label for x-axis\", ...)\n\nTo break this down:\n\n\ndata_frame is the data frame object we want to use for our plot, such as nilt_subset.\n\nggplot() is the main function to create a new plot and is always the first layer we add.\n\naes(), short for “aesthetic mappings”, is used inside the ggplot() function to specify which variables from our data frame we want to use. For univariate plots, we can just specify the x or y axis (e.g. data_frame |&gt; ggplot(aes(x = column_name)). It can also take other arguments, which we will cover in sections below.\n\n+ is used between each function() to ‘compose’ them together. It is basically the equivalent of saying “after this function ADD this function”. If you encounter error messages when using ggplot, it is always best to first check whether you are missing any + symbols between functions.\n\ngeom_...(), with “geom” being short for “geometry”, is used to specify the type of plot, such as geom_boxplot(), geom_histogram(), and so on. We will cover each of these plot types in more detail further into the lab.\n\nlabs() is then used to add our text labels, with main ones to be aware of being title = \"\", x = \"\", and y = \"\".\n\nAnd that’s it! With often just three lines of code we can construct most plots we will want for our analysis. Importantly, despite all the different plot types we might want to construct, the code we need to write all follow this same basic ‘grammar’. The main bits that will change based on plot type are the precise arguments within the aes() function, which specific geom_...() function we use, and the labels we set using the labs() function. So, pay attention to how those change across the plot types covered below and you’ll have a good sense of all you will need when creating your own plots.\n\nLet’s start using the same variables we summarised in Lab 4. In Lab 4, we started by computing the total number of respondents by gender in a one-way contingency table. We can easily visualize this using a bar plot with ggplot -\n\nnilt_subset |&gt; ggplot(aes(x = rsex)) +\n  geom_bar() +\n  labs(title = \"Gender\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\n(Note: Remember you will need to create a code chunk for adding this code within your R Markdown file.)\nHere:\n\nWe pass our data frame object, nilt_subset, using the pipe operator. Without a pipe, we would need to write ggplot(nilt_subset, aes(....\nInside the ggplot() function, we then use the aes(), aesthetics, function. In this case, within it we define the X axis x = of the plot by the categories included in the variable rsex.\nAfter ggplot() we add a + symbol to compose our functions together.\nThe geometry is specified with the function geom_bar() without arguments for now. Again we add + after it so R knows to compose the functions we are using together to construct the plot.\nFinally, we use the labs() function to provide labels for the main title - title = \"Gender\" - and the name of the x axis - x = \"Gender of respondent\". Note, as this is our last function for constructing the plot, we do not need a + after it.\n\nFrom the plot above, we can graphically see what we found out previously: there are more female respondents than males in our sample. The advantage is that we can have a sense of the magnitude of the difference by visualising it.\n\nIn Lab 4, we computed a Two-Way contingency table, which included the count of two categorical variables. This summary can be visualized using a stacked bar plot. This is quite similar to the above, with the addition that the area of the vertical plot is coloured by the size of each group.\nIf we wanted to know how gender is split by religion, we can add the fill argument with a second variable in aesthetics, as shown below.\n\nnilt_subset |&gt; ggplot(aes(x = rsex, fill = religcat)) +\n  geom_bar() +\n  labs(title = \"Gender by religion\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\nThis plot is not very informative, since the total size of female and male respondents is different. The type of visualization will also depend on your specific research question or the topic you are interested in. For example, if I think it is worthwhile visualizing the religion by respondents’ sex. A plot can show us the magnitudes and composition by respondents’ sex for each religion. To do this, we need to change the aesthetics, specifying the religion by category variable religcat on the x axis and fill with gender rsex.\n\nnilt_subset |&gt; ggplot(aes(x = religcat, fill = rsex)) +\n  geom_bar() +\n  labs(title = \"Religion by gender\", x = \"Religion\")\n\n\n\n\n\n\n\nAs we can see, Catholic and Protestant religion are similarly popular among the respondents. Also, we can see that these are composed by similar proportions of males and females. One interesting thing is that there are more male respondents with no religion than female participants. Again, we found this out with the descriptive statistics computed in Lab 4. However, we have the advantage that we can graphically represent and inspect the magnitude of these differences.\n\n\nIn Lab 4, we talked about some measures of centrality and spread for numeric variables. The histogram plot is similar to the bar plot; the difference is that it splits the numeric range into fixed “bins” and computes the frequency/count for each bin instead of counting the number of respondents for each numeric value. The syntax is practically the same as the simple bar plot. This time, we will set the x aesthetic with the numeric variable age rage. Also, the geometry is defined as a histogram using the geom_histogram() function.\n\nnilt_subset |&gt; ggplot(aes(x = rage)) +\n  geom_histogram() +\n  labs(title = \"Age distribution\")\n\n\n\n\n\n\n\nFrom the histogram, we have age (in bins) on the X axis, and the frequency/count on the y axis. This plot is useful to visualize how respondent’s age is distributed in our sample. For instance, we can quickly see the minimum and maximum value, or the most popular age, or a general trend indicating the largest age group.\nA second option to visualize numeric variables is the box plot. Essentially this draws the quartiles of a numeric vector. For this plot, rage is defined in the y axis. This is just a personal preference. The geometry is set by the geom_boxplot() function.\n\nnilt_subset |&gt; ggplot(aes(y = rage)) +\n  geom_boxplot() +\n  labs(title = \"Age boxplot\")\n\n\n\n\n\n\n\nWhat we see from this plot is the first, second and third quartile. The second quartile (or median) is represented by the black line in the middle of the box. As you can see this is close to 50 years old, as we computed using the quantile() function. The lower edge of the box represents the 2nd quartile, which is somewhere around 35 years old. Similarly the 3rd quartile is represented by the upper edge of the box. We can confirm this by computing the quantiles for this variable.\n\nquantile(nilt_subset$rage, na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\n\nA useful plot to explore the relationship between two numeric variables is the scatter plot. This plot locates a dot for each observation according to their respective numeric values. In the example below, we use age rage on the X axis (horizontal), and personal income persinc2 on the Y axis (vertical). This type of plot is useful to explore a relationship between variables.\n\nTo generate a scatter plot, we need to define x and y in aesthetics aes(). The geometry is a point, that we can specify using the geom_point() function. Note that we are specifying some further optional arguments within geom_point(). First, alpha regulates the opacity of the dots. This goes from 0.0 (completely translucent) to 1.0 (completely solid fill). Second, in we defined position as jitter. This arguments slightly moves the point away from their exact location. These two arguments are desired in this plot because the personal income bands overlap - meaning most points will be drawn directly ontop of each other. Adding some transparency and noise to their position with jitter (i.e. shifts dots slightly apart), can make it easier to visualize possible patterns.\n\nnilt_subset |&gt; ggplot(aes(x = rage, y = persinc2)) +\n  geom_point(alpha = 0.7, position = \"jitter\") +\n  labs(title = \"Personal income vs age\", x = \"Age\", y = \"Personal income (£)\")\n\n\n\n\n\n\n\nThere is not a clear pattern in our previous plot. However, it is interesting to note that most of the people younger than 25 years old earn less than £20K a year. Similarly, most of the people older than 75 earn less than £20K. And only very few earn over £60k a year (looking at the top of the plot).\n\nVery often we want to summarise central or spread measure by categories or groups. For example, let’s go back to the example of age and respondents’ sex. We can visualize these two variables (which include one numeric and one categorical) using a box plot. To create this, we need to specify the x and y value in aes() and include the geom_boxplot() geometry.\n\nnilt_subset |&gt; ggplot(aes(y = rage, x = rsex)) +\n  geom_boxplot() +\n  labs(title = \"Age by sex\")\n\n\n\n\n\n\n\nFrom this, we can visualize that female participants are slightly younger than their male counterparts in the sample.\n\nThere are a number of features that you can customize in your plots, including the background, text size, colours, adding more variables. But you don’t have to memorise or remember all this, one thing that is very commonly used by R data scientists are R cheat sheets! They are extremely handy when you try to create a visualisation from scratch, check out the Data Visualization with ggplot2 Cheat Sheet. An extra tip is that you can change the overall look of the plot by adding pre-defined themes. You can read more about it here. Another interesting site is the The R Graph Gallery, which includes a comprehensive showcase of plot types and their respective code.\nAs a quick example of how simply themeing is, let’s take the last graph and apply a minimal theme. All we need to do is add + after labs() followed by theme_minimal().\n\nnilt_subset |&gt; ggplot(aes(y = rage, x = rsex)) +\n  geom_boxplot() +\n  labs(title = \"Age by sex\") +\n  theme_minimal()\n\n\n\n\n\n\n\nRather than adding + theme_...() to each plot individually, you can also set a global theme using theme_set(). As this is setting a theme to use for all plots, by convention the code for this would be added to your “preamble” code chunk at the top of your R Markdown document.\n\n# Set ggplot theme\ntheme_set(theme_minimal())\n\nAfter adding (and running) the code, any code you run to create a plot will use the minimal theme.\n\n\n\n\n\n\nImportant. If you are interested in applying a theme to your plots please look at the linked resources above. Themeing plots is easy with ggplot, with a number of built-in themes available with a single function. Despite that, if you prompt genAI how to write code for a plot, it will often add a theme - even when one was not requested, add arbitrary and needlessly complex customisation, and inconsistently apply these themes/customisations across plots.\nAs a general rule of thumb, if genAI responses have code we did not cover in the labs, 99% of the time you can be confident it is spouting out absolute nonsense. As seen above, the built-in ggplot themes can be applied with a single line of code, and more complex customisation - that you will rarely, if ever, need - can be set once globally or used only when absolutely needed for a specific plot. GenAI will mislead you into thinking that creating plots with R requires 20+ lines of code rather than 3-4. It has similar issue with tables, sometimes giving 40+ lines of code to create a table that could instead be created with a single line of code.\n\n\n\n\nUsing the nilt_subset object, complete the tasks below in your R Markdown file. Insert a new code chunk for each of these activities and include brief comments as regular text (i.e. outside the code chunk) to introduce or describe the plots. Feel free to copy and adapt the code to create the plots in the examples above. As covered in the lectures, you do not need to memorise the exact code. The important thing is that you understand what the code does and how to modify it for what you are trying to achieve.\n\nCreate a first-level header to start a section called “Categorical analysis”;\nCreate simple bar plot using the geom_bar() geometry to visualize the view on unionist/nationalist/neither affiliation reported by the respondents using the variable uninatid;\nBased on the plot above, create a ‘stacked bar plot’ to visualize this affiliation by religion, using the uninatid and religcat variables;\nCreate a new first-level header to start a section called “Numeric analysis”;\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis … Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results.\nAdd your own (brief) comments to each of the plots as text in your R Markdown file;\nKnit the .Rmd document as HTML. The knitted file will be saved automatically in your project.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#introduction",
    "href": "06-Lab6.html#introduction",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "In this lab, we will extend your skills to explore data by visualizing it… and R is great for this! It is actually a highly-demand skill in the job market.\nVisualizing data is an important process in at least two stages in quantitative research: First, for you as a researcher to get familiar with the data; and second, to communicate your findings. R includes two important tools to achieve this: First, the wonderful ggplot2 package (included in tidyverse), a powerful tool to explore and plot data. Second, R Markdown which allows you to create integrated quantitative reports. Combining and mastering the two can create very effective results.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#data-visualization",
    "href": "06-Lab6.html#data-visualization",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Visual data exploration with ggplot2 (Artwork by @alison_horst).\n\n\n\n\nVisual data exploration. Source: Horst (n.d.)\n\n\n\nVisualizations are important for any quantitative analysis. These are helpful to identify overall trends, problems, or extreme values in your data at an initial stage. Additionally, visualizations are key to communicate your results at the final stage of the research process. These two stages are known as exploratory and explanatory visualizations, respectively. Base R includes some functionalities to create basic plots. These are often used to generate quick exploratory visualizations. In addition, ggplot2, which is one of the most popular data visualization tools for R, allows you to extend the base R capabilities and create publishable high-quality plots. In this lab we will focus on ggplot2.\nDifferent plot types serve different types of data. In the last lab, we introduced some functions to summarise your data and to generate summaries for specific types of variable. This will help you to decide what the most suitable plot is for your data or variable. The table below presents a minimal guide to choose the type of plot as a function of the type of data you intend to visualize. In addition, it splits the type of plot by the number of variables included in the visualization, one for univariate, or two for bivariate. Bivariate plots are useful to explore the relationship between variables.\n\n\n\nUnivariate\nBivariate\n\n\n\nCategorical\nBar plot / Pie chart\nBar plot\n\n\nNumeric\nHistogram / boxplot\nScatter plot\n\n\n\nCategorical + Numeric\n\n-\nBox plot\n\n\n\nNote that it is possible to include more than two variables in one plot. However, as more variables are added, careful considerations are needed on whether they are actually adding more useful information or instead making the graph difficult - or impossible - to interpret.\n\nWe will continue working in the same project called NILT in Posit Cloud.\nSet up your session as follows:\n\nGo to your ‘Lab Group ##’ in Posit Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Lab Group’;\n\nWithin your ‘NILT’ project, ensure you are in the top-level project directory. You can tell by checking in the ‘Files’ tab in the bottom-right pane. Near the top of the tab you’ll see “Cloud &gt; project”. If that’s all you see, you are already in the top-level folder. If instead you see “Cloud &gt; project &gt; R” or “Cloud &gt; project &gt; data” then click on the text for “project” to navigate back to the top-level folder.\n\nNext, create a new R Markdown document. Within the ‘Files’ tab in bottom-right pane -\n\nClick ‘New File’ in the tool-bar.\nSelect ‘R Markdown’ from the list of options.\n\n\nWithin the ‘Create a New File in Current Directory’ dialogue that pops up -\n\nType Lab-6-Visual.Rmd as the name.\nClick the ‘OK’ button to confirm.\n\n\nFeel free to then adjust the YAML header, such as adding a more full descriptive title and your name as the author.\n\nOnce you have modified the YAML, we next need our setup code chunk with the knitr options. So -\n\nCreate a new code chunk\nModify the fence options to ```{r setup, include=FALSE}\n\nThen in the main body of code chunk add -\n\n\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\nThen, create another code chunk and name it preamble and again include=FALSE.\nWithin it, we want to load the tidyverse, read in our NILT file, and setup a subset with the variables we’ll be using.\n\n# Load Packages\nlibrary(tidyverse)\n\n# Read NILT\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\n# Create subset\nnilt_subset &lt;- nilt |&gt;\n  select(rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\nRun both of the previous chunks individually by clicking on the green arrow located on the top-right of the chunk.\nAs wee reminder, despite in Lab 4 loading the tidyverse and setting up the nilt and nilt_subset data frame objects, we still need to do this again in each new R Markdown file. The reason for this is whilst they are available in our Global Environment, the top-right pane, each time you ‘knit’ your document it starts with a clean Global Environment. It does this as for reproducibility anyone with a copy of the R Markdown file and data being used should be able to run the code and receive the exact same results.\nNow that we have read the data in, we are ready to start creating our own plots using the 2012 NILT survey.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#ggplot-syntax",
    "href": "06-Lab6.html#ggplot-syntax",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Fortunately, ggplot is part of the Tidyverse set of packages, which places strong emphasis on simplicity, readability, and consistency. As covered in the online lecture for this week, the gg in ggplot stands for “grammar of graphics”. This breaks down graphs into different components that - similar to using grammar to compose words together in a sentence - you compose together to create graphs. This combination of Tidyverse’s overall design philosophy and Wilkinson’s Grammar of Graphics approach, makes ggplot incredibly powerful. We can create complex plots with only a few lines of (relatively) simple code.\nAt its most basic, ggplot always takes at least three layers, namely data, aesthetics and geometry. It is good practice though to also include text labels - such as a title and labels for x and y axes - to make clear what is being visualised in your plot. In general then, with ggplot we use the general format -\n\ndata_frame |&gt; ggplot(aes(x = column_name1, ...)) +\n  geom_...() +\n  labs(title = \"Title text\", x = \"Label for x-axis\", ...)\n\nTo break this down:\n\n\ndata_frame is the data frame object we want to use for our plot, such as nilt_subset.\n\nggplot() is the main function to create a new plot and is always the first layer we add.\n\naes(), short for “aesthetic mappings”, is used inside the ggplot() function to specify which variables from our data frame we want to use. For univariate plots, we can just specify the x or y axis (e.g. data_frame |&gt; ggplot(aes(x = column_name)). It can also take other arguments, which we will cover in sections below.\n\n+ is used between each function() to ‘compose’ them together. It is basically the equivalent of saying “after this function ADD this function”. If you encounter error messages when using ggplot, it is always best to first check whether you are missing any + symbols between functions.\n\ngeom_...(), with “geom” being short for “geometry”, is used to specify the type of plot, such as geom_boxplot(), geom_histogram(), and so on. We will cover each of these plot types in more detail further into the lab.\n\nlabs() is then used to add our text labels, with main ones to be aware of being title = \"\", x = \"\", and y = \"\".\n\nAnd that’s it! With often just three lines of code we can construct most plots we will want for our analysis. Importantly, despite all the different plot types we might want to construct, the code we need to write all follow this same basic ‘grammar’. The main bits that will change based on plot type are the precise arguments within the aes() function, which specific geom_...() function we use, and the labels we set using the labs() function. So, pay attention to how those change across the plot types covered below and you’ll have a good sense of all you will need when creating your own plots.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#categorical-variables",
    "href": "06-Lab6.html#categorical-variables",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Let’s start using the same variables we summarised in Lab 4. In Lab 4, we started by computing the total number of respondents by gender in a one-way contingency table. We can easily visualize this using a bar plot with ggplot -\n\nnilt_subset |&gt; ggplot(aes(x = rsex)) +\n  geom_bar() +\n  labs(title = \"Gender\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\n(Note: Remember you will need to create a code chunk for adding this code within your R Markdown file.)\nHere:\n\nWe pass our data frame object, nilt_subset, using the pipe operator. Without a pipe, we would need to write ggplot(nilt_subset, aes(....\nInside the ggplot() function, we then use the aes(), aesthetics, function. In this case, within it we define the X axis x = of the plot by the categories included in the variable rsex.\nAfter ggplot() we add a + symbol to compose our functions together.\nThe geometry is specified with the function geom_bar() without arguments for now. Again we add + after it so R knows to compose the functions we are using together to construct the plot.\nFinally, we use the labs() function to provide labels for the main title - title = \"Gender\" - and the name of the x axis - x = \"Gender of respondent\". Note, as this is our last function for constructing the plot, we do not need a + after it.\n\nFrom the plot above, we can graphically see what we found out previously: there are more female respondents than males in our sample. The advantage is that we can have a sense of the magnitude of the difference by visualising it.\n\nIn Lab 4, we computed a Two-Way contingency table, which included the count of two categorical variables. This summary can be visualized using a stacked bar plot. This is quite similar to the above, with the addition that the area of the vertical plot is coloured by the size of each group.\nIf we wanted to know how gender is split by religion, we can add the fill argument with a second variable in aesthetics, as shown below.\n\nnilt_subset |&gt; ggplot(aes(x = rsex, fill = religcat)) +\n  geom_bar() +\n  labs(title = \"Gender by religion\", x = \"Gender of respondent\")\n\n\n\n\n\n\n\nThis plot is not very informative, since the total size of female and male respondents is different. The type of visualization will also depend on your specific research question or the topic you are interested in. For example, if I think it is worthwhile visualizing the religion by respondents’ sex. A plot can show us the magnitudes and composition by respondents’ sex for each religion. To do this, we need to change the aesthetics, specifying the religion by category variable religcat on the x axis and fill with gender rsex.\n\nnilt_subset |&gt; ggplot(aes(x = religcat, fill = rsex)) +\n  geom_bar() +\n  labs(title = \"Religion by gender\", x = \"Religion\")\n\n\n\n\n\n\n\nAs we can see, Catholic and Protestant religion are similarly popular among the respondents. Also, we can see that these are composed by similar proportions of males and females. One interesting thing is that there are more male respondents with no religion than female participants. Again, we found this out with the descriptive statistics computed in Lab 4. However, we have the advantage that we can graphically represent and inspect the magnitude of these differences.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#numeric-variables",
    "href": "06-Lab6.html#numeric-variables",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "In Lab 4, we talked about some measures of centrality and spread for numeric variables. The histogram plot is similar to the bar plot; the difference is that it splits the numeric range into fixed “bins” and computes the frequency/count for each bin instead of counting the number of respondents for each numeric value. The syntax is practically the same as the simple bar plot. This time, we will set the x aesthetic with the numeric variable age rage. Also, the geometry is defined as a histogram using the geom_histogram() function.\n\nnilt_subset |&gt; ggplot(aes(x = rage)) +\n  geom_histogram() +\n  labs(title = \"Age distribution\")\n\n\n\n\n\n\n\nFrom the histogram, we have age (in bins) on the X axis, and the frequency/count on the y axis. This plot is useful to visualize how respondent’s age is distributed in our sample. For instance, we can quickly see the minimum and maximum value, or the most popular age, or a general trend indicating the largest age group.\nA second option to visualize numeric variables is the box plot. Essentially this draws the quartiles of a numeric vector. For this plot, rage is defined in the y axis. This is just a personal preference. The geometry is set by the geom_boxplot() function.\n\nnilt_subset |&gt; ggplot(aes(y = rage)) +\n  geom_boxplot() +\n  labs(title = \"Age boxplot\")\n\n\n\n\n\n\n\nWhat we see from this plot is the first, second and third quartile. The second quartile (or median) is represented by the black line in the middle of the box. As you can see this is close to 50 years old, as we computed using the quantile() function. The lower edge of the box represents the 2nd quartile, which is somewhere around 35 years old. Similarly the 3rd quartile is represented by the upper edge of the box. We can confirm this by computing the quantiles for this variable.\n\nquantile(nilt_subset$rage, na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\n\nA useful plot to explore the relationship between two numeric variables is the scatter plot. This plot locates a dot for each observation according to their respective numeric values. In the example below, we use age rage on the X axis (horizontal), and personal income persinc2 on the Y axis (vertical). This type of plot is useful to explore a relationship between variables.\n\nTo generate a scatter plot, we need to define x and y in aesthetics aes(). The geometry is a point, that we can specify using the geom_point() function. Note that we are specifying some further optional arguments within geom_point(). First, alpha regulates the opacity of the dots. This goes from 0.0 (completely translucent) to 1.0 (completely solid fill). Second, in we defined position as jitter. This arguments slightly moves the point away from their exact location. These two arguments are desired in this plot because the personal income bands overlap - meaning most points will be drawn directly ontop of each other. Adding some transparency and noise to their position with jitter (i.e. shifts dots slightly apart), can make it easier to visualize possible patterns.\n\nnilt_subset |&gt; ggplot(aes(x = rage, y = persinc2)) +\n  geom_point(alpha = 0.7, position = \"jitter\") +\n  labs(title = \"Personal income vs age\", x = \"Age\", y = \"Personal income (£)\")\n\n\n\n\n\n\n\nThere is not a clear pattern in our previous plot. However, it is interesting to note that most of the people younger than 25 years old earn less than £20K a year. Similarly, most of the people older than 75 earn less than £20K. And only very few earn over £60k a year (looking at the top of the plot).",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#mixed-data",
    "href": "06-Lab6.html#mixed-data",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Very often we want to summarise central or spread measure by categories or groups. For example, let’s go back to the example of age and respondents’ sex. We can visualize these two variables (which include one numeric and one categorical) using a box plot. To create this, we need to specify the x and y value in aes() and include the geom_boxplot() geometry.\n\nnilt_subset |&gt; ggplot(aes(y = rage, x = rsex)) +\n  geom_boxplot() +\n  labs(title = \"Age by sex\")\n\n\n\n\n\n\n\nFrom this, we can visualize that female participants are slightly younger than their male counterparts in the sample.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#r-cheatsheets",
    "href": "06-Lab6.html#r-cheatsheets",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "There are a number of features that you can customize in your plots, including the background, text size, colours, adding more variables. But you don’t have to memorise or remember all this, one thing that is very commonly used by R data scientists are R cheat sheets! They are extremely handy when you try to create a visualisation from scratch, check out the Data Visualization with ggplot2 Cheat Sheet. An extra tip is that you can change the overall look of the plot by adding pre-defined themes. You can read more about it here. Another interesting site is the The R Graph Gallery, which includes a comprehensive showcase of plot types and their respective code.\nAs a quick example of how simply themeing is, let’s take the last graph and apply a minimal theme. All we need to do is add + after labs() followed by theme_minimal().\n\nnilt_subset |&gt; ggplot(aes(y = rage, x = rsex)) +\n  geom_boxplot() +\n  labs(title = \"Age by sex\") +\n  theme_minimal()\n\n\n\n\n\n\n\nRather than adding + theme_...() to each plot individually, you can also set a global theme using theme_set(). As this is setting a theme to use for all plots, by convention the code for this would be added to your “preamble” code chunk at the top of your R Markdown document.\n\n# Set ggplot theme\ntheme_set(theme_minimal())\n\nAfter adding (and running) the code, any code you run to create a plot will use the minimal theme.\n\n\n\n\n\n\nImportant. If you are interested in applying a theme to your plots please look at the linked resources above. Themeing plots is easy with ggplot, with a number of built-in themes available with a single function. Despite that, if you prompt genAI how to write code for a plot, it will often add a theme - even when one was not requested, add arbitrary and needlessly complex customisation, and inconsistently apply these themes/customisations across plots.\nAs a general rule of thumb, if genAI responses have code we did not cover in the labs, 99% of the time you can be confident it is spouting out absolute nonsense. As seen above, the built-in ggplot themes can be applied with a single line of code, and more complex customisation - that you will rarely, if ever, need - can be set once globally or used only when absolutely needed for a specific plot. GenAI will mislead you into thinking that creating plots with R requires 20+ lines of code rather than 3-4. It has similar issue with tables, sometimes giving 40+ lines of code to create a table that could instead be created with a single line of code.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "06-Lab6.html#activity",
    "href": "06-Lab6.html#activity",
    "title": "Visual exploratory analysis",
    "section": "",
    "text": "Using the nilt_subset object, complete the tasks below in your R Markdown file. Insert a new code chunk for each of these activities and include brief comments as regular text (i.e. outside the code chunk) to introduce or describe the plots. Feel free to copy and adapt the code to create the plots in the examples above. As covered in the lectures, you do not need to memorise the exact code. The important thing is that you understand what the code does and how to modify it for what you are trying to achieve.\n\nCreate a first-level header to start a section called “Categorical analysis”;\nCreate simple bar plot using the geom_bar() geometry to visualize the view on unionist/nationalist/neither affiliation reported by the respondents using the variable uninatid;\nBased on the plot above, create a ‘stacked bar plot’ to visualize this affiliation by religion, using the uninatid and religcat variables;\nCreate a new first-level header to start a section called “Numeric analysis”;\nCreate a scatter plot about the relationship between personal income persinc2 on the Y axis and number of hours worked a week rhourswk on the X axis;\nFinally, create a box plot to visualize personal income persinc2 on the Y axis and self-reported level of happiness ruhappy on the x axis … Interesting result, Isn’t it? Talk to your lab group-mates and tutors about your results.\nAdd your own (brief) comments to each of the plots as text in your R Markdown file;\nKnit the .Rmd document as HTML. The knitted file will be saved automatically in your project.",
    "crumbs": [
      "**Lab 6** Visual Exploratory Analysis"
    ]
  },
  {
    "objectID": "help/copilot.html",
    "href": "help/copilot.html",
    "title": "Copilot - Saved Prompts",
    "section": "",
    "text": "After you send a prompt and Copilot finishes its response, hover your mouse over your prompt and use the bookmark icon in the options that pop-up:\n\n\nThis will open a dialogue where you can give your prompt a name and then click Save to confirm:\n\n\nWhen starting a new chat, you can access your saved prompt by clicking ‘See more’ in the bottom right:\n\n\nThen scroll down and you will see the ‘Prompt Gallery’ button in the bottom-right:\n\n\nThat will open a menu with the Copilot Prompt Gallery, where along the top row there is a button to access your saved prompts:\n\n\nAnd from there you can click the box for the prompt you want and Copilot will add the text of the saved prompt to the message box, where you can then just hit enter:\n\n\nIt’s also easier to access the Copilot Prompt Gallery after you send your first prompt. Instead of needing to click see more and scrolling down, a new ‘View prompts’ button gets added on the right just above the message box:\n\n\nFinally, by default, Copilot when accessing it via your student account still uses GPT-4o. You can though switch to GPT-5 by clicking the ‘Try GPT-5’ button in the top-right:\n\n\nDespite the bad press GPT-5 got at launch, it is a significant improvement on 4o for coding questions."
  },
  {
    "objectID": "help/r basics.html",
    "href": "help/r basics.html",
    "title": "Lab Workbook",
    "section": "",
    "text": "This appendix provides additional information on using R Markdown and an overview of the structure of the R Markdown document provided with the interpretive report project template.\nPlease ensure to review the relevant lab materials:\nSimilarly, if you encounter any errors, please ensure to check the R Issues FAQ first for any existing solutions.\nThe first half of this page provides an overview of R Markdown and the second half covers the Interpretive Report Template in more detail. Importantly, please ensure you at least read the [Preamble Code Chunk] section for information on best practice for installing and loading any additional packages."
  },
  {
    "objectID": "help/r basics.html#r-basics",
    "href": "help/r basics.html#r-basics",
    "title": "Lab Workbook",
    "section": "R Basics",
    "text": "R Basics\nR is a programming language used for data analysis and visualisation. A large community has built up around R through the combination of it being free and open-source software and a full featured programming language. This makes it possible for the community to contribute to and extend upon what it is possible to do with R. The Comprehensive R Archive Network, where packages are downloaded from when you install.packages(), helps illustrate the power that comes from an open-source community. It now lists 21,810 packages created by the R community.\nThese benefits are behind the rapid adoption of R within the social sciences, and academia and data analysis more broadly. However, compared to expensive closed source software where most analysis could be done through menus and mouse-clicks, using R requires learning the programming language. This is not the disadvantage it may at first seem. It does add an additional hurdle at the very start, but it also provides a far more powerful and flexible way to do data analysis. Menus are also not as simple and accessible as may first seem. A graph that can be created with a few lines of code in R may require going through multiple menus with 20+ mouse-clicks and entering text into multiple fields.\nRStudio\nRStudio is what is known as an ‘integrated development environment’, which is just a technical way of saying that it easier to write and run R code, manage files, view outputs, and so on within a (somewhat) user-friendly interface. R strictly speaking is merely the programming language. It comes bundled with a ‘command line console’ that runs the code, which is all you would see if you opened plain ‘R’ on your desktop.\nWithin RStudio the R console is placed in the bottom-left panel. The top-left is where ‘Sources’, such as R scripts and RMarkdown files, are opened.When you run a script, RMarkdown file, or individual code chunks, the lines of code they contain are sent by RStudio to the console. RStudio will then, as appropriate, add outputs from the console below code chunks or display them in the bottom-right Viewer panel.\nThe top-right panel includes the main R environment. The environment is the work space where all data and functions are stored during a session. When code that reads in a dataset is run, it stores the read dataset as a dataframe object in the environment. Loading a package similarly adds its functions as objects available in the environment.\nObjects and Assignment\nObjects are created in R by ‘assigning’ values/data to them. For example, the code below creates objects with the names a and b with numeric values 1 and 2 assigned to them respectively.\n\na &lt;- 1\nb &lt;- 2\n\nAfter running the above, the objects can be referred to by name in subsequent code, such as adding the values they contain together:\n\na + b\n\n[1] 3\n\n\nNote though that whilst R calculates the values of the two objects combined is 3, this output is not stored, as it is not being assigned to an object. If we wanted to store the value we could assign it to a new object:\n\nc &lt;- a + b # 1 + 2 = 3, so 'c' is assigned the value 3\n\nIn contrast, if we were to assign the a + b back to a, it is effectively saying “add a and b together and update a to store the combined value”.\n\na &lt;- a + b # 1 + 2 = 3, so 'a' is now assigned the value 3\na &lt;- a + b # 3 + 2 = 5, so 'a' is now assigned the value 5\na &lt;- a + b # 5 + 2 = 7, so 'a' is now assigned the value 7\na\n\n[1] 7\n\n\nThis is why it is important to ensure you create objects for values/data you want to store and take care that you do not accidentally assign changes to objects that you do not intend to make. If you receive an object not found error it is likely you have either mistyped the name of the object or have not assigned anything to it yet. If variables suddenly disappear or your graphs unexpectedly show different results when re-run, it is likely that you ran code that assigned a changed version of the data back to the object storing it.\ndata frames\ndataframe$variable\nrage\nfunctions\nreading in data sets\npackages\ntidyverse\ndocumentation and help"
  },
  {
    "objectID": "help/InterpretiveReportStructure.html",
    "href": "help/InterpretiveReportStructure.html",
    "title": "RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "This appendix provides additional information on using R Markdown and an overview of the structure of the R Markdown document provided with the interpretive report project template.\nPlease ensure to review the relevant lab materials:\n\n\nLab 5 session on R Markdown for an intro to R Markdown.\n\nLab 10 Session for Interpreting Quantitative Findings Report summative assessment for instructions on how to create a project on RStudio Cloud for your interpretive report using the provided template.\n\nSimilarly, if you encounter any errors, please ensure to check the R Issues FAQ first for any existing solutions.\nThe first half of this page provides an overview of R Markdown and the second half covers the Interpretive Report Template in more detail. Importantly, please ensure you at least read the Preamble Code Chunk section for information on best practice for installing and loading any additional packages.\n\nR is a programming language used for data analysis and visualisation. A large community has built up around R through the combination of it being free and open-source software and a full featured programming language. This makes it possible for the community to contribute to and extend upon what it is possible to do with R. The Comprehensive R Archive Network, where packages are downloaded from when you install.packages(), helps illustrate the power that comes from an open-source community. It now lists 21,810 packages created by the R community.\nThese benefits are behind the rapid adoption of R within the social sciences, and academia and data analysis more broadly. However, compared to expensive closed source software where most analysis could be done through menus and mouse-clicks, using R requires learning the programming language. This is not the disadvantage it may at first seem. It does add an additional hurdle at the very start, but it also provides a far more powerful and flexible way to do data analysis. Menus are also not as simple and accessible as may first seem. A graph that can be created with a few lines of code in R may require going through multiple menus with 20+ mouse-clicks and entering text into multiple fields.\n\nRStudio is what is known as an ‘integrated development environment’, which is just a technical way of saying that it easier to write and run R code, manage files, view outputs, and so on within a (somewhat) user-friendly interface. R strictly speaking is merely the programming language. It comes bundled with a ‘command line console’ that runs the code, which is all you would see if you opened plain ‘R’ on your desktop.\nWithin RStudio the R console is placed in the bottom-left panel. The top-left is where ‘Sources’, such as R scripts and RMarkdown files, are opened.When you run a script, RMarkdown file, or individual code chunks, the lines of code they contain are sent by RStudio to the console. RStudio will then, as appropriate, add outputs from the console below code chunks or display them in the bottom-right Viewer panel.\nThe top-right panel includes the main R environment. The environment is the work space where all data and functions are stored during a session. When code that reads in a dataset is run, it stores the read dataset as a dataframe object in the environment. Loading a package similarly adds its functions as objects available in the environment.\n\nObjects are created in R by ‘assigning’ values/data to them. For example, the code below creates objects with the names a and b with numeric values 1 and 2 assigned to them respectively.\n\na &lt;- 1\nb &lt;- 2\n\nAfter running the above, the objects can be referred to by name in subsequent code, such as adding the values they contain together:\n\na + b\n\n[1] 3\n\n\nNote though that whilst R calculates the values of the two objects combined is 3, this output is not stored, as it is not being assigned to an object. If we wanted to store the value we could assign it to a new object:\n\nc &lt;- a + b # 1 + 2 = 3, so 'c' is assigned the value 3\n\nIn contrast, if we were to assign the a + b back to a, it is effectively saying “add a and b together and update a to store the combined value”.\n\na &lt;- a + b # 1 + 2 = 3, so 'a' is now assigned the value 3\na &lt;- a + b # 3 + 2 = 5, so 'a' is now assigned the value 5\na &lt;- a + b # 5 + 2 = 7, so 'a' is now assigned the value 7\na\n\n[1] 7\n\n\nThis is why it is important to ensure you create objects for values/data you want to store and take care that you do not accidentally assign changes to objects that you do not intend to make. If you receive an object not found error it is likely you have either mistyped the name of the object or have not assigned anything to it yet. If variables suddenly disappear or your graphs unexpectedly show different results when re-run, it is likely that you ran code that assigned a changed version of the data back to the object storing it.\n\ndataframe$variable\nrage\n\nreading in data sets\n\n\n\n\nRMarkdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with Code Chunks that can be converted to multiple file formats, such as HTML or PDF. This makes RMarkdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis.\nRMarkdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text (see the Formatting section below). Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. RMarkdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text and passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format.\n\nRMarkdown uses simple, human-readable syntax for basic formatting. Whilst different to how you would write text in a Word document, it is easy to learn. This simple transparent formatting also helps avoid the hidden complexities of Word document, where formatting issues often arise from inconsistent syntax that is hidden from the user.\nBelow is a screenshot from RStudio with text using the key basic syntax for formatting your main text:\n\nknitr::include_graphics(\"./images/markdownraw.png\")\n\nAnd, this is how it appears when knitted:\n\nknitr::include_graphics(\"./images/markdownknitted.png\")\n\nHeadings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level:\n\nknitr::include_graphics(\"./images/headingsraw.png\")\n\nAnd, how it appears when knitted:\n\nknitr::include_graphics(\"./images/headingsknitted.png\")\n\n\nThe main thing in RMarkdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between:\n\nEach paragraph\nBefore and after a list\nBefore and after a header\n\nHere’s some example text in an RMarkdown document, the first without line spacing, the second with:\n\nknitr::include_graphics(\"./images/linespacingraw.png\")\n\nAnd, how this looks when knitted:\n\nknitr::include_graphics(\"./images/linespacingknitted.png\")\n\nSide-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in RMarkdown.\n\nThere are three main ways to create a code chunk:\n\nClick the ‘Insert Code Chunk’ button and from the drop-down select ‘R’ at the top. (See gif below)\nPress Ctrl + Alt + I (Windows & Linux) or Cmd + Option + I (macOS)\nManually typing three back-ticks with ‘r’ in curly brackets before your code and adding three back-ticks after.\n\n\nknitr::include_graphics(\"./images/insertcodechunk.gif\")\n\nCurly brackets at the start of a code chunk are used to specify {programming-language optional name, options = values}. Since we are using R, all our code chunks have {r ...}. Within code chunks, the pound sign / hash, #, at the start of a line is used to add any comments:\n\nknitr::include_graphics(\"./images/codechunk.png\")\n\nSide-note, this may seem confusing given hash signs are used for headers in the main text. However, inside of a code chunk, all text is treated as code for the language specified in the curly brackets. In R, hash signs are used for comments, so they are treated as comments within the code chunk rather than as Markdown headers.\n\nAdding a label is useful as a quick way to remind yourself the purpose of each code chunk. The quick outline (button at bottom of Source panel) can be used to jump to specific sections and code chunks in your document. If you provide a label for your code chunk it will also appear here:\n\nknitr::include_graphics(\"./images/chunkaddingname.gif\")\n\n\nOptions are used to specify how code chunks are handled when run/knitted. Two key ones we have used in the labs and in the project template:\n\n\necho - whether the code chunk is displayed in knitted files.\n\ninclude - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files.\n\nBy default, the code chunk and the output are displayed in knitted files. For the interpretive report, you do not want to display the code chunks in your knitted HTML file, so ensure you add echo=FALSE to the options for any new code chunk you create. Alternatively, see the Setup Code Chunk section for information on how to set echo=FALSE as a ‘global option’.\nExamples when working in RMarkdown document:\n\nknitr::include_graphics(\"./images/echoincluderaw.png\")\n\nAnd when knitted:\n\nknitr::include_graphics(\"./images/echoincludeknitted.png\")\n\n\nThere are multiple ways to run your R code.\nWithin the top-right of each code chunk there are two buttons:\n\nknitr::include_graphics(\"./images/runchunk.png\")\n\n\nRun Current Chunk - which will run the code within that code chunks only.\nRun All Chunks Above - which will run all code chunks from the top of your document down to this code chunk.\n\nThe second is useful when a code chunk depends upon others being run first. Remember the R environment maintains the results of any previously run code, so you do not need to continuously run all previous chunks. However, this can be useful when debugging issues or if you restart your R environment, losing the results of previously run code, and needing to re-run everything before this chunk.\nFurther options can be found in the ‘Run’ drop-down menu, accessed from the top-right of the Source Panel:\n\nknitr::include_graphics(\"./images/runoptions.png\")\n\nUseful additional ones here are:\n\n\nRun Selected Line(s) - running all lines you have manually selected using the mouse / text cursor.\n\nRestart R and Run All Chunks - incredibly useful when you need to reset your R environment, this can be useful when debugging an error to figure out whether the issue stems from your current code or code you previously ran but now removed.\n\nNote as well that many of these options have keyboard shortcuts listed. Learning these pays off long-term as you will be able to write and run your code without needing to move your hands from the keyboard.\n\nKnitting a document runs all your R code from top to bottom, then combines the results from this with the Markdown text to convert these into different file formats. YAML is used to specify which file formats to convert to; see the YAML Block section for information on how this is set up within the project template.\nImportantly, when you knit an RMarkdown document, all code is run sequentially from top to bottom in a clean R environment. This ensures the document is reproducible. Anyone with a copy can knit it and produce the same results. This requires though that code to load any required packages are included within a code chunk in the document. Simply loading a package via the Console adds it to your current R environment and manually run code chunks will be able to access the package. However, when knitting, the clean R environment won’t have access to the package, and you’ll receive an error message when the knit process tries to run a function that requires the package. This is why it is important to include a code chunk at the top of your document that loads all required packages. See the Preamble Code Chunk for how to do this in the RMarkdown file provided in the interpretive report project template.\nAssuming you have YAML specifying which file format(s) to convert to, all you then need to do each time is simply click the ‘Knit’ button:\n\nknitr::include_graphics(\"./images/knitting.png\")\n\n\nIt is possible to access an RMarkdown cheat sheet (as well as ones for ggplot2 and dplyr!) from within RStudio:\n\nknitr::include_graphics(\"./images/markdowncheetsheet.gif\")\n\nNote, from the same ‘Help’ menu, just below ‘Cheat Sheets’ is an option for ‘Keyboard Shortcuts Help’. This displays a screen with most of the shortcuts available within RStudio. This also includes a link to ‘See All Shortcuts…’.\n\nknitr::include_graphics(\"./images/keyboard-shortcuts.gif\")\n\n\n\nThe top of the RMarkdown document includes a YAML block, enclosed with three dashes, ---, at the start and end. The YAML block must be at the very start of the document, so do not add any new lines before it.\nYAML is often used with Markdown as it provides a simple human-readable structure for configuring metadata, such as the document title, author, date, and desired output when knitting:\n\nknitr::include_graphics(\"./images/templateyaml.png\")\n\nPlease remember to add your student number - and not your name - to line 3.\nTo save having to remember to update the date before knitting and submitting your final version, you can use the following in the YAML block:\n\ndate: \"`r format(Sys.time(), '%d/%m/%y')`\"\n\nWhen knitted that code will automatically add the current date in dd/mm/yy format (e.g. 04/12/24):\n\nknitr::include_graphics(\"./images/knitteddate.png\")\n\n\nAfter the YAML Block, there is a ‘setup’ code chunk that sets global options for knitr. The include=FALSE in its options means that this chunk and its output are not displayed in your knitted files:\n\nknitr::include_graphics(\"./images/templatesetup.png\")\n\nGlobal options apply to all code chunks in the document. This means anything set in the global options does not have to be individually added to each code chunk. Note, you can set a global option and still set a different one for a specific code chunk where needed. For example, if you set option=FALSE globally, but needed it to be TRUE for a specific code chunk, all you need to do is add option=TRUE to its options to override the global default.\nThe global options set in the setup code chunk are:\n\n\nmessage=FALSE, which hides all non-warning messages when knitting code chunks. For example, when loading a library it will sometimes display non-warning messages.\n\nwarning=FALSE, which hides all warning messages when knitting code chunks. Again, making sure that no warnings messages displayed when running code is included in your knitted file.\n\nImportantly, you do not want to display your code chunks in your knitted file, only the tables and graphs they generate. To achieve this you could add echo=FALSE to the options for each individual code chunk. Or – you can simply add it as a global option in the setup code chunk!\n\nknitr::include_graphics(\"./images/templatesetupechofalse.png\")\n\n\nIt is best practice to install and load any required packages and read in any data being used at the top of the RMarkdown file. This keeps all package management and data reading in a single location at the start of the document, making your code simpler and clearer. It further ensures that when running/knitting your code that all required packages are loaded and dataframes created before the rest of your code is run.\nThe preamble code block within the template provides a structure following this practice.\n\nknitr::include_graphics(\"./images/templatepreamble.png\")\n\n\n\nInstall packages if missing, outlined in pink. This code may look complex, but all it does is create a list of packages that are going to be used, assigning it to the object list.of.packages. This list is checked against packages already installed, creating a new list new.packages containing only the names of packages not already installed. The final line then basically says “if the length of new.packages is 1 or more, then install all packages in the new.packages list”.\n\nWord count addin if missing, outlined in blue. There is not an R package for calculating word counts, but there is what is known as an ‘addin’ for RStudio. The code here checks if the addin is already installed and if not it downloads a copy of the addin from GitHub. Please see the Word Count Code section for further information on how this addin is used in the document.\n\nLoad packages, outlined in yellow. This is where all packages used in the analysis are loaded.\n\nRead data, outlined in red. The dataset used for the assignment in read in and assigned to the nilt dataframe object.\n\nImportant, as a dataset is provided with the project template and read in already for you, you do not need to download or read in any other dataset. The assignment uses a version of the NILT dataset that includes more variables than the version of the data we used in the lab sessions. Downloading and reading in the dataset used in the labs will result in variables disappearing from the regression results table. Please see the R Issues FAQ for more info.\nIf you require additional packages, the best way to add this would be:\n\nWithin the Install packages if missing section, add the package name at the end of the list that is being assigned to ‘list.of.packages’\nWithin the Load packages section, add a new library() line under the existing ones.\n\nFor example, if needing to install and load ‘vtable’ these are the changes that you would make:\n\nknitr::include_graphics(\"./images/addingvtable.png\")\n\nAs a gif:\n\nknitr::include_graphics(\"./images/addingpackages.gif\")\n\nNote, moving all code for installing and loading of packages into this preamble code chunk means you do not need, and can safely remove, any other install.package() and library() lines from the rest of your code chunks. This will reduce risk of encountering error messages and make it easier to debug them if any do occur.\n\nTables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. Note, you will need to knit your document each time you want to check the updated word count.\nThis is how the code to calculate the word count looks within the RMarkdown file:\n\nknitr::include_graphics(\"./images/wordcountinline.png\")\n\nSide-note, surrounding code with single back-ticks, (`), creates an “inline code chunk”, enabling you to add short snippets of code. The r at the start specifies that the code is R, similar to adding r in curly brackets for code chunks.\nThis is then how it looks when knitting the project template (assuming you haven’t added any additional text yet):\n\nknitr::include_graphics(\"./images/knittedwordcount.png\")\n\nWe use this addin as the built-in RStudio word count, accessed via the ‘Edit’ menu at the top of the screen, includes all code and comments. So, despite the addin calculating ‘0’, RStudio will provide:\n\nknitr::include_graphics(\"./images/rstudiowordcount.png\")\n\nImportant, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to wordcountaddin::word_count(\"Assignmet2-template.Rmd\") - 193.\n\nThe RMarkdown document provided in the project template includes headed sections you can use alongside suggested word count for each and brief summary reminder of what to include in each section. The Course Handbook, pages 26-30, provide a more detailed breakdown of what to include in each section.\nScreenshot of the outline and commented suggestions:\n\nknitr::include_graphics(\"./images/templateoutline.png\")\n\nImportant, within the “Results and discussion” section, ensure to add your interpretation after the code chunk that creates the regression results table:\n\nknitr::include_graphics(\"./images/regressioninterpretativeaftercode.png\")\n\n\nThe code chunk that runs the multiple linear regression model and creates the table with the regression results can be found in the ‘Results and discussion’ section:\n\nknitr::include_graphics(\"./images/templateresults.png\")\n\n\nFirst the model is run, outlined in pink, and assigned to the model object.\nNext a list with labels to use for the independent and control variables in the regression table is created, outlined in blue, and assigned to the cov_labels object.\nFinally, the stargazer package is used to create the regression results table, outlined in red. It is passed the model and cov_label objects, highlighted in yellow, alongside arguments for outputting the table in HTML, a title, and caption & label to use for the dependent variable.\n\nStargazer produces well-formatted regression tables, but with the downside that when running the code chunk in RStudio, you will only see the raw HTML syntax that is created:\n\nknitr::include_graphics(\"./images/templatehtmltable.png\")\n\nIn order to view the table, you will need to knit your document and view the outputted HTML file:\n\nknitr::include_graphics(\"./images/knitting.png\")\n\nYour knitted HTML file should then open in a new popup window. However, you might instead receive a dialogue window saying that a pop-up was prevented from opening, if so just click ‘Try again’:\n\nknitr::include_graphics(\"./images/tryagain.png\")\n\nAlternatively, you can open the last knitted version of your HTML file from the ‘Files’ panel. Click the HTML file and select ‘View in Web Browser’:\n\nknitr::include_graphics(\"./images/openknittedhtml.gif\")\n\nBelow is how your regression results table should look in the knitted HTML file:\n\nknitr::include_graphics(\"./images/knittedresultstable.png\")\n\nIf it doesn’t, please see the R Issues FAQ.",
    "crumbs": [
      "RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/InterpretiveReportStructure.html#introduction",
    "href": "help/InterpretiveReportStructure.html#introduction",
    "title": "RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "This appendix provides additional information on using R Markdown and an overview of the structure of the R Markdown document provided with the interpretive report project template.\nPlease ensure to review the relevant lab materials:\n\n\nLab 5 session on R Markdown for an intro to R Markdown.\n\nLab 10 Session for Interpreting Quantitative Findings Report summative assessment for instructions on how to create a project on RStudio Cloud for your interpretive report using the provided template.\n\nSimilarly, if you encounter any errors, please ensure to check the R Issues FAQ first for any existing solutions.\nThe first half of this page provides an overview of R Markdown and the second half covers the Interpretive Report Template in more detail. Importantly, please ensure you at least read the Preamble Code Chunk section for information on best practice for installing and loading any additional packages.",
    "crumbs": [
      "RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/InterpretiveReportStructure.html#r-basics",
    "href": "help/InterpretiveReportStructure.html#r-basics",
    "title": "RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "R is a programming language used for data analysis and visualisation. A large community has built up around R through the combination of it being free and open-source software and a full featured programming language. This makes it possible for the community to contribute to and extend upon what it is possible to do with R. The Comprehensive R Archive Network, where packages are downloaded from when you install.packages(), helps illustrate the power that comes from an open-source community. It now lists 21,810 packages created by the R community.\nThese benefits are behind the rapid adoption of R within the social sciences, and academia and data analysis more broadly. However, compared to expensive closed source software where most analysis could be done through menus and mouse-clicks, using R requires learning the programming language. This is not the disadvantage it may at first seem. It does add an additional hurdle at the very start, but it also provides a far more powerful and flexible way to do data analysis. Menus are also not as simple and accessible as may first seem. A graph that can be created with a few lines of code in R may require going through multiple menus with 20+ mouse-clicks and entering text into multiple fields.\n\nRStudio is what is known as an ‘integrated development environment’, which is just a technical way of saying that it easier to write and run R code, manage files, view outputs, and so on within a (somewhat) user-friendly interface. R strictly speaking is merely the programming language. It comes bundled with a ‘command line console’ that runs the code, which is all you would see if you opened plain ‘R’ on your desktop.\nWithin RStudio the R console is placed in the bottom-left panel. The top-left is where ‘Sources’, such as R scripts and RMarkdown files, are opened.When you run a script, RMarkdown file, or individual code chunks, the lines of code they contain are sent by RStudio to the console. RStudio will then, as appropriate, add outputs from the console below code chunks or display them in the bottom-right Viewer panel.\nThe top-right panel includes the main R environment. The environment is the work space where all data and functions are stored during a session. When code that reads in a dataset is run, it stores the read dataset as a dataframe object in the environment. Loading a package similarly adds its functions as objects available in the environment.\n\nObjects are created in R by ‘assigning’ values/data to them. For example, the code below creates objects with the names a and b with numeric values 1 and 2 assigned to them respectively.\n\na &lt;- 1\nb &lt;- 2\n\nAfter running the above, the objects can be referred to by name in subsequent code, such as adding the values they contain together:\n\na + b\n\n[1] 3\n\n\nNote though that whilst R calculates the values of the two objects combined is 3, this output is not stored, as it is not being assigned to an object. If we wanted to store the value we could assign it to a new object:\n\nc &lt;- a + b # 1 + 2 = 3, so 'c' is assigned the value 3\n\nIn contrast, if we were to assign the a + b back to a, it is effectively saying “add a and b together and update a to store the combined value”.\n\na &lt;- a + b # 1 + 2 = 3, so 'a' is now assigned the value 3\na &lt;- a + b # 3 + 2 = 5, so 'a' is now assigned the value 5\na &lt;- a + b # 5 + 2 = 7, so 'a' is now assigned the value 7\na\n\n[1] 7\n\n\nThis is why it is important to ensure you create objects for values/data you want to store and take care that you do not accidentally assign changes to objects that you do not intend to make. If you receive an object not found error it is likely you have either mistyped the name of the object or have not assigned anything to it yet. If variables suddenly disappear or your graphs unexpectedly show different results when re-run, it is likely that you ran code that assigned a changed version of the data back to the object storing it.\n\ndataframe$variable\nrage\n\nreading in data sets",
    "crumbs": [
      "RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/InterpretiveReportStructure.html#rmarkdown",
    "href": "help/InterpretiveReportStructure.html#rmarkdown",
    "title": "RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "RMarkdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with Code Chunks that can be converted to multiple file formats, such as HTML or PDF. This makes RMarkdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis.\nRMarkdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text (see the Formatting section below). Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. RMarkdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text and passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format.\n\nRMarkdown uses simple, human-readable syntax for basic formatting. Whilst different to how you would write text in a Word document, it is easy to learn. This simple transparent formatting also helps avoid the hidden complexities of Word document, where formatting issues often arise from inconsistent syntax that is hidden from the user.\nBelow is a screenshot from RStudio with text using the key basic syntax for formatting your main text:\n\nknitr::include_graphics(\"./images/markdownraw.png\")\n\nAnd, this is how it appears when knitted:\n\nknitr::include_graphics(\"./images/markdownknitted.png\")\n\nHeadings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level:\n\nknitr::include_graphics(\"./images/headingsraw.png\")\n\nAnd, how it appears when knitted:\n\nknitr::include_graphics(\"./images/headingsknitted.png\")\n\n\nThe main thing in RMarkdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between:\n\nEach paragraph\nBefore and after a list\nBefore and after a header\n\nHere’s some example text in an RMarkdown document, the first without line spacing, the second with:\n\nknitr::include_graphics(\"./images/linespacingraw.png\")\n\nAnd, how this looks when knitted:\n\nknitr::include_graphics(\"./images/linespacingknitted.png\")\n\nSide-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in RMarkdown.\n\nThere are three main ways to create a code chunk:\n\nClick the ‘Insert Code Chunk’ button and from the drop-down select ‘R’ at the top. (See gif below)\nPress Ctrl + Alt + I (Windows & Linux) or Cmd + Option + I (macOS)\nManually typing three back-ticks with ‘r’ in curly brackets before your code and adding three back-ticks after.\n\n\nknitr::include_graphics(\"./images/insertcodechunk.gif\")\n\nCurly brackets at the start of a code chunk are used to specify {programming-language optional name, options = values}. Since we are using R, all our code chunks have {r ...}. Within code chunks, the pound sign / hash, #, at the start of a line is used to add any comments:\n\nknitr::include_graphics(\"./images/codechunk.png\")\n\nSide-note, this may seem confusing given hash signs are used for headers in the main text. However, inside of a code chunk, all text is treated as code for the language specified in the curly brackets. In R, hash signs are used for comments, so they are treated as comments within the code chunk rather than as Markdown headers.\n\nAdding a label is useful as a quick way to remind yourself the purpose of each code chunk. The quick outline (button at bottom of Source panel) can be used to jump to specific sections and code chunks in your document. If you provide a label for your code chunk it will also appear here:\n\nknitr::include_graphics(\"./images/chunkaddingname.gif\")\n\n\nOptions are used to specify how code chunks are handled when run/knitted. Two key ones we have used in the labs and in the project template:\n\n\necho - whether the code chunk is displayed in knitted files.\n\ninclude - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files.\n\nBy default, the code chunk and the output are displayed in knitted files. For the interpretive report, you do not want to display the code chunks in your knitted HTML file, so ensure you add echo=FALSE to the options for any new code chunk you create. Alternatively, see the Setup Code Chunk section for information on how to set echo=FALSE as a ‘global option’.\nExamples when working in RMarkdown document:\n\nknitr::include_graphics(\"./images/echoincluderaw.png\")\n\nAnd when knitted:\n\nknitr::include_graphics(\"./images/echoincludeknitted.png\")\n\n\nThere are multiple ways to run your R code.\nWithin the top-right of each code chunk there are two buttons:\n\nknitr::include_graphics(\"./images/runchunk.png\")\n\n\nRun Current Chunk - which will run the code within that code chunks only.\nRun All Chunks Above - which will run all code chunks from the top of your document down to this code chunk.\n\nThe second is useful when a code chunk depends upon others being run first. Remember the R environment maintains the results of any previously run code, so you do not need to continuously run all previous chunks. However, this can be useful when debugging issues or if you restart your R environment, losing the results of previously run code, and needing to re-run everything before this chunk.\nFurther options can be found in the ‘Run’ drop-down menu, accessed from the top-right of the Source Panel:\n\nknitr::include_graphics(\"./images/runoptions.png\")\n\nUseful additional ones here are:\n\n\nRun Selected Line(s) - running all lines you have manually selected using the mouse / text cursor.\n\nRestart R and Run All Chunks - incredibly useful when you need to reset your R environment, this can be useful when debugging an error to figure out whether the issue stems from your current code or code you previously ran but now removed.\n\nNote as well that many of these options have keyboard shortcuts listed. Learning these pays off long-term as you will be able to write and run your code without needing to move your hands from the keyboard.\n\nKnitting a document runs all your R code from top to bottom, then combines the results from this with the Markdown text to convert these into different file formats. YAML is used to specify which file formats to convert to; see the YAML Block section for information on how this is set up within the project template.\nImportantly, when you knit an RMarkdown document, all code is run sequentially from top to bottom in a clean R environment. This ensures the document is reproducible. Anyone with a copy can knit it and produce the same results. This requires though that code to load any required packages are included within a code chunk in the document. Simply loading a package via the Console adds it to your current R environment and manually run code chunks will be able to access the package. However, when knitting, the clean R environment won’t have access to the package, and you’ll receive an error message when the knit process tries to run a function that requires the package. This is why it is important to include a code chunk at the top of your document that loads all required packages. See the Preamble Code Chunk for how to do this in the RMarkdown file provided in the interpretive report project template.\nAssuming you have YAML specifying which file format(s) to convert to, all you then need to do each time is simply click the ‘Knit’ button:\n\nknitr::include_graphics(\"./images/knitting.png\")\n\n\nIt is possible to access an RMarkdown cheat sheet (as well as ones for ggplot2 and dplyr!) from within RStudio:\n\nknitr::include_graphics(\"./images/markdowncheetsheet.gif\")\n\nNote, from the same ‘Help’ menu, just below ‘Cheat Sheets’ is an option for ‘Keyboard Shortcuts Help’. This displays a screen with most of the shortcuts available within RStudio. This also includes a link to ‘See All Shortcuts…’.\n\nknitr::include_graphics(\"./images/keyboard-shortcuts.gif\")",
    "crumbs": [
      "RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/InterpretiveReportStructure.html#interpretive-report-template",
    "href": "help/InterpretiveReportStructure.html#interpretive-report-template",
    "title": "RMarkdown & the Interpretive Report Template",
    "section": "",
    "text": "The top of the RMarkdown document includes a YAML block, enclosed with three dashes, ---, at the start and end. The YAML block must be at the very start of the document, so do not add any new lines before it.\nYAML is often used with Markdown as it provides a simple human-readable structure for configuring metadata, such as the document title, author, date, and desired output when knitting:\n\nknitr::include_graphics(\"./images/templateyaml.png\")\n\nPlease remember to add your student number - and not your name - to line 3.\nTo save having to remember to update the date before knitting and submitting your final version, you can use the following in the YAML block:\n\ndate: \"`r format(Sys.time(), '%d/%m/%y')`\"\n\nWhen knitted that code will automatically add the current date in dd/mm/yy format (e.g. 04/12/24):\n\nknitr::include_graphics(\"./images/knitteddate.png\")\n\n\nAfter the YAML Block, there is a ‘setup’ code chunk that sets global options for knitr. The include=FALSE in its options means that this chunk and its output are not displayed in your knitted files:\n\nknitr::include_graphics(\"./images/templatesetup.png\")\n\nGlobal options apply to all code chunks in the document. This means anything set in the global options does not have to be individually added to each code chunk. Note, you can set a global option and still set a different one for a specific code chunk where needed. For example, if you set option=FALSE globally, but needed it to be TRUE for a specific code chunk, all you need to do is add option=TRUE to its options to override the global default.\nThe global options set in the setup code chunk are:\n\n\nmessage=FALSE, which hides all non-warning messages when knitting code chunks. For example, when loading a library it will sometimes display non-warning messages.\n\nwarning=FALSE, which hides all warning messages when knitting code chunks. Again, making sure that no warnings messages displayed when running code is included in your knitted file.\n\nImportantly, you do not want to display your code chunks in your knitted file, only the tables and graphs they generate. To achieve this you could add echo=FALSE to the options for each individual code chunk. Or – you can simply add it as a global option in the setup code chunk!\n\nknitr::include_graphics(\"./images/templatesetupechofalse.png\")\n\n\nIt is best practice to install and load any required packages and read in any data being used at the top of the RMarkdown file. This keeps all package management and data reading in a single location at the start of the document, making your code simpler and clearer. It further ensures that when running/knitting your code that all required packages are loaded and dataframes created before the rest of your code is run.\nThe preamble code block within the template provides a structure following this practice.\n\nknitr::include_graphics(\"./images/templatepreamble.png\")\n\n\n\nInstall packages if missing, outlined in pink. This code may look complex, but all it does is create a list of packages that are going to be used, assigning it to the object list.of.packages. This list is checked against packages already installed, creating a new list new.packages containing only the names of packages not already installed. The final line then basically says “if the length of new.packages is 1 or more, then install all packages in the new.packages list”.\n\nWord count addin if missing, outlined in blue. There is not an R package for calculating word counts, but there is what is known as an ‘addin’ for RStudio. The code here checks if the addin is already installed and if not it downloads a copy of the addin from GitHub. Please see the Word Count Code section for further information on how this addin is used in the document.\n\nLoad packages, outlined in yellow. This is where all packages used in the analysis are loaded.\n\nRead data, outlined in red. The dataset used for the assignment in read in and assigned to the nilt dataframe object.\n\nImportant, as a dataset is provided with the project template and read in already for you, you do not need to download or read in any other dataset. The assignment uses a version of the NILT dataset that includes more variables than the version of the data we used in the lab sessions. Downloading and reading in the dataset used in the labs will result in variables disappearing from the regression results table. Please see the R Issues FAQ for more info.\nIf you require additional packages, the best way to add this would be:\n\nWithin the Install packages if missing section, add the package name at the end of the list that is being assigned to ‘list.of.packages’\nWithin the Load packages section, add a new library() line under the existing ones.\n\nFor example, if needing to install and load ‘vtable’ these are the changes that you would make:\n\nknitr::include_graphics(\"./images/addingvtable.png\")\n\nAs a gif:\n\nknitr::include_graphics(\"./images/addingpackages.gif\")\n\nNote, moving all code for installing and loading of packages into this preamble code chunk means you do not need, and can safely remove, any other install.package() and library() lines from the rest of your code chunks. This will reduce risk of encountering error messages and make it easier to debug them if any do occur.\n\nTables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. Note, you will need to knit your document each time you want to check the updated word count.\nThis is how the code to calculate the word count looks within the RMarkdown file:\n\nknitr::include_graphics(\"./images/wordcountinline.png\")\n\nSide-note, surrounding code with single back-ticks, (`), creates an “inline code chunk”, enabling you to add short snippets of code. The r at the start specifies that the code is R, similar to adding r in curly brackets for code chunks.\nThis is then how it looks when knitting the project template (assuming you haven’t added any additional text yet):\n\nknitr::include_graphics(\"./images/knittedwordcount.png\")\n\nWe use this addin as the built-in RStudio word count, accessed via the ‘Edit’ menu at the top of the screen, includes all code and comments. So, despite the addin calculating ‘0’, RStudio will provide:\n\nknitr::include_graphics(\"./images/rstudiowordcount.png\")\n\nImportant, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to wordcountaddin::word_count(\"Assignmet2-template.Rmd\") - 193.\n\nThe RMarkdown document provided in the project template includes headed sections you can use alongside suggested word count for each and brief summary reminder of what to include in each section. The Course Handbook, pages 26-30, provide a more detailed breakdown of what to include in each section.\nScreenshot of the outline and commented suggestions:\n\nknitr::include_graphics(\"./images/templateoutline.png\")\n\nImportant, within the “Results and discussion” section, ensure to add your interpretation after the code chunk that creates the regression results table:\n\nknitr::include_graphics(\"./images/regressioninterpretativeaftercode.png\")\n\n\nThe code chunk that runs the multiple linear regression model and creates the table with the regression results can be found in the ‘Results and discussion’ section:\n\nknitr::include_graphics(\"./images/templateresults.png\")\n\n\nFirst the model is run, outlined in pink, and assigned to the model object.\nNext a list with labels to use for the independent and control variables in the regression table is created, outlined in blue, and assigned to the cov_labels object.\nFinally, the stargazer package is used to create the regression results table, outlined in red. It is passed the model and cov_label objects, highlighted in yellow, alongside arguments for outputting the table in HTML, a title, and caption & label to use for the dependent variable.\n\nStargazer produces well-formatted regression tables, but with the downside that when running the code chunk in RStudio, you will only see the raw HTML syntax that is created:\n\nknitr::include_graphics(\"./images/templatehtmltable.png\")\n\nIn order to view the table, you will need to knit your document and view the outputted HTML file:\n\nknitr::include_graphics(\"./images/knitting.png\")\n\nYour knitted HTML file should then open in a new popup window. However, you might instead receive a dialogue window saying that a pop-up was prevented from opening, if so just click ‘Try again’:\n\nknitr::include_graphics(\"./images/tryagain.png\")\n\nAlternatively, you can open the last knitted version of your HTML file from the ‘Files’ panel. Click the HTML file and select ‘View in Web Browser’:\n\nknitr::include_graphics(\"./images/openknittedhtml.gif\")\n\nBelow is how your regression results table should look in the knitted HTML file:\n\nknitr::include_graphics(\"./images/knittedresultstable.png\")\n\nIf it doesn’t, please see the R Issues FAQ.",
    "crumbs": [
      "RMarkdown & the Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/rstudio_helper.html",
    "href": "help/rstudio_helper.html",
    "title": "RStudio Helper GPT",
    "section": "",
    "text": "RStudio Helper is designed to address common issues when using GenAI to learn quantitative data analysis with R.\nYou can access it as a Copilot Saved Prompt and a ChatGPT Custom GPT:\nLink to Copilot Saved Prompt\nLink to ChatGPT Custom GPT\nOn this page, you will also find:\nTo be clear, your GenAI use on this course is not limited to RStudio Helper. Instead, it is made available to help you get more useful responses from GenAI. The instructions with explanations are provided on this page, so you can see how to achieve similar when writing your own prompts. Overtime as you build your quants skills and awareness of common issues in GenAI responses, you will want to experiment with writing prompts tailored to your needs and use-cases.\nWhilst this GPT is also set up with instructions to mitigate GenAI’s predilection towards undermining learning and disregard for academic integrity, due to the inherent limitations with GenAI care still needs to be taken when using it. Please ensure to read the information about permitted GenAI use for the assessments and the SLD’s Quick Guidance for Students page.\nPlease also read the Don’t Believe GenAI Bullshit section of the R Issues FAQ page. GenAI is useful but inherently flawed. When generating its responses it does not distinguish between fiction and fact. As a result, if you do not have a general understanding of the area / topic you are using it for, you will not have a sense of when its responses are nonsense."
  },
  {
    "objectID": "help/rstudio_helper.html#what-problems-does-this-gpt-address",
    "href": "help/rstudio_helper.html#what-problems-does-this-gpt-address",
    "title": "RStudio Helper GPT",
    "section": "What problems does this GPT address?",
    "text": "What problems does this GPT address?\nA key issue with GenAI models - such as ChatGPT, Gemini, and Claude - is that by default they are over-eager to do as much work as possible for you. The default response structure is “short opener, work done on behalf of user, suggestions for what else it can do”. An easy way to see this is to prompt GenAI “I need help improving the clarity of this paragraph -” along with a paragraph of text. More often than not, it will response with “Here’s a clearer and more concise version of your paragraph” and end with overly terse explanation for the changes. Sometimes it won’t even bother to offer an explanation.\nGenAI is also nowhere near as ‘intelligent’ as AI companies and online AI boosters claim. This is especially the case when using GenAI for R. GenAI models can generate over-convulted code, sometimes providing 10+ lines of code for something that can be done in 1-4 lines instead. Without including additional relevant information, responses tend towards being abstract and assuming a reasonable degree of prior knowledge. This can result in misleading and confusing responses as key information useful for beginners gets left out. Responses can also include fabricated data, whilst claiming the code it is spitting out is based on the dataset you provided it."
  },
  {
    "objectID": "help/rstudio_helper.html#instructions",
    "href": "help/rstudio_helper.html#instructions",
    "title": "RStudio Helper GPT",
    "section": "Instructions",
    "text": "Instructions\nBelow is a copy of the instructions used to create RStudio Helper GPT. If you hover your mouse over the gray box, you’ll see a clipboard appear in the top-right of it, that you can clip to copy the instructions in full to the clipboard.\n\n\n## Role and General Interaction\n\nRStudio Helper assists users learn R for quantitative social science. Users are honours-level undergraduate social science students. They are new to quantitative methods, statistics, and R. They are using RStudio via Posit Cloud, the tidyverse package, and R Markdown. Users are based in the UK, so use UK measurements.\n\nYou provide actionable advice through textbook style explanations and code chunks with detailed accessible documentation that breaks down and explains the code bit by bit. When users provide their own code with an error message or ask about writing code for a specific dataset, you always continue using textbook style examples and accessible documentation to guide users in learning how to debug error messages and write code themselves. Where appropriate include information relevant for data analysis and interpretation within the social sciences rather than making abstract simplistic statements about 'good' sample sizes and model fit results. \n\nYou have an ardent indefatigable desire to aid students learn quantitative analysis, doing so by giving detailed beginner-friendly explanations in a formal but friendly tone that ALWAYS follow the 'Golden Rules' below.\n\n## Golden Rules\n\nRule one: Support students in their learning, NEVER do the work for them. Academic integrity must always be maintained. Under no circumstances do you ever directly fix code provided to you, write code for specific datasets that can be copy and pasted, nor interpret statistical results on behalf of the user.\n\nRule two: Across all forms of response, NEVER use the exact dataset, variables, and values if these are provided by the user. You can use analogous examples, but keep it general. If a user is asking about a categorical variable on 'religcat' that stores value of respondents' religion, give a textbook example with another categorical variable such as employment. If they mention a variable for number of children, use an example for number of jobs. Never use an overly similar example, such as using 'annual income' in your example if the user mentioned 'income' or 'monthly income'.\n\nRule three: NEVER interpret statistical results for the user. If they provide a copy of a plot, table, or similar, NEVER interpret these for them. Instead give a general textbook explanation for how the type of graph, table, and so on can be interpreted, avoiding all specifics of what was provided to you. Within your explanation follow rule two and NEVER use the same variables and statistical results as provided by the user. For example, if their prompt mentioned employment status, use a different categorical variable for your explanation. Similarly, use different values and statistical results to the ones provided by the user.\n\nRule four: NEVER assume information about variables mentioned by the user. If a user mentions a variable for 'age' do not write a full response assuming it is interval or categorical. Instead first ask the user to clarify, with details for how they can check using R. Only once you have this information should you provide a full response.\n\n## Contextual Responses\n\nAdapt responses to the context of the learning environment. Write accessible explanations for social science students who are new to quantitative data analysis, RStudio, R, tidyverse, and R Markdown. The structure of R Markdown files and code chunks should follow best data analysis and coding practices.\n\n- Always use the tidyverse, 'tidyverse friendly' packages, and vtable or modelsummary for table packages.\n- Prefer `|&gt;` over `%&gt;%` for pipes.\n- Load the tidyverse rather than any specific individual package from it - installing it if have not done so already for the current project. For example, if needing ggplot2, load the tidyverse package and explain ggplot2 is part of the tidyverse.\n- When loading libraries, ALWAYS explain how to do this through a code chunk at the top of the R Markdown file with a reminder that libraries only need to be loaded once. NEVER provide code for loading libraries and analysis together in one code chunk.\n- When appropriate, remind users they can set global options through a code chunk at the top of their R Markdown file.\n- Make responses accessible by explaining all R & data analysis terms each time they are first used. Users are absolute beginners to R & may not know what terms like data frame, library, vector, function, object, plot, & so on mean.\n- Refer to relevant panels within RStudio, such as the Environment panel for checking a data frame or when installing a library explain how to install it through RStudio's console. Include a reminder of where on the screen the panel can be found.\n- Where relevant, inform users when the code covered returns console output, why not to use console output in knitted documents, and follow-up with suggestions for producing formatted outputs for knitted documents.\n- When customising ggplot plots, use existing complete themes or `theme()`, explaining how this supports a consistent look, and DO NOT hard-code arbitrary theme customisation into individual plots.\n- In code examples that require a dataset, either 1. write example code for how to load a dataset from a file such as csv or spss or 2. use a dataset that comes packaged with R or the tidyverse. NEVER write example data using vectors and for loops, as that is not real code students would use in practice.\n- For knitting issues, ask the user to check the YAML output field is html_document only as R table packages can require additional code to work with other file formats\n\n## Example response structure\n\nIn general, structure responses to provide a general explanation, more detailed breakdown, and summary of key information.\n\nFor example, when a user asks about an error message in their code:\n\n1. Explain what the error message means, including any technical jargon, with examples. Ask for more details about the error message if the initial question was vague. \n2. Explain step-by-step how to debug, trace, and fix the issue that produced the error. Include details when relevant for RStudio, Posit Cloud, tidyverse, and R Markdown. Remember to follow best practices, such as not loading libraries at the start of each code chunk, instead advising to load the library in a code chunk at the top of the R Markdown file. If a copy of the code was provided, help the user debug step by step and DO NOT simply rewrite the code for the user. Stick to analogous textbook examples, nothing that can be copied and pasted. \n3. Provide a summary checklist the user can use when encountering similar error messages in future.\n\nWhen a user asks to create a plot for a specific variable:\n\n1. Note that you are unable to provide the exact code to use, but can explain how to create a plot through a textbook example.\n2. Explain which variable types the plot should be used for.\n2. Explain the example step-by-step, from loading the tidyverse to writing the code with ggplot.\n3. Provide beginner-friendly and accessible documentation for how to create the plot type in general using ggplot.\n4. Provide a summary checklist with which variable types to use the plot for and the steps for creating the plot with ggplot.\n\n## Ending Responses\n\nBe proactive in building user understanding and encouraging exploration by ending responses with:\n\n- A 'Did You Know?' section with relevant tips and further information. For example, if the prompt was about creating a plot with ggplot, include information on customising colours. Similarly, provide tips, suggestions, and further into on RStudio, R Markdown, and the tidyverse where pertinent to the user's prompt.\n- A 'Explain Terminology' section that ALWAYS informs the user they can reply \"Explain all\" OR \"Explain [term]\" for more in-depth explanations of R & data analysis terms used in the response.\n\n## Formatting\n\nWhen nesting R code chunks inside code blocks use four backticks for the code block so the triple ones inside for R chunks are preserved."
  },
  {
    "objectID": "help/rstudio_helper.html#conversation-starters",
    "href": "help/rstudio_helper.html#conversation-starters",
    "title": "RStudio Helper GPT",
    "section": "Conversation Starters",
    "text": "Conversation Starters\nSome example prompts to try with RStudio Helper:\n\nWhat are the benefits of the tidyverse compared to base R?\nWhat are the steps to debug and fix an object not found error?\nHow is the mutate() function used?\nWhat are all the different ways to create and run R code chunks in RStudio?\nHow do I load my dataset into my R environment?\nHow can I go beyond standard boilerplate interpretations of statistical results?\nWhat can I create with R Markdown / Quarto beyond formatted reports?\nWhy does ‘#’ behave differently depending on whether it is inside or outwith a code chunk?\n\nProvide a summary of key R Markdown syntax covering headings, text, and code chunks. 2. Explain YAML and what variables can control with the YAML header in R Markdown files.\n\n\nExplain bit by bit how the “knitr::opts_chunk$set(…)” code that RStudio adds to top of its R Markdown template works 2. explain how I can set global and code chunk specific options\n\nExplain all basic R syntax and terms. Focus on those relevant for using R and tidyverse for quantitative analysis and working with data frames, including what symbols like ‘$’ do..\nHelp! My file won’t knit. All my code chunks run fine in RStudio. What can I check to figure out what is causing the problem?"
  },
  {
    "objectID": "help/rstudio_helper.html#breakdown",
    "href": "help/rstudio_helper.html#breakdown",
    "title": "RStudio Helper GPT",
    "section": "Breakdown",
    "text": "Breakdown\nBelow are bullet points explaining the reason behind what was included in each main section of the instructions.\nRole and General Interaction:\nThis section provides a role to the GenAI, some background context, and general initial instructions to shape responses and interaction with the user.\n\nOpens with information that it “assists users learn R for quantitative social science”.\nProvides general info about users, including what software and tools they will be using.\nA paragraph covers the general response principles, guiding it towards responses that are more equivalent to what would find in a textbook / online help page, rather than “here’s code to copy and paste”.\nEnds with a restatement that it is to support learning and provide “beginner-friendly explanations”.\n\nYou will see aspects from this initial section repeated across others. With shorter prompts you do not have to repeat information as often. However, the longer the prompt, usually the more often you need to give reminders of instructions that go against GenAI mode’s default behaviours.\nGolden Rules:\nGenAI models do not care about academic integrity and are incredibly bad at providing responses that support learning.\n\nRule one re-emphasises the importance of academic integrity with clarification of what that means for its responses.\nRule two encourages it to provide better responses that supports learning. Without this, prompts mentioning a variable for annual income would receive a response using a monthly income variable. This ensures responses still use comparable variables, just not ones that are overly similar.\nRule three further clarifies what not doing work for the user involves. Again without this, and despite all the rest of the instructions, GenAI models will too often default to spitting out its own interpretation rather than explaining how to interpret.\nRule four addresses the issue of GenAI being over-eager to make assumptions. When information is unclear, GenAI frequently makes ‘best guess’ assumptions rather than clarifying first. It will then happily spit out information based on wrong assumptions, and will a lot of time not even mention the assumptions made in the response.\n\nContextual Responses:\nThis section opens by re-inforcing that users are new to quantitative analysis and R. It repeats the key software and packages we are using on the course, and then has a long bullet point list to address common issues with default responses.\n\nUse the tidyverse and other packages covered in the labs. Without this information GenAI models tend towards generating overly code using ‘base R’ with no packages. That can result in 20+ lines of code for something you can do in one line with the tidyverse.\nInstruction to use the new built-in R pipe |&gt; over the older %&gt;%. Both will work, but it is advised to use the new one. Given it is relatively new, most of the data used to train GenAI uses the older pipe, which it defaults to unless told otherwise.\nAlways load the tidyverse rather than any subpackages. The tidyverse is a collection of packages. For example, by loading the tidyverse you are also loading dyplr, pplot2, and other packages. GenAI models, even when the initial prompt says that you are using the tidyverse, will spit out code that loads these packages individually rather than the tidyverse itself.\nInstruction to follow good data analysis practice and load all libraries used once in a code chunk at the top of the R Markdown file. GenAI responses instead tend to load the same packages again and again in every single code chunk it generates.\nSimilarly, when asking about how to set options for code chunks, responses tend to explain how to change these per code chunk, rather than informing users they can also set these globally.\nYet another reminder for it to explain all terms used, with a list of example terms. (You’ll notice GenAI responses still often fail to provide explanations for these.)\nA reminder to include any info about RStudio relevant to the prompt, helping the responses be less abstract.\nNote that it should inform users when they can create nicer formatted outputs for use in knitted documents rather than code which returns ‘raw outputs’.\nInstruction for it use existing ggplot themes and the theme() function. GenAI models have horrendous habit when prompted to customise a plot of adding dozens of individual lines of code for a specific plot. Not only does this result in a lot of unnecessary code, as it is specific to that plot, you have to repeat it for any other plots you are making as well. If you prompt genAI to do that, it will claim to have done the same, but often each time it spits out code for a new plot it’ll introduce subtle inconsistencies into the theming it is adding.\nGenAI models often default to generating code to fabricate a dataset. The instructions here make clear it should never generate fake data. Instead it should generate code for loading data from file or generate code using example datasets that are provided already with R and R packages.\nThe final bullet in the section advises it to ask users to check the YAML header in their R Markdown file if they report a knitting error. The most common cause of knitting issues on the module is from accidentally adding PDF as a file format to output when knitting. Some R table packages though require additional code when knitting to PDF, without which your file will fail to knit with vague error messages. Despite how common this is, GenAI will lead you down a path of installing 101 different packages and writing custom functions, all of which is entirely unnecessary and none of which will actually solve the issue.\n\nExample response structure:\nWith longer instructions it can help improve consistency to also include example(s) for how responses should be structured.\n\nThe first example sets out how to structure a response to a prompt about an error message, that re-emphasises the key information to include in its responses.\nThe second example sets out how to response to a prompt in a way consistent with the ‘Golden Rules’.\nAcross both, you will see that there is constant reiteration of ‘explain’, ‘explain’, ‘explain’. After explaining, responses should then ‘provide’ summary information that can add to your notes.\n\nEnding Responses:\nThe instructions here aim to flag other information you might find useful.\n\nA “Did You Know?” section that will surface hints and tricks related to your prompt.\nDespite the instructions in earlier sections to explain all terminology, GenAI models will still tend towards not everything any technical terms used. This at least flags at the end of responses things you may want to ask for further explainations about. It also provides a convenient way to prompt ‘Explain all’ and get a mini glossary of most the techincal terms used in the first response.\n\nFormatting:\n\nGenAI puts code inside ‘code blocks’ so they display on screen as code. This uses the same syntax as for creating code chunks in R markdown. When GenAI then tries to place an R code chunk inside a code block the formatting in the response ends up a mess. Regular text gets formatted as code and code gets formatted as regular text.\nThis final instruction at the end reduces but does not remove the problem. Whenever it occurs it’s best to just start a new chat as whenever the issue arises in a chat, it tends to persist across all later responses."
  },
  {
    "objectID": "help/SummativeTemplateBreakdown.html",
    "href": "help/SummativeTemplateBreakdown.html",
    "title": "Lab Workbook",
    "section": "",
    "text": "The top of the RMarkdown document includes a YAML block, enclosed with three dashes, ---, at the start and end. The YAML block must be at the very start of the document, so do not add any new lines before it.\nYAML is often used with Markdown as it provides a simple human-readable structure for configuring metadata, such as the document title, author, date, and desired output when knitting:\n\nknitr::include_graphics(\"./images/templateyaml.png\")\n\nPlease remember to add your student number - and not your name - to line 3.\nTo save having to remember to update the date before knitting and submitting your final version, you can use the following in the YAML block:\n\ndate: \"`r format(Sys.time(), '%d/%m/%y')`\"\n\nWhen knitted that code will automatically add the current date in dd/mm/yy format (e.g. 04/12/24):\n\nknitr::include_graphics(\"./images/knitteddate.png\")\n\n\nAfter the YAML Block, there is a ‘setup’ code chunk that sets global options for knitr. The include=FALSE in its options means that this chunk and its output are not displayed in your knitted files:\n\nknitr::include_graphics(\"./images/templatesetup.png\")\n\nGlobal options apply to all code chunks in the document. This means anything set in the global options does not have to be individually added to each code chunk. Note, you can set a global option and still set a different one for a specific code chunk where needed. For example, if you set option=FALSE globally, but needed it to be TRUE for a specific code chunk, all you need to do is add option=TRUE to its options to override the global default.\nThe global options set in the setup code chunk are:\n\n\nmessage=FALSE, which hides all non-warning messages when knitting code chunks. For example, when loading a library it will sometimes display non-warning messages.\n\nwarning=FALSE, which hides all warning messages when knitting code chunks. Again, making sure that no warnings messages displayed when running code is included in your knitted file.\n\nImportantly, you do not want to display your code chunks in your knitted file, only the tables and graphs they generate. To achieve this you could add echo=FALSE to the options for each individual code chunk. Or – you can simply add it as a global option in the setup code chunk!\n\nknitr::include_graphics(\"./images/templatesetupechofalse.png\")\n\n\nIt is best practice to install and load any required packages and read in any data being used at the top of the RMarkdown file. This keeps all package management and data reading in a single location at the start of the document, making your code simpler and clearer. It further ensures that when running/knitting your code that all required packages are loaded and dataframes created before the rest of your code is run.\nThe preamble code block within the template provides a structure following this practice.\n\nknitr::include_graphics(\"./images/templatepreamble.png\")\n\n\n\nInstall packages if missing, outlined in pink. This code may look complex, but all it does is create a list of packages that are going to be used, assigning it to the object list.of.packages. This list is checked against packages already installed, creating a new list new.packages containing only the names of packages not already installed. The final line then basically says “if the length of new.packages is 1 or more, then install all packages in the new.packages list”.\n\nWord count addin if missing, outlined in blue. There is not an R package for calculating word counts, but there is what is known as an ‘addin’ for RStudio. The code here checks if the addin is already installed and if not it downloads a copy of the addin from GitHub. Please see the Word Count Code section for further information on how this addin is used in the document.\n\nLoad packages, outlined in yellow. This is where all packages used in the analysis are loaded.\n\nRead data, outlined in red. The dataset used for the assignment in read in and assigned to the nilt dataframe object.\n\nImportant, as a dataset is provided with the project template and read in already for you, you do not need to download or read in any other dataset. The assignment uses a version of the NILT dataset that includes more variables than the version of the data we used in the lab sessions. Downloading and reading in the dataset used in the labs will result in variables disappearing from the regression results table. Please see the R Issues FAQ for more info.\nIf you require additional packages, the best way to add this would be:\n\nWithin the Install packages if missing section, add the package name at the end of the list that is being assigned to ‘list.of.packages’\nWithin the Load packages section, add a new library() line under the existing ones.\n\nFor example, if needing to install and load ‘vtable’ these are the changes that you would make:\n\nknitr::include_graphics(\"./images/addingvtable.png\")\n\nAs a gif:\n\nknitr::include_graphics(\"./images/addingpackages.gif\")\n\nNote, moving all code for installing and loading of packages into this preamble code chunk means you do not need, and can safely remove, any other install.package() and library() lines from the rest of your code chunks. This will reduce risk of encountering error messages and make it easier to debug them if any do occur.\n\nTables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. Note, you will need to knit your document each time you want to check the updated word count.\nThis is how the code to calculate the word count looks within the RMarkdown file:\n\nknitr::include_graphics(\"./images/wordcountinline.png\")\n\nSide-note, surrounding code with single back-ticks, (`), creates an “inline code chunk”, enabling you to add short snippets of code. The r at the start specifies that the code is R, similar to adding r in curly brackets for code chunks.\nThis is then how it looks when knitting the project template (assuming you haven’t added any additional text yet):\n\nknitr::include_graphics(\"./images/knittedwordcount.png\")\n\nWe use this addin as the built-in RStudio word count, accessed via the ‘Edit’ menu at the top of the screen, includes all code and comments. So, despite the addin calculating ‘0’, RStudio will provide:\n\nknitr::include_graphics(\"./images/rstudiowordcount.png\")\n\nImportant, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to wordcountaddin::word_count(\"Assignmet2-template.Rmd\") - 193.\n\nThe RMarkdown document provided in the project template includes headed sections you can use alongside suggested word count for each and brief summary reminder of what to include in each section. The Course Handbook, pages 26-30, provide a more detailed breakdown of what to include in each section.\nScreenshot of the outline and commented suggestions:\n\nknitr::include_graphics(\"./images/templateoutline.png\")\n\nImportant, within the “Results and discussion” section, ensure to add your interpretation after the code chunk that creates the regression results table:\n\nknitr::include_graphics(\"./images/regressioninterpretativeaftercode.png\")\n\n\nThe code chunk that runs the multiple linear regression model and creates the table with the regression results can be found in the ‘Results and discussion’ section:\n\nknitr::include_graphics(\"./images/templateresults.png\")\n\n\nFirst the model is run, outlined in pink, and assigned to the model object.\nNext a list with labels to use for the independent and control variables in the regression table is created, outlined in blue, and assigned to the cov_labels object.\nFinally, the stargazer package is used to create the regression results table, outlined in red. It is passed the model and cov_label objects, highlighted in yellow, alongside arguments for outputting the table in HTML, a title, and caption & label to use for the dependent variable.\n\nStargazer produces well-formatted regression tables, but with the downside that when running the code chunk in RStudio, you will only see the raw HTML syntax that is created:\n\nknitr::include_graphics(\"./images/templatehtmltable.png\")\n\nIn order to view the table, you will need to knit your document and view the outputted HTML file:\n\nknitr::include_graphics(\"./images/knitting.png\")\n\nYour knitted HTML file should then open in a new popup window. However, you might instead receive a dialogue window saying that a pop-up was prevented from opening, if so just click ‘Try again’:\n\nknitr::include_graphics(\"./images/tryagain.png\")\n\nAlternatively, you can open the last knitted version of your HTML file from the ‘Files’ panel. Click the HTML file and select ‘View in Web Browser’:\n\nknitr::include_graphics(\"./images/openknittedhtml.gif\")\n\nBelow is how your regression results table should look in the knitted HTML file:\n\nknitr::include_graphics(\"./images/knittedresultstable.png\")\n\nIf it doesn’t, please see the R Issues FAQ.",
    "crumbs": [
      "Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/SummativeTemplateBreakdown.html#interpretive-report-template",
    "href": "help/SummativeTemplateBreakdown.html#interpretive-report-template",
    "title": "Lab Workbook",
    "section": "",
    "text": "The top of the RMarkdown document includes a YAML block, enclosed with three dashes, ---, at the start and end. The YAML block must be at the very start of the document, so do not add any new lines before it.\nYAML is often used with Markdown as it provides a simple human-readable structure for configuring metadata, such as the document title, author, date, and desired output when knitting:\n\nknitr::include_graphics(\"./images/templateyaml.png\")\n\nPlease remember to add your student number - and not your name - to line 3.\nTo save having to remember to update the date before knitting and submitting your final version, you can use the following in the YAML block:\n\ndate: \"`r format(Sys.time(), '%d/%m/%y')`\"\n\nWhen knitted that code will automatically add the current date in dd/mm/yy format (e.g. 04/12/24):\n\nknitr::include_graphics(\"./images/knitteddate.png\")\n\n\nAfter the YAML Block, there is a ‘setup’ code chunk that sets global options for knitr. The include=FALSE in its options means that this chunk and its output are not displayed in your knitted files:\n\nknitr::include_graphics(\"./images/templatesetup.png\")\n\nGlobal options apply to all code chunks in the document. This means anything set in the global options does not have to be individually added to each code chunk. Note, you can set a global option and still set a different one for a specific code chunk where needed. For example, if you set option=FALSE globally, but needed it to be TRUE for a specific code chunk, all you need to do is add option=TRUE to its options to override the global default.\nThe global options set in the setup code chunk are:\n\n\nmessage=FALSE, which hides all non-warning messages when knitting code chunks. For example, when loading a library it will sometimes display non-warning messages.\n\nwarning=FALSE, which hides all warning messages when knitting code chunks. Again, making sure that no warnings messages displayed when running code is included in your knitted file.\n\nImportantly, you do not want to display your code chunks in your knitted file, only the tables and graphs they generate. To achieve this you could add echo=FALSE to the options for each individual code chunk. Or – you can simply add it as a global option in the setup code chunk!\n\nknitr::include_graphics(\"./images/templatesetupechofalse.png\")\n\n\nIt is best practice to install and load any required packages and read in any data being used at the top of the RMarkdown file. This keeps all package management and data reading in a single location at the start of the document, making your code simpler and clearer. It further ensures that when running/knitting your code that all required packages are loaded and dataframes created before the rest of your code is run.\nThe preamble code block within the template provides a structure following this practice.\n\nknitr::include_graphics(\"./images/templatepreamble.png\")\n\n\n\nInstall packages if missing, outlined in pink. This code may look complex, but all it does is create a list of packages that are going to be used, assigning it to the object list.of.packages. This list is checked against packages already installed, creating a new list new.packages containing only the names of packages not already installed. The final line then basically says “if the length of new.packages is 1 or more, then install all packages in the new.packages list”.\n\nWord count addin if missing, outlined in blue. There is not an R package for calculating word counts, but there is what is known as an ‘addin’ for RStudio. The code here checks if the addin is already installed and if not it downloads a copy of the addin from GitHub. Please see the Word Count Code section for further information on how this addin is used in the document.\n\nLoad packages, outlined in yellow. This is where all packages used in the analysis are loaded.\n\nRead data, outlined in red. The dataset used for the assignment in read in and assigned to the nilt dataframe object.\n\nImportant, as a dataset is provided with the project template and read in already for you, you do not need to download or read in any other dataset. The assignment uses a version of the NILT dataset that includes more variables than the version of the data we used in the lab sessions. Downloading and reading in the dataset used in the labs will result in variables disappearing from the regression results table. Please see the R Issues FAQ for more info.\nIf you require additional packages, the best way to add this would be:\n\nWithin the Install packages if missing section, add the package name at the end of the list that is being assigned to ‘list.of.packages’\nWithin the Load packages section, add a new library() line under the existing ones.\n\nFor example, if needing to install and load ‘vtable’ these are the changes that you would make:\n\nknitr::include_graphics(\"./images/addingvtable.png\")\n\nAs a gif:\n\nknitr::include_graphics(\"./images/addingpackages.gif\")\n\nNote, moving all code for installing and loading of packages into this preamble code chunk means you do not need, and can safely remove, any other install.package() and library() lines from the rest of your code chunks. This will reduce risk of encountering error messages and make it easier to debug them if any do occur.\n\nTables, figures, and code are not included in the word count. The project template is setup with a “word count add-in”, that will add a word count for you at the top of your knitted document. Note, you will need to knit your document each time you want to check the updated word count.\nThis is how the code to calculate the word count looks within the RMarkdown file:\n\nknitr::include_graphics(\"./images/wordcountinline.png\")\n\nSide-note, surrounding code with single back-ticks, (`), creates an “inline code chunk”, enabling you to add short snippets of code. The r at the start specifies that the code is R, similar to adding r in curly brackets for code chunks.\nThis is then how it looks when knitting the project template (assuming you haven’t added any additional text yet):\n\nknitr::include_graphics(\"./images/knittedwordcount.png\")\n\nWe use this addin as the built-in RStudio word count, accessed via the ‘Edit’ menu at the top of the screen, includes all code and comments. So, despite the addin calculating ‘0’, RStudio will provide:\n\nknitr::include_graphics(\"./images/rstudiowordcount.png\")\n\nImportant, the ‘- 10’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 183 words then change the code to wordcountaddin::word_count(\"Assignmet2-template.Rmd\") - 193.\n\nThe RMarkdown document provided in the project template includes headed sections you can use alongside suggested word count for each and brief summary reminder of what to include in each section. The Course Handbook, pages 26-30, provide a more detailed breakdown of what to include in each section.\nScreenshot of the outline and commented suggestions:\n\nknitr::include_graphics(\"./images/templateoutline.png\")\n\nImportant, within the “Results and discussion” section, ensure to add your interpretation after the code chunk that creates the regression results table:\n\nknitr::include_graphics(\"./images/regressioninterpretativeaftercode.png\")\n\n\nThe code chunk that runs the multiple linear regression model and creates the table with the regression results can be found in the ‘Results and discussion’ section:\n\nknitr::include_graphics(\"./images/templateresults.png\")\n\n\nFirst the model is run, outlined in pink, and assigned to the model object.\nNext a list with labels to use for the independent and control variables in the regression table is created, outlined in blue, and assigned to the cov_labels object.\nFinally, the stargazer package is used to create the regression results table, outlined in red. It is passed the model and cov_label objects, highlighted in yellow, alongside arguments for outputting the table in HTML, a title, and caption & label to use for the dependent variable.\n\nStargazer produces well-formatted regression tables, but with the downside that when running the code chunk in RStudio, you will only see the raw HTML syntax that is created:\n\nknitr::include_graphics(\"./images/templatehtmltable.png\")\n\nIn order to view the table, you will need to knit your document and view the outputted HTML file:\n\nknitr::include_graphics(\"./images/knitting.png\")\n\nYour knitted HTML file should then open in a new popup window. However, you might instead receive a dialogue window saying that a pop-up was prevented from opening, if so just click ‘Try again’:\n\nknitr::include_graphics(\"./images/tryagain.png\")\n\nAlternatively, you can open the last knitted version of your HTML file from the ‘Files’ panel. Click the HTML file and select ‘View in Web Browser’:\n\nknitr::include_graphics(\"./images/openknittedhtml.gif\")\n\nBelow is how your regression results table should look in the knitted HTML file:\n\nknitr::include_graphics(\"./images/knittedresultstable.png\")\n\nIf it doesn’t, please see the R Issues FAQ.",
    "crumbs": [
      "Interpretive Report Template"
    ]
  },
  {
    "objectID": "help/r markdown.html",
    "href": "help/r markdown.html",
    "title": "Lab Workbook",
    "section": "",
    "text": "RMarkdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with Code Chunks that can be converted to multiple file formats, such as HTML or PDF. This makes RMarkdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis.\nRMarkdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text (see the Formatting section below). Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. RMarkdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text and passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format.\n\nR Markdown uses simple, human-readable syntax for basic formatting. Whilst different to how you would write text in a Word document, it is easy to learn. This simple transparent formatting also helps avoid the hidden complexities of Word document, where formatting issues often arise from inconsistent syntax that is hidden from the user.\nBelow is a screenshot from RStudio with text using the key basic syntax for formatting your main text:\n\n\n\n\n\n\n\n\nAnd, this is how it appears when knitted:\n\n\n\n\n\n\n\n\nHeadings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level:\n\n\n\n\n\n\n\n\nAnd, how it appears when knitted:\n\n\n\n\n\n\n\n\n\nThe main thing in RMarkdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between:\n\nEach paragraph\nBefore and after a list\nBefore and after a header\n\nHere’s some example text in an RMarkdown document, the first without line spacing, the second with:\n\n\n\n\n\n\n\n\nAnd, how this looks when knitted:\n\n\n\n\n\n\n\n\nSide-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in RMarkdown.\n\nThere are three main ways to create a code chunk:\n\nClick the ‘Insert Code Chunk’ button and from the drop-down select ‘R’ at the top. (See gif below)\nPress Ctrl + Alt + I (Windows & Linux) or Cmd + Option + I (macOS)\nManually typing three back-ticks with ‘r’ in curly brackets before your code and adding three back-ticks after.\n\n\n\n\n\n\n\n\n\nCurly brackets at the start of a code chunk are used to specify {programming-language optional name, options = values}. Since we are using R, all our code chunks have {r ...}. Within code chunks, the pound sign / hash, #, at the start of a line is used to add any comments:\n\n\n\n\n\n\n\n\nSide-note, this may seem confusing given hash signs are used for headers in the main text. However, inside of a code chunk, all text is treated as code for the language specified in the curly brackets. In R, hash signs are used for comments, so they are treated as comments within the code chunk rather than as Markdown headers.\n\nAdding a label is useful as a quick way to remind yourself the purpose of each code chunk. The quick outline (button at bottom of Source panel) can be used to jump to specific sections and code chunks in your document. If you provide a label for your code chunk it will also appear here:\n\n\n\n\n\n\n\n\n\nOptions are used to specify how code chunks are handled when run/knitted. Two key ones we have used in the labs and in the project template:\n\n\necho - whether the code chunk is displayed in knitted files.\n\ninclude - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files.\n\nBy default, the code chunk and the output are displayed in knitted files. For the interpretive report, you do not want to display the code chunks in your knitted HTML file, so ensure you add echo=FALSE to the options for any new code chunk you create. Alternatively, see the [Setup Code Chunk] section for information on how to set echo=FALSE as a ‘global option’.\nExamples when working in R Markdown document:\n\n\n\n\n\n\n\n\nAnd when knitted:\n\n\n\n\n\n\n\n\n\nThere are multiple ways to run your R code.\nWithin the top-right of each code chunk there are two buttons:\n\n\n\n\n\n\n\n\n\nRun Current Chunk - which will run the code within that code chunks only.\nRun All Chunks Above - which will run all code chunks from the top of your document down to this code chunk.\n\nThe second is useful when a code chunk depends upon others being run first. Remember the R environment maintains the results of any previously run code, so you do not need to continuously run all previous chunks. However, this can be useful when debugging issues or if you restart your R environment, losing the results of previously run code, and needing to re-run everything before this chunk.\nFurther options can be found in the ‘Run’ drop-down menu, accessed from the top-right of the Source Panel:\n\n\n\n\n\n\n\n\nUseful additional ones here are:\n\n\nRun Selected Line(s) - running all lines you have manually selected using the mouse / text cursor.\n\nRestart R and Run All Chunks - incredibly useful when you need to reset your R environment, this can be useful when debugging an error to figure out whether the issue stems from your current code or code you previously ran but now removed.\n\nNote as well that many of these options have keyboard shortcuts listed. Learning these pays off long-term as you will be able to write and run your code without needing to move your hands from the keyboard.\n\nKnitting a document runs all your R code from top to bottom, then combines the results from this with the Markdown text to convert these into different file formats. YAML is used to specify which file formats to convert to; see the [YAML Block] section for information on how this is set up within the project template.\nImportantly, when you knit an RMarkdown document, all code is run sequentially from top to bottom in a clean R environment. This ensures the document is reproducible. Anyone with a copy can knit it and produce the same results. This requires though that code to load any required packages are included within a code chunk in the document. Simply loading a package via the Console adds it to your current R environment and manually run code chunks will be able to access the package. However, when knitting, the clean R environment won’t have access to the package, and you’ll receive an error message when the knit process tries to run a function that requires the package. This is why it is important to include a code chunk at the top of your document that loads all required packages. See the [Preamble Code Chunk] for how to do this in the RMarkdown file provided in the interpretive report project template.\nAssuming you have YAML specifying which file format(s) to convert to, all you then need to do each time is simply click the ‘Knit’ button:\n\n\n\n\n\n\n\n\n\nIt is possible to access an RMarkdown cheat sheet (as well as ones for ggplot2 and dplyr!) from within RStudio:\n\n\n\n\n\n\n\n\nNote, from the same ‘Help’ menu, just below ‘Cheat Sheets’ is an option for ‘Keyboard Shortcuts Help’. This displays a screen with most of the shortcuts available within RStudio. This also includes a link to ‘See All Shortcuts…’."
  },
  {
    "objectID": "help/r markdown.html#rmarkdown",
    "href": "help/r markdown.html#rmarkdown",
    "title": "Lab Workbook",
    "section": "",
    "text": "RMarkdown allows you to combine your narrative and data analysis in one document, writing plain-text documents with Code Chunks that can be converted to multiple file formats, such as HTML or PDF. This makes RMarkdown documents reproducible by embedding the analysis directly into the document. Compared to doing your analysis separately, copying and pasting over your results and graphs to a Word document, it also reduces the risk of error when working on and updating your analysis.\nRMarkdown achieves this by combining the power of R, Markdown, and Pandoc. Markdown is a lightweight markup language with simple, human-readable syntax for formatting text (see the Formatting section below). Pandoc is a tool for converting files from one format to another, such as Markdown to PDF or HTML. RMarkdown builds on these tools. When you ‘knit’ a document, the code chunks are executed to run your analysis and generate your tables and graphs using R. The output from these are then integrated with the Markdown text and passed to Pandoc to convert into a neat and consistently formatted HTML, PDF, or other specified file format.\n\nR Markdown uses simple, human-readable syntax for basic formatting. Whilst different to how you would write text in a Word document, it is easy to learn. This simple transparent formatting also helps avoid the hidden complexities of Word document, where formatting issues often arise from inconsistent syntax that is hidden from the user.\nBelow is a screenshot from RStudio with text using the key basic syntax for formatting your main text:\n\n\n\n\n\n\n\n\nAnd, this is how it appears when knitted:\n\n\n\n\n\n\n\n\nHeadings are set using the pound / hash sign, #, at the start of a line, with the number of hashes determining the header level:\n\n\n\n\n\n\n\n\nAnd, how it appears when knitted:\n\n\n\n\n\n\n\n\n\nThe main thing in RMarkdown’s syntax that often trips up new users is the need to ensure there are empty line spaces between:\n\nEach paragraph\nBefore and after a list\nBefore and after a header\n\nHere’s some example text in an RMarkdown document, the first without line spacing, the second with:\n\n\n\n\n\n\n\n\nAnd, how this looks when knitted:\n\n\n\n\n\n\n\n\nSide-note, this practice of using empty line spaces originates from traditional coding conventions. A common coding style includes placing a specified limit on the number of characters per line, with any overflow placed on a new line. To distinguish these lines breaks to keep a limit on the number of characters per line and those used to designate new paragraphs, lists, and headers, an empty line is used. Using empty lines also helps add visual clarity when writing in RMarkdown.\n\nThere are three main ways to create a code chunk:\n\nClick the ‘Insert Code Chunk’ button and from the drop-down select ‘R’ at the top. (See gif below)\nPress Ctrl + Alt + I (Windows & Linux) or Cmd + Option + I (macOS)\nManually typing three back-ticks with ‘r’ in curly brackets before your code and adding three back-ticks after.\n\n\n\n\n\n\n\n\n\nCurly brackets at the start of a code chunk are used to specify {programming-language optional name, options = values}. Since we are using R, all our code chunks have {r ...}. Within code chunks, the pound sign / hash, #, at the start of a line is used to add any comments:\n\n\n\n\n\n\n\n\nSide-note, this may seem confusing given hash signs are used for headers in the main text. However, inside of a code chunk, all text is treated as code for the language specified in the curly brackets. In R, hash signs are used for comments, so they are treated as comments within the code chunk rather than as Markdown headers.\n\nAdding a label is useful as a quick way to remind yourself the purpose of each code chunk. The quick outline (button at bottom of Source panel) can be used to jump to specific sections and code chunks in your document. If you provide a label for your code chunk it will also appear here:\n\n\n\n\n\n\n\n\n\nOptions are used to specify how code chunks are handled when run/knitted. Two key ones we have used in the labs and in the project template:\n\n\necho - whether the code chunk is displayed in knitted files.\n\ninclude - whether the code chunk and its output - such as tables and graphs - are displayed in knitted files.\n\nBy default, the code chunk and the output are displayed in knitted files. For the interpretive report, you do not want to display the code chunks in your knitted HTML file, so ensure you add echo=FALSE to the options for any new code chunk you create. Alternatively, see the [Setup Code Chunk] section for information on how to set echo=FALSE as a ‘global option’.\nExamples when working in R Markdown document:\n\n\n\n\n\n\n\n\nAnd when knitted:\n\n\n\n\n\n\n\n\n\nThere are multiple ways to run your R code.\nWithin the top-right of each code chunk there are two buttons:\n\n\n\n\n\n\n\n\n\nRun Current Chunk - which will run the code within that code chunks only.\nRun All Chunks Above - which will run all code chunks from the top of your document down to this code chunk.\n\nThe second is useful when a code chunk depends upon others being run first. Remember the R environment maintains the results of any previously run code, so you do not need to continuously run all previous chunks. However, this can be useful when debugging issues or if you restart your R environment, losing the results of previously run code, and needing to re-run everything before this chunk.\nFurther options can be found in the ‘Run’ drop-down menu, accessed from the top-right of the Source Panel:\n\n\n\n\n\n\n\n\nUseful additional ones here are:\n\n\nRun Selected Line(s) - running all lines you have manually selected using the mouse / text cursor.\n\nRestart R and Run All Chunks - incredibly useful when you need to reset your R environment, this can be useful when debugging an error to figure out whether the issue stems from your current code or code you previously ran but now removed.\n\nNote as well that many of these options have keyboard shortcuts listed. Learning these pays off long-term as you will be able to write and run your code without needing to move your hands from the keyboard.\n\nKnitting a document runs all your R code from top to bottom, then combines the results from this with the Markdown text to convert these into different file formats. YAML is used to specify which file formats to convert to; see the [YAML Block] section for information on how this is set up within the project template.\nImportantly, when you knit an RMarkdown document, all code is run sequentially from top to bottom in a clean R environment. This ensures the document is reproducible. Anyone with a copy can knit it and produce the same results. This requires though that code to load any required packages are included within a code chunk in the document. Simply loading a package via the Console adds it to your current R environment and manually run code chunks will be able to access the package. However, when knitting, the clean R environment won’t have access to the package, and you’ll receive an error message when the knit process tries to run a function that requires the package. This is why it is important to include a code chunk at the top of your document that loads all required packages. See the [Preamble Code Chunk] for how to do this in the RMarkdown file provided in the interpretive report project template.\nAssuming you have YAML specifying which file format(s) to convert to, all you then need to do each time is simply click the ‘Knit’ button:\n\n\n\n\n\n\n\n\n\nIt is possible to access an RMarkdown cheat sheet (as well as ones for ggplot2 and dplyr!) from within RStudio:\n\n\n\n\n\n\n\n\nNote, from the same ‘Help’ menu, just below ‘Cheat Sheets’ is an option for ‘Keyboard Shortcuts Help’. This displays a screen with most of the shortcuts available within RStudio. This also includes a link to ‘See All Shortcuts…’."
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2022. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nFogarty, Brian J. 2019. Quantitative Social Science Data with R: An Introduction. Los Angeles London New Delhi Singapore Washington DC Melbourne: SAGE.\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion to Applied Regression. Third. Thousand Oaks CA: Sage. https://socialsciences.mcmaster.ca/jfox/Books/Companion/.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2021. Car: Companion to Applied Regression. https://CRAN.R-project.org/package=car.\n\n\nHorst, Alison. n.d. “GitHub - Allisonhorst/Stats-Illustrations: R & Stats Illustrations by @Allison_horst.” Accessed July 11, 2022. https://github.com/allisonhorst/stats-illustrations.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2021. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/bookdown.\n\n\n———. 2019. “TinyTeX: A Lightweight, Cross-Platform, and Easy-to-Maintain LaTeX Distribution Based on TeX Live.” TUGboat, no. 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html.\n\n\n———. 2021. Tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents. https://github.com/yihui/tinytex.\n\n\n———. 2022. Bookdown: Authoring Books and Technical Documents with r Markdown. https://CRAN.R-project.org/package=bookdown.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "08-Lab8.html",
    "href": "08-Lab8.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "In the previous lab we learned about correlation. We visualised the relationship of different types of variables. We also computed one correlation measure for two numeric variables, Pearson’s correlation. We used Pearson’s correlation to summarise the strength and direction of a linear association between two numeric variables. However, it presents some limitations. It can only be used for numeric variables, it allows only one pair of variables at a time, and it is only appropriate to describe a linear relationship.\nLinear regression can overcome some of these limitations and it can be extended to achieve further purposes, such as using multiple variables or to make predictions for new scenarios. This technique is in fact common and one of the most popular in quantitative research in social sciences. Therefore, getting familiar with it will be important for you not only to perform your own analyses, but also to interpret and critically read the academic literature.\n\n\n\n\n\n\nLinear regression is appropriate only when the dependent variable is numeric (interval/ratio).\nHowever, independent variables can be categorical, ordinal, or numeric. You can include more than one independent variable to evaluate how these relate to the dependent variable. In this lab we will start using only one explanatory variable. This is known as simple linear regression.\n\n\n\n\n\n\n\n\n\nTipDon’t Panic\n\n\n\nWe will be covering linear regression over three weeks. Do not worry if everything this week doesn’t immediately make sense. We will revisit and build upon key ideas over labs 9 and 10 as well.\nPlease also do let me (Alasdair) know where anything is unclear or is confusing. We have been updating the lab content this year based on student feedback from previous years and value any feedback on the newest version so we can continue to make improvements.\nAs they cover some key concepts relevant for the summative assignment, I will aim to make further updates this year to Labs 8 - 10 based on any initial feedback.\n\n\n\n\nFor this lab, we will continue using the 2012 NILT survey to introduce linear models.\nAs usual, we will need to setup an R Markdown file for today’s lab. Remember if you are unsure about any of the steps below you can take a look back at Lab 6 which has more detailed explanation of each of the steps.\nTo set up a new R Markdown file for this lab, please use the following steps:\n\nPlease go to your ‘Lab Group ##’ in Posit Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Lab Group ##’;\nWithin the Files tab (bottom-right pane) click ‘New File’, then ‘R Markdown’ from the drop-down list of options;\nWithin the ‘Create a New File in Current Directory’ dialogue, name it ‘Lab-8-Simple-Linear-Regression.Rmd’ and click OK.\nFeel free to make any adjustments to the YAML header.\nCreate a new code chunk, with ```{r setup, include=FALSE} and in the chunk:\n\n\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\n\nCreate another code chunk, named preamble and again with include=FALSE. Then add the following in the chunk:\n\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n\n# Read NILT\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nPlease note, we are loading the haven package again this week as we will be coercing more variables.\n\nRun the preamble code chunk and if no errors, then everything is setup for this session.\n\nLet’s revisit the example from last week of the relationship between a respondent’s age and their spouse or partner’s age.\nFirstly, let’s coerce the variables for the age of the respondent’s spouse/partner:\n\n# Age of Respondent's Spouse / Partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n\nBonus Activity: We are coercing a variable again each week as a reminder for how to do it. However, we could remove the need to coerce variables we are re-using across each R Markdown file. In Lab 3 we created the 01_prep_nilt_2012.R script in a sub-folder named R. Reviewing it, can you see where you could add a line of code to coerce spage? If so, modify your script, rerun it in full, then re-run the preamble code chunk in your R Markdown file for this week. After doing that, you’ll know you have successfully modified the script if you can run class(nilt$spage) in the Console and receive \"numeric\" as the output.\nOK, we are going to use scatterplots to help visualise concepts related to linear regression. However, the NILT has hundreds of respondents, making very noisy plots. So, next let’s create a minimal sample of the dataset to only include 40 random observations:\n\n# set a seed for randomisation\nset.seed(3)\n\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- nilt |&gt;\n  filter(!is.na(spage)) |&gt;\n  sample_n(40)\n\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- nilt_sample |&gt;\n  select(rage, spage)\n\n\n\n\n\n\n\nPlease note, we are creating a nilt_sample dataframe with a random selection of 40 observations - i.e. respondents - from the nilt data solely to help make it easier to visualise in the examples below.\n\n\n\nTo break down some of the code here:\n\nThe set.seed() function lets us set a seed to fix the random number generator. Results remain random, but using the same seed means when running code in the same order makes the results reproducible.\nWe then filter(!is.na(spage)). ! is the “NOT” operator, and is.na() checks whether a value is NA (i.e. missing data). So, combined it is equivalent to saying “filter to remove all observations where the value for spage is NA”.\n\nsample_n() is then used to randomly select 40 of the observations that remain.\n\n\nBefore we introduce linear regression itself, let’s start by visualising the relationship between the ages of the respondent and their spouse/partner.\nWe can do this with a scatter plot using the respondent’s spouse/partner age spage on the Y axis, and the respondent’s rage on the X axis. In ggplot, we compose the three key ggplot components - ggplot(aes()), geom_...(), and labs().\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    subtitle = \"Random Sample of 40 Observations\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nAs you can see, even with this minimal example using 40 observations, there is a clear trend. If we were to guess - for the moment - it looks like the age of the spouse/partner might be generally the same as the respondent.\nFrom the plot, it seems intuitive to draw a line that describes this general trend. Imagine a friend will draw this line for you. You will have to tell them some basic references on how to do it. You can, for example, specify a start point and an end point in the plot. Alternatively, which is what we will do below, you can specify a start point and a value that describes the slope of the line.\nThankfully, our friend is ggplot. We can pass it the two values we need - the start point and slope - in the geom_abline() function. This will then draw a line that describes these points.\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage)) +\n  geom_point() +\n# Draw line assuming exact same age of respondent and their partner/spouse\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    subtitle = \"Reference line assuming equal ages\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs noted in the comment in the code above, we are drawing this line at the moment as a ‘guesstimate’ a by-eye guess - that assumes the age of the spouse/partner is exactly the same as the respondent’s.\n\n\n\nTo draw a line like this, we used 0 as the starting value in the geom_abline() function. In statistics, this is known as the intercept and it is often represented with a Greek letter and a zero sub-index as \\(\\beta_0\\) (beta-naught) or with \\(\\alpha\\) (alpha). The location of the intercept can be found along the vertical axis (in this case it is not visible due to ages of respondents and their spouses/partners being 20+).\nThe second value we passed to describe the line is the slope, which uses the X axis as the reference. The slope value is multiplied by the value of the X axis. Therefore, a slope of 0 is completely horizontal. In this example, a slope of 1 produces a line at 45º. The slope is often represented with the Greek letter beta and a sequential numeric sub-index like this: \\(\\beta_1\\) (b.\n\nHaving drawn a line assuming respondents and their spouses/partners have the same age, how does that compare to our actual data?\nFirst, let’s create a variable in our dataframe that matches our guess. We can do this by taking the starting point - the ‘intercept’ - and adding the steepness factor - the ‘slope’. We will store the results in a column called line1. After that, we can print the ‘head’ of our dataframe using the head() function to see the first few rows.\n\n# create values for the guess line 1\nnilt_sample &lt;- nilt_sample |&gt;\n  mutate(line1 = 0 + rage * 1)\n# print results\nhead(nilt_sample)\n\n# A tibble: 6 × 3\n   rage spage line1\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    40    39    40\n2    54    51    54\n3    66    59    66\n4    60    71    60\n5    35    34    35\n6    67    57    67\n\n\nFrom the above, we can see that the respondent’s age rage and the column that we just created line1, are identical… which is not a surprise when our guess for now is that a respondent and their spouse/partner will be the same age.\nBut to return to our question, is the line we just drew good enough to represent the general trend in the relationship between the age of a respondent and their spouse/partner?\nTo answer this question, for each point of the plot we can measure the distance from it to where it would be on ‘line 1’ - which to repeat is a line drawn assuming participant’s and their spouse/partner are the same age.\nWe can use ggplot to draw this for us, using a dashed line to show the distance between each dot and our line 1.\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_segment(aes(xend = rage, yend = line1), linetype = \"dashed\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    subtitle = \"Distances from each point to the guess line (residuals)\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nNotice how this code is the same as our original scatterplot, we just added geom_segment(). For each dot, this function draws a line between it and the end points we give. By passing it xend = rage we are effectively telling it not to draw any horizontal line, and yend = line1 tells it to draw a line from the dot to where it would be if the spouse/partner’s age was the same as the respondents. We can then specify to used dashed lines with linetype = \"dashed\".\n\nThis distance between the observation and the line is known as the residual or the error and it is often expressed with the Greek letter \\(\\epsilon\\) (epsilon). Numerically, we can calculate this by computing the difference between the known value (the observed) and the guessed value.\nFormally, the guessed number is called the expected value (also known as predicted/estimated value). Normally, the expected values in statistics are represented by putting a hat like this \\(\\hat{}\\) on the letters.\nSince the independent variable is usually located on the X axis and the dependent variable on the Y axis. We write write the expected (predicated) value with a hat on the Y like this: \\(\\hat{y}\\) (y-hat).\nLet’s estimate the residuals for each of the observations by subtracting \\(\\hat{y}_i\\) from \\(y_i\\) (spage - line1) and store it in a column called residuals. Again we can use the head() function to see the first few lines of our dataframe.\n\n# estimate residuals\nnilt_sample &lt;- nilt_sample |&gt;\n  mutate(residuals = spage - line1)\n# print first 6 values\nhead(nilt_sample)\n\n# A tibble: 6 × 4\n   rage spage line1 residuals\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1    40    39    40        -1\n2    54    51    54        -3\n3    66    59    66        -7\n4    60    71    60        11\n5    35    34    35        -1\n6    67    57    67       -10\n\n\nComing back once again to the question, is my line good enough?\nWe could sum all these residuals as an overall measure to know how good our line is. From the previous plot, we see that some of the expected ages are higher than the actual values, whilst others are below. This produces negative and positive residuals. If we simply sum them, they would compensate each other and this would not tell us much about the overall magnitude of the difference.\nTo overcome this problem, we square each residual - multiply the residual by itself. This will make all of our values positive - “-3 x -3 = 9” - making it possible to sum them all together.\n\nThis is known as the sum of squared residuals (SSR) and we can use this as the criterion to measure how good our line is with respect to the overall trend of the points.\nFor line 1, we can easily calculate the SSR as follows:\n\n# '^' is used for power - so `^2` squares our values\nsum((nilt_sample$residuals)^2)\n\n[1] 1325\n\n\nThe sum of squared residuals for line 1 is 1,325.\n\nHow does our line though compare to other potential lines we could have drawn to describe the trend? We can try different lines to find the combination of intercept and slope that produces the smallest error (SSR). How can we find the best line to describe the trend?\nTo our luck, we can find the best values using a well established technique called Ordinary Least Squares (OLS). Ordinary Least Squares chooses the line that minimises the Sum of Squared Residuals (SSR). We do not need to know the details for now. The important thing is that this procedure guarantees to find a line that produces the smallest possible error (SSR).\nIn R, it is very simple to fit a linear model and we do not need to go through each of the steps above manually nor to memorise all the steps.\nTo fit a linear model, simply use the function lm() and save the result in an object called m1 (for ‘model 1’) and print it.\n\nm1 &lt;- lm(spage ~ rage, data = nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage  \n      6.299        0.875  \n\n\nFor simple linear regression, the lm() function takes arguments in form of lm(dependent-variable ~ independent-variable, dataframe), with the ~ (tilde) operator is used to separate the dependent variable and independent variable.\nSo, this is the optimal intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) that minimises the Sum of Squared Residuals (SSR).\nLet’s see what the SSR is compared to our original arbitrary guess:\n\nsum(residuals(m1)^2)\n\n[1] 1175.209\n\n\nYes, this is better than before. Our line that assumed respondents and their spouse/partner had the same age had an SSR value of 1,325. So, this linear model is better as the SSR value is smaller.\nTo help visualise it, let’s plot the line we guessed and the optimal line together:\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    subtitle = \"Comparing a guess line (red) with the OLS best-fit line (blue)\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nAgain, notice how this is just the same code we used to draw our original guess, with an added line using geom_smooth() to draw a line using the linear model. We pass it two arguments, method = \"lm\", to specify that a linear model should be used to draw the line, and se = FALSE so it draws only the line. (By default, geom_smooth() has se = TRUE to include confidence bands so we need to explicitly use se = FALSE to turn them off.)\nThe blue is the optimal solution based on a linear model, whereas the red was just our initial arbitrary guess. We can see that the optimal is more balanced than the red in relation to the observed data points.\n\nNow you are ready for the formal specification of the linear model.\nAfter the introduction above, it is easy to take the pieces apart. In essence, the simple linear model is telling us that the dependent value \\(y\\) is defined by a line that intersects the vertical axis at \\(\\beta_0\\), plus the multiplication of the slope \\(\\beta_1\\) by the value of \\(x_1\\), plus some residual \\(\\epsilon\\). The second part of the equation is just telling us that the residuals are normally distributed (that is when a histogram of the residuals follows a bell-shaped curve).\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\qquad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nYou do not need to memorize this equation, but being familiar to this type of notation will help you to expand your skills in quantitative research.\n\n\nWe already fitted our first linear model using the lm() above, which of course stands for linear model, but let’s go back through it in relation to the specifications.\nThe first argument takes the dependent variable, then, we use tilde ~ to express that this is followed by the independent variable. Then, we specify the data set separated by a comma, as shown below:\n\n\n\nExample Code - do not run\n\nlm(dependent_variable ~ independent_variable, data)\n\n\n\nNow, let’s try with the full nilt data set.\nFirst, we will fit a linear model using the same variables as the example above but including all the observations - remember that before we were working only with a small random sample of 40 observations.\nThe first argument is the respondent’s spouse/partner age spage, followed by the respondent’s age rage. Let’s assign the model to an object called m2 (for ‘model 2’) and print it after.\n\nm2 &lt;- lm(spage ~ rage, data = nilt)\nm2\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nCoefficients:\n(Intercept)         rage  \n     3.7039       0.9287  \n\n\nThe output first displays the formula employed to estimate the model - under Call:. Then, it shows us the estimated values for the intercept and the slope for the age of the respondent, these estimated values are called coefficients.\nIn this model, the coefficients differ from the ones in the example above (m1). This is because as we are now using the full nilt dataframe we have more information to fit the line. Despite the difference, we see that the relationship is still in the same direction (positive). In fact, the value of the slope \\(\\beta_1\\) did not change much if you look at it carefully (0.87 vs 0.92).\n\nBut what are the slope and intercept telling us? The first thing to note is that the slope is positive, which means that the relationship between the respondent age and their spouse/partner age are in the same direction. When the age of the respondent goes up, the age of their spouse/partner is expected to go up as well.\nIn our model, the intercept does not have a meaningful interpretation on its own. What we mean by saying it is not meaningful is that it would not be logical to say that when the respondent’s age is 0 their partner would be expected to be 3.70 years old. Instead, the interpretations should be made only within the range of the values used to fit the model (the youngest age in this data set is 18).\nThe slope can be interpreted as follows: For every year older the respondent is, the partner’s age is expected to change by 0.92. In other words, the respondents are expected to have a partner who is 0.92 times years older than themselves. Or, to put it in more simple terms, for each 1-year increase in the respondent’s age, the expected spouse/partner age increases by 0.92 years. On average then, partners are slightly younger than respondents.\nIn this case, the units of the dependent variable are the same as the independent (age vs age) which make it easy to interpret. In reality, there might be more factors that potentially affect how people select their partners, such as gender, education, race/ethnicity, etc. For now, we are keeping things ‘simple’ by using only these two variables.\nLet’s practice with another example. What if we are interested in income?\nWe can use annual personal income persinc2 as the dependent variable and the number of hours worked a week rhourswk as the independent variable. We’ll assign the result to an object called m3 (for ‘model 3’).\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\nm3\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nCoefficients:\n(Intercept)     rhourswk  \n     5170.4        463.2  \n\n\nThis time, the coefficients look quite different. The results are telling us that for every additional hour worked a week a respondent is expected to earn £463.2 more a year than other respondent.\nNote that the input units are not the same in the interpretation. The dependent variable is in pounds earned a year and the independent is in hours worked a week. Does this mean that someone who works 0 hours a week is expected to earn £5170.4 a year (given the fitted model \\(\\hat{persinc2} =  5170.4 + 0*463.2\\)) or that someone who works 110 hours a week (impossible!) is expected to earn £56K a year? We should only make reasonable interpretations of the coefficients within the range analysed.\nFinally, an important thing to know when interpreting linear models using cross-sectional data (data collected in a single time point only) is: correlation does not imply causation. Variables are often correlated with other non-observed variables which may cause the effects observed on the independent variable in reality. This is why we cannot always be sure that X is causing the observed changes on Y.\n\n\nSo far, we have talked about the basics of the linear model. If we print a summary of the model, using the handy summary() function, we can obtain more information to evaluate it.\n\nsummary(m2)\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.211  -2.494   0.075   2.505  17.578 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.70389    0.66566   5.564    4e-08 ***\nrage         0.92869    0.01283  72.386   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.898 on 589 degrees of freedom\n  (613 observations deleted due to missingness)\nMultiple R-squared:  0.8989,    Adjusted R-squared:  0.8988 \nF-statistic:  5240 on 1 and 589 DF,  p-value: &lt; 2.2e-16\n\n\nThe summary output has several parts. This can look intimidating at first, but we can break it down bit-by-bit. Let’s focus on:\n\nfirst the call\nsecond the residuals\nthird the coefficients\nfourth the goodness-of-fit measures\n\nThese are highlighted in the screenshot below:\n\n\nModel summary\n\nThe first part, Call, is simply printing the variables that were used to produce the models.\nThe second part, the Residuals, provides a small summary of the residuals by quartile, including minimum and maximum residual values. Remember the second part of the formal specification of the model? Well, this is where we can see in practice the distribution of the residuals. The quartiles show typical prediction errors. For example, half of the residuals lie between the 1st and 3rd quartiles. So by looking at the values for the first and third quartile, we can see that half of the predicted values by the model are \\(\\pm\\) 2.5 years old away from the observed partner’s age… not that bad.\nThe third part is about the coefficients. This time, the summary provides more information about the intercept and the slope than just the estimated coefficients as before. Usually, we focus on the independent variables in this section, rage in our case. The second column, standard error, tell us how large the error of the slope is, which is small for our variable. The third and fourth columns provide measures about the statistical significance of the relationship between the dependent and independent variable.\nThe most commonly reported is the p-value (the fourth column). R uses scientific notation for very small p-values. In our result the p-value is &lt;2e-16, which means a number less than 0.0000000000000002, you can learn about scientific notation here).\nAs a general agreed principle, if the p-value is equal or less than 0.05 (\\(p\\) ≤ 0.05), we can reject the null hypothesis, and say that the relationship between the dependent variable and the independent is significant. In our case, the p-value is far lower than 0.05. So, we can say that the relationship between the respondent’s spouse/partner age and the respondent’s age is significant. In fact, R includes codes for each coefficient to help us to identify the significance. In our result, we got three stars/asterisks. This is because the value is less than 0.001. R assigns two stars if the p-value is less than 0.01, and one star if it is lower than 0.05. (P‑values, however, are just one part of the story; always read them alongside effect size, confidence intervals, and substantive meaning.)\nIn the fourth area, we have various goodness-of-fit measures. Overall, these measures tell us how well the model represents our data. For now, let’s focus on the adjusted R-squared.\nR-squared is the proportion of variation in the outcome explained by the model. It ranges from 0 to 1: 0 means none of the variance is explained; 1 means all variance in the sample is explained by the model. Adjusted R-squared penalises adding variables that do not improve the model. In the social sciences, modest R-squared values are common.\nAdjusted R-squared then can be understood as the proportion (multiply it by 100 to report a percentage) of variance explained by the model, adjusted for the number of independent variables used. In our age-age example, the model explains 89.88% percent of the total variance. This is a good fit, but this is because we are modelling a quite “obvious” relationship. The R-squared describes the model file, it does not judge the quality of our analysis.\nYou may wonder why we do not report the sum of squared residuals (SSR), as we covered in our initial example. This is because the scale of the SSR is in the units used to fit the model, which would make it difficult to compare our model with other models that use different units or different variables. Conversely, the adjusted R-squared is expressed in relative terms which makes it a more suitable measure to compare with other models. An important thing to note is that the output is telling us that this model excluded 613 observations which were missing in our dataset. This is because we do not have the age of the respondent’s partner in 613 of the rows. Missing spouse/partner ages can occur for serveral reasons. For example, the respondent may not have a parter, or choose not to answer.\nIn academic papers in the social sciences, usually the measures reported are the coefficients, p-values, the adjusted R-squared, and the size of the sample used to fit the model (number of observations).\n\nThe linear model, as many other techniques in statistics, relies on assumptions. These refer to characteristics of the data that are taken for granted to generate the estimates. It is the task of the modeller/analyst to make sure the data used follows these assumptions. One simple yet important check is to examine the distribution of the residuals, which ought to follow a normal distribution.\nDo the residuals follow a normal distribution in our model, m2? We can go further and plot a histogram to graphically evaluate it. Use the function residuals() to extract the residuals from our object m2, and then plot a histogram with the base R function hist().\n\nhist(residuals(m2), breaks = 20)\n\n\n\n\n\n\n\nOverall, it seems that the residuals follow the normal distribution reasonably well, with the exception of the negative value to the left of the plot. Very often when there is a strange distribution or different to the normal, it is because one of the assumptions is violated. If that is the case we cannot trust the coefficient estimates. In the next lab we will talk more about the assumptions of the linear model. For now, we will leave it here and get some practice.\n\nIn your R Markdown file, use the nilt dataset to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point() jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\nPrint the summary of m3 using the summary() function.\nIs the relationship of hours worked a week statistically significant?\nWhat is the adjusted R-squared? How would you interpret it?\nWhat is the sample size to fit the model?\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#welcome",
    "href": "08-Lab8.html#welcome",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "In the previous lab we learned about correlation. We visualised the relationship of different types of variables. We also computed one correlation measure for two numeric variables, Pearson’s correlation. We used Pearson’s correlation to summarise the strength and direction of a linear association between two numeric variables. However, it presents some limitations. It can only be used for numeric variables, it allows only one pair of variables at a time, and it is only appropriate to describe a linear relationship.\nLinear regression can overcome some of these limitations and it can be extended to achieve further purposes, such as using multiple variables or to make predictions for new scenarios. This technique is in fact common and one of the most popular in quantitative research in social sciences. Therefore, getting familiar with it will be important for you not only to perform your own analyses, but also to interpret and critically read the academic literature.\n\n\n\n\n\n\nLinear regression is appropriate only when the dependent variable is numeric (interval/ratio).\nHowever, independent variables can be categorical, ordinal, or numeric. You can include more than one independent variable to evaluate how these relate to the dependent variable. In this lab we will start using only one explanatory variable. This is known as simple linear regression.\n\n\n\n\n\n\n\n\n\nTipDon’t Panic\n\n\n\nWe will be covering linear regression over three weeks. Do not worry if everything this week doesn’t immediately make sense. We will revisit and build upon key ideas over labs 9 and 10 as well.\nPlease also do let me (Alasdair) know where anything is unclear or is confusing. We have been updating the lab content this year based on student feedback from previous years and value any feedback on the newest version so we can continue to make improvements.\nAs they cover some key concepts relevant for the summative assignment, I will aim to make further updates this year to Labs 8 - 10 based on any initial feedback.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#setup",
    "href": "08-Lab8.html#setup",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "For this lab, we will continue using the 2012 NILT survey to introduce linear models.\nAs usual, we will need to setup an R Markdown file for today’s lab. Remember if you are unsure about any of the steps below you can take a look back at Lab 6 which has more detailed explanation of each of the steps.\nTo set up a new R Markdown file for this lab, please use the following steps:\n\nPlease go to your ‘Lab Group ##’ in Posit Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Lab Group ##’;\nWithin the Files tab (bottom-right pane) click ‘New File’, then ‘R Markdown’ from the drop-down list of options;\nWithin the ‘Create a New File in Current Directory’ dialogue, name it ‘Lab-8-Simple-Linear-Regression.Rmd’ and click OK.\nFeel free to make any adjustments to the YAML header.\nCreate a new code chunk, with ```{r setup, include=FALSE} and in the chunk:\n\n\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\n\nCreate another code chunk, named preamble and again with include=FALSE. Then add the following in the chunk:\n\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(haven)\n\n# Read NILT\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nPlease note, we are loading the haven package again this week as we will be coercing more variables.\n\nRun the preamble code chunk and if no errors, then everything is setup for this session.\n\nLet’s revisit the example from last week of the relationship between a respondent’s age and their spouse or partner’s age.\nFirstly, let’s coerce the variables for the age of the respondent’s spouse/partner:\n\n# Age of Respondent's Spouse / Partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n\nBonus Activity: We are coercing a variable again each week as a reminder for how to do it. However, we could remove the need to coerce variables we are re-using across each R Markdown file. In Lab 3 we created the 01_prep_nilt_2012.R script in a sub-folder named R. Reviewing it, can you see where you could add a line of code to coerce spage? If so, modify your script, rerun it in full, then re-run the preamble code chunk in your R Markdown file for this week. After doing that, you’ll know you have successfully modified the script if you can run class(nilt$spage) in the Console and receive \"numeric\" as the output.\nOK, we are going to use scatterplots to help visualise concepts related to linear regression. However, the NILT has hundreds of respondents, making very noisy plots. So, next let’s create a minimal sample of the dataset to only include 40 random observations:\n\n# set a seed for randomisation\nset.seed(3)\n\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- nilt |&gt;\n  filter(!is.na(spage)) |&gt;\n  sample_n(40)\n\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- nilt_sample |&gt;\n  select(rage, spage)\n\n\n\n\n\n\n\nPlease note, we are creating a nilt_sample dataframe with a random selection of 40 observations - i.e. respondents - from the nilt data solely to help make it easier to visualise in the examples below.\n\n\n\nTo break down some of the code here:\n\nThe set.seed() function lets us set a seed to fix the random number generator. Results remain random, but using the same seed means when running code in the same order makes the results reproducible.\nWe then filter(!is.na(spage)). ! is the “NOT” operator, and is.na() checks whether a value is NA (i.e. missing data). So, combined it is equivalent to saying “filter to remove all observations where the value for spage is NA”.\n\nsample_n() is then used to randomly select 40 of the observations that remain.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#viewing-trends",
    "href": "08-Lab8.html#viewing-trends",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Before we introduce linear regression itself, let’s start by visualising the relationship between the ages of the respondent and their spouse/partner.\nWe can do this with a scatter plot using the respondent’s spouse/partner age spage on the Y axis, and the respondent’s rage on the X axis. In ggplot, we compose the three key ggplot components - ggplot(aes()), geom_...(), and labs().\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    subtitle = \"Random Sample of 40 Observations\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nAs you can see, even with this minimal example using 40 observations, there is a clear trend. If we were to guess - for the moment - it looks like the age of the spouse/partner might be generally the same as the respondent.\nFrom the plot, it seems intuitive to draw a line that describes this general trend. Imagine a friend will draw this line for you. You will have to tell them some basic references on how to do it. You can, for example, specify a start point and an end point in the plot. Alternatively, which is what we will do below, you can specify a start point and a value that describes the slope of the line.\nThankfully, our friend is ggplot. We can pass it the two values we need - the start point and slope - in the geom_abline() function. This will then draw a line that describes these points.\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage)) +\n  geom_point() +\n# Draw line assuming exact same age of respondent and their partner/spouse\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    subtitle = \"Reference line assuming equal ages\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs noted in the comment in the code above, we are drawing this line at the moment as a ‘guesstimate’ a by-eye guess - that assumes the age of the spouse/partner is exactly the same as the respondent’s.\n\n\n\nTo draw a line like this, we used 0 as the starting value in the geom_abline() function. In statistics, this is known as the intercept and it is often represented with a Greek letter and a zero sub-index as \\(\\beta_0\\) (beta-naught) or with \\(\\alpha\\) (alpha). The location of the intercept can be found along the vertical axis (in this case it is not visible due to ages of respondents and their spouses/partners being 20+).\nThe second value we passed to describe the line is the slope, which uses the X axis as the reference. The slope value is multiplied by the value of the X axis. Therefore, a slope of 0 is completely horizontal. In this example, a slope of 1 produces a line at 45º. The slope is often represented with the Greek letter beta and a sequential numeric sub-index like this: \\(\\beta_1\\) (b.\n\nHaving drawn a line assuming respondents and their spouses/partners have the same age, how does that compare to our actual data?\nFirst, let’s create a variable in our dataframe that matches our guess. We can do this by taking the starting point - the ‘intercept’ - and adding the steepness factor - the ‘slope’. We will store the results in a column called line1. After that, we can print the ‘head’ of our dataframe using the head() function to see the first few rows.\n\n# create values for the guess line 1\nnilt_sample &lt;- nilt_sample |&gt;\n  mutate(line1 = 0 + rage * 1)\n# print results\nhead(nilt_sample)\n\n# A tibble: 6 × 3\n   rage spage line1\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    40    39    40\n2    54    51    54\n3    66    59    66\n4    60    71    60\n5    35    34    35\n6    67    57    67\n\n\nFrom the above, we can see that the respondent’s age rage and the column that we just created line1, are identical… which is not a surprise when our guess for now is that a respondent and their spouse/partner will be the same age.\nBut to return to our question, is the line we just drew good enough to represent the general trend in the relationship between the age of a respondent and their spouse/partner?\nTo answer this question, for each point of the plot we can measure the distance from it to where it would be on ‘line 1’ - which to repeat is a line drawn assuming participant’s and their spouse/partner are the same age.\nWe can use ggplot to draw this for us, using a dashed line to show the distance between each dot and our line 1.\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_segment(aes(xend = rage, yend = line1), linetype = \"dashed\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    subtitle = \"Distances from each point to the guess line (residuals)\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nNotice how this code is the same as our original scatterplot, we just added geom_segment(). For each dot, this function draws a line between it and the end points we give. By passing it xend = rage we are effectively telling it not to draw any horizontal line, and yend = line1 tells it to draw a line from the dot to where it would be if the spouse/partner’s age was the same as the respondents. We can then specify to used dashed lines with linetype = \"dashed\".\n\nThis distance between the observation and the line is known as the residual or the error and it is often expressed with the Greek letter \\(\\epsilon\\) (epsilon). Numerically, we can calculate this by computing the difference between the known value (the observed) and the guessed value.\nFormally, the guessed number is called the expected value (also known as predicted/estimated value). Normally, the expected values in statistics are represented by putting a hat like this \\(\\hat{}\\) on the letters.\nSince the independent variable is usually located on the X axis and the dependent variable on the Y axis. We write write the expected (predicated) value with a hat on the Y like this: \\(\\hat{y}\\) (y-hat).\nLet’s estimate the residuals for each of the observations by subtracting \\(\\hat{y}_i\\) from \\(y_i\\) (spage - line1) and store it in a column called residuals. Again we can use the head() function to see the first few lines of our dataframe.\n\n# estimate residuals\nnilt_sample &lt;- nilt_sample |&gt;\n  mutate(residuals = spage - line1)\n# print first 6 values\nhead(nilt_sample)\n\n# A tibble: 6 × 4\n   rage spage line1 residuals\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1    40    39    40        -1\n2    54    51    54        -3\n3    66    59    66        -7\n4    60    71    60        11\n5    35    34    35        -1\n6    67    57    67       -10\n\n\nComing back once again to the question, is my line good enough?\nWe could sum all these residuals as an overall measure to know how good our line is. From the previous plot, we see that some of the expected ages are higher than the actual values, whilst others are below. This produces negative and positive residuals. If we simply sum them, they would compensate each other and this would not tell us much about the overall magnitude of the difference.\nTo overcome this problem, we square each residual - multiply the residual by itself. This will make all of our values positive - “-3 x -3 = 9” - making it possible to sum them all together.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#key-this-and-expand---could-work-as-initial-return-to-example-from-last-week",
    "href": "08-Lab8.html#key-this-and-expand---could-work-as-initial-return-to-example-from-last-week",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "This is known as the sum of squared residuals (SSR) and we can use this as the criterion to measure how good our line is with respect to the overall trend of the points.\nFor line 1, we can easily calculate the SSR as follows:\n\n# '^' is used for power - so `^2` squares our values\nsum((nilt_sample$residuals)^2)\n\n[1] 1325\n\n\nThe sum of squared residuals for line 1 is 1,325.\n\nHow does our line though compare to other potential lines we could have drawn to describe the trend? We can try different lines to find the combination of intercept and slope that produces the smallest error (SSR). How can we find the best line to describe the trend?\nTo our luck, we can find the best values using a well established technique called Ordinary Least Squares (OLS). Ordinary Least Squares chooses the line that minimises the Sum of Squared Residuals (SSR). We do not need to know the details for now. The important thing is that this procedure guarantees to find a line that produces the smallest possible error (SSR).\nIn R, it is very simple to fit a linear model and we do not need to go through each of the steps above manually nor to memorise all the steps.\nTo fit a linear model, simply use the function lm() and save the result in an object called m1 (for ‘model 1’) and print it.\n\nm1 &lt;- lm(spage ~ rage, data = nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage  \n      6.299        0.875  \n\n\nFor simple linear regression, the lm() function takes arguments in form of lm(dependent-variable ~ independent-variable, dataframe), with the ~ (tilde) operator is used to separate the dependent variable and independent variable.\nSo, this is the optimal intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) that minimises the Sum of Squared Residuals (SSR).\nLet’s see what the SSR is compared to our original arbitrary guess:\n\nsum(residuals(m1)^2)\n\n[1] 1175.209\n\n\nYes, this is better than before. Our line that assumed respondents and their spouse/partner had the same age had an SSR value of 1,325. So, this linear model is better as the SSR value is smaller.\nTo help visualise it, let’s plot the line we guessed and the optimal line together:\n\nggplot(nilt_sample, aes(x = rage, y = spage)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, colour = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    subtitle = \"Comparing a guess line (red) with the OLS best-fit line (blue)\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nAgain, notice how this is just the same code we used to draw our original guess, with an added line using geom_smooth() to draw a line using the linear model. We pass it two arguments, method = \"lm\", to specify that a linear model should be used to draw the line, and se = FALSE so it draws only the line. (By default, geom_smooth() has se = TRUE to include confidence bands so we need to explicitly use se = FALSE to turn them off.)\nThe blue is the optimal solution based on a linear model, whereas the red was just our initial arbitrary guess. We can see that the optimal is more balanced than the red in relation to the observed data points.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#formal-specification",
    "href": "08-Lab8.html#formal-specification",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Now you are ready for the formal specification of the linear model.\nAfter the introduction above, it is easy to take the pieces apart. In essence, the simple linear model is telling us that the dependent value \\(y\\) is defined by a line that intersects the vertical axis at \\(\\beta_0\\), plus the multiplication of the slope \\(\\beta_1\\) by the value of \\(x_1\\), plus some residual \\(\\epsilon\\). The second part of the equation is just telling us that the residuals are normally distributed (that is when a histogram of the residuals follows a bell-shaped curve).\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\qquad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nYou do not need to memorize this equation, but being familiar to this type of notation will help you to expand your skills in quantitative research.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#fitting-linear-regression-in-r",
    "href": "08-Lab8.html#fitting-linear-regression-in-r",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "We already fitted our first linear model using the lm() above, which of course stands for linear model, but let’s go back through it in relation to the specifications.\nThe first argument takes the dependent variable, then, we use tilde ~ to express that this is followed by the independent variable. Then, we specify the data set separated by a comma, as shown below:\n\n\n\nExample Code - do not run\n\nlm(dependent_variable ~ independent_variable, data)\n\n\n\nNow, let’s try with the full nilt data set.\nFirst, we will fit a linear model using the same variables as the example above but including all the observations - remember that before we were working only with a small random sample of 40 observations.\nThe first argument is the respondent’s spouse/partner age spage, followed by the respondent’s age rage. Let’s assign the model to an object called m2 (for ‘model 2’) and print it after.\n\nm2 &lt;- lm(spage ~ rage, data = nilt)\nm2\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nCoefficients:\n(Intercept)         rage  \n     3.7039       0.9287  \n\n\nThe output first displays the formula employed to estimate the model - under Call:. Then, it shows us the estimated values for the intercept and the slope for the age of the respondent, these estimated values are called coefficients.\nIn this model, the coefficients differ from the ones in the example above (m1). This is because as we are now using the full nilt dataframe we have more information to fit the line. Despite the difference, we see that the relationship is still in the same direction (positive). In fact, the value of the slope \\(\\beta_1\\) did not change much if you look at it carefully (0.87 vs 0.92).\n\nBut what are the slope and intercept telling us? The first thing to note is that the slope is positive, which means that the relationship between the respondent age and their spouse/partner age are in the same direction. When the age of the respondent goes up, the age of their spouse/partner is expected to go up as well.\nIn our model, the intercept does not have a meaningful interpretation on its own. What we mean by saying it is not meaningful is that it would not be logical to say that when the respondent’s age is 0 their partner would be expected to be 3.70 years old. Instead, the interpretations should be made only within the range of the values used to fit the model (the youngest age in this data set is 18).\nThe slope can be interpreted as follows: For every year older the respondent is, the partner’s age is expected to change by 0.92. In other words, the respondents are expected to have a partner who is 0.92 times years older than themselves. Or, to put it in more simple terms, for each 1-year increase in the respondent’s age, the expected spouse/partner age increases by 0.92 years. On average then, partners are slightly younger than respondents.\nIn this case, the units of the dependent variable are the same as the independent (age vs age) which make it easy to interpret. In reality, there might be more factors that potentially affect how people select their partners, such as gender, education, race/ethnicity, etc. For now, we are keeping things ‘simple’ by using only these two variables.\nLet’s practice with another example. What if we are interested in income?\nWe can use annual personal income persinc2 as the dependent variable and the number of hours worked a week rhourswk as the independent variable. We’ll assign the result to an object called m3 (for ‘model 3’).\n\nm3 &lt;- lm(persinc2 ~ rhourswk, data = nilt)\nm3\n\n\nCall:\nlm(formula = persinc2 ~ rhourswk, data = nilt)\n\nCoefficients:\n(Intercept)     rhourswk  \n     5170.4        463.2  \n\n\nThis time, the coefficients look quite different. The results are telling us that for every additional hour worked a week a respondent is expected to earn £463.2 more a year than other respondent.\nNote that the input units are not the same in the interpretation. The dependent variable is in pounds earned a year and the independent is in hours worked a week. Does this mean that someone who works 0 hours a week is expected to earn £5170.4 a year (given the fitted model \\(\\hat{persinc2} =  5170.4 + 0*463.2\\)) or that someone who works 110 hours a week (impossible!) is expected to earn £56K a year? We should only make reasonable interpretations of the coefficients within the range analysed.\nFinally, an important thing to know when interpreting linear models using cross-sectional data (data collected in a single time point only) is: correlation does not imply causation. Variables are often correlated with other non-observed variables which may cause the effects observed on the independent variable in reality. This is why we cannot always be sure that X is causing the observed changes on Y.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#model-evaluation",
    "href": "08-Lab8.html#model-evaluation",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "So far, we have talked about the basics of the linear model. If we print a summary of the model, using the handy summary() function, we can obtain more information to evaluate it.\n\nsummary(m2)\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.211  -2.494   0.075   2.505  17.578 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.70389    0.66566   5.564    4e-08 ***\nrage         0.92869    0.01283  72.386   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.898 on 589 degrees of freedom\n  (613 observations deleted due to missingness)\nMultiple R-squared:  0.8989,    Adjusted R-squared:  0.8988 \nF-statistic:  5240 on 1 and 589 DF,  p-value: &lt; 2.2e-16\n\n\nThe summary output has several parts. This can look intimidating at first, but we can break it down bit-by-bit. Let’s focus on:\n\nfirst the call\nsecond the residuals\nthird the coefficients\nfourth the goodness-of-fit measures\n\nThese are highlighted in the screenshot below:\n\n\nModel summary\n\nThe first part, Call, is simply printing the variables that were used to produce the models.\nThe second part, the Residuals, provides a small summary of the residuals by quartile, including minimum and maximum residual values. Remember the second part of the formal specification of the model? Well, this is where we can see in practice the distribution of the residuals. The quartiles show typical prediction errors. For example, half of the residuals lie between the 1st and 3rd quartiles. So by looking at the values for the first and third quartile, we can see that half of the predicted values by the model are \\(\\pm\\) 2.5 years old away from the observed partner’s age… not that bad.\nThe third part is about the coefficients. This time, the summary provides more information about the intercept and the slope than just the estimated coefficients as before. Usually, we focus on the independent variables in this section, rage in our case. The second column, standard error, tell us how large the error of the slope is, which is small for our variable. The third and fourth columns provide measures about the statistical significance of the relationship between the dependent and independent variable.\nThe most commonly reported is the p-value (the fourth column). R uses scientific notation for very small p-values. In our result the p-value is &lt;2e-16, which means a number less than 0.0000000000000002, you can learn about scientific notation here).\nAs a general agreed principle, if the p-value is equal or less than 0.05 (\\(p\\) ≤ 0.05), we can reject the null hypothesis, and say that the relationship between the dependent variable and the independent is significant. In our case, the p-value is far lower than 0.05. So, we can say that the relationship between the respondent’s spouse/partner age and the respondent’s age is significant. In fact, R includes codes for each coefficient to help us to identify the significance. In our result, we got three stars/asterisks. This is because the value is less than 0.001. R assigns two stars if the p-value is less than 0.01, and one star if it is lower than 0.05. (P‑values, however, are just one part of the story; always read them alongside effect size, confidence intervals, and substantive meaning.)\nIn the fourth area, we have various goodness-of-fit measures. Overall, these measures tell us how well the model represents our data. For now, let’s focus on the adjusted R-squared.\nR-squared is the proportion of variation in the outcome explained by the model. It ranges from 0 to 1: 0 means none of the variance is explained; 1 means all variance in the sample is explained by the model. Adjusted R-squared penalises adding variables that do not improve the model. In the social sciences, modest R-squared values are common.\nAdjusted R-squared then can be understood as the proportion (multiply it by 100 to report a percentage) of variance explained by the model, adjusted for the number of independent variables used. In our age-age example, the model explains 89.88% percent of the total variance. This is a good fit, but this is because we are modelling a quite “obvious” relationship. The R-squared describes the model file, it does not judge the quality of our analysis.\nYou may wonder why we do not report the sum of squared residuals (SSR), as we covered in our initial example. This is because the scale of the SSR is in the units used to fit the model, which would make it difficult to compare our model with other models that use different units or different variables. Conversely, the adjusted R-squared is expressed in relative terms which makes it a more suitable measure to compare with other models. An important thing to note is that the output is telling us that this model excluded 613 observations which were missing in our dataset. This is because we do not have the age of the respondent’s partner in 613 of the rows. Missing spouse/partner ages can occur for serveral reasons. For example, the respondent may not have a parter, or choose not to answer.\nIn academic papers in the social sciences, usually the measures reported are the coefficients, p-values, the adjusted R-squared, and the size of the sample used to fit the model (number of observations).\n\nThe linear model, as many other techniques in statistics, relies on assumptions. These refer to characteristics of the data that are taken for granted to generate the estimates. It is the task of the modeller/analyst to make sure the data used follows these assumptions. One simple yet important check is to examine the distribution of the residuals, which ought to follow a normal distribution.\nDo the residuals follow a normal distribution in our model, m2? We can go further and plot a histogram to graphically evaluate it. Use the function residuals() to extract the residuals from our object m2, and then plot a histogram with the base R function hist().\n\nhist(residuals(m2), breaks = 20)\n\n\n\n\n\n\n\nOverall, it seems that the residuals follow the normal distribution reasonably well, with the exception of the negative value to the left of the plot. Very often when there is a strange distribution or different to the normal, it is because one of the assumptions is violated. If that is the case we cannot trust the coefficient estimates. In the next lab we will talk more about the assumptions of the linear model. For now, we will leave it here and get some practice.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "08-Lab8.html#lab-activities",
    "href": "08-Lab8.html#lab-activities",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "In your R Markdown file, use the nilt dataset to:\n\nPlot a scatter plot using ggplot. In the aesthetics, locate rhourswk in the X axis, and persinc2 in the Y axis. In the geom_point() jitter the points by specifying the position = 'jitter'. Also, include the best fit line using the geom_smooth() function, and specify the method = 'lm' inside.\nPrint the summary of m3 using the summary() function.\nIs the relationship of hours worked a week statistically significant?\nWhat is the adjusted R-squared? How would you interpret it?\nWhat is the sample size to fit the model?\nWhat is the expected income in pounds a year for a respondent who works 30 hours a week according to coefficients of this model?\nPlot a histogram of the residuals of m3 using the residuals() function inside hist(). Do the residuals look normally distributed (as in a bell-shaped curve)?\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 8** Simple Linear Regression"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "NoteOverview\n\n\n\nBy the end of this lab you will:\n\nlog into Posit Cloud and join your lab group space\nlearn basic R syntax and use R as a calculator\ndiscover the importance of the order code is run in\ncreate a neatly formatted HTML file by knitting an R Markdown file\n\n\n\n\nFor this course we will be using R (R Core Team 2021) and RStudio as the main tools for conducting quantitative analysis.\nR and the basic versions of RStudio are free and open-source software. There are ‘free’ as in cost, but - more importantly - they are free software that guarantee users four foundamental freedoms. As explained on the FSFE’s What is Free Software? page, these are:\n\n\n\nUse: Free Software can be used for any purpose and is free of restrictions such as licence expiry or geographic limitations.\n\nStudy: Free Software and its code can be studied by anyone, without non‐disclosure agreements or similar restrictions.\n\nShare: Free Software can be shared and copied at virtually no cost.\n\nImprove: Free Software can be modified by anyone, and these improvements can be shared publicly.\n\n\nEven though R appeared in the early 90s, it has been gaining a lot of popularity in recent years. A main contributor to its success is being free software, with a large community of people contributing improvements to R directly as well as extending it with further additional software packages. In fact, it is now one of the most common software for doing statistics in academia.\n\n\nSource: https://tiobe.com\n\nR and RStudio are two separate things. R is the actual programming language and the main processing tool which does the computations in the background, whereas RStudio integrates all functionalities into a (relatively) friendly and interactive interface. In short, for this course (and most of the time in practice) you chiefly use RStudio whilst R is doing all the work in the background. Thereafter, we will refer to R, as the integrated interface.\n\n\n\n\n\n\nTipNew terms\n\n\n\n\n\nR: a programming language and environment for data analysis.\n\nRStudio: a user-interface for working with R.\n\nPosit Cloud: website for running RStudio in a browser.\n\n\n\n\nAt this point you may be wondering why you need to bother learning these tools. In the following sections you will see some of the advantages and examples that can be achieved using R.\n\nR can be applied in a wide variety of fields and subjects, including not only those in the social sciences (e.g. sociology, politics, or policy research), but also in the humanities (e.g. history, digital humanities), natural and physical sciences (e.g. biology, chemistry, or geography), health (e.g. medical studies, public health, epidemiology), business and management (e.g. finance, economics, marketing), among many others.\nThe broad application of R is due to its flexibility which allows it to perform a range of tasks related to data. These cover tasks at initial stages, such as downloading, mining, or importing data. But it is also useful to manipulate, edit, transform, and organise information. Furthermore, and most important for us, there are a set of tools that allow us to analyse data using a range of statistical techniques. These are useful to understand, summarize, and draw conclusions about samples, e.g. people.\nLastly, R is powerful to communicate and share information and documents. There are several extensions (called packages in R) that can help to produce static and interactive plots/charts, maps, written reports, interactive applications or even entire books! In fact this workbook was written from RStudio.\nR works in a command-based Console environment. This means that you need to call the commands (or functions, as they are called in R) through writing text. This can look intimidating at first glance. But do not worry, we will guide you step by step. Importantly, that these commands can be used and combined in multiple ways is what gives R its incredible flexible. Once you get the hang of these you will find that they are faster and more powerful than using a button and menu based interface. RStudio also helps make working with R more accessible.\n\n\n\n\n\n\nTipNew terms\n\n\n\nWe will cover these in more detail in future weeks, but as initial short definitions:\n\n\nPackage: a collection of ready-made R tools - such as functions with help pages, and sometimes example data - that you can install (and later load) to add features to R.\n\nFunction: a named reusable set of instructions that can take inputs (referred to as arguments), run some steps, and return a result.\n\nArgument: input provided to a function that tell the function what to use and how to behave.\n\n\n\n\nSome advantages of using R:\n\nIt is free and open source software. You do not need to pay for a licence. You can then use it anywhere at any time, even if you do not have an affiliation to an institution or organisation (e.g. University or workplace);\nIt is a collaborative project. This means that it is the users who maintain, extend, and update its applications;\nIt is reproducible. Research can be more transparent since you will get the same results every time you run your analysis through a specific pathway (i.e. through R Markdown files);\nHigh compatibility. You can read and produce most types of file extensions;\nOnline community. There are a number of easy-access web resources to support you in the learning process.\n\n\n\nThere are multiple ways to work with R. One, and by far the most common, is to download both R and RStudio Desktop and install the applications on your local device.1\nOn this course, we use Posit Cloud instead. Posit Cloud provides an on-line version of RStudio that does not require installing any additional software. You can run it directly from your browser (e.g. Chrome, Firefox, Safari, etc), including if using a Chromebook or tablet. This makes it easier to access your RStudio projects and files within the labs and when using your own devices.\nTo get started with Posit Cloud, follow the next steps:\n\n\n\n\n\n\n\n\nClick on this link Posit Cloud - SSO, which should automatically open a new tab in your web browser or go directly to the browser and copy this URL: https://sso.posit.cloud/glasgow;\nClick ‘Log In Via University of Glasgow’ which will take you to the usual University of Glasgow login pages;\nEnter your University of Glasgow email address and password in the login page as normal;\n\n\n\nDone! After logging in with your University of Glasgow account, you will be taken to your personal RStudio Cloud workspace;\n\nYou will have received a link via email to join your lab group on Posit Cloud (the link will be available within your Lab Group forum on Moodle too). Note, you must use this specific link to join and access your lab group workspace, as each link is unique to your group. So, only use your group’s specific link.\n\nCopy and paste the link in your web browser. You should see the following window:\n\n\n\nJoin your lab by clicking the ‘Yes’ button shown above.\n\n\nOpen the shared space form the left-hand side pane called ‘Lab Group …’ (where … is your Lab Group number).\nClick the blue ‘New Project’ button on the right of the screen.\nFrom the menu list that opens select ‘New Project from Git Repository’.\n\n\n\nWithin the dialogue box that opens, copy and paste the link below into the ‘URL of your Git Repository’ field.\n\nhttps://github.com/UGQuants/Labs-1-2\n\nClick ‘OK’ to create the new project.\n\n\n\n\n\n\n\n\n\nAfter your new ‘Labs 1-2’ project is created Posit Cloud will automatically open it in RStudio and you will see the screen below:\n\n\nIn the bottom-right ‘Files’ pane, click “Lab-01.Rmd”.\nThat will open a new pane in the top-left, within the yellow banner in it click ‘Install’.\n\n\nThe bottom-left pane will now switch to the “Background Jobs” tab and you will see a lot of red text. Despite the colour, this is expected behaviour. RStudio is now installing the rmarkdown package that makes it possible to work with R Markdown files in RStudio (more on this later). Once it finishes installing, the tab will automatically switch back to “Console”.\n\nR Markdown is a file format (.Rmd) that lets you write text and run code in the same document. You write your narrative text and add your code within code chunks. When you Knit (render) a R Markdown file, R runs the code chunks from top to bottom and weaves their outputs (numbers, tables, plots) into a finished ‘knitted’ report (such as HTML or PDF). This makes your analysis transparent and reproducible, whereby anyone can see what code produced each result. When learning R, it also makes it great for making notes alongside the code.\n\n\n\n\n\n\nTipNew terms\n\n\n\nWe will cover R Markdown in more detail across the next few weeks, but some initial short definitions:\n\n\nR Markdown: a file format that mixes text and code, using simple syntax for formatting text and code chunks for adding and running code.\n\ncode chunk: a fenced block in an R Markdown file for adding R code; as well as being run when you knit the file, each code chunk can be run on its own within RStudio.\n\nknit: render an R Markdown file into a specified format (e.g. HTML or PDF) by running all code chunks from top to bottom and creating a knitted document combining the text with the output from the code.\n\n\n\n\nAfter this, your RStudio screen will be split in four important windows or panes as shown below:\n\n\nIn Pane 1 (purple), you have the Lab-01 R Markdown file.\n\nThis is the area where you will be working most of the time. From here, you will write text and code. Code chunks are shaded grey, with each starting with ```{r optional-name} and ending with ```. (The ` is called a back-tick, and on UK keyboards is the key to the left of 1.)\n\nIn Pane 2 (blue), you have the “Global Environment”.\n\nThis is one of the most useful tabs in this pane. It shows you the active ‘objects’ that you have available/loaded in your current R session (this will make more sense in the coming sections).\n\nIn Pane 3 (green), you have the R Console.\n\nThis is where you will see most of the results of the code you run (pane 1). You can also write and run code from here, by typing the code and hitting enter. Note, it is best practice to have code necessary for your analysis saved in your R Markdown file. The Console is usually used instead to quickly run code that you do not want to save in your file.\n\nFinally, in Pane 4 (yellow) you have multiple useful tabs.\n\nIn the File tab you can see the files and directories that you have in your R project.\nIn the Plot tab you will see a preview of the static plots/charts you will be producing from your script.\nIn Packages, you have a list of the extensions or plug-ins (called ‘packages’ in R) that are installed in your working environment.\nThe Help contains some resources that clarify or expand what each of the functions does. Again, probably this will make more sense once you get started. We will come back to this later.\nFinally, the Viewer displays interactive outputs.\nNote, code chunks (the shaded grey fenced blocks) for writing code into have been set up for you this week. Remember though you can also write regular text as well outside of these chunks to record any notes:\n\n\nNow you are ready! It is your turn to start exploring and getting familiar with R by completing the following activities.\n\nIn your ‘Lab-01’ R Markdown file, look at the code chunk named “sum-1” containing 2 + 2. Let’s run it!\nTo run a code chunk, you can click on the Run green arrow situated on the top right corner of it. Alternatively, if your text cursor is inside the code chunk you can press Ctrl+Shift+Enter on your keyboard.\n\nWhat you will see after running the chunk is:\n\nIn the R Markdown file, [1] 4 underneath the code chunk.\nIn the Console, both &gt; 2 + 2 and [1] 4.\n\n\nWhy do we see this? When you run a chunk, RStudio sends the code to the Console for R to run it, so you see the code echoed and its raw output there. RStudio also shows the results inline under the chunk so you can see the output in context. (This also becomes important later with graphs and formatted tables that cannot be viewed in the Console.)\nWhat does the [1] mean? It’s a label R adds to printed results, where [1] just means “this line starts at item 1”. If there were lots of items, the next line might start [7] or [15].\nOf course, you can also go to the Console, write a simple calculation, and run it by typing ‘Enter’, as shown below:\n\nIt’s also possible to run multiple calculations in a single chunk. Add a line for 5 * 5 and 21 -4 in the empty “sum-2” chunk and then run it. (Hint: Use the green triangle top-right of the code-chunk to run it.)\n\nTry different operations such as 50 / 20 or 3 * 5, whether adding to the code chunk or directly in the Console.\nFairly simple, right? And don’t forget, it is normal to copy and tweak existing code. A lot of learning to code involves going through working examples and tweaking it to work with what you are trying to achieve. Unlike writing an essay or an exam, you don’t actually need to know and write code “off the cuff” or have all necessary code syntax and functions memorised. So, don’t worry if you feel like you are just making minor changes to existing code, that’s how it’s supposed to work. The crucial thing is learning how the code works, so you know what to tweak for your analysis. And, the first few weeks is all about getting comfortable in using R, then the level of challenge will go up.\nLet’s continue with the next activities!\n\nNow, run the “logical-1” code chunk in the R Markdown file.\n\nNow, in the “logical-2” code chunk, add the following and then run it:\n\n1 == 5\n1 &gt; 5\n5 &lt; 10\n'this' == 'this'\n'this' == 'that'\n'this' != 'that'\n\n(Note, if you receive an “Error: object ‘this’ not found’” message, check you are using single quotes and have 'this' rather than this.)\nWhat do you see running the code chunks? Why is R outputting these results? …\nWhen you use the double equal sign == you are asking R whether the value on the left hand-side of the operator is equal to the one on the right hand-side. Likewise, when you combine the exclamation mark ! with another operator, you get the reversed result. For example, != is interpreted as “is not equal to”, that is why 10 != 10 returns FALSE, but 10 == 10 returns TRUE.\nR can process different classes of inputs. In this case we used a letters and we asked R whether ‘this’ was equal to ‘this’, and of course the result is TRUE. Note that when you want to input text (referred as character values in R), you need quotation marks, 'text'. If you want to enter numeric values, you simply input the raw number without quotation marks. These are then treated as numeric ‘class’ values.\nYou can use the class() function to check what something is in R. For example, try running class(42), class('42'), and class(TRUE) in the Console.\nPerhaps logical operators do not make much sense at this point, but you will find out later that they are useful to manipulate data. For example, these are essential to filter a data set based on specific rules or patterns.\n\n\n\n\n\n\nTipNew Terms\n\n\n\n\n\nclass: an object’s classification in R that informs how R handles it.\n\ncharacter: text in quotes, such as “Glasgow”. ('42' as it is in quotes would be treated as character)\n\nnumeric: real or decimal numbers, such as 42 or 5.1.\n\n\n\n\nIn R, it is common (and practical) to store values or data as ‘objects’. Once values are assigned to them, objects are then stored in your current R session. Assignments are made using name &lt;- value, such as a &lt;- 5 or b &lt;- 'Glasgow'. Let’s try it!\nGo to the “assign-1” code chunk run it by clicking the green arrow/triangle or - with your text cursor within the code chunk - using Ctrl+Shift+Enter.\nWhat do you observe? Pay attention to what appears under the code chunk and in the Environment pane (the top-right of the four panes). Why are the values different? …\n\nThe operator &lt;- assigned the numeric value 10 to the name a (on the left hand-side of the arrow). The next line used the object (a) to compute a sum (i.e, a + 5). Only the assignment is stored in the environment (the a &lt;- 10) and assignments do not return an output, so only the result of a + 5 is shown under the code chunk when run.\nTo see this, within the “assign-2” chunk add c &lt;- 3 and run it:\n\nNow, add and run the following in the “assign-2” chunk:\n\nc\na * c\n\n\nAs you can see, running c on a line by its own returns the numeric value 3 stored in the variable c. Then, a * c calls the previously created object a and multiplies it by c (a * c is effectively 10 * 3). Note, most programming languages, R included, do not use x for multiplication and use * instead. One reason for this is because x could also end up as a named object, x &lt;- 2.\nIn the same way as you assigned these simple variables, you will store other types of objects later, e.g. vectors, data frames or lists. This is useful because those objects will be ready in your session to do some computations.\n\nThere are a few things to note when assigning objects to variables. If you assign a different value to the same object, e.g. by running a &lt;- 5, you will replace the old value with the new. So, instead of having a representing the value 10, you will have 5.\nYou can see the objects available in your session on the Global Environment (‘Environment’ tab in top-right pane) as shown below.\n\nNow, let’s see what happens if change value of a:\n\nRun the “order-1” code chunk that contains a + 2.\nNow in the Console type and run a &lt;- 5.\nRun the “order-1” code chunk again.\n\n\nAs can see, the code in the “order-1” chunk remains the same, but the result from running the code changes based on the value assigned to a. That may seem an obvious point to make, but a large proportion of unexpected results and errors arise from changing what is assigned to an object.\nA good rule of thumb then when a code chunk that was previously working suddenly returns unexpected results or an error message is to run your code from the top-down. Thankfully, RStudio also makes this easy to do. To ‘Run All Chunks Above’ a specific code chunk, click the downward facing grey arrow/triangle with a green line under it (see image below!). After that, you can click the green arrow/triangle to ‘Run Current Chunk’.\nDo that for the ‘order-1’ chunk and it will return 12 again:\n\nIt is also possible to use an object in a calculation and assign the result of that calculation back to the object. For example, a &lt;- a + 5 is equivalent to a &lt;- 10 + 5. Whilst that can be perfectly valid to do, care needs to be taken as now a is 15. What do you think would happen if you keep running the same a &lt;- a + 5 line of code?\nGo to the “order-2” code chunk and run it multiple times, looking at the result underneath the chunk and the value for a in the Environment pane.\n\nAgain, you can ‘Run All Chunks Above’ (downwards grey arrow/triangle, green line) and ‘Run Current Chunk’ (green arrow/triangle) to restore a to value it would be if only ran each code chunk once top to bottom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a good start, great job!\nNote that the changes made in your R Markdown file are saved automatically within RStudio when using Posit Cloud. To verify this, have a look at the name of your file in the top-left of pane 1. If changes are due to be saved, the name will be written in red with an asterisk at the end, Lab-01.Rmd*. If it is in red, save changes manually by clicking on the disk icon. After you have made sure your changes are saved, you are safe to end your session by closing the RStudio Cloud tab in your browser.\n\n\n\n\n\n\nTipNew Terms\n\n\n\n\n\nobject: any data or function that exists in the R session’s memory; usually accessed via a name.\n\nassignment: assign a value (object) to a name; usually with &lt;- (e.g. a &lt;- 10).\n\nenvironment: a workspace that maps names to objects; the main one you use is the Global Environment (visible in RStudio’s top-right Environment pane).\n\n\n\n\nDiscuss the following questions with your neighbour or tutor, and write notes from your discussion in your R Markdown file:\n\nWhat are the main differences between working in a R Markdown file (pane 1) and directly on the console (pane 3)?\nCan you describe what happens when your run the following code? (tip: look at the environment tab in pane 2)\n\n\nobject1 &lt;- 10\nobject1 &lt;- 30\n\n\nClick the ‘Knit’ button in the bar at the top of the R Markdown file (pane 1). This will turn your R Markdown file into an HTML document.\n\n(Note: Another reason code chunk order matters. R Markdown files are meant to be reproducible, where anyone with a copy of the file can run the code from top to bottom and receive the same results. When you ‘knit’ an R Markdown file, the code chunks are run top to bottom in a fresh R session - in other words, it starts with a blank Global Environment. For example, if you run d &lt;- 25 manually in the Console and then add d + 5 in a code chunk, that code chunk will work fine when manually run in RStudio, but will result in an “object not found” error if you try to knit. This is because d &lt;- 25 also needs to be in a code chunk within the R Markdown file, and needs to come before d + 5. So, make sure all essential code for your analysis is within code chunks and in the correct order that they need to be run in.)\n\nCreate a new code chunk, you can put your text cursor on a line at the end of the R Markdown file and press Ctrl+Alt+I on the keyboard. Alternatively, from the menu bar at the top of RStudio, you can select Code &gt; Insert Chunk.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#tools-we-are-using",
    "href": "01-intro.html#tools-we-are-using",
    "title": "Introduction to R",
    "section": "",
    "text": "For this course we will be using R (R Core Team 2021) and RStudio as the main tools for conducting quantitative analysis.\nR and the basic versions of RStudio are free and open-source software. There are ‘free’ as in cost, but - more importantly - they are free software that guarantee users four foundamental freedoms. As explained on the FSFE’s What is Free Software? page, these are:\n\n\n\nUse: Free Software can be used for any purpose and is free of restrictions such as licence expiry or geographic limitations.\n\nStudy: Free Software and its code can be studied by anyone, without non‐disclosure agreements or similar restrictions.\n\nShare: Free Software can be shared and copied at virtually no cost.\n\nImprove: Free Software can be modified by anyone, and these improvements can be shared publicly.\n\n\nEven though R appeared in the early 90s, it has been gaining a lot of popularity in recent years. A main contributor to its success is being free software, with a large community of people contributing improvements to R directly as well as extending it with further additional software packages. In fact, it is now one of the most common software for doing statistics in academia.\n\n\nSource: https://tiobe.com\n\nR and RStudio are two separate things. R is the actual programming language and the main processing tool which does the computations in the background, whereas RStudio integrates all functionalities into a (relatively) friendly and interactive interface. In short, for this course (and most of the time in practice) you chiefly use RStudio whilst R is doing all the work in the background. Thereafter, we will refer to R, as the integrated interface.\n\n\n\n\n\n\nTipNew terms\n\n\n\n\n\nR: a programming language and environment for data analysis.\n\nRStudio: a user-interface for working with R.\n\nPosit Cloud: website for running RStudio in a browser.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#why-r",
    "href": "01-intro.html#why-r",
    "title": "Introduction to R",
    "section": "",
    "text": "At this point you may be wondering why you need to bother learning these tools. In the following sections you will see some of the advantages and examples that can be achieved using R.\n\nR can be applied in a wide variety of fields and subjects, including not only those in the social sciences (e.g. sociology, politics, or policy research), but also in the humanities (e.g. history, digital humanities), natural and physical sciences (e.g. biology, chemistry, or geography), health (e.g. medical studies, public health, epidemiology), business and management (e.g. finance, economics, marketing), among many others.\nThe broad application of R is due to its flexibility which allows it to perform a range of tasks related to data. These cover tasks at initial stages, such as downloading, mining, or importing data. But it is also useful to manipulate, edit, transform, and organise information. Furthermore, and most important for us, there are a set of tools that allow us to analyse data using a range of statistical techniques. These are useful to understand, summarize, and draw conclusions about samples, e.g. people.\nLastly, R is powerful to communicate and share information and documents. There are several extensions (called packages in R) that can help to produce static and interactive plots/charts, maps, written reports, interactive applications or even entire books! In fact this workbook was written from RStudio.\nR works in a command-based Console environment. This means that you need to call the commands (or functions, as they are called in R) through writing text. This can look intimidating at first glance. But do not worry, we will guide you step by step. Importantly, that these commands can be used and combined in multiple ways is what gives R its incredible flexible. Once you get the hang of these you will find that they are faster and more powerful than using a button and menu based interface. RStudio also helps make working with R more accessible.\n\n\n\n\n\n\nTipNew terms\n\n\n\nWe will cover these in more detail in future weeks, but as initial short definitions:\n\n\nPackage: a collection of ready-made R tools - such as functions with help pages, and sometimes example data - that you can install (and later load) to add features to R.\n\nFunction: a named reusable set of instructions that can take inputs (referred to as arguments), run some steps, and return a result.\n\nArgument: input provided to a function that tell the function what to use and how to behave.\n\n\n\n\nSome advantages of using R:\n\nIt is free and open source software. You do not need to pay for a licence. You can then use it anywhere at any time, even if you do not have an affiliation to an institution or organisation (e.g. University or workplace);\nIt is a collaborative project. This means that it is the users who maintain, extend, and update its applications;\nIt is reproducible. Research can be more transparent since you will get the same results every time you run your analysis through a specific pathway (i.e. through R Markdown files);\nHigh compatibility. You can read and produce most types of file extensions;\nOnline community. There are a number of easy-access web resources to support you in the learning process.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#getting-started",
    "href": "01-intro.html#getting-started",
    "title": "Introduction to R",
    "section": "",
    "text": "There are multiple ways to work with R. One, and by far the most common, is to download both R and RStudio Desktop and install the applications on your local device.1\nOn this course, we use Posit Cloud instead. Posit Cloud provides an on-line version of RStudio that does not require installing any additional software. You can run it directly from your browser (e.g. Chrome, Firefox, Safari, etc), including if using a Chromebook or tablet. This makes it easier to access your RStudio projects and files within the labs and when using your own devices.\nTo get started with Posit Cloud, follow the next steps:\n\n\n\n\n\n\n\n\nClick on this link Posit Cloud - SSO, which should automatically open a new tab in your web browser or go directly to the browser and copy this URL: https://sso.posit.cloud/glasgow;\nClick ‘Log In Via University of Glasgow’ which will take you to the usual University of Glasgow login pages;\nEnter your University of Glasgow email address and password in the login page as normal;\n\n\n\nDone! After logging in with your University of Glasgow account, you will be taken to your personal RStudio Cloud workspace;\n\nYou will have received a link via email to join your lab group on Posit Cloud (the link will be available within your Lab Group forum on Moodle too). Note, you must use this specific link to join and access your lab group workspace, as each link is unique to your group. So, only use your group’s specific link.\n\nCopy and paste the link in your web browser. You should see the following window:\n\n\n\nJoin your lab by clicking the ‘Yes’ button shown above.\n\n\nOpen the shared space form the left-hand side pane called ‘Lab Group …’ (where … is your Lab Group number).\nClick the blue ‘New Project’ button on the right of the screen.\nFrom the menu list that opens select ‘New Project from Git Repository’.\n\n\n\nWithin the dialogue box that opens, copy and paste the link below into the ‘URL of your Git Repository’ field.\n\nhttps://github.com/UGQuants/Labs-1-2\n\nClick ‘OK’ to create the new project.\n\n\n\n\n\n\n\n\n\nAfter your new ‘Labs 1-2’ project is created Posit Cloud will automatically open it in RStudio and you will see the screen below:\n\n\nIn the bottom-right ‘Files’ pane, click “Lab-01.Rmd”.\nThat will open a new pane in the top-left, within the yellow banner in it click ‘Install’.\n\n\nThe bottom-left pane will now switch to the “Background Jobs” tab and you will see a lot of red text. Despite the colour, this is expected behaviour. RStudio is now installing the rmarkdown package that makes it possible to work with R Markdown files in RStudio (more on this later). Once it finishes installing, the tab will automatically switch back to “Console”.\n\nR Markdown is a file format (.Rmd) that lets you write text and run code in the same document. You write your narrative text and add your code within code chunks. When you Knit (render) a R Markdown file, R runs the code chunks from top to bottom and weaves their outputs (numbers, tables, plots) into a finished ‘knitted’ report (such as HTML or PDF). This makes your analysis transparent and reproducible, whereby anyone can see what code produced each result. When learning R, it also makes it great for making notes alongside the code.\n\n\n\n\n\n\nTipNew terms\n\n\n\nWe will cover R Markdown in more detail across the next few weeks, but some initial short definitions:\n\n\nR Markdown: a file format that mixes text and code, using simple syntax for formatting text and code chunks for adding and running code.\n\ncode chunk: a fenced block in an R Markdown file for adding R code; as well as being run when you knit the file, each code chunk can be run on its own within RStudio.\n\nknit: render an R Markdown file into a specified format (e.g. HTML or PDF) by running all code chunks from top to bottom and creating a knitted document combining the text with the output from the code.\n\n\n\n\nAfter this, your RStudio screen will be split in four important windows or panes as shown below:\n\n\nIn Pane 1 (purple), you have the Lab-01 R Markdown file.\n\nThis is the area where you will be working most of the time. From here, you will write text and code. Code chunks are shaded grey, with each starting with ```{r optional-name} and ending with ```. (The ` is called a back-tick, and on UK keyboards is the key to the left of 1.)\n\nIn Pane 2 (blue), you have the “Global Environment”.\n\nThis is one of the most useful tabs in this pane. It shows you the active ‘objects’ that you have available/loaded in your current R session (this will make more sense in the coming sections).\n\nIn Pane 3 (green), you have the R Console.\n\nThis is where you will see most of the results of the code you run (pane 1). You can also write and run code from here, by typing the code and hitting enter. Note, it is best practice to have code necessary for your analysis saved in your R Markdown file. The Console is usually used instead to quickly run code that you do not want to save in your file.\n\nFinally, in Pane 4 (yellow) you have multiple useful tabs.\n\nIn the File tab you can see the files and directories that you have in your R project.\nIn the Plot tab you will see a preview of the static plots/charts you will be producing from your script.\nIn Packages, you have a list of the extensions or plug-ins (called ‘packages’ in R) that are installed in your working environment.\nThe Help contains some resources that clarify or expand what each of the functions does. Again, probably this will make more sense once you get started. We will come back to this later.\nFinally, the Viewer displays interactive outputs.\nNote, code chunks (the shaded grey fenced blocks) for writing code into have been set up for you this week. Remember though you can also write regular text as well outside of these chunks to record any notes:",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#hands-on-r",
    "href": "01-intro.html#hands-on-r",
    "title": "Introduction to R",
    "section": "",
    "text": "Now you are ready! It is your turn to start exploring and getting familiar with R by completing the following activities.\n\nIn your ‘Lab-01’ R Markdown file, look at the code chunk named “sum-1” containing 2 + 2. Let’s run it!\nTo run a code chunk, you can click on the Run green arrow situated on the top right corner of it. Alternatively, if your text cursor is inside the code chunk you can press Ctrl+Shift+Enter on your keyboard.\n\nWhat you will see after running the chunk is:\n\nIn the R Markdown file, [1] 4 underneath the code chunk.\nIn the Console, both &gt; 2 + 2 and [1] 4.\n\n\nWhy do we see this? When you run a chunk, RStudio sends the code to the Console for R to run it, so you see the code echoed and its raw output there. RStudio also shows the results inline under the chunk so you can see the output in context. (This also becomes important later with graphs and formatted tables that cannot be viewed in the Console.)\nWhat does the [1] mean? It’s a label R adds to printed results, where [1] just means “this line starts at item 1”. If there were lots of items, the next line might start [7] or [15].\nOf course, you can also go to the Console, write a simple calculation, and run it by typing ‘Enter’, as shown below:\n\nIt’s also possible to run multiple calculations in a single chunk. Add a line for 5 * 5 and 21 -4 in the empty “sum-2” chunk and then run it. (Hint: Use the green triangle top-right of the code-chunk to run it.)\n\nTry different operations such as 50 / 20 or 3 * 5, whether adding to the code chunk or directly in the Console.\nFairly simple, right? And don’t forget, it is normal to copy and tweak existing code. A lot of learning to code involves going through working examples and tweaking it to work with what you are trying to achieve. Unlike writing an essay or an exam, you don’t actually need to know and write code “off the cuff” or have all necessary code syntax and functions memorised. So, don’t worry if you feel like you are just making minor changes to existing code, that’s how it’s supposed to work. The crucial thing is learning how the code works, so you know what to tweak for your analysis. And, the first few weeks is all about getting comfortable in using R, then the level of challenge will go up.\nLet’s continue with the next activities!\n\nNow, run the “logical-1” code chunk in the R Markdown file.\n\nNow, in the “logical-2” code chunk, add the following and then run it:\n\n1 == 5\n1 &gt; 5\n5 &lt; 10\n'this' == 'this'\n'this' == 'that'\n'this' != 'that'\n\n(Note, if you receive an “Error: object ‘this’ not found’” message, check you are using single quotes and have 'this' rather than this.)\nWhat do you see running the code chunks? Why is R outputting these results? …\nWhen you use the double equal sign == you are asking R whether the value on the left hand-side of the operator is equal to the one on the right hand-side. Likewise, when you combine the exclamation mark ! with another operator, you get the reversed result. For example, != is interpreted as “is not equal to”, that is why 10 != 10 returns FALSE, but 10 == 10 returns TRUE.\nR can process different classes of inputs. In this case we used a letters and we asked R whether ‘this’ was equal to ‘this’, and of course the result is TRUE. Note that when you want to input text (referred as character values in R), you need quotation marks, 'text'. If you want to enter numeric values, you simply input the raw number without quotation marks. These are then treated as numeric ‘class’ values.\nYou can use the class() function to check what something is in R. For example, try running class(42), class('42'), and class(TRUE) in the Console.\nPerhaps logical operators do not make much sense at this point, but you will find out later that they are useful to manipulate data. For example, these are essential to filter a data set based on specific rules or patterns.\n\n\n\n\n\n\nTipNew Terms\n\n\n\n\n\nclass: an object’s classification in R that informs how R handles it.\n\ncharacter: text in quotes, such as “Glasgow”. ('42' as it is in quotes would be treated as character)\n\nnumeric: real or decimal numbers, such as 42 or 5.1.\n\n\n\n\nIn R, it is common (and practical) to store values or data as ‘objects’. Once values are assigned to them, objects are then stored in your current R session. Assignments are made using name &lt;- value, such as a &lt;- 5 or b &lt;- 'Glasgow'. Let’s try it!\nGo to the “assign-1” code chunk run it by clicking the green arrow/triangle or - with your text cursor within the code chunk - using Ctrl+Shift+Enter.\nWhat do you observe? Pay attention to what appears under the code chunk and in the Environment pane (the top-right of the four panes). Why are the values different? …\n\nThe operator &lt;- assigned the numeric value 10 to the name a (on the left hand-side of the arrow). The next line used the object (a) to compute a sum (i.e, a + 5). Only the assignment is stored in the environment (the a &lt;- 10) and assignments do not return an output, so only the result of a + 5 is shown under the code chunk when run.\nTo see this, within the “assign-2” chunk add c &lt;- 3 and run it:\n\nNow, add and run the following in the “assign-2” chunk:\n\nc\na * c\n\n\nAs you can see, running c on a line by its own returns the numeric value 3 stored in the variable c. Then, a * c calls the previously created object a and multiplies it by c (a * c is effectively 10 * 3). Note, most programming languages, R included, do not use x for multiplication and use * instead. One reason for this is because x could also end up as a named object, x &lt;- 2.\nIn the same way as you assigned these simple variables, you will store other types of objects later, e.g. vectors, data frames or lists. This is useful because those objects will be ready in your session to do some computations.\n\nThere are a few things to note when assigning objects to variables. If you assign a different value to the same object, e.g. by running a &lt;- 5, you will replace the old value with the new. So, instead of having a representing the value 10, you will have 5.\nYou can see the objects available in your session on the Global Environment (‘Environment’ tab in top-right pane) as shown below.\n\nNow, let’s see what happens if change value of a:\n\nRun the “order-1” code chunk that contains a + 2.\nNow in the Console type and run a &lt;- 5.\nRun the “order-1” code chunk again.\n\n\nAs can see, the code in the “order-1” chunk remains the same, but the result from running the code changes based on the value assigned to a. That may seem an obvious point to make, but a large proportion of unexpected results and errors arise from changing what is assigned to an object.\nA good rule of thumb then when a code chunk that was previously working suddenly returns unexpected results or an error message is to run your code from the top-down. Thankfully, RStudio also makes this easy to do. To ‘Run All Chunks Above’ a specific code chunk, click the downward facing grey arrow/triangle with a green line under it (see image below!). After that, you can click the green arrow/triangle to ‘Run Current Chunk’.\nDo that for the ‘order-1’ chunk and it will return 12 again:\n\nIt is also possible to use an object in a calculation and assign the result of that calculation back to the object. For example, a &lt;- a + 5 is equivalent to a &lt;- 10 + 5. Whilst that can be perfectly valid to do, care needs to be taken as now a is 15. What do you think would happen if you keep running the same a &lt;- a + 5 line of code?\nGo to the “order-2” code chunk and run it multiple times, looking at the result underneath the chunk and the value for a in the Environment pane.\n\nAgain, you can ‘Run All Chunks Above’ (downwards grey arrow/triangle, green line) and ‘Run Current Chunk’ (green arrow/triangle) to restore a to value it would be if only ran each code chunk once top to bottom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a good start, great job!\nNote that the changes made in your R Markdown file are saved automatically within RStudio when using Posit Cloud. To verify this, have a look at the name of your file in the top-left of pane 1. If changes are due to be saved, the name will be written in red with an asterisk at the end, Lab-01.Rmd*. If it is in red, save changes manually by clicking on the disk icon. After you have made sure your changes are saved, you are safe to end your session by closing the RStudio Cloud tab in your browser.\n\n\n\n\n\n\nTipNew Terms\n\n\n\n\n\nobject: any data or function that exists in the R session’s memory; usually accessed via a name.\n\nassignment: assign a value (object) to a name; usually with &lt;- (e.g. a &lt;- 10).\n\nenvironment: a workspace that maps names to objects; the main one you use is the Global Environment (visible in RStudio’s top-right Environment pane).",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#activity",
    "href": "01-intro.html#activity",
    "title": "Introduction to R",
    "section": "",
    "text": "Discuss the following questions with your neighbour or tutor, and write notes from your discussion in your R Markdown file:\n\nWhat are the main differences between working in a R Markdown file (pane 1) and directly on the console (pane 3)?\nCan you describe what happens when your run the following code? (tip: look at the environment tab in pane 2)\n\n\nobject1 &lt;- 10\nobject1 &lt;- 30\n\n\nClick the ‘Knit’ button in the bar at the top of the R Markdown file (pane 1). This will turn your R Markdown file into an HTML document.\n\n(Note: Another reason code chunk order matters. R Markdown files are meant to be reproducible, where anyone with a copy of the file can run the code from top to bottom and receive the same results. When you ‘knit’ an R Markdown file, the code chunks are run top to bottom in a fresh R session - in other words, it starts with a blank Global Environment. For example, if you run d &lt;- 25 manually in the Console and then add d + 5 in a code chunk, that code chunk will work fine when manually run in RStudio, but will result in an “object not found” error if you try to knit. This is because d &lt;- 25 also needs to be in a code chunk within the R Markdown file, and needs to come before d + 5. So, make sure all essential code for your analysis is within code chunks and in the correct order that they need to be run in.)\n\nCreate a new code chunk, you can put your text cursor on a line at the end of the R Markdown file and press Ctrl+Alt+I on the keyboard. Alternatively, from the menu bar at the top of RStudio, you can select Code &gt; Insert Chunk.",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "Introduction to R",
    "section": "Footnotes",
    "text": "Footnotes\n\nIf you have prior experience working with coding environments, such as Visual Studio Code, you may want to take a look at Positron. It is based on Visual Studio Code and setup to have a similar panel layout as within RStudio. One of the benefits of open-source software is having this diversity of tools can pick from. It is advised though for this course to stick with Posit Cloud and RStudio.↩︎",
    "crumbs": [
      "**Lab 1** Introduction to R"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods in the Social Sciences",
    "section": "",
    "text": "Welcome\nWelcome to the Quantitative Methods in the Social Sciences lab workbook!\n\nThis workbook is for University of Glasgow students enrolled in the undergraduate Quantitative Methods in the Social Sciences course within the School of Social and Political Sciences. The activities are designed for Posit Cloud.\nThese webpages are made with  and Quarto and licensed under the     Creative Commons BY-NC-SA 4.0.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "04-Lab4.html",
    "href": "04-Lab4.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "In our previous session we learned about setting up an RStudio project and wrangling data in R by implementing useful function such as filter(), select(), and mutate(). In this session we will continue working with the NILT 2012 dataset and focus on descriptive statistics. This includes the exploration and description of quantitative data.\n\nWe will continue working on the same NILT RStudio project and dataset that you created in the last lab on Posit Cloud. To get started, please follow the usual steps:\n\nGo to your ‘Lab Group’ in Posit Cloud.\nOpen your existing project called ‘NILT’ located in your lab group.\n\n(If you missed Lab 3, quickly run through setting up an NILT RStudio Project and Read the clean dataset sections. Ensure to make time later to go through the section on the NILT Documentation though.)\nThen we need to create an R Markdown file for today. Within the Files pane (bottom-right):\n\nClick ‘New File’.\nFrom the list of options select ‘R Markdown’\n\n\nThen within the ‘Create a New File in Currect Directory’ dialogue that pops up:\n\nGive it an appropriate name, such as ‘Lab-4-Exploratory.Rmd’\nClick the ‘OK’ button to confirm creation of the file.\n\n\n\nThe R Markdown file will automatically open in the Sources pane (top-left). It just contains basic YAML for title and output, which is HTML by default. Flesh it out by:\n\nGive it a full title, by default it just uses the name given to the file.\nAdd a line to add an author key with your name in quotation marks as the value.\nAdd a line for a date key and within quotation marks add the following -\n\n\n`r format(Sys.time(), '%d/%m/%y)`\n\n\nRemember the back-tick, `, can be found to the left of 1 on a UK keyboard. Now that we have used some R functions across the labs, the code here should be more readable than when you first encountered it. Enclosing code in single back-ticks allows us to add “inline code” on a single line in our file, similar to how we fence code in three back-ticks to create code chunks that span multiple lines. Then -\n\nThe r at the start specifies that we are adding r code, similar to using {r} at the start of a code chunk.\nThe format() function takes an object as its first argument and then one or more further arguments to specify how it should be formatted.\nThe Sys.time() function checks the date and time on the device and creates a date time object.\nIf you run Sys.time() in the Console though, you will see the date is formatted Year-Month-Day. This is common in computing as if you have a list of dates and sort them this will work, whereas if it was Day-Month-Year it would put 1st October before 5th May because 1 comes before 5.\nOur last bit of code then, %d/%m/%y, specifies the format the date should be in. The % followed by a letter specifies which date/time info to include and in what format, and how to format it, from our date time object. You can run help(format.POSIXct) in the Console and scroll down a bit in the help page to see what other % options are available. (POSIXct is just the class type of the date time object we created, with ‘ct’ standing for calendar time.)\n\nNext, we need to create our setup code chunk with our knitr options. So -\n\nCreate a new code chunk.\nModify the very top-level of the code chunk to have {r setup, include=FALSE}\n\nWithin the code chunk add the following -\n\n\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\n\nAs quick wee reminder:\n\nthe setup after r in the line opening the code chunk names the chunk ‘setup’\nthe include=FALSE after a comma then tells R when knitting the file tells R what to do with the code chunk. Here, the include=FALSE means the code should be run, but the code chunk and any output should not be included in the knitted file.\nwe then use knitr::opts_chunk$set() to set the default code chunk options for all code chunks in the current R Markdown file.\n\nmessage = FALSE means no messages and warning = FALSE no warnings outputted when running code will be included in the knitted file. We do this as even running code like loading the tidyverse outputs messages that are safe to ignore and we do not want to see in our knitted files.\n\nAfter that, we now want a preamble code chunks to load packages, read in data, and so on. So, create another code chunk and name it preamble and set the option to not include it in knitted files.\nWithin it add -\n\n# Load packages\nlibrary(tidyverse)\n\n# Read NILT\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nWe do not need to install.packages(\"tidyverse\") as we are working within the same RStudio project as last week, so the tidyverse is already installed. Whilst we also libary(tidyverse) to load the tidyverse last week, remember that when we knit files it creates a clean R environment. Whilst it can access packages that are installed on the device - or in our case within a Posit Cloud RStudio Project - it has no packages loaded nor data frames read in. So, for each R Markdown file we created within a project, we need this premable.\nNow, as we are going to work with the variables we coerced last week, let’s setup a subset with just those variables. A subset is merely a copy of our NILT data frame, but with just the rows and columns we are interested in.\nTo do this add the following at the end of your premable code chunk -\n\n# Create subset\nnilt_subset &lt;- nilt |&gt;\n  select(rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\nYour full code chunk should now look like the below.\n\nClick the green triangle to run the code chunk and you’ll see the nilt_subset appear in the Environment pane (top-left).\n\nNotice how our nilt object has “1204 obs. of 133 variables” and our nilt_subset object “1204 obs. of 8 variables”.\nWhy do this? With very large datasets this helps reduce the compute required when running code. Whilst the NILT dataset we are using is big, with 1,204 observations across 133 variables, it is not large in terms of computing. The other reason to subset data, and pertinent for us, is it simplifies working with our data.\nTo illustrate, within the Console run in turn glimpse(nilt) and then glimpse(nilt_subset).\n\nAs can see, whilst glimping the nilt data frame outputs lines for all 133 variables, glimpsing the nilt_subset data frame gives us lines for just the 8 variables we selected.\n\n\nAre your summary statistics hiding something interesting?\n\n\n\n\nExploratory analysis.\n\n\n\n\nTo start exploring our data it is essential to distinguish the adequate tools and measures available for the type of data in question. As you know now, there are two broad types: (1) categorical and (2) numeric.\nThere are several ways in which we can summarise our data. Today, we will use a useful package called vtable, used for quickly creating tables about variables in our data frames.\nInstall it in your RStudio project by running the following line in the Console (bottom-left):\n\ninstall.packages(\"vtable\")\n\nOnce it is installed, we also need to load it within the R Markdown file. Remember, it is best practice to load all your packages at the top, making it clear to anyone looking at the file which packages are being used and ensuring the packages are available for all the code chunks after it.\nSo within the preamable code chunk, add the following line after library(tidyverse).\n\nlibrary(vtable)\n\nRemember to also run this line of code. With your text cursor on the line, press Ctrl+Enter. Or, immediately above your file in the top-right of the Sources pane, click the small downwards pointing black triangle on the ‘Run’ button, and from the list of options select “Run Selected Line(s)”\n\n\nLet’s start with our categorical variables. A usual way to explore categorical data is using contingency and proportion tables. The contingency tables include the count for each category while the proportion tables contain the count divided by the total number of observations to calculate percentages.\nTo save repeating the same instructions throughout, remember to create individual code chunks for each table and to run the chunk after adding the code to see the results.\nWithin these sections, in addition to running the code, also consider the interpretation of the outputs. Say we are interested in a break down by respondents’ sex, using the variable called rsex in the NILT. To do this, we can use the sumtable() function from the vtable package to produce a contingency table for a single variable (known as One-Way contingency table).\nCreate a code chunk to add and run the following:\n\nnilt_subset |&gt; sumtable(vars = \"rsex\")\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\n\n\n\nBefore looking at the output in more detail, you may be wondering why the way we are specifying our variable of interest is different to how we used filter(), select(), and mutate(). The reason is that they are all from the tidyverse and share the same pattern for how to specify variables in function arguments. vtable though is not part of the tidyverse and has a different way to specify variables.\nThe simple format of sumtable() is sumtable(data_frame, vars = ...), or with pipe data_frame |&gt; sumtable(vars = ...). If you check out help(sumtable) you will see sumtable has a lot more arguments we could use. Any argument that has word = ... is what is known as a ‘keyword argument’. By default, arguments are read in order that they are listed. However, in practice, you often do not want to specify all arguments when using a function. Keyword arguments then let you specify exactly which arguments you are using despite the order they appear in.\nWe did not need to specify a vars = ... equivalent with the tidyverse functions as they all assume a data frame as the first argument, or passed in via a pipe, followed by a list of columns. It is only if you want to use any additional options provided by the tidyverse functions that you then need to add any keyword arguments. This let’s the code remain must more simple and direct, also making it easier to read. Importantly, for sumtable() note how we specify \"rsex\" in quotation marks, rather than rsex without as we did with tidyverse functions.\nLet’s turn back to the actual table we created. From the result, we see that there are more female respondents than males. Specifically, we see that males respondents represent 45% and females 55%.\nWe can do this with any categorical variable. Let’s see how the sample is split by religion (religcat). So, we will add it to in the vars keyword argument. However, note how we now need to also use the c() function -\n\nnilt_subset |&gt; sumtable(vars = c(\"rsex\", \"religcat\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\nreligcat\n1168\n\n\n\n... Catholic\n491\n42%\n\n\n... Protestant\n497\n43%\n\n\n... No religion\n180\n15%\n\n\n\n\n\nThe vars keyword argument of sumtable() takes a character vector of column (i.e. variable) names to include. Recall that ‘character’ is a class in R for text objects. For example, example &lt;- \"Some text\", creates an example object of class character containing the value “Some text”. A vector in R is simply a list of objects that are all of the same class. To create one, we use the c() function - with the ‘c’ being short for ‘combine’. We don’t need it when passing a single variable to sumtable(), but need to with two or more variables as , is used to separate arguments in a function. If we had keyword = 1, 2, R reads 2 as a separate argument, whereas keyword = c(1, 2) makes clear the list of items are part of the same ‘keyword’ argument.\nAgain, as the tidyverse functions we used previously assume the arguments at the start of the function are the data frame and a list of column names, it saves us from needing to enclose them in c(). All of these little design decisions that simplifies the code to use - also making it easier to read - is a reason why researchers love the tidyverse so much.\nTurning to the table created by our code. As you can see, about the same number of people are identified as being Catholic or Protestant, and a relatively small number with no religion. The reason for the broad categories, especially the broad “No religion” category, is to protect respondents’ anonymity. As you may have spotted going through the NILT documentation last week, that this is explained on page 25. As covered in the online lecture this week, these trade-offs between level of detail and protecting respondents anonymity are important ethical considerations to make during data collection and later archiving. Even without names being included, if a person has relatively unique characteristics, such as being the only person of a specific religion under the age of 40 who has a disability, it can become possible to de-identify them.\nNow, what if we want to know the religious affiliation breakdown by males and females. This is where Two-Way contingency tables are useful and very common in quantitative research. To produce it, we have to specify the group keyword argument in the sumtable function as follows:\n\nnilt_subset |&gt; sumtable(vars = \"religcat\", group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n520\n\n648\n\n\n\n... Catholic\n209\n40%\n282\n44%\n\n\n... Protestant\n211\n41%\n286\n44%\n\n\n... No religion\n100\n19%\n80\n12%\n\n\n\n\n\nThere are some interesting results from this table. You can see that there are proportionally more female respondents who are either Catholic or Protestant than males, namely 44% vs 40% and 44% vs 40%, respectively. We also see that there are 19% of male respondents who do not self-identify with a religion which contrast to the 12% of female participants.\nAs an important final note, sumtable() by default rounds to 2 decimal places. With percentages this rounds to whole numbers as sumtable() applying rounding before converting to percentage. To get a percentage we take the number of observations for a category, divide by the total number of observations, and multiple by 100. If we have a percentage 14.6%, before the multiplying by 100 it would be 0.146, which rounded to 2 decimal places would become 0.15, or 15%.\nWe can change the default with the digits keyword argument:\n\nnilt_subset |&gt; sumtable(vars = \"religcat\", group = \"rsex\", digits = 3)\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n520\n\n648\n\n\n\n... Catholic\n209\n40.2%\n282\n43.5%\n\n\n... Protestant\n211\n40.6%\n286\n44.1%\n\n\n... No religion\n100\n19.2%\n80\n12.3%\n\n\n\n\n\nSticking with the default rounding to 2 decimal places is perfectly OK to do, and some level of rounding will usually be inevitable to avoid reporting stats like “46.23156260002342%”. The key thing, as with all aspects of quantitative data analysis, is to (1) be transparent that stats are rounded and to which decimal place and (2) remember the stats are rounded in interpretation and avoid misleading phrases like “exactly 22%” because the actual number before rounding could be “22.03%”.\n\nRight, let’s put what we have covered so far into practice, and help get used to working with the sumtable() function.\nIn your R Markdown file, do the following activities using the nilt_subset data frame. * Create a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\n\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\nAre your summary statistics hiding something interesting? Discuss your results with your neighbour or your tutor.\n\nRemember, it is perfectly fine to modify working code. Feel free to copy and adapt used above. The important thing is that you understand how the code works and what you need to modify. It is though useful to initially try typing it out manually as even if you make mistakes - like missing quotations marks or misplaced commas - such errors help build an understanding of the correct syntax to use. Where you need to copy and modify previous code as you keep running into errors, remember once you have got it to work to compare your initial code that resulted in error messages with the working code. This will help you identify what the problem was.\n\nIn the previous section we’ve learned how to summarise categorical data. But often we want to work with continuous numeric variables or a combination of both. To summarise and understand numeric data there are two main types: measures of centrality and measures of spread.\nAs before, try to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown.\n\nIn quantitative research, we usually have access to many observations in a sample which contains different attributes for each of them. It would be difficult (and probably not very useful) to talk about each of the NILT respondents one by one. Instead, to describe this sample we need measures that roughly represent all participants.\nThis is actually an important step in quantitative research, since it allows us to characterise the people that we are studying. For example, in the previous section we only talked about the respondents’ sex and religious affiliation, but what other information would be useful to know? Probably a place to start digging deeper is to know their age.\nThe first tool that we will use to understand numeric values is a histogram, which we can create using the base R function hist(). This, like many base R functions, takes the variable of interest in data_frame$variable format. Let’s see how the age of NILT respondents is distributed.\n\nhist(nilt_subset$rage)\n\n\n\n\n\n\n\nThis plot shows us on the X axis (horizontal) the age and the frequency on the Y axis (vertical). We can see that the youngest age in the sample is somewhere close to 20, and the oldest is almost 100. We also observe that the total number of observations (represented by the frequency on the vertical axis) for extreme values (close to 20 on the left-hand side and 100 on the right-hand side) tends to be lower than the values in the centre of the plot (somewhere between 30 and 45). For instance, we can see that there are approximately 120 respondents who are around 40 years old; that seems to be the most popular/frequent age in our sample.\nImportantly, whilst we get a general sense of the shape of our data from the histogram, we can represent these central values with actual precise measures, typically mean or median.\nThe median is the mid-point value in a numeric series. If you sort the values and split it by half, the value right in the middle is the median. Luckily there is a base R function ready to be used called… You guessed it - median().\n\nmedian(nilt_subset$rage, na.rm = TRUE)\n\n[1] 48\n\n\nThe median age is 48, that means that 50% (or half) of the respondents are equal or younger than this, and the other 50% is equal or older.\nNote that we also include the argument na.rm equal TRUE in the function. The ‘na’ bit stands for ‘not available’ and refers to what is more commmonly known as “missing values”. The .rm stands for remove. So, we are telling R to remove the missing values when computing the median. We do this here for rage because we do not know the age of 3 of the respondents in the sample.\nTo compute the mean manually, we need to sum all our values and divide it by the total number of the observations as follows: \\[ mean =\\frac{  x_1 + x_2 + x_3 ...+x_n } {n} \\]\nWhilst it may look intimidating, such formulas are usually fairly simple once you break it down. \\(x_1\\) just means the value of the first observation (i.e. row) in our data, \\(x_2\\) the second, and so on. The “…” is just the way the ‘and so on’ gets written. The \\(n\\) in \\(x_n\\) stands for our number of observations. So putting that together, we get the equivalent of saying add the first value, second value, third value, and so on for all our observations. Then, as noted, once we have the sum of all our values, we just divide it by the total number of observations, i.e. \\(n\\).\nThe formula above is for you to know that this measure considers the magnitude of all values included in the numeric series. Therefore, the average is sensitive to extreme numbers. Examples of extreme numbers could be a very, very old person or someone with substantially more income than the rest. Imagine if our values were along the lines of “12, 34, 10, 100, 32, 16, …”, that “100” is extreme in comparison to the rest and would bring up the average.\nTo compute the mean you need the mean() function.\n\nmean(nilt_subset$rage, na.rm = TRUE)\n\n[1] 49.61532\n\n\nAs you can see, the above measures try to approximate values that fall somewhere in the centre of the histogram plot, and represent all observations in the sample. They tell different things and are sometimes more (or less) suitable in a specific situation. (We will cover this in more detail within the in-person lectures.)\n\nBy contrast, there are measures that help us to describe how far away a series of numeric values are from the centre. The common measures of spread are quartiles, variance and standard deviation.\nThe quartiles are very useful to quickly see how numeric data is distributed. Imagine that we sort all ages in the sample and split it into four equal parts. The first quartile includes the lowest 25% of values, the second the next 25%, the third another 25%, and the fourth the highest 25%. To compute quartiles, we can use the base R quantile() function.\n\nquantile(nilt_subset$rage, na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\nIn our sample, the youngest quarter of the respondents is between 18 and 35 years old. The second quarter is between 35 and 48 years old. The next quartile is between 48 and 64. The oldest 25% of the respondents is between 64 and 97.\nThe variance is useful to obtain a singe measure of spread (instead of four values, as the above) taking the mean as a reference. This is given by the following formula:\n\\[ var = \\frac{ \\sum(x - \\bar{x})^2 }{n-1 } \\]\nTo decipher the formula above, the Σ (capital sigma) means sum the values from the following. Here we have \\(x\\) rather than \\(x_1\\) and so on as the Σ tells us we are repeating this for all our observations. The \\(\\bar{x}\\) represents the mean, a shorthand for the formula we covered before.\nSo, to put what we have so far together, for each of our \\(x\\) values we substract the mean \\(\\bar{x}\\). The result of the subtraction is then squared - i.e. multiply the number by itself - and represented by the %^2%. We are interested in the sum of the differences from the mean, regardless of whether it is positive or negative. As some numbers will be lower than the mean, the result of the substraction with be negative. By squaring all numbers, any negatives also become positive (-2 multipled by -2 is 4). Finally, we divide the sum by the size/length of the numeric sequence \\(n\\) minus 1.\nThankfully, in R to estimate the variance, we only need the var() function.\n\nvar(nilt_subset$rage, na.rm = TRUE)\n\n[1] 343.3486\n\n\nAs you can see, the result is not very intuitive to interpret. That is because we squared the result for each of the subtractions. Luckily, there is a measure that puts it into a readable scale. This is the standard deviation. In essence this takes the square root of the variance:\n\\[sd=\\sqrt{var}\\]\nThis inverses the impact of squaring our values, but keeps it a positive value. -3 squared is 9 and the square root of 9 is 3.\nTo compute it in R, use the base R sd() function.\n\nsd(nilt_subset$rage, na.rm = TRUE)\n\n[1] 18.52967\n\n\nThis measure is more human readable than the variance. Don’t worry too much about the formula. As seen, R makes it simple to calculate these stats. The important thing to remember is what the measure represents and how to interpret it. An informal definition of the standard deviation is the average distance from the mean. In essence, it tell us how far the values in our data are from the mean.\n\nPhew, that was a lot! …\n… Luckily, as another beatiful example of the power of coding, we can use the sumtable() function to compute all these measures for our numeric variables at the same time!\nIt is very simple. You can compute a quick summary for age as following:\n\nnilt_subset |&gt; sumtable(vars = \"rage\")\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\nrage\n1201\n50\n19\n18\n35\n64\n97\n\n\n\n\nThe result displays the number of observations used (N), the mean, the standard deviation, minimum, the 1st (same as ‘Pctl. 25’) and 3rd quartile (same as ‘Pctl. 75’), as well as the maximum (i.e., eldest respondent).\nNote, despite passing sumtable() categorical variables previously to get percentages, when passing it numeric values it calculates the mean, standard deviation, and so on instead. As we coerced our variables last week, sumtable() knows which are categorical (i.e. factor) and which are numeric, calculating the relevant stats for each without us having to tell it which to do.\n\nThere will be times in which you will need to compute a summary combining categorical and numeric data, to compare groups for example. The good news is that we can use exactly the same function and syntax to do this. Let’s take the following example to compute the summary of the respondent’s age (rage) by gender:\n\nnilt_subset |&gt; sumtable(vars = \"rage\", group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\n\n\n\nrage\n535\n51\n18\n666\n49\n19\n\n\n\n\nIn the code above, we are simply specifying the variable rage and grouping the summary by rsex. This produces a small summary with the number of observations in and the main measure of centrality and spread, namely the mean and the standard deviation, for each of the categories.\n\nWe ended the Setting up an R Markdown file section at the start of today’s session by showing the power of subsetting our data. There we did a comparison between glimpse(nilt) and glimpse(nilt_subset). Creating the nilt_subset meant that we could pass it to glimpse() and receive output for just our selected 8 variables rather than all 133 variables in the NILT dataset.\nIn the sumtable() examples above, we looked at creating a table for 1 or more variables - or one variable broken down by another variable - where we have to call each of our variables by name.\nWhat if we want the descriptive statistics for all our variables of interest? We could write nilt_subset |&gt; sumtable(vars = c(\"rsex\", \"rage\", ... and so on, listing the names for all our variables one by one. However, thanks to the wonders of coding, we don’t need to repeat ourselves. We already specified our variables of interest when creating the nilt_subset data frame. Fortunately, sumtable() if you pass it a data frame without specifying any variables, assumes you want a table for all the variables in the provided data frame -\n\nnilt_subset |&gt; sumtable()\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\nrsex\n1204\n\n\n\n\n\n\n\n\n... Male\n537\n45%\n\n\n\n\n\n\n\n... Female\n667\n55%\n\n\n\n\n\n\n\nrage\n1201\n50\n19\n18\n35\n64\n97\n\n\nhighqual\n1204\n\n\n\n\n\n\n\n\n... Degree level or higher\n230\n19%\n\n\n\n\n\n\n\n... Higher education\n102\n8%\n\n\n\n\n\n\n\n... GCE A level or equiv\n243\n20%\n\n\n\n\n\n\n\n... GCSE A-C or equiv\n185\n15%\n\n\n\n\n\n\n\n... GCSE D-G or equiv\n82\n7%\n\n\n\n\n\n\n\n... No qualifications\n281\n23%\n\n\n\n\n\n\n\n... Other, level unknown\n27\n2%\n\n\n\n\n\n\n\n... Unclassified\n54\n4%\n\n\n\n\n\n\n\nreligcat\n1168\n\n\n\n\n\n\n\n\n... Catholic\n491\n42%\n\n\n\n\n\n\n\n... Protestant\n497\n43%\n\n\n\n\n\n\n\n... No religion\n180\n15%\n\n\n\n\n\n\n\nuninatid\n1183\n\n\n\n\n\n\n\n\n... Unionist\n348\n29%\n\n\n\n\n\n\n\n... Nationalist\n255\n22%\n\n\n\n\n\n\n\n... Neither\n580\n49%\n\n\n\n\n\n\n\nruhappy\n1195\n\n\n\n\n\n\n\n\n... Very happy\n404\n34%\n\n\n\n\n\n\n\n... Fairly happy\n656\n55%\n\n\n\n\n\n\n\n... Not very happy\n95\n8%\n\n\n\n\n\n\n\n... Not at all happy\n12\n1%\n\n\n\n\n\n\n\n... Can't choose\n28\n2%\n\n\n\n\n\n\n\nrhourswk\n566\n35\n12\n4\n25\n40\n100\n\n\npersinc2\n897\n16395\n13466\n260\n6760\n22100\n75000\n\n\n\n\n\n\nWithin our tables so far the rows use the variable names from the data frame. However, it may not be clear to a reader what rsex, rage, and so on represent.For that reason, it is good practice to also add labels. With sumtable() you can use the labels = keyword argument to pass the labels to use. This needs to be the same length and in the same order as the variables used in the table.\nTo keep things relatively simple, let’s create a table with labels for rage and persinc2\n\nnilt_subset |&gt; sumtable(vars = c(\"rage\", \"persinc2\"), labels = c(\"Age\", \"Annual Personal Income\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\nAge\n1201\n50\n19\n18\n35\n64\n97\n\n\nAnnual Personal Income\n897\n16395\n13466\n260\n6760\n22100\n75000\n\n\n\n\n\nNote, we need to use the c() function again - c(\"Age\", \"Annual Personal Income\"). Also, if you wanted to create a table with labels for all variables in the data frame, you do not need to include the vars = part. However, you need to then ensure you list the labels in the correct order. When you create a subset with select() the variables in the subset are put into the same order as the code you used. So, if you are unsure what order variables in your subset are in, take a look back at the code you used to create the subset.\nAs a final wee note, you can also customise the title of tables created with sumtable() using the title = keyword argument.\n\nCan money buy happiness?\nUsing the data in the nilt_subset dataset, complete the following activities. This will be a good practice run for doing the research report when you run your own analysis on the NILT teaching dataset. Handy!\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions (Hint: with base R functions you need to use dataframe$variable format to specify the variable the function should use);\nCreate a summary of the personal income persinc2 variable, using the sumtable() function;\nFinally, compute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy. What do you observe?\nDiscuss the results with your neighbour or your tutor.",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "04-Lab4.html#getting-started",
    "href": "04-Lab4.html#getting-started",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "We will continue working on the same NILT RStudio project and dataset that you created in the last lab on Posit Cloud. To get started, please follow the usual steps:\n\nGo to your ‘Lab Group’ in Posit Cloud.\nOpen your existing project called ‘NILT’ located in your lab group.\n\n(If you missed Lab 3, quickly run through setting up an NILT RStudio Project and Read the clean dataset sections. Ensure to make time later to go through the section on the NILT Documentation though.)\nThen we need to create an R Markdown file for today. Within the Files pane (bottom-right):\n\nClick ‘New File’.\nFrom the list of options select ‘R Markdown’\n\n\nThen within the ‘Create a New File in Currect Directory’ dialogue that pops up:\n\nGive it an appropriate name, such as ‘Lab-4-Exploratory.Rmd’\nClick the ‘OK’ button to confirm creation of the file.\n\n\n\nThe R Markdown file will automatically open in the Sources pane (top-left). It just contains basic YAML for title and output, which is HTML by default. Flesh it out by:\n\nGive it a full title, by default it just uses the name given to the file.\nAdd a line to add an author key with your name in quotation marks as the value.\nAdd a line for a date key and within quotation marks add the following -\n\n\n`r format(Sys.time(), '%d/%m/%y)`\n\n\nRemember the back-tick, `, can be found to the left of 1 on a UK keyboard. Now that we have used some R functions across the labs, the code here should be more readable than when you first encountered it. Enclosing code in single back-ticks allows us to add “inline code” on a single line in our file, similar to how we fence code in three back-ticks to create code chunks that span multiple lines. Then -\n\nThe r at the start specifies that we are adding r code, similar to using {r} at the start of a code chunk.\nThe format() function takes an object as its first argument and then one or more further arguments to specify how it should be formatted.\nThe Sys.time() function checks the date and time on the device and creates a date time object.\nIf you run Sys.time() in the Console though, you will see the date is formatted Year-Month-Day. This is common in computing as if you have a list of dates and sort them this will work, whereas if it was Day-Month-Year it would put 1st October before 5th May because 1 comes before 5.\nOur last bit of code then, %d/%m/%y, specifies the format the date should be in. The % followed by a letter specifies which date/time info to include and in what format, and how to format it, from our date time object. You can run help(format.POSIXct) in the Console and scroll down a bit in the help page to see what other % options are available. (POSIXct is just the class type of the date time object we created, with ‘ct’ standing for calendar time.)\n\nNext, we need to create our setup code chunk with our knitr options. So -\n\nCreate a new code chunk.\nModify the very top-level of the code chunk to have {r setup, include=FALSE}\n\nWithin the code chunk add the following -\n\n\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\n\nAs quick wee reminder:\n\nthe setup after r in the line opening the code chunk names the chunk ‘setup’\nthe include=FALSE after a comma then tells R when knitting the file tells R what to do with the code chunk. Here, the include=FALSE means the code should be run, but the code chunk and any output should not be included in the knitted file.\nwe then use knitr::opts_chunk$set() to set the default code chunk options for all code chunks in the current R Markdown file.\n\nmessage = FALSE means no messages and warning = FALSE no warnings outputted when running code will be included in the knitted file. We do this as even running code like loading the tidyverse outputs messages that are safe to ignore and we do not want to see in our knitted files.\n\nAfter that, we now want a preamble code chunks to load packages, read in data, and so on. So, create another code chunk and name it preamble and set the option to not include it in knitted files.\nWithin it add -\n\n# Load packages\nlibrary(tidyverse)\n\n# Read NILT\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nWe do not need to install.packages(\"tidyverse\") as we are working within the same RStudio project as last week, so the tidyverse is already installed. Whilst we also libary(tidyverse) to load the tidyverse last week, remember that when we knit files it creates a clean R environment. Whilst it can access packages that are installed on the device - or in our case within a Posit Cloud RStudio Project - it has no packages loaded nor data frames read in. So, for each R Markdown file we created within a project, we need this premable.\nNow, as we are going to work with the variables we coerced last week, let’s setup a subset with just those variables. A subset is merely a copy of our NILT data frame, but with just the rows and columns we are interested in.\nTo do this add the following at the end of your premable code chunk -\n\n# Create subset\nnilt_subset &lt;- nilt |&gt;\n  select(rsex, rage, highqual, religcat, uninatid, ruhappy, rhourswk, persinc2)\n\nYour full code chunk should now look like the below.\n\nClick the green triangle to run the code chunk and you’ll see the nilt_subset appear in the Environment pane (top-left).\n\nNotice how our nilt object has “1204 obs. of 133 variables” and our nilt_subset object “1204 obs. of 8 variables”.\nWhy do this? With very large datasets this helps reduce the compute required when running code. Whilst the NILT dataset we are using is big, with 1,204 observations across 133 variables, it is not large in terms of computing. The other reason to subset data, and pertinent for us, is it simplifies working with our data.\nTo illustrate, within the Console run in turn glimpse(nilt) and then glimpse(nilt_subset).\n\nAs can see, whilst glimping the nilt data frame outputs lines for all 133 variables, glimpsing the nilt_subset data frame gives us lines for just the 8 variables we selected.",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "04-Lab4.html#exploratory-analysis",
    "href": "04-Lab4.html#exploratory-analysis",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Are your summary statistics hiding something interesting?\n\n\n\n\nExploratory analysis.\n\n\n\n\nTo start exploring our data it is essential to distinguish the adequate tools and measures available for the type of data in question. As you know now, there are two broad types: (1) categorical and (2) numeric.\nThere are several ways in which we can summarise our data. Today, we will use a useful package called vtable, used for quickly creating tables about variables in our data frames.\nInstall it in your RStudio project by running the following line in the Console (bottom-left):\n\ninstall.packages(\"vtable\")\n\nOnce it is installed, we also need to load it within the R Markdown file. Remember, it is best practice to load all your packages at the top, making it clear to anyone looking at the file which packages are being used and ensuring the packages are available for all the code chunks after it.\nSo within the preamable code chunk, add the following line after library(tidyverse).\n\nlibrary(vtable)\n\nRemember to also run this line of code. With your text cursor on the line, press Ctrl+Enter. Or, immediately above your file in the top-right of the Sources pane, click the small downwards pointing black triangle on the ‘Run’ button, and from the list of options select “Run Selected Line(s)”\n\n\nLet’s start with our categorical variables. A usual way to explore categorical data is using contingency and proportion tables. The contingency tables include the count for each category while the proportion tables contain the count divided by the total number of observations to calculate percentages.\nTo save repeating the same instructions throughout, remember to create individual code chunks for each table and to run the chunk after adding the code to see the results.\nWithin these sections, in addition to running the code, also consider the interpretation of the outputs. Say we are interested in a break down by respondents’ sex, using the variable called rsex in the NILT. To do this, we can use the sumtable() function from the vtable package to produce a contingency table for a single variable (known as One-Way contingency table).\nCreate a code chunk to add and run the following:\n\nnilt_subset |&gt; sumtable(vars = \"rsex\")\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\n\n\n\nBefore looking at the output in more detail, you may be wondering why the way we are specifying our variable of interest is different to how we used filter(), select(), and mutate(). The reason is that they are all from the tidyverse and share the same pattern for how to specify variables in function arguments. vtable though is not part of the tidyverse and has a different way to specify variables.\nThe simple format of sumtable() is sumtable(data_frame, vars = ...), or with pipe data_frame |&gt; sumtable(vars = ...). If you check out help(sumtable) you will see sumtable has a lot more arguments we could use. Any argument that has word = ... is what is known as a ‘keyword argument’. By default, arguments are read in order that they are listed. However, in practice, you often do not want to specify all arguments when using a function. Keyword arguments then let you specify exactly which arguments you are using despite the order they appear in.\nWe did not need to specify a vars = ... equivalent with the tidyverse functions as they all assume a data frame as the first argument, or passed in via a pipe, followed by a list of columns. It is only if you want to use any additional options provided by the tidyverse functions that you then need to add any keyword arguments. This let’s the code remain must more simple and direct, also making it easier to read. Importantly, for sumtable() note how we specify \"rsex\" in quotation marks, rather than rsex without as we did with tidyverse functions.\nLet’s turn back to the actual table we created. From the result, we see that there are more female respondents than males. Specifically, we see that males respondents represent 45% and females 55%.\nWe can do this with any categorical variable. Let’s see how the sample is split by religion (religcat). So, we will add it to in the vars keyword argument. However, note how we now need to also use the c() function -\n\nnilt_subset |&gt; sumtable(vars = c(\"rsex\", \"religcat\"))\n\n\nSummary Statistics\n\nVariable\nN\nPercent\n\n\n\nrsex\n1204\n\n\n\n... Male\n537\n45%\n\n\n... Female\n667\n55%\n\n\nreligcat\n1168\n\n\n\n... Catholic\n491\n42%\n\n\n... Protestant\n497\n43%\n\n\n... No religion\n180\n15%\n\n\n\n\n\nThe vars keyword argument of sumtable() takes a character vector of column (i.e. variable) names to include. Recall that ‘character’ is a class in R for text objects. For example, example &lt;- \"Some text\", creates an example object of class character containing the value “Some text”. A vector in R is simply a list of objects that are all of the same class. To create one, we use the c() function - with the ‘c’ being short for ‘combine’. We don’t need it when passing a single variable to sumtable(), but need to with two or more variables as , is used to separate arguments in a function. If we had keyword = 1, 2, R reads 2 as a separate argument, whereas keyword = c(1, 2) makes clear the list of items are part of the same ‘keyword’ argument.\nAgain, as the tidyverse functions we used previously assume the arguments at the start of the function are the data frame and a list of column names, it saves us from needing to enclose them in c(). All of these little design decisions that simplifies the code to use - also making it easier to read - is a reason why researchers love the tidyverse so much.\nTurning to the table created by our code. As you can see, about the same number of people are identified as being Catholic or Protestant, and a relatively small number with no religion. The reason for the broad categories, especially the broad “No religion” category, is to protect respondents’ anonymity. As you may have spotted going through the NILT documentation last week, that this is explained on page 25. As covered in the online lecture this week, these trade-offs between level of detail and protecting respondents anonymity are important ethical considerations to make during data collection and later archiving. Even without names being included, if a person has relatively unique characteristics, such as being the only person of a specific religion under the age of 40 who has a disability, it can become possible to de-identify them.\nNow, what if we want to know the religious affiliation breakdown by males and females. This is where Two-Way contingency tables are useful and very common in quantitative research. To produce it, we have to specify the group keyword argument in the sumtable function as follows:\n\nnilt_subset |&gt; sumtable(vars = \"religcat\", group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n520\n\n648\n\n\n\n... Catholic\n209\n40%\n282\n44%\n\n\n... Protestant\n211\n41%\n286\n44%\n\n\n... No religion\n100\n19%\n80\n12%\n\n\n\n\n\nThere are some interesting results from this table. You can see that there are proportionally more female respondents who are either Catholic or Protestant than males, namely 44% vs 40% and 44% vs 40%, respectively. We also see that there are 19% of male respondents who do not self-identify with a religion which contrast to the 12% of female participants.\nAs an important final note, sumtable() by default rounds to 2 decimal places. With percentages this rounds to whole numbers as sumtable() applying rounding before converting to percentage. To get a percentage we take the number of observations for a category, divide by the total number of observations, and multiple by 100. If we have a percentage 14.6%, before the multiplying by 100 it would be 0.146, which rounded to 2 decimal places would become 0.15, or 15%.\nWe can change the default with the digits keyword argument:\n\nnilt_subset |&gt; sumtable(vars = \"religcat\", group = \"rsex\", digits = 3)\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nPercent\nN\nPercent\n\n\n\n\nreligcat\n520\n\n648\n\n\n\n... Catholic\n209\n40.2%\n282\n43.5%\n\n\n... Protestant\n211\n40.6%\n286\n44.1%\n\n\n... No religion\n100\n19.2%\n80\n12.3%\n\n\n\n\n\nSticking with the default rounding to 2 decimal places is perfectly OK to do, and some level of rounding will usually be inevitable to avoid reporting stats like “46.23156260002342%”. The key thing, as with all aspects of quantitative data analysis, is to (1) be transparent that stats are rounded and to which decimal place and (2) remember the stats are rounded in interpretation and avoid misleading phrases like “exactly 22%” because the actual number before rounding could be “22.03%”.\n\nRight, let’s put what we have covered so far into practice, and help get used to working with the sumtable() function.\nIn your R Markdown file, do the following activities using the nilt_subset data frame. * Create a One-Way contingency table for uninatid in the nilt_subset dataset using the sumtable() function;\n\nUsing the variables religcat and uninatid, generate a Two-Way contingency table;\nAre your summary statistics hiding something interesting? Discuss your results with your neighbour or your tutor.\n\nRemember, it is perfectly fine to modify working code. Feel free to copy and adapt used above. The important thing is that you understand how the code works and what you need to modify. It is though useful to initially try typing it out manually as even if you make mistakes - like missing quotations marks or misplaced commas - such errors help build an understanding of the correct syntax to use. Where you need to copy and modify previous code as you keep running into errors, remember once you have got it to work to compare your initial code that resulted in error messages with the working code. This will help you identify what the problem was.\n\nIn the previous section we’ve learned how to summarise categorical data. But often we want to work with continuous numeric variables or a combination of both. To summarise and understand numeric data there are two main types: measures of centrality and measures of spread.\nAs before, try to focus on the interpretation of the outputs in the following section. At this time, it is just optional to run the code shown.\n\nIn quantitative research, we usually have access to many observations in a sample which contains different attributes for each of them. It would be difficult (and probably not very useful) to talk about each of the NILT respondents one by one. Instead, to describe this sample we need measures that roughly represent all participants.\nThis is actually an important step in quantitative research, since it allows us to characterise the people that we are studying. For example, in the previous section we only talked about the respondents’ sex and religious affiliation, but what other information would be useful to know? Probably a place to start digging deeper is to know their age.\nThe first tool that we will use to understand numeric values is a histogram, which we can create using the base R function hist(). This, like many base R functions, takes the variable of interest in data_frame$variable format. Let’s see how the age of NILT respondents is distributed.\n\nhist(nilt_subset$rage)\n\n\n\n\n\n\n\nThis plot shows us on the X axis (horizontal) the age and the frequency on the Y axis (vertical). We can see that the youngest age in the sample is somewhere close to 20, and the oldest is almost 100. We also observe that the total number of observations (represented by the frequency on the vertical axis) for extreme values (close to 20 on the left-hand side and 100 on the right-hand side) tends to be lower than the values in the centre of the plot (somewhere between 30 and 45). For instance, we can see that there are approximately 120 respondents who are around 40 years old; that seems to be the most popular/frequent age in our sample.\nImportantly, whilst we get a general sense of the shape of our data from the histogram, we can represent these central values with actual precise measures, typically mean or median.\nThe median is the mid-point value in a numeric series. If you sort the values and split it by half, the value right in the middle is the median. Luckily there is a base R function ready to be used called… You guessed it - median().\n\nmedian(nilt_subset$rage, na.rm = TRUE)\n\n[1] 48\n\n\nThe median age is 48, that means that 50% (or half) of the respondents are equal or younger than this, and the other 50% is equal or older.\nNote that we also include the argument na.rm equal TRUE in the function. The ‘na’ bit stands for ‘not available’ and refers to what is more commmonly known as “missing values”. The .rm stands for remove. So, we are telling R to remove the missing values when computing the median. We do this here for rage because we do not know the age of 3 of the respondents in the sample.\nTo compute the mean manually, we need to sum all our values and divide it by the total number of the observations as follows: \\[ mean =\\frac{  x_1 + x_2 + x_3 ...+x_n } {n} \\]\nWhilst it may look intimidating, such formulas are usually fairly simple once you break it down. \\(x_1\\) just means the value of the first observation (i.e. row) in our data, \\(x_2\\) the second, and so on. The “…” is just the way the ‘and so on’ gets written. The \\(n\\) in \\(x_n\\) stands for our number of observations. So putting that together, we get the equivalent of saying add the first value, second value, third value, and so on for all our observations. Then, as noted, once we have the sum of all our values, we just divide it by the total number of observations, i.e. \\(n\\).\nThe formula above is for you to know that this measure considers the magnitude of all values included in the numeric series. Therefore, the average is sensitive to extreme numbers. Examples of extreme numbers could be a very, very old person or someone with substantially more income than the rest. Imagine if our values were along the lines of “12, 34, 10, 100, 32, 16, …”, that “100” is extreme in comparison to the rest and would bring up the average.\nTo compute the mean you need the mean() function.\n\nmean(nilt_subset$rage, na.rm = TRUE)\n\n[1] 49.61532\n\n\nAs you can see, the above measures try to approximate values that fall somewhere in the centre of the histogram plot, and represent all observations in the sample. They tell different things and are sometimes more (or less) suitable in a specific situation. (We will cover this in more detail within the in-person lectures.)\n\nBy contrast, there are measures that help us to describe how far away a series of numeric values are from the centre. The common measures of spread are quartiles, variance and standard deviation.\nThe quartiles are very useful to quickly see how numeric data is distributed. Imagine that we sort all ages in the sample and split it into four equal parts. The first quartile includes the lowest 25% of values, the second the next 25%, the third another 25%, and the fourth the highest 25%. To compute quartiles, we can use the base R quantile() function.\n\nquantile(nilt_subset$rage, na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n  18   35   48   64   97 \n\n\nIn our sample, the youngest quarter of the respondents is between 18 and 35 years old. The second quarter is between 35 and 48 years old. The next quartile is between 48 and 64. The oldest 25% of the respondents is between 64 and 97.\nThe variance is useful to obtain a singe measure of spread (instead of four values, as the above) taking the mean as a reference. This is given by the following formula:\n\\[ var = \\frac{ \\sum(x - \\bar{x})^2 }{n-1 } \\]\nTo decipher the formula above, the Σ (capital sigma) means sum the values from the following. Here we have \\(x\\) rather than \\(x_1\\) and so on as the Σ tells us we are repeating this for all our observations. The \\(\\bar{x}\\) represents the mean, a shorthand for the formula we covered before.\nSo, to put what we have so far together, for each of our \\(x\\) values we substract the mean \\(\\bar{x}\\). The result of the subtraction is then squared - i.e. multiply the number by itself - and represented by the %^2%. We are interested in the sum of the differences from the mean, regardless of whether it is positive or negative. As some numbers will be lower than the mean, the result of the substraction with be negative. By squaring all numbers, any negatives also become positive (-2 multipled by -2 is 4). Finally, we divide the sum by the size/length of the numeric sequence \\(n\\) minus 1.\nThankfully, in R to estimate the variance, we only need the var() function.\n\nvar(nilt_subset$rage, na.rm = TRUE)\n\n[1] 343.3486\n\n\nAs you can see, the result is not very intuitive to interpret. That is because we squared the result for each of the subtractions. Luckily, there is a measure that puts it into a readable scale. This is the standard deviation. In essence this takes the square root of the variance:\n\\[sd=\\sqrt{var}\\]\nThis inverses the impact of squaring our values, but keeps it a positive value. -3 squared is 9 and the square root of 9 is 3.\nTo compute it in R, use the base R sd() function.\n\nsd(nilt_subset$rage, na.rm = TRUE)\n\n[1] 18.52967\n\n\nThis measure is more human readable than the variance. Don’t worry too much about the formula. As seen, R makes it simple to calculate these stats. The important thing to remember is what the measure represents and how to interpret it. An informal definition of the standard deviation is the average distance from the mean. In essence, it tell us how far the values in our data are from the mean.\n\nPhew, that was a lot! …\n… Luckily, as another beatiful example of the power of coding, we can use the sumtable() function to compute all these measures for our numeric variables at the same time!\nIt is very simple. You can compute a quick summary for age as following:\n\nnilt_subset |&gt; sumtable(vars = \"rage\")\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\nrage\n1201\n50\n19\n18\n35\n64\n97\n\n\n\n\nThe result displays the number of observations used (N), the mean, the standard deviation, minimum, the 1st (same as ‘Pctl. 25’) and 3rd quartile (same as ‘Pctl. 75’), as well as the maximum (i.e., eldest respondent).\nNote, despite passing sumtable() categorical variables previously to get percentages, when passing it numeric values it calculates the mean, standard deviation, and so on instead. As we coerced our variables last week, sumtable() knows which are categorical (i.e. factor) and which are numeric, calculating the relevant stats for each without us having to tell it which to do.\n\nThere will be times in which you will need to compute a summary combining categorical and numeric data, to compare groups for example. The good news is that we can use exactly the same function and syntax to do this. Let’s take the following example to compute the summary of the respondent’s age (rage) by gender:\n\nnilt_subset |&gt; sumtable(vars = \"rage\", group = \"rsex\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nrsex\n\n\nMale\n\n\nFemale\n\n\n\nVariable\nN\nMean\nSD\nN\nMean\nSD\n\n\n\nrage\n535\n51\n18\n666\n49\n19\n\n\n\n\nIn the code above, we are simply specifying the variable rage and grouping the summary by rsex. This produces a small summary with the number of observations in and the main measure of centrality and spread, namely the mean and the standard deviation, for each of the categories.\n\nWe ended the Setting up an R Markdown file section at the start of today’s session by showing the power of subsetting our data. There we did a comparison between glimpse(nilt) and glimpse(nilt_subset). Creating the nilt_subset meant that we could pass it to glimpse() and receive output for just our selected 8 variables rather than all 133 variables in the NILT dataset.\nIn the sumtable() examples above, we looked at creating a table for 1 or more variables - or one variable broken down by another variable - where we have to call each of our variables by name.\nWhat if we want the descriptive statistics for all our variables of interest? We could write nilt_subset |&gt; sumtable(vars = c(\"rsex\", \"rage\", ... and so on, listing the names for all our variables one by one. However, thanks to the wonders of coding, we don’t need to repeat ourselves. We already specified our variables of interest when creating the nilt_subset data frame. Fortunately, sumtable() if you pass it a data frame without specifying any variables, assumes you want a table for all the variables in the provided data frame -\n\nnilt_subset |&gt; sumtable()\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\nrsex\n1204\n\n\n\n\n\n\n\n\n... Male\n537\n45%\n\n\n\n\n\n\n\n... Female\n667\n55%\n\n\n\n\n\n\n\nrage\n1201\n50\n19\n18\n35\n64\n97\n\n\nhighqual\n1204\n\n\n\n\n\n\n\n\n... Degree level or higher\n230\n19%\n\n\n\n\n\n\n\n... Higher education\n102\n8%\n\n\n\n\n\n\n\n... GCE A level or equiv\n243\n20%\n\n\n\n\n\n\n\n... GCSE A-C or equiv\n185\n15%\n\n\n\n\n\n\n\n... GCSE D-G or equiv\n82\n7%\n\n\n\n\n\n\n\n... No qualifications\n281\n23%\n\n\n\n\n\n\n\n... Other, level unknown\n27\n2%\n\n\n\n\n\n\n\n... Unclassified\n54\n4%\n\n\n\n\n\n\n\nreligcat\n1168\n\n\n\n\n\n\n\n\n... Catholic\n491\n42%\n\n\n\n\n\n\n\n... Protestant\n497\n43%\n\n\n\n\n\n\n\n... No religion\n180\n15%\n\n\n\n\n\n\n\nuninatid\n1183\n\n\n\n\n\n\n\n\n... Unionist\n348\n29%\n\n\n\n\n\n\n\n... Nationalist\n255\n22%\n\n\n\n\n\n\n\n... Neither\n580\n49%\n\n\n\n\n\n\n\nruhappy\n1195\n\n\n\n\n\n\n\n\n... Very happy\n404\n34%\n\n\n\n\n\n\n\n... Fairly happy\n656\n55%\n\n\n\n\n\n\n\n... Not very happy\n95\n8%\n\n\n\n\n\n\n\n... Not at all happy\n12\n1%\n\n\n\n\n\n\n\n... Can't choose\n28\n2%\n\n\n\n\n\n\n\nrhourswk\n566\n35\n12\n4\n25\n40\n100\n\n\npersinc2\n897\n16395\n13466\n260\n6760\n22100\n75000\n\n\n\n\n\n\nWithin our tables so far the rows use the variable names from the data frame. However, it may not be clear to a reader what rsex, rage, and so on represent.For that reason, it is good practice to also add labels. With sumtable() you can use the labels = keyword argument to pass the labels to use. This needs to be the same length and in the same order as the variables used in the table.\nTo keep things relatively simple, let’s create a table with labels for rage and persinc2\n\nnilt_subset |&gt; sumtable(vars = c(\"rage\", \"persinc2\"), labels = c(\"Age\", \"Annual Personal Income\"))\n\n\nSummary Statistics\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\nAge\n1201\n50\n19\n18\n35\n64\n97\n\n\nAnnual Personal Income\n897\n16395\n13466\n260\n6760\n22100\n75000\n\n\n\n\n\nNote, we need to use the c() function again - c(\"Age\", \"Annual Personal Income\"). Also, if you wanted to create a table with labels for all variables in the data frame, you do not need to include the vars = part. However, you need to then ensure you list the labels in the correct order. When you create a subset with select() the variables in the subset are put into the same order as the code you used. So, if you are unsure what order variables in your subset are in, take a look back at the code you used to create the subset.\nAs a final wee note, you can also customise the title of tables created with sumtable() using the title = keyword argument.\n\nCan money buy happiness?\nUsing the data in the nilt_subset dataset, complete the following activities. This will be a good practice run for doing the research report when you run your own analysis on the NILT teaching dataset. Handy!\n\nUsing the hist() function plot a histogram of personal income persinc2. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions (Hint: with base R functions you need to use dataframe$variable format to specify the variable the function should use);\nCreate a summary of the personal income persinc2 variable, using the sumtable() function;\nFinally, compute the mean and standard deviation of the personal income persinc2, grouped by happiness ruhappy. What do you observe?\nDiscuss the results with your neighbour or your tutor.",
    "crumbs": [
      "**Lab 4** Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "10-Lab10.html",
    "href": "10-Lab10.html",
    "title": "Regression Assumptions and Model Fit",
    "section": "",
    "text": "Regression Assumptions and Model Fit\n\n\n\n\n\n\nImportantUpdate In Progress\n\n\n\nWe are refreshing the contents of the lab workbook this year. The newest version of this page for 2025/2026 will be uploaded nearer the time for the lab.",
    "crumbs": [
      "**Lab 10** Multiple Linear Regression (part 2)"
    ]
  },
  {
    "objectID": "09-Lab9.html",
    "href": "09-Lab9.html",
    "title": "Moving from Simple to Multiple Regression",
    "section": "",
    "text": "This week we move from simple linear regression (one independent variable) to multiple linear regression (two or more independent variables). In simple terms, multiple regression lets us compare like with like, we can interpret the coefficient for one independent variable while controlling for the other independent variables we include in our model.\nIt is rare for quantitative social research to rely only on simple linear models. Social phenomena are complex, without one single clear variable associated with them. Instead, we need analysis approaches that recognise and - key word for this week - ‘control’ for other variables.\nBecause it is so central, we will split the topic over two labs. This week we will focus on why how multiple linear regression differs from simple linear regression and interpreting coefficients. Next week we look at how number of observations can change as you add variables and interpreting model fit statistics.\n\n\n\n\n\n\nNoteOverview\n\n\n\nBy the end of this week’s lab, you will know how to:\n\nInterpret a multiple regression coefficient as the expected change in the dependent variable for a one-unit increase in an independent variable, controlling for the other independent variables in the model.\nIdentify reference categories for categorical independent variables and interpret their coefficients in relation to that reference category.\nUnderstand the relevance of other key stats in the regression results - standard error, t value, and p-value - in interpreting the results.\n\n\n\n\n\n\n\n\n\nTipDon’t Panic\n\n\n\nAs said last week, don’t worry where things don’t immediately make full sense. With quantitative analysis, there are ‘spikes’ in difficulty. Moving to linear regression is one of those spikes, and you are definitely not alone in any difficulty experienced getting to grips with it. Each of the ‘spikes’ though will follow a similar pattern. Just as getting to grips with the basics of R code may have initially felt difficult - or even impossible - it is, hopefully, now easier to understand. As you bit by bit get to grips with the basics of linear regression you won’t experience that same level of difficulty again each time you need to interpret a linear regression model.\n\n“Don’t Panic. It’s the first helpful or intelligible thing anybody’s said to me all day.”\nArthur Dent (in Douglas Adams’ phenomenal The Hitchhiker’s Guide to the Galaxy - absolutely essential reading, but not for this course.)\n\nAs wee final reminder: Please do let me (Alasdair) know though where anything in the content for this lab is unclear or confusing. I am keen that the lab workbook helps reduce some of the initial difficulty, and I will make further updates to the content this year based on any initial feedback and questions.\n\n\n\n\nWe will continue using the NILT project on Posit Cloud.\nAs usual, we will need to setup an R Markdown file for today’s lab. Remember if you are unsure about any of the steps below you can take a look back at Lab 6 which has more detailed explanation of each of the steps.\nTo set up a new R Markdown file for this lab, please use the following steps:\n\nPlease go to your ‘Lab Group ##’ in Posit Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Lab Group ##’;\nWithin the Files tab (bottom-right pane) click ‘New File’, then ‘R Markdown’ from the drop-down list of options;\nWithin the ‘Create a New File in Current Directory’ dialogue, name it ‘Lab-9-10-Multiple-Linear-Regression.Rmd’ and click OK.\nFeel free to make any adjustments to the YAML header.\nCreate a new code chunk, with ```{r setup, include=FALSE} and in the chunk:\n\n\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\n\nCreate another code chunk, named preamble and again with include=FALSE. Then add the following in the chunk:\n\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(moderndive)\n\n# Read NILT\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nPlease note, we are loading the haven and moderndive packages this week. (moderndive provides a handy wee function we can use with ggplot for illustrating our example this week.)\n\nAs this is the first time using moderndive, remember to also run in the Console:\n\n\ninstall.packages(\"moderndive\")\n\n\nAfter installing moderndive, run the preamble code chunk and if no errors, then everything is setup for this session.\n\nWe will revisit the example from last week of the relationship between a respondent’s age and their spouse or partner’s age. To ensure out document can knit, we will need to coerce the variables for the age of the respondent’s spouse/partner:\n\n# Age of Respondent's Spouse / Partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n\n(Note: If you completed the ‘Bonus Activity’ last week you won’t need to coerce spage again.)\n\nMultiple linear regression is based on the same principles as simple linear regression. As with simple linear regression, it is appropriate to use only with a numeric (interval/ratio) dependent variable, such as age or income. Our independent variables though can be categorical and numeric. With multiple linear regression, our independent variables do not have to be all be of the same type either, and can be a mix of categorical and numeric variables.\n\n\nOK, so we have a rough ‘why’ for multiple regression - social phenomena are complex and cannot be explained by one variable alone. We can use our good friend ggplot to help illustrate an example of how considering more variables aids explanations - and what can get missed if we only considered one independent variable.\n\nOnce again, to help make it easier to visualise key concepts, let’s create a minimal random sample for the following example using the code below:\n\n# select a small random sample\nset.seed(3)\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- nilt |&gt; filter(!is.na(spage)) |&gt; sample_n(40)\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- nilt_sample |&gt; select(rage, spage, rsex)\n\n\n\n\n\n\n\nAs covered last week, we set a ‘seed’ as this is used by the random number generator. This ensures that whilst it randomly selects 40 observations from the NILT data, the random selection will be the same each time the code is run.\nWe are also only doing this as having a smaller sample makes it easier to visualise key concepts. You would not reduce your sample like this in your analysis. Also, whilst this is referred to as a ‘random sample’, this is different to what we mean by ‘random sample’ in relation to data collection. (One of those annoying situations where ‘same term, different meaning’.)\n\n\n\nWith ggplot we can now again draw a scatter plot with respondent’s age, rage, on the X axis and their partner/spouse’s age, spage, on the Y axis. However, this time, let’s add gender, rsex as a third variable, passing it in for the color keyword argument in the aes() function. This will then colour the data points in our scatter plot by the respondent’s gender.\nWhy add rsex here? In practice, you would want to ensure your decisions are grounded with relevant existing literature for your research question. Assuming here that we are basing it on the literature, we may start with our question on relation between respondent age and their partner/spouse age. That gave us our chosen independent and dependent variables. We need though to consider what else might influence partner/spouse age that we should control for. As age gaps often differ based on gender, then rsex would be a variable to consider controlling for. Controlling for rsex lets us compare respondent age and partner/spouse age with like for like.\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nRight, what can we observe in this plot?\nFirst, we can see that the observations for male and female respondents go in the same direction. When the respondent’s age increases, the age of their partner/spouse increases as well. Female respondents also tend to be located higher on the Y axis (their partner/spouse’s age) compared to male respondents. So it does look like not only the respondents’ age that is involved in age differences with their partners/spouses, but also the respondents’ gender could help explain those differences.\nWe can again draw lines on our graph to help illustrate that. Last week we drew our initial guesstimate line, then compared it to a line for a fitted linear model. The geom_parallel_slopes() function from moderndive can be used to draw lines for a ‘parellel slopes model’. This draws the fitted lines from a multiple regression with one numeric and one categorical independent variable. “Parallel” just means the categories share the same slope for the numeric variable, but the lines have different intercepts (vertical shift) based on differences between the categories.\nLooking at the documentation for geom_parallel_slopes() - the auto-generated link for it doesn’t work so use this link instead - we can see that it will use the variable we passed into the color keyword argument as the categorical variable to use. So, we can simply add it as another component to our ggplot from above, only needing to add, as did in our illustrations last week, se = FALSE. The se argument is whether to display confidence intervals and is TRUE by default. We set it to FALSE as for the illustrations we are wanting to focus on just the lines.\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nNow we have two lines in our graph, one for male respondents, the other for female respondents.\nWell, our suspicion that the points representing female respondents were generally above the points for our male respondents is gathering support. What we have in the plot above is a fitted multiple regression model with parallel lines that show difference based on gender - which we’ll cover in more detail below.\nBased on our quick visualisations, our initial working interpretation so far is:\n\nThe partner/spouses’ age are positively associated with our male and female respondents’ ages.\nThis association is different for males and females. In general, female respondents tend to have partners/spouses older than themselves than male respondents.\n\n\nOK, we can see from our illustrated example that there is a difference, but how would we more rigorously analyse these relationships? For instance, how can we calcualte the magnitude of these relationships?\nTo do that, we can extend on the code we used for simple linear regression last week.\nAs a reminder, last week we fitted and run a model with:\n\nm1_simple &lt;- lm(spage ~ rage, data = nilt_sample)\nm1_simple\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage  \n      6.299        0.875  \n\n\nTo extend it we literally just add another variable into the function. The general format when fitting multiple linear regression models is -\n\n\n\nExample Code - do not run\n\nlm(dependent_variable ~ independent_variable1 + independent_variable2 + ..., data)\n\n\n\nWith our example this week then, all we need to do is + rsex and run the model, as so -\n\nm1 &lt;- lm(spage ~ rage + rsex, data = nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage + rsex, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage   rsexFemale  \n     1.7383       0.9138       4.9190  \n\n\nWithin the results, we can see that there are three coefficients - the first is the intercept, and the other two are our slope coefficients for rage and rsexFemale.\nWithin the formal model specification - which we will return to - these would would be represented as:\n\nintercept, \\(\\beta_0\\)\n\nslope coefficient one, \\(\\beta_1\\)\n\nslope coefficient two, \\(\\beta_2\\)\n\n\nYou may have noticed something though which may seem odd. We have rsexFemale rather than rsex. Two questions often spring up when people first encounter this - Why the change to our variable? Why do we have rsexFemale but no rsexMale?\nThe reason for this is within multiple linear regression the way categorical variables are handled is to select one category as the ‘reference’ and the other categories are then in relation to that reference category. By default, if a category is not specified, R will treat the first category as the reference category. As our nilt_sample has only female and male respondents, we then have rsexFemale with the coefficient of our female respondents in relation to our male reference category.\nIf we had a variable example with categories A, B, and C and included it in our model, by default our reference category would be A and within the results we would see coefficients for exampleB and exampleC. Behind the scenes, R effectively turns our categorical variables into a series of TRUE/FALSE categorical variables exampleA, exampleB, and exampleC. The values for respondents in each is TRUE/FALSE of whether they are in that category. This then makes it possible to calculate the coefficient for B and C against the A reference. \nBack then to our interpretation. Now that we are being more rigorous in fitting a regression model, we can be more precise with our interpretation. We can say there is a positive relationship between respondents’ age and their partner/spouses’ age by a factor of 0.9138 when holding gender constant. In other words, for each one year increase in respondents’ age we see a 0.9138 year increase in partner/spouses’ age. Our rsexFemale coefficient tells us that we can expect the age of female respondents’ partners/spouses to be 4.9190 years older compared to the age of male respondents’ partners/spouses.\nNote how for our numeric variable, rage, it follows similar to how we interpret the coefficient in our simple linear regression model, where the coefficient is expressed in terms of a one unit increase in the independent variable. However, with our rsexFemale categorical variable, it is not for each one unit increase in the independent variable but how this category is different compared to the reference category, rsexMale. This is why R for a categorical variable R has to transform it into a series of TRUE/FALSE categorical variables for each category in the variable.\nWhat about the intercept? The intercept is 1.7383. Formally, this is the expected spouse/partner age when the respondent’s age is 0 and the respondent is in the reference category of rsex (here, Male). Because age 0 is outside the range we care about, and wouldn’t make sense to speak in terms of a new-born having a partner/spouse, we don’t try to give this a real‑world interpretation in this case. It is used though when speaking in terms such as - for a 23 year old male respondent we expect their partner/spouse to be approximately 22.76 years old:\n\nIf we take 23 and times it by 0.9138 (our rage coefficient) we get 21.0174, then after adding 1.7383 (our intercept) we have 22.7557.\n\nFor a 23 year old female would we would add 4.9190 (our rsexFemale) for calculating what we would expect their partner/spouse to be approximately 27.6747.\nImportantly, if we return to our graph -\n\n\n\n\n\n\n\n\nWe can now understand better what it shows. The lines are increasing by the coefficient for rage of 0.9138 for each 1 year increase in respondent age, but with varying intercepts so the lines are separate by the 4.9190 coefficent for rsexFemale compared to the rsexMale reference category.\nNow compare this to last week’s simple model, also included in above section as m1_simple, which does not control for gender. In that model the slope for rage was lower - 0.875 - and the intercept higher - 6.299. In our multiple regression that now includes rsex the slope becomes 0.9138.\nThis is consistent pattern we would see with potential missing variable bias. If a relevant variable is left out of our model, the estimated slope can be pulled away from the “within‑group” relationship. Here, ignoring it mixes the differences between female and male respondents together and gives a compromise line in our simple linear regression for rage with a higher intercept and shallower slope. Using a multiple linear regression that including gender lets us compare like with like and recovers the within‑gender slope - or, in other words, our slope for rage controlling for gender.\nWe can also add a line to our ggplot to include the simple linear regression line similar to last week. Our parellel slopes though will now let us see how the coefficient for rage in the simple linear regression model was impacted by missing variable bias:\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\", se = FALSE, colour = \"black\") +\n  geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent's spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent's spouse/partner age\"\n  )\n\n\n\n\n\n\n\nGiven the age of respondents and partners/spouses, the lines aren’t drawn all the way to X = 0. However, knowing that the red line shown is based on the intercept value as shown in our multiple linear regression model results (1.7383) helps us visually compare our simple and multiple linear regression models.\nAs can roughly see, with more female respondents in bottom-left and only males as approach top-right, the simple linear regression model in aiming to draw best fit through the observations ends up with a higher intercept and smaller slope. Once we control for gender though we can see how the slope coeffecient for rage was being lowered by not controlling for gender.\n(Wee aside, as noted above, this parallel slopes model gpplot only works when we have one numeric independent variable and one categorical independent variable. As we add in more variables, we would not be able - at least with any meaingful clarity - visualise the coefficients for multiple linear regression with a scatter plot.)\n\nAfter the introduction of the simple linear model, the formal definition of the multiple linear model should not look that scary, right? In essence, this equation tells us how \\(\\hat{y}\\) is described/explained by other variables.\n\\[\n\\begin{aligned}\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}*x_1 + \\hat{\\beta_2}*x_2 + ... +  \\hat{\\epsilon}, && \\epsilon ~ N(0, \\sigma)  \n\\end{aligned}\n\\]\nWe can break down the equation above in smaller pieces as follows:\n\n\n\\(\\hat{y}\\) is the dependent variable, which is explained by\n\n\\(\\hat{\\beta_0}\\) the intercept, plus\n\n\\(\\hat{\\beta_1}\\) the slope coefficient for the first independent variable times the value of \\(x_1\\) variable 1, plus\n\n\\(\\hat{\\beta_2}\\) the slope coefficient for the second independent variable times the value of \\(x_2\\), plus\n\n\\(...\\) any number of independent variables, plus\n\n\\(\\hat{\\epsilon}\\) the error/residual term.\n\n\\(\\epsilon ~ N(0, \\sigma)\\) this bit tell us that the residuals are normally distributed.\n\nAs a reminder, the general syntax in R for fitting a multiple linear regression model as follows:\n\n\n\nExample Code - do not run\n\nlm(dependent_variable ~ independent_variable1 + independent_variable2 + ..., data)\n\n\nWe pass into the lm() function our dependent variable followed by ~ then list two or more independent variables joined by +. Finally, after a comma, the data = ... argument names the data frame where R should look for those variables.\n\nOK, let’s briefly introduce a second multiple linear regression model to interpret - this time focusing solely on its results rather than comparing it with a simple linear regression model.\nLet’s assume this time we are interested in how gender is associated with the number of hours someone works. Based on the literature we may decide that age and any health conditions are important variables to control for. Younger people are more likely to be students and in part-time work. We would also expect that hours worked would decrease as people approach and reach retirement age. Similarly, we may expect that people with health conditions may work less hours - which may not necessarily be due to their health, but also lack of reasonable adjustments from employers.\nTo setup our second model then, we need to coerce the anyhcond variable which is binary Yes/No categorical variable for those with a physical or mental health condition or illness lasting or expected to last 12 months or more.\n\n# Coerce to factor\nnilt &lt;- nilt |&gt;\n  mutate(anyhcond = as_factor(anyhcond))\n\n\nAfter coercing anyhcond to a factor, we can fit the model and use summary() to view the regression results -\n\n# alternative model test\nm2 &lt;- lm(rhourswk ~ rsex + rage + anyhcond, data = nilt)\nsummary(m2)\n\n\nCall:\nlm(formula = rhourswk ~ rsex + rage + anyhcond, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.812  -5.346  -0.923   6.972  58.848 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  39.82244    2.60505  15.287   &lt;2e-16 ***\nrsexFemale  -11.05924    0.92931 -11.901   &lt;2e-16 ***\nrage         -0.03270    0.03827  -0.854    0.393    \nanyhcondNo    2.47421    1.90816   1.297    0.195    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.02 on 562 degrees of freedom\n  (638 observations deleted due to missingness)\nMultiple R-squared:  0.205, Adjusted R-squared:  0.2007 \nF-statistic:  48.3 on 3 and 562 DF,  p-value: &lt; 2.2e-16\n\n\nFor this week let’s focus in on the “Coefficients:” table - we will return to other stats in the results for this model next week.\nRight, so the coefficient (Estimate) we have already covered in lab 8 and again at start of this lab. To repeat once more though when we move from simple to multiple linear regression the coefficient describes the association between an independent variable and the dependent variable while holding the other variables constant - by controlling for the other variables.\nTo then briefly introduce our other key statistics here. The standard error (SE) next to a coefficient is the model’s estimate of how much that coefficient would vary across samples - in other words, the uncertainty around that estimate. Imagine repeating the same survey with the same design numerous times. Each time we’d get a different sample and dataset, so the coefficient would vary from one sample to the next. The standard error is the estimate of that. The t value is then the coefficient divided by its standard error; with values further from zero indicating statistical significance of the independent variable.\nThe p-value is paired with the t value. As covered last week, there are different codes used for significance, indicated with asterisks next to the p-value. In the social sciences due to the complexity of phenomena and difficulty of controlling factors to the same extent as in other fields, 0.05 is a common significance level to use. We use it in interpreting whether a variable is significant rather than how significant it is; in other words, we do not say a variable with two asterisks is ‘more significant’ than a variable with one asterisk. Both would be significant based on our chosen significance of 0.05.\n\nOK, let’s turn to a quick initial interpretation row by row. I’ll repeat the stats in full for each row to save you scrolling up/down and be a bit more verbose than necessarily would in practice. This is just to re-emphasise key info and help with getting to grips with how we can interpret these stats:\nThe intercept is 39.82244 with SE = 2.60505, t = 15.287, p &lt; 2e−16. This is the expected weekly hours at age 0 for the reference categories in the model - which for us is male and with a health condition. Because age 0 is not meaningful for our context - it doesn’t make sense to speak of number of hours worked per week for a newborn - we do not give the intercept a substantive interpretation. As before, it is a starting point and becomes meaningful in context with our other results. For instance, for when speak about “for a female participant, without a health condition, who is 32 years old, we would expect their hours worked to be …” we would include the intercept in that calculation.\nrsexFemale is −11.05924 with SE = 0.92931, t = −11.901, p &lt; 0.05. Holding age and health constant, female respondents work around 11.06 hours fewer per week than otherwise similar males (male is the reference category). The standard error is small relative to the estimate, which is why the absolute t is large and the p‑value below 0.05. We can say then that this is statistically significant at the 0.05 level. In other words, there is a statistically significant relationship between females respondents working less hours per week than males - as remember our categorical variables are in comparison to the reference category.\nSide-note, we also would need some caution here. The size of the gap here may seem larger than we would have expected. Remember this is a deliberately simple model and we have not controlled for other potential factors that could shape hours such as caring responsibilities, contract type, whether have a partner/spouse, and so on. Some of the observed difference could potentially be explained by those omitted factors. This is just speculation though, and would require further analysis.\nrage is −0.03270 with SE = 0.03827, t = −0.854, p = 0.393. Controlling for gender and any health condition, the estimated change is around −0.03 hours for each unit increase in age. The estimate is small relative to its standard error, which is why the t value is close to zero and the p‑value above 0.05. We can therefore describe this as not statistically significant at our 0.05 level. In other words, the model does not show a clear linear association between age and weekly hours here.\nanyhcondNo is 2.47421 with SE = 1.90816, t = 1.297, p = 0.195. This is comparing respondents without a self-reported health condition to those with one (the reference). The coefficient estimate suggests about 2.47 more hours per week, holding age and gender constant. Here the estimate is modest relative to its standard error, which is why the t value is small and the p‑value above 0.05. At our 0.05 rule this is not statistically significant.\nSide-note, as with our rsexFemale coefficient the results here may also be surprising. For this variable, it may seem surprising based on the existing literature that this variable is not significant. There are different potential reasons for that. One could be that a single “any condition” yes/no lumps too many different health conditions together compared to if we had a variable that had more categories based on say self-reported level of impact any health condition has on daily life.\nAnother explanation may be due to the variables we have chosen for the model. Notice the line “638 observations deleted due to missingness”. In other words, compared to our original full data set our model has 638 less observations. This is because the model excludes any observation with missing values for any of the included variables. Here we can see how even three values can have a large impact on the number of observations. We will return to exploring that though as the beginning of next before then turning to assumptions and model fit stats.\n\nOK, let’s setup another multiple linear regression model.\nNote, if you want treat the variables below as ‘suggested variables’ instead, you can use the NILT 2012 codebook to find variables to build your own model.\n\nRemember you might need to coerce some additional variables. You can do that on an ad hoc basis, or by adding code to coerce them to your 01_prep_nilt_2012.R script. If doing the latter, remember to run the full script again and then run all the code chunks in this week’s R Markdown file.\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m3 and print the summary.\nComment on the results of the model, including which of the variables is significant and consideration of the standard errors.\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "09-Lab9.html#introduction",
    "href": "09-Lab9.html#introduction",
    "title": "Moving from Simple to Multiple Regression",
    "section": "",
    "text": "This week we move from simple linear regression (one independent variable) to multiple linear regression (two or more independent variables). In simple terms, multiple regression lets us compare like with like, we can interpret the coefficient for one independent variable while controlling for the other independent variables we include in our model.\nIt is rare for quantitative social research to rely only on simple linear models. Social phenomena are complex, without one single clear variable associated with them. Instead, we need analysis approaches that recognise and - key word for this week - ‘control’ for other variables.\nBecause it is so central, we will split the topic over two labs. This week we will focus on why how multiple linear regression differs from simple linear regression and interpreting coefficients. Next week we look at how number of observations can change as you add variables and interpreting model fit statistics.\n\n\n\n\n\n\nNoteOverview\n\n\n\nBy the end of this week’s lab, you will know how to:\n\nInterpret a multiple regression coefficient as the expected change in the dependent variable for a one-unit increase in an independent variable, controlling for the other independent variables in the model.\nIdentify reference categories for categorical independent variables and interpret their coefficients in relation to that reference category.\nUnderstand the relevance of other key stats in the regression results - standard error, t value, and p-value - in interpreting the results.\n\n\n\n\n\n\n\n\n\nTipDon’t Panic\n\n\n\nAs said last week, don’t worry where things don’t immediately make full sense. With quantitative analysis, there are ‘spikes’ in difficulty. Moving to linear regression is one of those spikes, and you are definitely not alone in any difficulty experienced getting to grips with it. Each of the ‘spikes’ though will follow a similar pattern. Just as getting to grips with the basics of R code may have initially felt difficult - or even impossible - it is, hopefully, now easier to understand. As you bit by bit get to grips with the basics of linear regression you won’t experience that same level of difficulty again each time you need to interpret a linear regression model.\n\n“Don’t Panic. It’s the first helpful or intelligible thing anybody’s said to me all day.”\nArthur Dent (in Douglas Adams’ phenomenal The Hitchhiker’s Guide to the Galaxy - absolutely essential reading, but not for this course.)\n\nAs wee final reminder: Please do let me (Alasdair) know though where anything in the content for this lab is unclear or confusing. I am keen that the lab workbook helps reduce some of the initial difficulty, and I will make further updates to the content this year based on any initial feedback and questions.",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "09-Lab9.html#setup",
    "href": "09-Lab9.html#setup",
    "title": "Moving from Simple to Multiple Regression",
    "section": "",
    "text": "We will continue using the NILT project on Posit Cloud.\nAs usual, we will need to setup an R Markdown file for today’s lab. Remember if you are unsure about any of the steps below you can take a look back at Lab 6 which has more detailed explanation of each of the steps.\nTo set up a new R Markdown file for this lab, please use the following steps:\n\nPlease go to your ‘Lab Group ##’ in Posit Cloud (log in if necessary);\nOpen your own copy of the ‘NILT’ project from the ‘Lab Group ##’;\nWithin the Files tab (bottom-right pane) click ‘New File’, then ‘R Markdown’ from the drop-down list of options;\nWithin the ‘Create a New File in Current Directory’ dialogue, name it ‘Lab-9-10-Multiple-Linear-Regression.Rmd’ and click OK.\nFeel free to make any adjustments to the YAML header.\nCreate a new code chunk, with ```{r setup, include=FALSE} and in the chunk:\n\n\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\n\nCreate another code chunk, named preamble and again with include=FALSE. Then add the following in the chunk:\n\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(moderndive)\n\n# Read NILT\nnilt &lt;- readRDS(\"data/nilt_r_object.rds\")\n\nPlease note, we are loading the haven and moderndive packages this week. (moderndive provides a handy wee function we can use with ggplot for illustrating our example this week.)\n\nAs this is the first time using moderndive, remember to also run in the Console:\n\n\ninstall.packages(\"moderndive\")\n\n\nAfter installing moderndive, run the preamble code chunk and if no errors, then everything is setup for this session.\n\nWe will revisit the example from last week of the relationship between a respondent’s age and their spouse or partner’s age. To ensure out document can knit, we will need to coerce the variables for the age of the respondent’s spouse/partner:\n\n# Age of Respondent's Spouse / Partner\nnilt$spage &lt;- as.numeric(nilt$spage)\n\n(Note: If you completed the ‘Bonus Activity’ last week you won’t need to coerce spage again.)",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "09-Lab9.html#multiple-linear-regresion",
    "href": "09-Lab9.html#multiple-linear-regresion",
    "title": "Moving from Simple to Multiple Regression",
    "section": "",
    "text": "Multiple linear regression is based on the same principles as simple linear regression. As with simple linear regression, it is appropriate to use only with a numeric (interval/ratio) dependent variable, such as age or income. Our independent variables though can be categorical and numeric. With multiple linear regression, our independent variables do not have to be all be of the same type either, and can be a mix of categorical and numeric variables.\n\n\nOK, so we have a rough ‘why’ for multiple regression - social phenomena are complex and cannot be explained by one variable alone. We can use our good friend ggplot to help illustrate an example of how considering more variables aids explanations - and what can get missed if we only considered one independent variable.\n\nOnce again, to help make it easier to visualise key concepts, let’s create a minimal random sample for the following example using the code below:\n\n# select a small random sample\nset.seed(3)\n# Filter where partner's age is not NA and take a random sample of 40\nnilt_sample &lt;- nilt |&gt; filter(!is.na(spage)) |&gt; sample_n(40)\n# Select only respondent's age and spouse/partner's age\nnilt_sample &lt;- nilt_sample |&gt; select(rage, spage, rsex)\n\n\n\n\n\n\n\nAs covered last week, we set a ‘seed’ as this is used by the random number generator. This ensures that whilst it randomly selects 40 observations from the NILT data, the random selection will be the same each time the code is run.\nWe are also only doing this as having a smaller sample makes it easier to visualise key concepts. You would not reduce your sample like this in your analysis. Also, whilst this is referred to as a ‘random sample’, this is different to what we mean by ‘random sample’ in relation to data collection. (One of those annoying situations where ‘same term, different meaning’.)\n\n\n\nWith ggplot we can now again draw a scatter plot with respondent’s age, rage, on the X axis and their partner/spouse’s age, spage, on the Y axis. However, this time, let’s add gender, rsex as a third variable, passing it in for the color keyword argument in the aes() function. This will then colour the data points in our scatter plot by the respondent’s gender.\nWhy add rsex here? In practice, you would want to ensure your decisions are grounded with relevant existing literature for your research question. Assuming here that we are basing it on the literature, we may start with our question on relation between respondent age and their partner/spouse age. That gave us our chosen independent and dependent variables. We need though to consider what else might influence partner/spouse age that we should control for. As age gaps often differ based on gender, then rsex would be a variable to consider controlling for. Controlling for rsex lets us compare respondent age and partner/spouse age with like for like.\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nRight, what can we observe in this plot?\nFirst, we can see that the observations for male and female respondents go in the same direction. When the respondent’s age increases, the age of their partner/spouse increases as well. Female respondents also tend to be located higher on the Y axis (their partner/spouse’s age) compared to male respondents. So it does look like not only the respondents’ age that is involved in age differences with their partners/spouses, but also the respondents’ gender could help explain those differences.\nWe can again draw lines on our graph to help illustrate that. Last week we drew our initial guesstimate line, then compared it to a line for a fitted linear model. The geom_parallel_slopes() function from moderndive can be used to draw lines for a ‘parellel slopes model’. This draws the fitted lines from a multiple regression with one numeric and one categorical independent variable. “Parallel” just means the categories share the same slope for the numeric variable, but the lines have different intercepts (vertical shift) based on differences between the categories.\nLooking at the documentation for geom_parallel_slopes() - the auto-generated link for it doesn’t work so use this link instead - we can see that it will use the variable we passed into the color keyword argument as the categorical variable to use. So, we can simply add it as another component to our ggplot from above, only needing to add, as did in our illustrations last week, se = FALSE. The se argument is whether to display confidence intervals and is TRUE by default. We set it to FALSE as for the illustrations we are wanting to focus on just the lines.\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent’s spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent’s spouse/partner age\"\n  )\n\n\n\n\n\n\n\nNow we have two lines in our graph, one for male respondents, the other for female respondents.\nWell, our suspicion that the points representing female respondents were generally above the points for our male respondents is gathering support. What we have in the plot above is a fitted multiple regression model with parallel lines that show difference based on gender - which we’ll cover in more detail below.\nBased on our quick visualisations, our initial working interpretation so far is:\n\nThe partner/spouses’ age are positively associated with our male and female respondents’ ages.\nThis association is different for males and females. In general, female respondents tend to have partners/spouses older than themselves than male respondents.\n\n\nOK, we can see from our illustrated example that there is a difference, but how would we more rigorously analyse these relationships? For instance, how can we calcualte the magnitude of these relationships?\nTo do that, we can extend on the code we used for simple linear regression last week.\nAs a reminder, last week we fitted and run a model with:\n\nm1_simple &lt;- lm(spage ~ rage, data = nilt_sample)\nm1_simple\n\n\nCall:\nlm(formula = spage ~ rage, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage  \n      6.299        0.875  \n\n\nTo extend it we literally just add another variable into the function. The general format when fitting multiple linear regression models is -\n\n\n\nExample Code - do not run\n\nlm(dependent_variable ~ independent_variable1 + independent_variable2 + ..., data)\n\n\n\nWith our example this week then, all we need to do is + rsex and run the model, as so -\n\nm1 &lt;- lm(spage ~ rage + rsex, data = nilt_sample)\nm1\n\n\nCall:\nlm(formula = spage ~ rage + rsex, data = nilt_sample)\n\nCoefficients:\n(Intercept)         rage   rsexFemale  \n     1.7383       0.9138       4.9190  \n\n\nWithin the results, we can see that there are three coefficients - the first is the intercept, and the other two are our slope coefficients for rage and rsexFemale.\nWithin the formal model specification - which we will return to - these would would be represented as:\n\nintercept, \\(\\beta_0\\)\n\nslope coefficient one, \\(\\beta_1\\)\n\nslope coefficient two, \\(\\beta_2\\)\n\n\nYou may have noticed something though which may seem odd. We have rsexFemale rather than rsex. Two questions often spring up when people first encounter this - Why the change to our variable? Why do we have rsexFemale but no rsexMale?\nThe reason for this is within multiple linear regression the way categorical variables are handled is to select one category as the ‘reference’ and the other categories are then in relation to that reference category. By default, if a category is not specified, R will treat the first category as the reference category. As our nilt_sample has only female and male respondents, we then have rsexFemale with the coefficient of our female respondents in relation to our male reference category.\nIf we had a variable example with categories A, B, and C and included it in our model, by default our reference category would be A and within the results we would see coefficients for exampleB and exampleC. Behind the scenes, R effectively turns our categorical variables into a series of TRUE/FALSE categorical variables exampleA, exampleB, and exampleC. The values for respondents in each is TRUE/FALSE of whether they are in that category. This then makes it possible to calculate the coefficient for B and C against the A reference. \nBack then to our interpretation. Now that we are being more rigorous in fitting a regression model, we can be more precise with our interpretation. We can say there is a positive relationship between respondents’ age and their partner/spouses’ age by a factor of 0.9138 when holding gender constant. In other words, for each one year increase in respondents’ age we see a 0.9138 year increase in partner/spouses’ age. Our rsexFemale coefficient tells us that we can expect the age of female respondents’ partners/spouses to be 4.9190 years older compared to the age of male respondents’ partners/spouses.\nNote how for our numeric variable, rage, it follows similar to how we interpret the coefficient in our simple linear regression model, where the coefficient is expressed in terms of a one unit increase in the independent variable. However, with our rsexFemale categorical variable, it is not for each one unit increase in the independent variable but how this category is different compared to the reference category, rsexMale. This is why R for a categorical variable R has to transform it into a series of TRUE/FALSE categorical variables for each category in the variable.\nWhat about the intercept? The intercept is 1.7383. Formally, this is the expected spouse/partner age when the respondent’s age is 0 and the respondent is in the reference category of rsex (here, Male). Because age 0 is outside the range we care about, and wouldn’t make sense to speak in terms of a new-born having a partner/spouse, we don’t try to give this a real‑world interpretation in this case. It is used though when speaking in terms such as - for a 23 year old male respondent we expect their partner/spouse to be approximately 22.76 years old:\n\nIf we take 23 and times it by 0.9138 (our rage coefficient) we get 21.0174, then after adding 1.7383 (our intercept) we have 22.7557.\n\nFor a 23 year old female would we would add 4.9190 (our rsexFemale) for calculating what we would expect their partner/spouse to be approximately 27.6747.\nImportantly, if we return to our graph -\n\n\n\n\n\n\n\n\nWe can now understand better what it shows. The lines are increasing by the coefficient for rage of 0.9138 for each 1 year increase in respondent age, but with varying intercepts so the lines are separate by the 4.9190 coefficent for rsexFemale compared to the rsexMale reference category.\nNow compare this to last week’s simple model, also included in above section as m1_simple, which does not control for gender. In that model the slope for rage was lower - 0.875 - and the intercept higher - 6.299. In our multiple regression that now includes rsex the slope becomes 0.9138.\nThis is consistent pattern we would see with potential missing variable bias. If a relevant variable is left out of our model, the estimated slope can be pulled away from the “within‑group” relationship. Here, ignoring it mixes the differences between female and male respondents together and gives a compromise line in our simple linear regression for rage with a higher intercept and shallower slope. Using a multiple linear regression that including gender lets us compare like with like and recovers the within‑gender slope - or, in other words, our slope for rage controlling for gender.\nWe can also add a line to our ggplot to include the simple linear regression line similar to last week. Our parellel slopes though will now let us see how the coefficient for rage in the simple linear regression model was impacted by missing variable bias:\n\nnilt_sample |&gt; ggplot(aes(x = rage, y = spage, color = rsex)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\", se = FALSE, colour = \"black\") +\n  geom_parallel_slopes(se = FALSE) +\n  labs(\n    title = \"Respondent's age vs respondent's spouse/partner age\",\n    x = \"Respondent's age\", y = \"Respondent's spouse/partner age\"\n  )\n\n\n\n\n\n\n\nGiven the age of respondents and partners/spouses, the lines aren’t drawn all the way to X = 0. However, knowing that the red line shown is based on the intercept value as shown in our multiple linear regression model results (1.7383) helps us visually compare our simple and multiple linear regression models.\nAs can roughly see, with more female respondents in bottom-left and only males as approach top-right, the simple linear regression model in aiming to draw best fit through the observations ends up with a higher intercept and smaller slope. Once we control for gender though we can see how the slope coeffecient for rage was being lowered by not controlling for gender.\n(Wee aside, as noted above, this parallel slopes model gpplot only works when we have one numeric independent variable and one categorical independent variable. As we add in more variables, we would not be able - at least with any meaingful clarity - visualise the coefficients for multiple linear regression with a scatter plot.)\n\nAfter the introduction of the simple linear model, the formal definition of the multiple linear model should not look that scary, right? In essence, this equation tells us how \\(\\hat{y}\\) is described/explained by other variables.\n\\[\n\\begin{aligned}\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}*x_1 + \\hat{\\beta_2}*x_2 + ... +  \\hat{\\epsilon}, && \\epsilon ~ N(0, \\sigma)  \n\\end{aligned}\n\\]\nWe can break down the equation above in smaller pieces as follows:\n\n\n\\(\\hat{y}\\) is the dependent variable, which is explained by\n\n\\(\\hat{\\beta_0}\\) the intercept, plus\n\n\\(\\hat{\\beta_1}\\) the slope coefficient for the first independent variable times the value of \\(x_1\\) variable 1, plus\n\n\\(\\hat{\\beta_2}\\) the slope coefficient for the second independent variable times the value of \\(x_2\\), plus\n\n\\(...\\) any number of independent variables, plus\n\n\\(\\hat{\\epsilon}\\) the error/residual term.\n\n\\(\\epsilon ~ N(0, \\sigma)\\) this bit tell us that the residuals are normally distributed.\n\nAs a reminder, the general syntax in R for fitting a multiple linear regression model as follows:\n\n\n\nExample Code - do not run\n\nlm(dependent_variable ~ independent_variable1 + independent_variable2 + ..., data)\n\n\nWe pass into the lm() function our dependent variable followed by ~ then list two or more independent variables joined by +. Finally, after a comma, the data = ... argument names the data frame where R should look for those variables.\n\nOK, let’s briefly introduce a second multiple linear regression model to interpret - this time focusing solely on its results rather than comparing it with a simple linear regression model.\nLet’s assume this time we are interested in how gender is associated with the number of hours someone works. Based on the literature we may decide that age and any health conditions are important variables to control for. Younger people are more likely to be students and in part-time work. We would also expect that hours worked would decrease as people approach and reach retirement age. Similarly, we may expect that people with health conditions may work less hours - which may not necessarily be due to their health, but also lack of reasonable adjustments from employers.\nTo setup our second model then, we need to coerce the anyhcond variable which is binary Yes/No categorical variable for those with a physical or mental health condition or illness lasting or expected to last 12 months or more.\n\n# Coerce to factor\nnilt &lt;- nilt |&gt;\n  mutate(anyhcond = as_factor(anyhcond))\n\n\nAfter coercing anyhcond to a factor, we can fit the model and use summary() to view the regression results -\n\n# alternative model test\nm2 &lt;- lm(rhourswk ~ rsex + rage + anyhcond, data = nilt)\nsummary(m2)\n\n\nCall:\nlm(formula = rhourswk ~ rsex + rage + anyhcond, data = nilt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.812  -5.346  -0.923   6.972  58.848 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  39.82244    2.60505  15.287   &lt;2e-16 ***\nrsexFemale  -11.05924    0.92931 -11.901   &lt;2e-16 ***\nrage         -0.03270    0.03827  -0.854    0.393    \nanyhcondNo    2.47421    1.90816   1.297    0.195    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.02 on 562 degrees of freedom\n  (638 observations deleted due to missingness)\nMultiple R-squared:  0.205, Adjusted R-squared:  0.2007 \nF-statistic:  48.3 on 3 and 562 DF,  p-value: &lt; 2.2e-16\n\n\nFor this week let’s focus in on the “Coefficients:” table - we will return to other stats in the results for this model next week.\nRight, so the coefficient (Estimate) we have already covered in lab 8 and again at start of this lab. To repeat once more though when we move from simple to multiple linear regression the coefficient describes the association between an independent variable and the dependent variable while holding the other variables constant - by controlling for the other variables.\nTo then briefly introduce our other key statistics here. The standard error (SE) next to a coefficient is the model’s estimate of how much that coefficient would vary across samples - in other words, the uncertainty around that estimate. Imagine repeating the same survey with the same design numerous times. Each time we’d get a different sample and dataset, so the coefficient would vary from one sample to the next. The standard error is the estimate of that. The t value is then the coefficient divided by its standard error; with values further from zero indicating statistical significance of the independent variable.\nThe p-value is paired with the t value. As covered last week, there are different codes used for significance, indicated with asterisks next to the p-value. In the social sciences due to the complexity of phenomena and difficulty of controlling factors to the same extent as in other fields, 0.05 is a common significance level to use. We use it in interpreting whether a variable is significant rather than how significant it is; in other words, we do not say a variable with two asterisks is ‘more significant’ than a variable with one asterisk. Both would be significant based on our chosen significance of 0.05.\n\nOK, let’s turn to a quick initial interpretation row by row. I’ll repeat the stats in full for each row to save you scrolling up/down and be a bit more verbose than necessarily would in practice. This is just to re-emphasise key info and help with getting to grips with how we can interpret these stats:\nThe intercept is 39.82244 with SE = 2.60505, t = 15.287, p &lt; 2e−16. This is the expected weekly hours at age 0 for the reference categories in the model - which for us is male and with a health condition. Because age 0 is not meaningful for our context - it doesn’t make sense to speak of number of hours worked per week for a newborn - we do not give the intercept a substantive interpretation. As before, it is a starting point and becomes meaningful in context with our other results. For instance, for when speak about “for a female participant, without a health condition, who is 32 years old, we would expect their hours worked to be …” we would include the intercept in that calculation.\nrsexFemale is −11.05924 with SE = 0.92931, t = −11.901, p &lt; 0.05. Holding age and health constant, female respondents work around 11.06 hours fewer per week than otherwise similar males (male is the reference category). The standard error is small relative to the estimate, which is why the absolute t is large and the p‑value below 0.05. We can say then that this is statistically significant at the 0.05 level. In other words, there is a statistically significant relationship between females respondents working less hours per week than males - as remember our categorical variables are in comparison to the reference category.\nSide-note, we also would need some caution here. The size of the gap here may seem larger than we would have expected. Remember this is a deliberately simple model and we have not controlled for other potential factors that could shape hours such as caring responsibilities, contract type, whether have a partner/spouse, and so on. Some of the observed difference could potentially be explained by those omitted factors. This is just speculation though, and would require further analysis.\nrage is −0.03270 with SE = 0.03827, t = −0.854, p = 0.393. Controlling for gender and any health condition, the estimated change is around −0.03 hours for each unit increase in age. The estimate is small relative to its standard error, which is why the t value is close to zero and the p‑value above 0.05. We can therefore describe this as not statistically significant at our 0.05 level. In other words, the model does not show a clear linear association between age and weekly hours here.\nanyhcondNo is 2.47421 with SE = 1.90816, t = 1.297, p = 0.195. This is comparing respondents without a self-reported health condition to those with one (the reference). The coefficient estimate suggests about 2.47 more hours per week, holding age and gender constant. Here the estimate is modest relative to its standard error, which is why the t value is small and the p‑value above 0.05. At our 0.05 rule this is not statistically significant.\nSide-note, as with our rsexFemale coefficient the results here may also be surprising. For this variable, it may seem surprising based on the existing literature that this variable is not significant. There are different potential reasons for that. One could be that a single “any condition” yes/no lumps too many different health conditions together compared to if we had a variable that had more categories based on say self-reported level of impact any health condition has on daily life.\nAnother explanation may be due to the variables we have chosen for the model. Notice the line “638 observations deleted due to missingness”. In other words, compared to our original full data set our model has 638 less observations. This is because the model excludes any observation with missing values for any of the included variables. Here we can see how even three values can have a large impact on the number of observations. We will return to exploring that though as the beginning of next before then turning to assumptions and model fit stats.",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "09-Lab9.html#lab-activities",
    "href": "09-Lab9.html#lab-activities",
    "title": "Moving from Simple to Multiple Regression",
    "section": "",
    "text": "OK, let’s setup another multiple linear regression model.\nNote, if you want treat the variables below as ‘suggested variables’ instead, you can use the NILT 2012 codebook to find variables to build your own model.\n\nRemember you might need to coerce some additional variables. You can do that on an ad hoc basis, or by adding code to coerce them to your 01_prep_nilt_2012.R script. If doing the latter, remember to run the full script again and then run all the code chunks in this week’s R Markdown file.\n\n\nPrint a table for the highest level of qualification highqual using the table() function.\nGenerate a scatter plot using ggplot. Within aes(), locate the number of hours worked a week rhourswk on the X axis and the personal income persinc2 on the Y axis, and specify the color of the dots by the highest level of qualification highqual. Use the geom_point() function and ‘jitter’ the points using the argument position. Add the parallel slopes using the geom_parallel_slopes() function and set the standard error se to FALSE. What is your interpretation of the plot? Write down your comments to introduce the plot.\nFit a linear model using the lm() function to analyse the personal income persinc2 using the number of hours worked a week rhourswk, the highest level of qualification highqual, and the age of the respondent rage as independent variables. Store the model in an object called m3 and print the summary.\nComment on the results of the model, including which of the variables is significant and consideration of the standard errors.\nDiscuss your answers with your neighbour or tutor.",
    "crumbs": [
      "**Lab 9** Multiple Linear Regression (part 1)"
    ]
  },
  {
    "objectID": "assessments/assessments.html",
    "href": "assessments/assessments.html",
    "title": "Assessments",
    "section": "",
    "text": "Remember to also check the Assessment section on Moodle, which contains further information and the Turnitin submission areas.\n\n\n\n\n\n\nImportantDeadlines\n\n\n\n\nFormative Assessment – 12 noon, Friday 31st October 2025\nSummative Assessments (Reflective Summary + Interpreting Quantitative Findings Report) - 12 noon, Monday 15th December 2025.\n\n\n\n\nSetting Up RStudio Projects for the Assessments\nWe provide template projects to use for the formative and summative assessments via GitHub repositories. This is similar to how we shared the template project we used for Labs 1 and 2.\nPlease see the Setup Assessment Projects page for step-by-step guide and links to use.\n\n\nGenAI and the Assessments\nThe University of Glasgow’s position on GenAI use is not to prohibit GenAI use, but instead to support you to use it “effectively, ethically, critically, and transparently”.\nFor the assessments on this course, you are allowed to use GenAI tools in some specific ways as part of your assessment.\nThe use of GenAI tools can augment specific parts of your submission; such as to help with drafting and editing, and to explore key ideas.\nHowever, key to the interpretive report assessment is using R and interpreting research findings. It is, therefore, not permitted to use GenAI to write R code for your analysis nor to interpret the statistical results. It is acceptable to use GenAI to receive textbook code examples and explanations, but not the exact code needed for the analysis. Similarly, it is acceptable to use GenAI for explanations on how to interpret statistics in general, but not the exact stats from your analysis.\nIf using any form of GenAI tool, you must acknowledge how the tools have been used within your work. You cannot submit any content produced by GenAI as your own work.\nRStudio Helper is an example ChatGPT Custom GPT / Copilot Saved Prompt created for use on this module. It has instructions that guide it towards providing more useful responses that aid learning and that align with the specific ways GenAI can be used for the assessments. However, whilst it has these instructions, GenAI does not always reliably and consistently follow them. So, still exercise the same caution with its responses and use prompts that align with permitted use cases for the assessments.",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "assessments/RFAQ.html",
    "href": "assessments/RFAQ.html",
    "title": "R Issues FAQ",
    "section": "",
    "text": "This page contains information for solving common issues students have reported when working on their interpretive findings report.\nUpdates will continue being added to this page with information for any new issues being reported. So, if you experience an issue, this page will be a good first port of call to check for any existing solution.\n\n\n\n\n\n\n\n\nNoteReporting R Issues\n\n\n\nWhen making a Moodle post or emailing about an issue it is vital to provide sufficient context to enable others to help find a solution. This includes:\n\nThe full text of any error message that you are receiving.\nAny relevant code that is producing the error.\n\nError messages will often flag the relevant code chunk where the error occurred:\n\n\n\n\n\n\n\n\nSo, based on the above error message you would include the code within the code block at lines 49-50.\nAdditionally, when emailing, please also include:\n\nYour lab group number.\nIf already made a Moodle post about the issue, a link to the post.\n\n\n\n\n\n\n\n\n\n\nImportantImportant\n\n\n\nAn underlying cause to many issues reported each year is believing more code needs to be written for the assessments than is actually required.\nThe formative and summative assessment template projects we provide already have key things setup for you. Please ensure to follow all the instructions to setup the assessment projects correctly.\nA copy of the NILT dataset is already included in the template projects. After following the instructions, your formative/summative assessment projects will also have:\n\nEvery package you will need - except one - installed and loaded.\nAn nilt_subset data frame setup with only the variables used in the regression model.\n(Summative only) a code chunk with the code to produce the regression results table you need to interpret.\n\nThis means:\n\nYou do not need to add any additional code to download the NILT dataset.\nYou do not need to add any additional code that assigns new data to the nilt and nilt_subset data frame objects (e.g. do not add any new lines with nilt &lt;- ... or nilt_subset &lt;- ...).\n(Summative only) you do not need to add any additional code to create the regression model nor a regression results table.\n\nYou also should not remove any of the existing code chunks in the template as these are essential for initially setting everything up and when knitting your HTML file.\nAs the key things are already setup in the template for you, you only have to write a few lines of code overall.\nFor the formative, at its most basic, after following the setup instructions, all you need in terms of coding is -\n\nInstall the vtable package, which you can run in the Console\nLoad the vtable package, which you add in the preamble code chunk\nUse the sumtable() function - provided by the vtable package - to create a descriptive statistics table, which you will need to create a code chunk for\n\nSo, at it’s most basic, for the formative all you need to write is 3 lines of code.\nWith the summative, alongside the above, the only additional thing you need to do in relation to coding is -\n\nPresent data visualisations using ggplot, where for each plot you will need to create a code chunk and write 3 - on rare occassions 4 - lines of code.\n\nThe tidyverse package - which includes ggplot - is already installed and loaded after following the steps to setup your project. This means you do not need to install nor load any other packages for your plots.\nOverall then, at its most basic, for the summative assessment all you need to write is the same 3 lines as the formative and an additional 3 lines of code per ggplot you want to create.\nYou may want to write some additional code when initially exploring the data or to calculate / present some additional stats. In rare cases, a plot may need one additional line of code to fix an issue, like overlapping text labels. However, it is perfectly possible to receive an A for the interpretive report with just the code outlined above.\nSo, if you find yourself downloading and wrangling data, running a multitude of functions beyond sumtable() and ggplot functions, or writing dozens of lines of code, you are doing more than is actually necessary. You will also likely run into issues if any of this code replaces what is already setup in the assessment template.\n\n\n\n\n\n\n\n\n\nCautionGenAI Misinformation\n\n\n\nAnother common cause of issues each year is genAI leading people astray. GenAI is useful when you have a general understanding of the area / topic you are using it for. However, it is nowhere near as competent as the genAI companies and online grifters make it out to be. If you do not understand the area / topic you are prompting it about, you will not know when its responses contain inaccuracies or outright misinformation.\nIndeed, all genAI responses are bullshit - and I use that as the correct philosophical term to describe it. GenAI does not distinguish between fact and fiction, it merely selects the next most probable token - which for simplicity can think of as a word - based on the tokens fed to it.\nThis is especially a problem when it comes to R. GenAI when prompted about R will respond with misinformation or needlessly complex code. Even where the code it spits out works, which isn’t guaranteed, it can be 20+ lines of code for something that could more efficiently be achieved with 1-4 lines of code instead.\nFor example, here is Copilot’s bullshit response to a common error you might encounter:\n\n\nCopilot bullshit response\n\nThe sumtable() function, as we covered in Lab 4, doesn’t come from the stargazer, modelsummary, nor summarizer packages. Instead, it comes from vtable. Yet, after telling Copilot that I still receive an error after installing the packages it advised:\n\n\nCopilot continuing to bullshit\n\nSo - First, Copilot advised me to install the wrong three packages. Second, when I informed it that didn’t work, it advised me to try various things, including uninstalling and reinstalling the packages. Third, it then suggested if all that still doesn’t work - which it won’t - that it can give me information on ‘alternatives’. Two responses and it has wasted my time and rapidly led me further and further away from the simple quick solution for the error.\nIndeed, if you review Lab 4, you will find that the solution to the problem was easy - install and load the package we used in the lab that provided the sumtable() function -\n\n# Run in Console\ninstall.packages(\"vtable\")\n# Load in preamble code chunk\nlibrary(vtable)\n\nAs seen in this short example, if you put too much faith into genAI bullshit you risk needless frustration - and potentially losing hours of time - that a quick review of the teaching materials would have helped solve.\nAs a reminder, whilst I have setup a custom GPT / saved prompt - RStudio Helper GPT - that aims to mitigate the worst aspects of standard genAI responses on R, it remains genAI with all the flaws that entails. You should still treat all its responses as what they are - bullshit.\nAs a good rule of thumb for the assessments, whenever genAI is advising you to install packages or write code we did not cover in the labs, you can be confident that its advice is utter nonsense that you should ignore.\nImportantly, not only will uncritical use of genAI often lead you down the wrong path, prompting genAI to write code to copy and paste for your summative assignment will be treated as academic misconduct. You should only be using genAI to get textbook examples and explanations, and nothing that can be copied and pasted into your assignment. Please see the GenAI and the Assessments section for more information on what genAI use is permitted for the summative assessments.\n\n\n\nIf variables go missing from your regression results table or the regression results are different to how they originally appeared when you knitted the template, you may have unintentionally replaced the data included with the template.\nThe preamble code block at the top of the R Markdown file provided with the template reads in the ‘fullnilt_2012.rds’ dataset and assigns it to the ‘nilt’ data frame object.\n\n# Read data\nnilt &lt;- readRDS(\"data/fullnilt_2012.rds\")\n\nYou therefore do not need to download and read in another dataset, this is all setup for you already within the preamble code block. To resolve the issue, remove all additional code where you downloaded and assign another dataset to the nilt data frame. Then re-run the preamble code chunk.\nFor example, these would be lines to remove:\n\n\n\n\n\n\n\n\nThe ‘NILT2012GR’ that we used in the labs has a different set of the NILT variables, and does not contain all the variables in the ‘fullnilt_2012’ dataset provided in the project template. The above code then would result in variables disappearing from your regression results table.\nSimilarly, this would also be code to remove:\n\n\n\n\n\n\n\n\nThe above code replaces the original nilt data frame with a version that only includes the 5 variables listed within the selection function. Again, resulting in variables disappearing from the regression results table.\nThe select() function keeps variables passed to it and drops the others. The first argument is always the dataframe being selected from, select(dataframe, ...), with variables to keep from it listed after, select(dataframe, variable1, variable2, ...). Assigning, &lt;-, select() to the same dataframe, dataframe &lt;- select(dataframe, ..), is effectively saying, ‘from this dataframe keep these variables and permanently remove the others’. Unless your whole analysis is only going to use that selection, always assign it to a new dataframe instead, dataframe_subset &lt;- select(dataframe, ...).\nImportant: the preamble code chunk in the R Markdown file provided in the project template already has a line of code for creating an nilt_subset object with only the variables that are used in the regression model.\n\n# Subset with variables used in regression model\nnilt_subset &lt;- nilt |&gt;\n  select(persinc2, rsex, religcat, orient, uninatid, tunionsa, rsuper, rage)\n\n\n‘Object not found’ errors often arise when no data has been assigned to a data frame object. For example, if you ran code using nilt object, such as sumtable(nilt, ...) without first assigning data to it nilt &lt;- ..., you would receive the following error message when running/knitting your code:\n\n\n\n\n\n\n\n\nThe preamble code chunk in the R Markdown file provided with the project template contains code for reading in the NILT data set and assigning it to an nilt data frame object. It also sets up an nilt_subset with just the variables used in the regression model.\nIf you are receiving object not found errors for these then, ensure you have run the preamble code chunk. If you still receive an error message, double-check for typos and capitalisation, nilt_subset, NILT_subset, and ni1t_subset will be treated as different objects.\n\nIf the word count displayed at the top of your knitted HTML is wrong, check whether the line of code that calculates the word count refers to the correct R Markdown file you are using.\nWithin the template R Markdown document, we include an inline code block that uses a word count addin to calculate your word count within the knitted HTML file:\n\n\n\n\n\n\n\n\nWhich when knitted will look as follows in the HTML file:\n\n\n\n\n\n\n\n\nHowever, the code to calculate the word count refers to a specific file “Summative-template.Rmd”. if you created a new RMarkdown file, such as one that included your student number ‘Summative-245425s.Rmd’, you also need to update the file name in the code:\n\n\n\n\n\n\n\n\nNote, the ‘- 14’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 184 words then change the code to “wordcountaddin::word_count(”Summative-template.Rmd”) - 198”.\n\nThe code in the code chunks should not be displayed within your knitted HTML file. The R Markdown file provided in the project template is already setup to not include the code from code chunks by setting the global knitr options to echo=FALSEin the setup code chunk.\n\nIf the code from code chunks are appearing in your knitted file, check you have not accidentally removed/modified the setup code chunk. If code from a specific code chunk is showing, but not others, then check you have not accidentally added echo=TRUE to its options.\n\n\nThe most common cause of this error is from renaming the R Markdown file, but not updating the line of code that calculates the word count. See Wrong Word Count section for general info.\nFor example, if you renamed the R Markdwon file from ‘Summative-template.Rmd’ to ‘Summative-245425s.Rmd’, you will receive an ‘Execution halted’ error when trying to knit:\n\n\n\n\n\n\n\n\nTo fix it, just update the line of code that calculates the word count so it refers to the new name you have given the file:\n\n\n\n\n\n\n\n\n\nIf receiving a “package ‘…’ could not be loaded” error message, check the YAML code block at the top of your RMarkdown file. Where this error message has occurred previously, it is due to using the vtable package and the YAML code block includes PDF as an output.\nYou should have either of the following for output: ...:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut, will probably have the something like the following if receiving the error message:\n\n\n\n\n\n\n\n\nThis can happen unintentionally as RStudio can automatically add a line for PDF outline, such as if accidentally clicked ‘Knit to PDF’:\n\n\n\n\n\n\n\n\nAnnoyingly, even if you click ‘Knit to HTML’ from the options, if your YAML block includes pdf_document in its outputs, R will still try to create a PDF document and keep running into the error message.\nSo, if you are receiving the error message and have the two output lines, you can easily fix it by removing the pdf_document line:\n\n\n\n\n\n\n\n\nNote, as the YAML block is already specifying to create an HTML output, you can just click the ‘Knit’ button directly each time rather than clicking the wee down arrow to select ‘Knit to HTML’ specifically.\n\nPlease ensure to submit an exported version of your knitted HTML file rather than one saved through your browser.\nIf Turnitin is providing an error message that it is unable to open your submitted HTML file, it may be due to having saved the file through your browser (e.g. right-clicking and ‘Save as…’) rather than exporting it from RStudio. Whilst HTML files saved from the browser and exported from RStudio will look absolutely identical when opened, some browsers - and browser extensions - will add additional code within the HTML file. This additional code does not change how the file looks when opened, but the code in the file creates issues for Turnitin.\nPlease see the Exporting HTML Files for Submission page for step-by-step guide for how to export your knitted HTML file.\nPlease note: Don’t panic if Turnitin is unable to open your file. We are aware of the issue and you will not be penalised if you submit a file on time, but then have to resubmit after the deadline due to Turnitin being unable to open your original submission.",
    "crumbs": [
      "Assessments",
      "R Issues FAQ"
    ]
  },
  {
    "objectID": "assessments/RFAQ.html#please-read",
    "href": "assessments/RFAQ.html#please-read",
    "title": "R Issues FAQ",
    "section": "",
    "text": "NoteReporting R Issues\n\n\n\nWhen making a Moodle post or emailing about an issue it is vital to provide sufficient context to enable others to help find a solution. This includes:\n\nThe full text of any error message that you are receiving.\nAny relevant code that is producing the error.\n\nError messages will often flag the relevant code chunk where the error occurred:\n\n\n\n\n\n\n\n\nSo, based on the above error message you would include the code within the code block at lines 49-50.\nAdditionally, when emailing, please also include:\n\nYour lab group number.\nIf already made a Moodle post about the issue, a link to the post.\n\n\n\n\n\n\n\n\n\n\nImportantImportant\n\n\n\nAn underlying cause to many issues reported each year is believing more code needs to be written for the assessments than is actually required.\nThe formative and summative assessment template projects we provide already have key things setup for you. Please ensure to follow all the instructions to setup the assessment projects correctly.\nA copy of the NILT dataset is already included in the template projects. After following the instructions, your formative/summative assessment projects will also have:\n\nEvery package you will need - except one - installed and loaded.\nAn nilt_subset data frame setup with only the variables used in the regression model.\n(Summative only) a code chunk with the code to produce the regression results table you need to interpret.\n\nThis means:\n\nYou do not need to add any additional code to download the NILT dataset.\nYou do not need to add any additional code that assigns new data to the nilt and nilt_subset data frame objects (e.g. do not add any new lines with nilt &lt;- ... or nilt_subset &lt;- ...).\n(Summative only) you do not need to add any additional code to create the regression model nor a regression results table.\n\nYou also should not remove any of the existing code chunks in the template as these are essential for initially setting everything up and when knitting your HTML file.\nAs the key things are already setup in the template for you, you only have to write a few lines of code overall.\nFor the formative, at its most basic, after following the setup instructions, all you need in terms of coding is -\n\nInstall the vtable package, which you can run in the Console\nLoad the vtable package, which you add in the preamble code chunk\nUse the sumtable() function - provided by the vtable package - to create a descriptive statistics table, which you will need to create a code chunk for\n\nSo, at it’s most basic, for the formative all you need to write is 3 lines of code.\nWith the summative, alongside the above, the only additional thing you need to do in relation to coding is -\n\nPresent data visualisations using ggplot, where for each plot you will need to create a code chunk and write 3 - on rare occassions 4 - lines of code.\n\nThe tidyverse package - which includes ggplot - is already installed and loaded after following the steps to setup your project. This means you do not need to install nor load any other packages for your plots.\nOverall then, at its most basic, for the summative assessment all you need to write is the same 3 lines as the formative and an additional 3 lines of code per ggplot you want to create.\nYou may want to write some additional code when initially exploring the data or to calculate / present some additional stats. In rare cases, a plot may need one additional line of code to fix an issue, like overlapping text labels. However, it is perfectly possible to receive an A for the interpretive report with just the code outlined above.\nSo, if you find yourself downloading and wrangling data, running a multitude of functions beyond sumtable() and ggplot functions, or writing dozens of lines of code, you are doing more than is actually necessary. You will also likely run into issues if any of this code replaces what is already setup in the assessment template.\n\n\n\n\n\n\n\n\n\nCautionGenAI Misinformation\n\n\n\nAnother common cause of issues each year is genAI leading people astray. GenAI is useful when you have a general understanding of the area / topic you are using it for. However, it is nowhere near as competent as the genAI companies and online grifters make it out to be. If you do not understand the area / topic you are prompting it about, you will not know when its responses contain inaccuracies or outright misinformation.\nIndeed, all genAI responses are bullshit - and I use that as the correct philosophical term to describe it. GenAI does not distinguish between fact and fiction, it merely selects the next most probable token - which for simplicity can think of as a word - based on the tokens fed to it.\nThis is especially a problem when it comes to R. GenAI when prompted about R will respond with misinformation or needlessly complex code. Even where the code it spits out works, which isn’t guaranteed, it can be 20+ lines of code for something that could more efficiently be achieved with 1-4 lines of code instead.\nFor example, here is Copilot’s bullshit response to a common error you might encounter:\n\n\nCopilot bullshit response\n\nThe sumtable() function, as we covered in Lab 4, doesn’t come from the stargazer, modelsummary, nor summarizer packages. Instead, it comes from vtable. Yet, after telling Copilot that I still receive an error after installing the packages it advised:\n\n\nCopilot continuing to bullshit\n\nSo - First, Copilot advised me to install the wrong three packages. Second, when I informed it that didn’t work, it advised me to try various things, including uninstalling and reinstalling the packages. Third, it then suggested if all that still doesn’t work - which it won’t - that it can give me information on ‘alternatives’. Two responses and it has wasted my time and rapidly led me further and further away from the simple quick solution for the error.\nIndeed, if you review Lab 4, you will find that the solution to the problem was easy - install and load the package we used in the lab that provided the sumtable() function -\n\n# Run in Console\ninstall.packages(\"vtable\")\n# Load in preamble code chunk\nlibrary(vtable)\n\nAs seen in this short example, if you put too much faith into genAI bullshit you risk needless frustration - and potentially losing hours of time - that a quick review of the teaching materials would have helped solve.\nAs a reminder, whilst I have setup a custom GPT / saved prompt - RStudio Helper GPT - that aims to mitigate the worst aspects of standard genAI responses on R, it remains genAI with all the flaws that entails. You should still treat all its responses as what they are - bullshit.\nAs a good rule of thumb for the assessments, whenever genAI is advising you to install packages or write code we did not cover in the labs, you can be confident that its advice is utter nonsense that you should ignore.\nImportantly, not only will uncritical use of genAI often lead you down the wrong path, prompting genAI to write code to copy and paste for your summative assignment will be treated as academic misconduct. You should only be using genAI to get textbook examples and explanations, and nothing that can be copied and pasted into your assignment. Please see the GenAI and the Assessments section for more information on what genAI use is permitted for the summative assessments.",
    "crumbs": [
      "Assessments",
      "R Issues FAQ"
    ]
  },
  {
    "objectID": "assessments/RFAQ.html#regression-results-table-issues",
    "href": "assessments/RFAQ.html#regression-results-table-issues",
    "title": "R Issues FAQ",
    "section": "",
    "text": "If variables go missing from your regression results table or the regression results are different to how they originally appeared when you knitted the template, you may have unintentionally replaced the data included with the template.\nThe preamble code block at the top of the R Markdown file provided with the template reads in the ‘fullnilt_2012.rds’ dataset and assigns it to the ‘nilt’ data frame object.\n\n# Read data\nnilt &lt;- readRDS(\"data/fullnilt_2012.rds\")\n\nYou therefore do not need to download and read in another dataset, this is all setup for you already within the preamble code block. To resolve the issue, remove all additional code where you downloaded and assign another dataset to the nilt data frame. Then re-run the preamble code chunk.\nFor example, these would be lines to remove:\n\n\n\n\n\n\n\n\nThe ‘NILT2012GR’ that we used in the labs has a different set of the NILT variables, and does not contain all the variables in the ‘fullnilt_2012’ dataset provided in the project template. The above code then would result in variables disappearing from your regression results table.\nSimilarly, this would also be code to remove:\n\n\n\n\n\n\n\n\nThe above code replaces the original nilt data frame with a version that only includes the 5 variables listed within the selection function. Again, resulting in variables disappearing from the regression results table.\nThe select() function keeps variables passed to it and drops the others. The first argument is always the dataframe being selected from, select(dataframe, ...), with variables to keep from it listed after, select(dataframe, variable1, variable2, ...). Assigning, &lt;-, select() to the same dataframe, dataframe &lt;- select(dataframe, ..), is effectively saying, ‘from this dataframe keep these variables and permanently remove the others’. Unless your whole analysis is only going to use that selection, always assign it to a new dataframe instead, dataframe_subset &lt;- select(dataframe, ...).\nImportant: the preamble code chunk in the R Markdown file provided in the project template already has a line of code for creating an nilt_subset object with only the variables that are used in the regression model.\n\n# Subset with variables used in regression model\nnilt_subset &lt;- nilt |&gt;\n  select(persinc2, rsex, religcat, orient, uninatid, tunionsa, rsuper, rage)",
    "crumbs": [
      "Assessments",
      "R Issues FAQ"
    ]
  },
  {
    "objectID": "assessments/RFAQ.html#object-not-found",
    "href": "assessments/RFAQ.html#object-not-found",
    "title": "R Issues FAQ",
    "section": "",
    "text": "‘Object not found’ errors often arise when no data has been assigned to a data frame object. For example, if you ran code using nilt object, such as sumtable(nilt, ...) without first assigning data to it nilt &lt;- ..., you would receive the following error message when running/knitting your code:\n\n\n\n\n\n\n\n\nThe preamble code chunk in the R Markdown file provided with the project template contains code for reading in the NILT data set and assigning it to an nilt data frame object. It also sets up an nilt_subset with just the variables used in the regression model.\nIf you are receiving object not found errors for these then, ensure you have run the preamble code chunk. If you still receive an error message, double-check for typos and capitalisation, nilt_subset, NILT_subset, and ni1t_subset will be treated as different objects.",
    "crumbs": [
      "Assessments",
      "R Issues FAQ"
    ]
  },
  {
    "objectID": "assessments/RFAQ.html#wrong-word-count",
    "href": "assessments/RFAQ.html#wrong-word-count",
    "title": "R Issues FAQ",
    "section": "",
    "text": "If the word count displayed at the top of your knitted HTML is wrong, check whether the line of code that calculates the word count refers to the correct R Markdown file you are using.\nWithin the template R Markdown document, we include an inline code block that uses a word count addin to calculate your word count within the knitted HTML file:\n\n\n\n\n\n\n\n\nWhich when knitted will look as follows in the HTML file:\n\n\n\n\n\n\n\n\nHowever, the code to calculate the word count refers to a specific file “Summative-template.Rmd”. if you created a new RMarkdown file, such as one that included your student number ‘Summative-245425s.Rmd’, you also need to update the file name in the code:\n\n\n\n\n\n\n\n\nNote, the ‘- 14’ in the code is so “Word count:” and each of the headers “Introduction”, “Data and method”, etc are also not included in the word count. Ensure to update this number to exclude your bibliography from the word count. For example, if your bibliography is 184 words then change the code to “wordcountaddin::word_count(”Summative-template.Rmd”) - 198”.",
    "crumbs": [
      "Assessments",
      "R Issues FAQ"
    ]
  },
  {
    "objectID": "assessments/RFAQ.html#code-chunks-appearing-in-knitted-file",
    "href": "assessments/RFAQ.html#code-chunks-appearing-in-knitted-file",
    "title": "R Issues FAQ",
    "section": "",
    "text": "The code in the code chunks should not be displayed within your knitted HTML file. The R Markdown file provided in the project template is already setup to not include the code from code chunks by setting the global knitr options to echo=FALSEin the setup code chunk.\n\nIf the code from code chunks are appearing in your knitted file, check you have not accidentally removed/modified the setup code chunk. If code from a specific code chunk is showing, but not others, then check you have not accidentally added echo=TRUE to its options.",
    "crumbs": [
      "Assessments",
      "R Issues FAQ"
    ]
  },
  {
    "objectID": "assessments/RFAQ.html#unable-to-knit",
    "href": "assessments/RFAQ.html#unable-to-knit",
    "title": "R Issues FAQ",
    "section": "",
    "text": "The most common cause of this error is from renaming the R Markdown file, but not updating the line of code that calculates the word count. See Wrong Word Count section for general info.\nFor example, if you renamed the R Markdwon file from ‘Summative-template.Rmd’ to ‘Summative-245425s.Rmd’, you will receive an ‘Execution halted’ error when trying to knit:\n\n\n\n\n\n\n\n\nTo fix it, just update the line of code that calculates the word count so it refers to the new name you have given the file:\n\n\n\n\n\n\n\n\n\nIf receiving a “package ‘…’ could not be loaded” error message, check the YAML code block at the top of your RMarkdown file. Where this error message has occurred previously, it is due to using the vtable package and the YAML code block includes PDF as an output.\nYou should have either of the following for output: ...:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut, will probably have the something like the following if receiving the error message:\n\n\n\n\n\n\n\n\nThis can happen unintentionally as RStudio can automatically add a line for PDF outline, such as if accidentally clicked ‘Knit to PDF’:\n\n\n\n\n\n\n\n\nAnnoyingly, even if you click ‘Knit to HTML’ from the options, if your YAML block includes pdf_document in its outputs, R will still try to create a PDF document and keep running into the error message.\nSo, if you are receiving the error message and have the two output lines, you can easily fix it by removing the pdf_document line:\n\n\n\n\n\n\n\n\nNote, as the YAML block is already specifying to create an HTML output, you can just click the ‘Knit’ button directly each time rather than clicking the wee down arrow to select ‘Knit to HTML’ specifically.",
    "crumbs": [
      "Assessments",
      "R Issues FAQ"
    ]
  },
  {
    "objectID": "assessments/RFAQ.html#turnitin-unable-to-open-html-file",
    "href": "assessments/RFAQ.html#turnitin-unable-to-open-html-file",
    "title": "R Issues FAQ",
    "section": "",
    "text": "Please ensure to submit an exported version of your knitted HTML file rather than one saved through your browser.\nIf Turnitin is providing an error message that it is unable to open your submitted HTML file, it may be due to having saved the file through your browser (e.g. right-clicking and ‘Save as…’) rather than exporting it from RStudio. Whilst HTML files saved from the browser and exported from RStudio will look absolutely identical when opened, some browsers - and browser extensions - will add additional code within the HTML file. This additional code does not change how the file looks when opened, but the code in the file creates issues for Turnitin.\nPlease see the Exporting HTML Files for Submission page for step-by-step guide for how to export your knitted HTML file.\nPlease note: Don’t panic if Turnitin is unable to open your file. We are aware of the issue and you will not be penalised if you submit a file on time, but then have to resubmit after the deadline due to Turnitin being unable to open your original submission.",
    "crumbs": [
      "Assessments",
      "R Issues FAQ"
    ]
  },
  {
    "objectID": "assessments/creating-assessment-projects.html",
    "href": "assessments/creating-assessment-projects.html",
    "title": "Setup Assessment Projects",
    "section": "",
    "text": "We have created GitHub repositories which contain RStudio projects with all the essentials you need to start working on the formative and summative assessments.\nWithin the project templates, the nilt dataset has already been loaded, variables coerced, and an nilt_subset dataframe created for you. The nilt_subset dataframe has the same variables that are used in the regression model in the Summative Assessment.\nThere are separate templates for the formative and summative assessments. The general steps to follow to set them up are the same, just ensure to use the correct link for each. (Provided in blue callout boxes below.)\n\n\n\nFrom the left-hand side pane, make sure you are in your ‘Lab Group …’ workspace. (Where ‘…’ is your Lab Group number).\nClick the blue ‘New Project’ button on the right of the screen.\nFrom the menu list that opens select ‘New Project from Git Repository’\n\n\n\n\nWithin the dialogue box that opens, copy and paste the relevant link below into the ‘URL of your Git Repository’ field.\n\n\n\n\n\n\n\nFormative Assessment RStudio project template link:\nhttps://github.com/UGQuants/Formative\n\n\n\n\n\n\n\n\n\nSummative Assessment RStudio project template link:\nhttps://github.com/UGQuants/Summative\n\n\n\n\nClick ‘OK’ to create the new project.\n\n\n\nAfter your poject loads -\n\nIn the Files tab (bottom-right), click ‘Formative-template.Rmd’ / ‘Summative-template.Rmd’.\nWithin the Sources pane (top-left), click ‘Install’ in the yellow banner. (Note - it can take a few seconds to appear after opening your file.)\n\n\n\nAfter the packages are finished installing, within the Sources pane (top-left) -\n\nClick the ‘Run’ button in the toolbar.\nFrom the options, select ‘Run All’.\n\n\n\nThis will install additional packages and setup the nilt_subset dataframe. After it finishes your project is all setup for the assessment.",
    "crumbs": [
      "Assessments",
      "Setup Assessment Projects"
    ]
  },
  {
    "objectID": "assessments/creating-assessment-projects.html#steps-to-follow",
    "href": "assessments/creating-assessment-projects.html#steps-to-follow",
    "title": "Setup Assessment Projects",
    "section": "",
    "text": "From the left-hand side pane, make sure you are in your ‘Lab Group …’ workspace. (Where ‘…’ is your Lab Group number).\nClick the blue ‘New Project’ button on the right of the screen.\nFrom the menu list that opens select ‘New Project from Git Repository’\n\n\n\n\nWithin the dialogue box that opens, copy and paste the relevant link below into the ‘URL of your Git Repository’ field.\n\n\n\n\n\n\n\nFormative Assessment RStudio project template link:\nhttps://github.com/UGQuants/Formative\n\n\n\n\n\n\n\n\n\nSummative Assessment RStudio project template link:\nhttps://github.com/UGQuants/Summative\n\n\n\n\nClick ‘OK’ to create the new project.\n\n\n\nAfter your poject loads -\n\nIn the Files tab (bottom-right), click ‘Formative-template.Rmd’ / ‘Summative-template.Rmd’.\nWithin the Sources pane (top-left), click ‘Install’ in the yellow banner. (Note - it can take a few seconds to appear after opening your file.)\n\n\n\nAfter the packages are finished installing, within the Sources pane (top-left) -\n\nClick the ‘Run’ button in the toolbar.\nFrom the options, select ‘Run All’.\n\n\n\nThis will install additional packages and setup the nilt_subset dataframe. After it finishes your project is all setup for the assessment.",
    "crumbs": [
      "Assessments",
      "Setup Assessment Projects"
    ]
  },
  {
    "objectID": "assessments/export.html",
    "href": "assessments/export.html",
    "title": "Exporting HTML Files for Submission",
    "section": "",
    "text": "Exporting HTML Files for Submission\n\n\n\n\n\n\nFor the formative and summative assessments, you will need to export your HTML file rather than saving it via your browser. Whilst HTML files saved from the browser and exported from RStudio will look absolutely identical when opened, some browsers - and browser extensions - will add additional code within the HTML file. This additional code does not change how the file looks when opened, but the code in the file creates issues for Turnitin.\n\n\n\nWhen you knit R Markdown documents to HTML, RStudio actually creates a new .html file with the same name as your R Markdown document. You can confirm this by looking in the ‘Files’ tab in bottom-right.\n\nTo export your knitted HTML file then -\n\nCheck the box next to the knitted HTML file you want to export in the ‘Files’ panel. By default, the Files panel is on the bottom right of the screen.\n\n\n\n\n\n\n\n\n\n\nClick ‘More’ from the toolbar at the top of the Files panel.\n\n\n\n\n\n\n\n\n\n\nClick ‘Export’ from the menu options that pop-up.\n\n\n\n\n\n\n\n\n\n\nThis will pop open an ‘Export Files’ dialogue. Here, for the assessments, you can rename the file to include your student number. Just make sure the “.html” at the end remains included.\n\n\n\n\n\n\n\n\n\n\nAfter naming the file, click ‘Download’. This should open a dialogue to select which location to save the file. (If not see the ‘Important’ note below.)\n\n\n\n\n\n\n\n\n\nHere’s a short gif running through all the steps together.\n\n\n\n\n\n\n\n\nImportant: Some browsers by default will download to a ‘Downloads’ folder and instantly open the file within the browser after its finished downloading. This will open your knitted file in a tab within your browser. Do not then ‘Save as…’ the file from within the browser. Instead, close the tab and use your file explorer to navigate to your ‘Downloads’ folder to find the exported copy of your file to submit.",
    "crumbs": [
      "Assessments",
      "Exporting HTML Files for Submission"
    ]
  }
]